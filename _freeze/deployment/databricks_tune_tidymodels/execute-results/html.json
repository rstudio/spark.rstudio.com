{
  "hash": "2ca5b73bb3d224195d922daabbb02a82",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Tune Tidymodel in Databricks\nexecute:\n  eval: true\n  freeze: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n## Intro\n\n`sparklyr` makes it possible to off-load a Tidymodels grid search model\ntuning to Databricks. [Tidymodels](https://www.tidymodels.org/) is a\ncollection of packages that are designed to work together to provide\neverything from resampling, to preprocessing, to model tuning, to\nperformance measurement.\n\nModel tuning is a time consuming process because of the number of tuning\nparameter combinations that need to be processed. Running the\ncombinations in parallel saves a significant amount of time. Tidymodels\nprovides ways to run parallel tuning, but not in Spark. `sparklyr` not\nonly enables this possibility, but it also makes it very simple to\naccess this functionality.\n\nIn Tidymodels, the\n[`tune_grid()`](https://tune.tidymodels.org/reference/tune_grid.html)\nfunction is called to execute the grid search locally. To run in\nDatabricks, simply call `sparklyr`'s `tune_grid_spark()` instead. It\naccepts the exact same arguments as `tune_grid()` does, see @fig-code.\n\n::::::: {#fig-code}\n:::::: columns\n::: {.column width=\"38%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\ntune::tune_grid(\n  object = my_model, \n  preprocessor = my_recipe, \n  resamples = my_resamples\n  \n  )\n```\n:::\n\n\nRun locally\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"58%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nsparklyr::tune_grid_spark(\n  object = my_model, \n  preprocessor = my_recipe, \n  resamples = my_resamples,\n  sc = my_conn # Only additional requirement\n  )\n```\n:::\n\n\nRun remotely in Databricks\n:::\n::::::\n\nComparing tune and sparklyr function calls\n:::::::\n\n`sparklyr` will automatically upload the needed R object to Databricks,\nsuch as the resample object, model specification, and the preprocessing\nsteps. It will then run the tuning in parallel taking advantage of the R\nintegration in Databricks. Lastly, it will collect all of the results\nback to your local R session, and return a `tune_results` object that is\nindistinguishable from one made directly by Tidymodels, , see\n@fig-diagram.\n\n::: {#fig-diagram}\n![](/images/guides/tidymodels-db/diagram-1.png)\n\nHow *tune_grid_spark()* works with Databricks\n:::\n\n## Example\n\n### Setup the Tidymodels objects\n\nThe model building, the data pre-processing and re-sampling steps remain\nthe exact same. `sparklyr` will upload the R objects resulting from each\nstep onto the Databricks cluster, so that it can process the\nhyper-tuning using the exact same information.\n\nTo demonstrate the functionality in this article, we start with a\nstandard tuning workflow. In other words, there is nothing different to\ndo in Tidymodels to setup for the actual model tuning process:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(readmission)\n\nset.seed(1)\n\n# Data set resampling \nreadmission_splits <- initial_split(readmission, strata = readmitted)\n\nreadmission_folds <- vfold_cv(\n  data = training(readmission_splits),\n  strata = readmitted\n  )\n\n# Data pre-processing\nrecipe_basic <- recipe(readmitted ~ ., data = readmission) |>\n  step_mutate(\n    race = factor(case_when(\n      !(race %in% c(\"Caucasian\", \"African American\")) ~ \"Other\",\n      .default = race\n    ))\n  ) |>\n  step_unknown(all_nominal_predictors()) |>\n  step_YeoJohnson(all_numeric_predictors()) |>\n  step_normalize(all_numeric_predictors()) |>\n  step_dummy(all_nominal_predictors())\n\n# Model specification\nspec_bt <- boost_tree(\n  mode = \"classification\",\n  mtry = tune(), # Part of the hyper-parameters to tune\n  learn_rate = tune(), # Part of the hyper-parameters to tune\n  trees = 10\n  )\n```\n:::\n\n\n### Tune in Databricks Connect\n\nThe first step is to connect to the Databricks cluster. This operation\nis only needed one time during the R session:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n\nsc <- spark_connect(\n  method = \"databricks_connect\", \n  cluster_id = \"1218-000327-q970zsow\" # Replace with your own cluster's ID\n  )\n#> ℹ Retrieving info for cluster:'1218-000327-q970zsow'\n#> ✔ Cluster: '1218-000327-q970zsow' | DBR: '17.3' [722ms]\n#> \n#> ℹ\n#> ✔ Python environment: 'Managed `uv` environment' [16.2s]\n#> \n#> ℹ Connecting to '8 cores'\n#> ✔ Connected to: '8 cores' [12ms]\n#> \n```\n:::\n\n\nThe next step is to call `tune_grid_spark()`. We will pass the model,\nrecipe and re-sample objects prepared in the previous section. The only\naddition is to have the process run in `verbose` mode, this is an\noptional argument. It will display the steps and timings for each:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_results <- tune_grid_spark(\n  sc = sc, \n  object = spec_bt, \n  preprocessor = recipe_basic, \n  resamples = readmission_folds,\n  control = control_grid(verbose = TRUE)\n  )\n#> i Creating pre-processing data to finalize 1 unknown parameter: \"mtry\"\n#> ℹ Uploading model, pre-processor, and other info to the Spark session\n#> ✔ Uploading model, pre-processor, and other info to the Spark session [602ms]\n#> \n#> ℹ Uploading the re-samples to the Spark session\n#> ✔ Uploading the re-samples to the Spark session [2.2s]\n#> \n#> ℹ Copying the grid to the Spark session\n#> ✔ Copying the grid to the Spark session [178ms]\n#> \n#> ℹ Executing the model tuning in Spark\n#> ✔ Executing the model tuning in Spark [17.4s]\n#> \n```\n:::\n\n\nThe `spark_results` object can be now used as if it was a tuned object\nthat was prepared locally.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(spark_results)\n```\n\n::: {.cell-output-display}\n![](databricks_tune_tidymodels_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## R and Python libraries\n\nThere are Python and R packages that need to be pre-installed in the\nDatabricks cluster:\n\n**Python**\n\n-   `rpy2`\n\n**R**\n\n-   `tidymodels`\n\n-   `reticulate`\n\n-   The modeling package used by `parsnip` in the tuning\n\nInstallation is a simple operation that is done via your Databricks web\nportal. Here are the instructions that shows you how to do that:\n[Databricks - Cluster\nLibraries](https://docs.databricks.com/en/libraries/cluster-libraries.html).\n\n### Install programmatically (optional)\n\nThe [`brickster`](https://databrickslabs.github.io/brickster/) package\nis a complete toolkit for interacting with Databricks. It can be used to\nprogrammatically install the Python and R libraries. It requires that\nthe user has rights to modify the libraries in target cluster.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brickster)\n\n# DBRs have a fixed snapshot date to source the R packages. Redirecting  \n# to the 'latest' snapshot to get the most recent package versions.\nrepo <- \"https://databricks.packagemanager.posit.co/cran/__linux__/noble/latest\"\n\n# Builds the list of R packages, pointing them to the 'latest' snapshot\nr_libs <- libraries(\n  lib_cran(\"reticulate\", repo = repo),\n  lib_cran(\"tidymodels\", repo = repo),\n  lib_cran(\"xgboost\", repo = repo) # (optional) Used in the example\n)\n\ndb_libs_install(\"1218-000327-q970zsow\", r_libs)\n\n# Builds the Python package object. Specifying the version of `rpy2` to install.\npy_lib <- lib_pypi(\"rpy2==3.6.4\") |>\n  libraries()\n\ndb_libs_install(\"1218-000327-q970zsow\", py_lib)\n```\n:::\n\n\nAs time goes by, there may be other modeling R packages that need to be\ninstalled in Spark. Here is a template that can be used to install a\nsingle package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brickster)\n\nlib_cran(\n  package = \"[Missing package]\", \n  repo = \"https://databricks.packagemanager.posit.co/cran/__linux__/noble/latest\"\n  ) |> \n  libraries() |> \n  db_libs_install(\"[Your cluster ID]\", libraries =  _)\n\n```\n:::\n\n",
    "supporting": [
      "databricks_tune_tidymodels_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}