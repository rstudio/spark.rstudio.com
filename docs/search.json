[
  {
    "objectID": "packages/sparklyr/latest/reference/ml-constructors.html",
    "href": "packages/sparklyr/latest/reference/ml-constructors.html",
    "title": "Constructors for Pipeline Stages",
    "section": "",
    "text": "R/ml_transformer_and_estimator.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-constructors.html#ml-constructors",
    "href": "packages/sparklyr/latest/reference/ml-constructors.html#ml-constructors",
    "title": "Constructors for Pipeline Stages",
    "section": "ml-constructors",
    "text": "ml-constructors"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-constructors.html#description",
    "href": "packages/sparklyr/latest/reference/ml-constructors.html#description",
    "title": "Constructors for Pipeline Stages",
    "section": "Description",
    "text": "Description\nFunctions for developers writing extensions for Spark ML."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-constructors.html#usage",
    "href": "packages/sparklyr/latest/reference/ml-constructors.html#usage",
    "title": "Constructors for Pipeline Stages",
    "section": "Usage",
    "text": "Usage\nnew_ml_transformer(jobj, ..., class = character()) \n\nnew_ml_prediction_model(jobj, ..., class = character()) \n\nnew_ml_classification_model(jobj, ..., class = character()) \n\nnew_ml_probabilistic_classification_model(jobj, ..., class = character()) \n\nnew_ml_clustering_model(jobj, ..., class = character()) \n\nnew_ml_estimator(jobj, ..., class = character()) \n\nnew_ml_predictor(jobj, ..., class = character()) \n\nnew_ml_classifier(jobj, ..., class = character()) \n\nnew_ml_probabilistic_classifier(jobj, ..., class = character())"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-constructors.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml-constructors.html#arguments",
    "title": "Constructors for Pipeline Stages",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\njobj\nPointer to the pipeline stage object.\n\n\n…\n(Optional) additional attributes of the object.\n\n\nclass\nName of class."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj-class.html",
    "href": "packages/sparklyr/latest/reference/spark_jobj-class.html",
    "title": "spark_jobj class",
    "section": "",
    "text": "R/connection_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj-class.html#spark_jobj-class",
    "href": "packages/sparklyr/latest/reference/spark_jobj-class.html#spark_jobj-class",
    "title": "spark_jobj class",
    "section": "spark_jobj-class",
    "text": "spark_jobj-class"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj-class.html#description",
    "href": "packages/sparklyr/latest/reference/spark_jobj-class.html#description",
    "title": "spark_jobj class",
    "section": "Description",
    "text": "Description\nspark_jobj class"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stop.html",
    "href": "packages/sparklyr/latest/reference/stream_stop.html",
    "title": "Stops a Spark Stream",
    "section": "",
    "text": "R/stream_operations.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stop.html#stream_stop",
    "href": "packages/sparklyr/latest/reference/stream_stop.html#stream_stop",
    "title": "Stops a Spark Stream",
    "section": "stream_stop",
    "text": "stream_stop"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stop.html#description",
    "href": "packages/sparklyr/latest/reference/stream_stop.html#description",
    "title": "Stops a Spark Stream",
    "section": "Description",
    "text": "Description\nStops processing data from a Spark stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stop.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_stop.html#usage",
    "title": "Stops a Spark Stream",
    "section": "Usage",
    "text": "Usage\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stop.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_stop.html#arguments",
    "title": "Stops a Spark Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nstream\nThe spark stream object to be stopped."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_array_sort.html",
    "href": "packages/sparklyr/latest/reference/hof_array_sort.html",
    "title": "Sorts array using a custom comparator",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_array_sort.html#hof_array_sort",
    "href": "packages/sparklyr/latest/reference/hof_array_sort.html#hof_array_sort",
    "title": "Sorts array using a custom comparator",
    "section": "hof_array_sort",
    "text": "hof_array_sort"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_array_sort.html#description",
    "href": "packages/sparklyr/latest/reference/hof_array_sort.html#description",
    "title": "Sorts array using a custom comparator",
    "section": "Description",
    "text": "Description\nApplies a custom comparator function to sort an array (this is essentially a dplyr wrapper to the array_sort(expr, func) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_array_sort.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_array_sort.html#usage",
    "title": "Sorts array using a custom comparator",
    "section": "Usage",
    "text": "Usage\nhof_array_sort(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_array_sort.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_array_sort.html#arguments",
    "title": "Sorts array using a custom comparator",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\nfunc\nThe comparator function to apply (it should take 2 array elements as arguments and return an integer, with a return value of -1 indicating the first element is less than the second, 0 indicating equality, or 1 indicating the first element is greater than the second)\n\n\nexpr\nThe array being sorted, could be any SQL expression evaluating to an array (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the sorted result (default: expr)\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_array_sort.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_array_sort.html#examples",
    "title": "Sorts array using a custom comparator",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\", version = \"3.0.0\") \ncopy_to( \n  sc, \n  tibble::tibble( \n    # x contains 2 arrays each having elements in ascending order \n    x = list(1:5, 6:10) \n  ) \n) %>% \n  # now each array from x gets sorted in descending order \n  hof_array_sort(~ as.integer(sign(.y - .x))) \n#> # Source: spark<?> [?? x 1]\n#>   x        \n#>   <list>   \n#> 1 <dbl [5]>\n#> 2 <dbl [5]>"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html",
    "title": "Feature Transformation – VectorIndexer (Estimator)",
    "section": "",
    "text": "R/ml_feature_vector_indexer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#ft_vector_indexer",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#ft_vector_indexer",
    "title": "Feature Transformation – VectorIndexer (Estimator)",
    "section": "ft_vector_indexer",
    "text": "ft_vector_indexer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#description",
    "title": "Feature Transformation – VectorIndexer (Estimator)",
    "section": "Description",
    "text": "Description\nIndexing categorical feature columns in a dataset of Vector."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#usage",
    "title": "Feature Transformation – VectorIndexer (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_vector_indexer( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  handle_invalid = \"error\", \n  max_categories = 20, \n  uid = random_string(\"vector_indexer_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#arguments",
    "title": "Feature Transformation – VectorIndexer (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error”\n\n\nmax_categories\nThreshold for the number of values a categorical feature can take. If a feature is found to have > max_categories values, then it is declared continuous. Must be greater than or equal to 2. Defaults to 20.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#details",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#details",
    "title": "Feature Transformation – VectorIndexer (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#value",
    "title": "Feature Transformation – VectorIndexer (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#see-also",
    "title": "Feature Transformation – VectorIndexer (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html",
    "title": "Retrieves or sets whether coalescing contiguous shuffle partitions is enabled",
    "section": "",
    "text": "R/spark_context_config.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#spark_coalesce_shuffle_partitions",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#spark_coalesce_shuffle_partitions",
    "title": "Retrieves or sets whether coalescing contiguous shuffle partitions is enabled",
    "section": "spark_coalesce_shuffle_partitions",
    "text": "spark_coalesce_shuffle_partitions"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#description",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#description",
    "title": "Retrieves or sets whether coalescing contiguous shuffle partitions is enabled",
    "section": "Description",
    "text": "Description\nRetrieves or sets whether coalescing contiguous shuffle partitions is enabled"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#usage",
    "title": "Retrieves or sets whether coalescing contiguous shuffle partitions is enabled",
    "section": "Usage",
    "text": "Usage\nspark_coalesce_shuffle_partitions(sc, enable = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#arguments",
    "title": "Retrieves or sets whether coalescing contiguous shuffle partitions is enabled",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nenable\nWhether to enable coalescing of contiguous shuffle partitions. Defaults to NULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#see-also",
    "title": "Retrieves or sets whether coalescing contiguous shuffle partitions is enabled",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_adaptive_query_execution(), spark_advisory_shuffle_partition_size(), spark_auto_broadcast_join_threshold(), spark_coalesce_initial_num_partitions(), spark_coalesce_min_num_partitions(), spark_session_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html",
    "href": "packages/sparklyr/latest/reference/ft_idf.html",
    "title": "Feature Transformation – IDF (Estimator)",
    "section": "",
    "text": "R/ml_feature_idf.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#ft_idf",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#ft_idf",
    "title": "Feature Transformation – IDF (Estimator)",
    "section": "ft_idf",
    "text": "ft_idf"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#description",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#description",
    "title": "Feature Transformation – IDF (Estimator)",
    "section": "Description",
    "text": "Description\nCompute the Inverse Document Frequency (IDF) given a collection of documents."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#usage",
    "title": "Feature Transformation – IDF (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_idf( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  min_doc_freq = 0, \n  uid = random_string(\"idf_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#arguments",
    "title": "Feature Transformation – IDF (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nmin_doc_freq\nThe minimum number of documents in which a term should appear. Default: 0\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#details",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#details",
    "title": "Feature Transformation – IDF (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#value",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#value",
    "title": "Feature Transformation – IDF (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#see-also",
    "title": "Feature Transformation – IDF (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html",
    "title": "Read a CSV file into a Spark DataFrame",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html#spark_read_csv",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html#spark_read_csv",
    "title": "Read a CSV file into a Spark DataFrame",
    "section": "spark_read_csv",
    "text": "spark_read_csv"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html#description",
    "title": "Read a CSV file into a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRead a tabular data file into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html#usage",
    "title": "Read a CSV file into a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nspark_read_csv( \n  sc, \n  name = NULL, \n  path = name, \n  header = TRUE, \n  columns = NULL, \n  infer_schema = is.null(columns), \n  delimiter = \",\", \n  quote = \"\\\"\", \n  escape = \"\\\\\", \n  charset = \"UTF-8\", \n  null_value = NULL, \n  options = list(), \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html#arguments",
    "title": "Read a CSV file into a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nheader\nBoolean; should the first row of data be used as a header? Defaults to TRUE.\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\ninfer_schema\nBoolean; should column types be automatically inferred? Requires one extra pass over the data. Defaults to is.null(columns).\n\n\ndelimiter\nThe character used to delimit each column. Defaults to ','.\n\n\nquote\nThe character used as a quote. Defaults to '\"'.\n\n\nescape\nThe character used to escape other characters. Defaults to '\\'.\n\n\ncharset\nThe character set. Defaults to \"UTF-8\".\n\n\nnull_value\nThe character to use for null, or missing, values. Defaults to NULL.\n\n\noptions\nA list of strings with additional options.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html#details",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html#details",
    "title": "Read a CSV file into a Spark DataFrame",
    "section": "Details",
    "text": "Details\nYou can read data from HDFS (hdfs://), S3 (s3a://), as well as the local file system (file://).\nIf you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf spark.hadoop.fs.s3a.access.key, spark.hadoop.fs.s3a.secret.key or any of the methods outlined in the aws-sdk documentation Working with AWS credentials\nIn order to work with the newer s3a:// protocol also set the values for spark.hadoop.fs.s3a.impl and spark.hadoop.fs.s3a.endpoint. In addition, to support v4 of the S3 api be sure to pass the -Dcom.amazonaws.services.s3.enableV4 driver options for the config key spark.driver.extraJavaOptions\nFor instructions on how to configure s3n:// check the hadoop documentation: s3n authentication properties\nWhen header is FALSE, the column names are generated with a V prefix; e.g. V1, V2, ...."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html#see-also",
    "title": "Read a CSV file into a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_json.html",
    "href": "packages/sparklyr/latest/reference/spark_write_json.html",
    "title": "Write a Spark DataFrame to a JSON file",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_json.html#spark_write_json",
    "href": "packages/sparklyr/latest/reference/spark_write_json.html#spark_write_json",
    "title": "Write a Spark DataFrame to a JSON file",
    "section": "spark_write_json",
    "text": "spark_write_json"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_json.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_json.html#description",
    "title": "Write a Spark DataFrame to a JSON file",
    "section": "Description",
    "text": "Description\nSerialize a Spark DataFrame to the JavaScript Object Notation format."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_json.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_json.html#usage",
    "title": "Write a Spark DataFrame to a JSON file",
    "section": "Usage",
    "text": "Usage\nspark_write_json( \n  x, \n  path, \n  mode = NULL, \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_json.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_json.html#arguments",
    "title": "Write a Spark DataFrame to a JSON file",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_json.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_json.html#see-also",
    "title": "Write a Spark DataFrame to a JSON file",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html",
    "href": "packages/sparklyr/latest/reference/spark_config.html",
    "title": "Read Spark Configuration",
    "section": "",
    "text": "R/config_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html#spark_config",
    "href": "packages/sparklyr/latest/reference/spark_config.html#spark_config",
    "title": "Read Spark Configuration",
    "section": "spark_config",
    "text": "spark_config"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html#description",
    "href": "packages/sparklyr/latest/reference/spark_config.html#description",
    "title": "Read Spark Configuration",
    "section": "Description",
    "text": "Description\nRead Spark Configuration"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_config.html#usage",
    "title": "Read Spark Configuration",
    "section": "Usage",
    "text": "Usage\nspark_config(file = \"config.yml\", use_default = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_config.html#arguments",
    "title": "Read Spark Configuration",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nfile\nName of the configuration file\n\n\nuse_default\nTRUE to use the built-in defaults provided in this package"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html#details",
    "href": "packages/sparklyr/latest/reference/spark_config.html#details",
    "title": "Read Spark Configuration",
    "section": "Details",
    "text": "Details\nRead Spark configuration using the config package."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html#value",
    "href": "packages/sparklyr/latest/reference/spark_config.html#value",
    "title": "Read Spark Configuration",
    "section": "Value",
    "text": "Value\nNamed list with configuration data"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html",
    "title": "Feature Transformation – HashingTF (Transformer)",
    "section": "",
    "text": "R/ml_feature_hashing_tf.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html#ft_hashing_tf",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html#ft_hashing_tf",
    "title": "Feature Transformation – HashingTF (Transformer)",
    "section": "ft_hashing_tf",
    "text": "ft_hashing_tf"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html#description",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html#description",
    "title": "Feature Transformation – HashingTF (Transformer)",
    "section": "Description",
    "text": "Description\nMaps a sequence of terms to their term frequencies using the hashing trick."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html#usage",
    "title": "Feature Transformation – HashingTF (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_hashing_tf( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  binary = FALSE, \n  num_features = 2^18, \n  uid = random_string(\"hashing_tf_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html#arguments",
    "title": "Feature Transformation – HashingTF (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nbinary\nBinary toggle to control term frequency counts. If true, all non-zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. (default = FALSE)\n\n\nnum_features\nNumber of features. Should be greater than 0. (default = 2^18)\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html#value",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html#value",
    "title": "Feature Transformation – HashingTF (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html#see-also",
    "title": "Feature Transformation – HashingTF (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_dir.html",
    "href": "packages/sparklyr/latest/reference/spark_home_dir.html",
    "title": "Find the SPARK_HOME directory for a version of Spark",
    "section": "",
    "text": "R/spark_home.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_dir.html#spark_home_dir",
    "href": "packages/sparklyr/latest/reference/spark_home_dir.html#spark_home_dir",
    "title": "Find the SPARK_HOME directory for a version of Spark",
    "section": "spark_home_dir",
    "text": "spark_home_dir"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_dir.html#description",
    "href": "packages/sparklyr/latest/reference/spark_home_dir.html#description",
    "title": "Find the SPARK_HOME directory for a version of Spark",
    "section": "Description",
    "text": "Description\nFind the SPARK_HOME directory for a given version of Spark that was previously installed using spark_install."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_dir.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_home_dir.html#usage",
    "title": "Find the SPARK_HOME directory for a version of Spark",
    "section": "Usage",
    "text": "Usage\nspark_home_dir(version = NULL, hadoop_version = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_dir.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_home_dir.html#arguments",
    "title": "Find the SPARK_HOME directory for a version of Spark",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nversion\nVersion of Spark\n\n\nhadoop_version\nVersion of Hadoop"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_dir.html#value",
    "href": "packages/sparklyr/latest/reference/spark_home_dir.html#value",
    "title": "Find the SPARK_HOME directory for a version of Spark",
    "section": "Value",
    "text": "Value\nPath to SPARK_HOME (or NULL if the specified version was not found)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html",
    "href": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html",
    "title": "Retrieve the Spark connection’s SQL catalog implementation property",
    "section": "",
    "text": "R/spark_connection.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#get_spark_sql_catalog_implementation",
    "href": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#get_spark_sql_catalog_implementation",
    "title": "Retrieve the Spark connection’s SQL catalog implementation property",
    "section": "get_spark_sql_catalog_implementation",
    "text": "get_spark_sql_catalog_implementation"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#description",
    "href": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#description",
    "title": "Retrieve the Spark connection’s SQL catalog implementation property",
    "section": "Description",
    "text": "Description\nRetrieve the Spark connection’s SQL catalog implementation property"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#usage",
    "href": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#usage",
    "title": "Retrieve the Spark connection’s SQL catalog implementation property",
    "section": "Usage",
    "text": "Usage\nget_spark_sql_catalog_implementation(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#arguments",
    "href": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#arguments",
    "title": "Retrieve the Spark connection’s SQL catalog implementation property",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nspark_connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#value",
    "href": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#value",
    "title": "Retrieve the Spark connection’s SQL catalog implementation property",
    "section": "Value",
    "text": "Value\nspark.sql.catalogImplementation property from the connection’s runtime configuration"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_config.html",
    "href": "packages/sparklyr/latest/reference/connection_config.html",
    "title": "Read configuration values for a connection",
    "section": "",
    "text": "R/spark_connection.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_config.html#connection_config",
    "href": "packages/sparklyr/latest/reference/connection_config.html#connection_config",
    "title": "Read configuration values for a connection",
    "section": "connection_config",
    "text": "connection_config"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_config.html#description",
    "href": "packages/sparklyr/latest/reference/connection_config.html#description",
    "title": "Read configuration values for a connection",
    "section": "Description",
    "text": "Description\nRead configuration values for a connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_config.html#usage",
    "href": "packages/sparklyr/latest/reference/connection_config.html#usage",
    "title": "Read configuration values for a connection",
    "section": "Usage",
    "text": "Usage\nconnection_config(sc, prefix, not_prefix = list())"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_config.html#arguments",
    "href": "packages/sparklyr/latest/reference/connection_config.html#arguments",
    "title": "Read configuration values for a connection",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nspark_connection\n\n\nprefix\nPrefix to read parameters for (e.g. spark.context., spark.sql., etc.)\n\n\nnot_prefix\nPrefix to not include."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_config.html#value",
    "href": "packages/sparklyr/latest/reference/connection_config.html#value",
    "title": "Read configuration values for a connection",
    "section": "Value",
    "text": "Value\nNamed list of config parameters (note that if a prefix was specified then the names will not include the prefix)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_insert_table.html",
    "href": "packages/sparklyr/latest/reference/spark_insert_table.html",
    "title": "Inserts a Spark DataFrame into a Spark table",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_insert_table.html#spark_insert_table",
    "href": "packages/sparklyr/latest/reference/spark_insert_table.html#spark_insert_table",
    "title": "Inserts a Spark DataFrame into a Spark table",
    "section": "spark_insert_table",
    "text": "spark_insert_table"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_insert_table.html#description",
    "href": "packages/sparklyr/latest/reference/spark_insert_table.html#description",
    "title": "Inserts a Spark DataFrame into a Spark table",
    "section": "Description",
    "text": "Description\nInserts a Spark DataFrame into a Spark table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_insert_table.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_insert_table.html#usage",
    "title": "Inserts a Spark DataFrame into a Spark table",
    "section": "Usage",
    "text": "Usage\nspark_insert_table( \n  x, \n  name, \n  mode = NULL, \n  overwrite = FALSE, \n  options = list(), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_insert_table.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_insert_table.html#arguments",
    "title": "Inserts a Spark DataFrame into a Spark table",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nname\nThe name to assign to the newly generated table.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_insert_table.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_insert_table.html#see-also",
    "title": "Inserts a Spark DataFrame into a Spark table",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj.html",
    "href": "packages/sparklyr/latest/reference/spark_jobj.html",
    "title": "Retrieve a Spark JVM Object Reference",
    "section": "",
    "text": "R/core_jobj.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj.html#spark_jobj",
    "href": "packages/sparklyr/latest/reference/spark_jobj.html#spark_jobj",
    "title": "Retrieve a Spark JVM Object Reference",
    "section": "spark_jobj",
    "text": "spark_jobj"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj.html#description",
    "href": "packages/sparklyr/latest/reference/spark_jobj.html#description",
    "title": "Retrieve a Spark JVM Object Reference",
    "section": "Description",
    "text": "Description\nThis S3 generic is used for accessing the underlying Java Virtual Machine (JVM) Spark objects associated with R objects. These objects act as references to Spark objects living in the JVM. Methods on these objects can be called with the invoke family of functions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_jobj.html#usage",
    "title": "Retrieve a Spark JVM Object Reference",
    "section": "Usage",
    "text": "Usage\nspark_jobj(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_jobj.html#arguments",
    "title": "Retrieve a Spark JVM Object Reference",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn R object containing, or wrapping, a spark_jobj.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_jobj.html#see-also",
    "title": "Retrieve a Spark JVM Object Reference",
    "section": "See Also",
    "text": "See Also\ninvoke, for calling methods on Java object references."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/join.tbl_spark.html",
    "href": "packages/sparklyr/latest/reference/join.tbl_spark.html",
    "title": "Join Spark tbls.",
    "section": "",
    "text": "R/dplyr_join.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/join.tbl_spark.html#join.tbl_spark",
    "href": "packages/sparklyr/latest/reference/join.tbl_spark.html#join.tbl_spark",
    "title": "Join Spark tbls.",
    "section": "join.tbl_spark",
    "text": "join.tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/join.tbl_spark.html#description",
    "href": "packages/sparklyr/latest/reference/join.tbl_spark.html#description",
    "title": "Join Spark tbls.",
    "section": "Description",
    "text": "Description\nThese functions are wrappers around their dplyr equivalents that set Spark SQL-compliant values for the suffix argument by replacing dots (.) with underscores (_). See [join] for a description of the general purpose of the functions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/join.tbl_spark.html#usage",
    "href": "packages/sparklyr/latest/reference/join.tbl_spark.html#usage",
    "title": "Join Spark tbls.",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'tbl_spark'\ninner_join( \n  x, \n  y, \n  by = NULL, \n  copy = FALSE, \n  suffix = c(\"_x\", \"_y\"), \n  auto_index = FALSE, \n  ..., \n  sql_on = NULL \n) \n\n## S3 method for class 'tbl_spark'\nleft_join( \n  x, \n  y, \n  by = NULL, \n  copy = FALSE, \n  suffix = c(\"_x\", \"_y\"), \n  auto_index = FALSE, \n  ..., \n  sql_on = NULL \n) \n\n## S3 method for class 'tbl_spark'\nright_join( \n  x, \n  y, \n  by = NULL, \n  copy = FALSE, \n  suffix = c(\"_x\", \"_y\"), \n  auto_index = FALSE, \n  ..., \n  sql_on = NULL \n) \n\n## S3 method for class 'tbl_spark'\nfull_join( \n  x, \n  y, \n  by = NULL, \n  copy = FALSE, \n  suffix = c(\"_x\", \"_y\"), \n  auto_index = FALSE, \n  ..., \n  sql_on = NULL \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/join.tbl_spark.html#arguments",
    "href": "packages/sparklyr/latest/reference/join.tbl_spark.html#arguments",
    "title": "Join Spark tbls.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx, y\nA pair of lazy data frames backed by database queries.\n\n\nby\nA character vector of variables to join by. If NULL, the default, *_join() will perform a natural join, using all variables in common across x and y. A message lists the variables so that you can check they’re correct; suppress the message by supplying by explicitly. To join by different variables on x and y, use a named vector. For example, by = c(\"a\" = \"b\") will match x$a to y$b. To join by multiple variables, use a vector with length > 1. For example, by = c(\"a\", \"b\") will match x$a to y$a and x$b to y$b. Use a named vector to match different variables in x and y. For example, by = c(\"a\" = \"b\", \"c\" = \"d\") will match x$a to y$b and x$c to y$d. To perform a cross-join, generating all combinations of x and y, use by = character().\n\n\ncopy\nIf x and y are not from the same data source, and copy is TRUE, then y will be copied into a temporary table in same database as x. *_join() will automatically run ANALYZE on the created table in the hope that this will make you queries as efficient as possible by giving more data to the query planner. This allows you to join tables across srcs, but it’s potentially expensive operation so you must opt into it.\n\n\nsuffix\nIf there are non-joined duplicate variables in x and y, these suffixes will be added to the output to disambiguate them. Should be a character vector of length 2.\n\n\nauto_index\nif copy is TRUE, automatically create indices for the variables in by. This may speed up the join if there are matching indexes in x.\n\n\n…\nOther parameters passed onto methods.\n\n\nsql_on\nA custom join predicate as an SQL expression. Usually joins use column equality, but you can perform more complex queries by supply sql_on which should be a SQL expression that uses LHS and RHS aliases to refer to the left-hand side or right-hand side of the join respectively."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect.html",
    "href": "packages/sparklyr/latest/reference/collect.html",
    "title": "Collect",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect.html#collect",
    "href": "packages/sparklyr/latest/reference/collect.html#collect",
    "title": "Collect",
    "section": "collect",
    "text": "collect"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect.html#description",
    "href": "packages/sparklyr/latest/reference/collect.html#description",
    "title": "Collect",
    "section": "Description",
    "text": "Description\nSee collect for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html",
    "title": "Feature Transformation – Binarizer (Transformer)",
    "section": "",
    "text": "R/ml_feature_binarizer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#ft_binarizer",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#ft_binarizer",
    "title": "Feature Transformation – Binarizer (Transformer)",
    "section": "ft_binarizer",
    "text": "ft_binarizer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#description",
    "title": "Feature Transformation – Binarizer (Transformer)",
    "section": "Description",
    "text": "Description\nApply thresholding to a column, such that values less than or equal to the threshold are assigned the value 0.0, and values greater than the threshold are assigned the value 1.0. Column output is numeric for compatibility with other modeling functions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#usage",
    "title": "Feature Transformation – Binarizer (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_binarizer( \n  x, \n  input_col, \n  output_col, \n  threshold = 0, \n  uid = random_string(\"binarizer_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#arguments",
    "title": "Feature Transformation – Binarizer (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nthreshold\nThreshold used to binarize continuous features.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#value",
    "title": "Feature Transformation – Binarizer (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#examples",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#examples",
    "title": "Feature Transformation – Binarizer (Transformer)",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nlibrary(dplyr) \nsc <- spark_connect(master = \"local\") \niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \niris_tbl %>% \n  ft_binarizer( \n    input_col = \"Sepal_Length\", \n    output_col = \"Sepal_Length_bin\", \n    threshold = 5 \n  ) %>% \n  select(Sepal_Length, Sepal_Length_bin, Species) \n#> # Source: spark<?> [?? x 3]\n#>    Sepal_Length Sepal_Length_bin Species\n#>           <dbl>            <dbl> <chr>  \n#>  1          5.1                1 setosa \n#>  2          4.9                0 setosa \n#>  3          4.7                0 setosa \n#>  4          4.6                0 setosa \n#>  5          5                  0 setosa \n#>  6          5.4                1 setosa \n#>  7          4.6                0 setosa \n#>  8          5                  0 setosa \n#>  9          4.4                0 setosa \n#> 10          4.9                0 setosa \n#> # … with more rows"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#see-also",
    "title": "Feature Transformation – Binarizer (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/dplyr_hof.html",
    "href": "packages/sparklyr/latest/reference/dplyr_hof.html",
    "title": "dplyr wrappers for Apache Spark higher order functions",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/dplyr_hof.html#dplyr_hof",
    "href": "packages/sparklyr/latest/reference/dplyr_hof.html#dplyr_hof",
    "title": "dplyr wrappers for Apache Spark higher order functions",
    "section": "dplyr_hof",
    "text": "dplyr_hof"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/dplyr_hof.html#description",
    "href": "packages/sparklyr/latest/reference/dplyr_hof.html#description",
    "title": "dplyr wrappers for Apache Spark higher order functions",
    "section": "Description",
    "text": "Description\nThese methods implement dplyr grammars for Apache Spark higher order functions"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_forall.html",
    "href": "packages/sparklyr/latest/reference/hof_forall.html",
    "title": "Checks whether all elements in an array satisfy a predicate",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_forall.html#hof_forall",
    "href": "packages/sparklyr/latest/reference/hof_forall.html#hof_forall",
    "title": "Checks whether all elements in an array satisfy a predicate",
    "section": "hof_forall",
    "text": "hof_forall"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_forall.html#description",
    "href": "packages/sparklyr/latest/reference/hof_forall.html#description",
    "title": "Checks whether all elements in an array satisfy a predicate",
    "section": "Description",
    "text": "Description\nChecks whether the predicate specified holds for all elements in an array (this is essentially a dplyr wrapper to the forall(expr, pred) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_forall.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_forall.html#usage",
    "title": "Checks whether all elements in an array satisfy a predicate",
    "section": "Usage",
    "text": "Usage\nhof_forall(x, pred, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_forall.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_forall.html#arguments",
    "title": "Checks whether all elements in an array satisfy a predicate",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\npred\nThe predicate to test (it should take an array element as argument and return a boolean value)\n\n\nexpr\nThe array being tested, could be any SQL expression evaluating to an array (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the boolean result (default: expr)\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_forall.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_forall.html#examples",
    "title": "Checks whether all elements in an array satisfy a predicate",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"3.0.0\") \ndf <- tibble::tibble( \n  x = list(c(1, 2, 3, 4, 5), c(6, 7, 8, 9, 10)), \n  y = list(c(1, 4, 2, 8, 5), c(7, 1, 4, 2, 8)), \n) \nsdf <- sdf_copy_to(sc, df, overwrite = TRUE) \nall_positive_tbl <- sdf %>% \n  hof_forall(pred = ~ .x > 0, expr = y, dest_col = all_positive) %>% \n  dplyr::select(all_positive)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html",
    "title": "Feature Transformation – Imputer (Estimator)",
    "section": "",
    "text": "R/ml_feature_imputer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#ft_imputer",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#ft_imputer",
    "title": "Feature Transformation – Imputer (Estimator)",
    "section": "ft_imputer",
    "text": "ft_imputer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#description",
    "title": "Feature Transformation – Imputer (Estimator)",
    "section": "Description",
    "text": "Description\nImputation estimator for completing missing values, either using the mean or the median of the columns in which the missing values are located. The input columns should be of numeric type. This function requires Spark 2.2.0+."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#usage",
    "title": "Feature Transformation – Imputer (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_imputer( \n  x, \n  input_cols = NULL, \n  output_cols = NULL, \n  missing_value = NULL, \n  strategy = \"mean\", \n  uid = random_string(\"imputer_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#arguments",
    "title": "Feature Transformation – Imputer (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_cols\nThe names of the input columns\n\n\noutput_cols\nThe names of the output columns.\n\n\nmissing_value\nThe placeholder for the missing values. All occurrences of missing_value will be imputed. Note that null values are always treated as missing.\n\n\nstrategy\nThe imputation strategy. Currently only “mean” and “median” are supported. If “mean”, then replace missing values using the mean value of the feature. If “median”, then replace missing values using the approximate median value of the feature. Default: mean\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#details",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#details",
    "title": "Feature Transformation – Imputer (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#value",
    "title": "Feature Transformation – Imputer (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#see-also",
    "title": "Feature Transformation – Imputer (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html",
    "title": "Feature Transformation – StopWordsRemover (Transformer)",
    "section": "",
    "text": "R/ml_feature_stop_words_remover.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#ft_stop_words_remover",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#ft_stop_words_remover",
    "title": "Feature Transformation – StopWordsRemover (Transformer)",
    "section": "ft_stop_words_remover",
    "text": "ft_stop_words_remover"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#description",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#description",
    "title": "Feature Transformation – StopWordsRemover (Transformer)",
    "section": "Description",
    "text": "Description\nA feature transformer that filters out stop words from input."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#usage",
    "title": "Feature Transformation – StopWordsRemover (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_stop_words_remover( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  case_sensitive = FALSE, \n  stop_words = ml_default_stop_words(spark_connection(x), \"english\"), \n  uid = random_string(\"stop_words_remover_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#arguments",
    "title": "Feature Transformation – StopWordsRemover (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\ncase_sensitive\nWhether to do a case sensitive comparison over the stop words.\n\n\nstop_words\nThe words to be filtered out.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#value",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#value",
    "title": "Feature Transformation – StopWordsRemover (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#see-also",
    "title": "Feature Transformation – StopWordsRemover (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nml_default_stop_words\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html",
    "href": "packages/sparklyr/latest/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\nspark_connect() spark_connection_is_open() spark_disconnect() spark_disconnect_all() spark_submit()\nManage Spark Connections\n\n\nspark_connect() spark_connection_is_open() spark_disconnect() spark_disconnect_all() spark_submit()\nManage Spark Connections\n\n\nspark_config()\nRead Spark Configuration\n\n\nspark_install() spark_uninstall() spark_install_dir() spark_install_tar() spark_installed_versions() spark_available_versions()\nDownload and install various versions of Spark\n\n\nspark_log()\nView Entries in the Spark Log\n\n\nspark_web()\nOpen the Spark web interface"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-data",
    "href": "packages/sparklyr/latest/reference/index.html#spark-data",
    "title": "sparklyr",
    "section": "Spark Data",
    "text": "Spark Data\n\n\n\nFunction(s)\nDescription\n\n\n\n\nspark_read()\nRead file(s) into a Spark DataFrame using a custom reader\n\n\nspark_read_avro()\nRead Apache Avro data into a Spark DataFrame.\n\n\nspark_read_binary()\nRead binary data into a Spark DataFrame.\n\n\nspark_read_csv()\nRead a CSV file into a Spark DataFrame\n\n\nspark_read_delta()\nRead from Delta Lake into a Spark DataFrame.\n\n\nspark_read_image()\nRead image data into a Spark DataFrame.\n\n\nspark_read_jdbc()\nRead from JDBC connection into a Spark DataFrame.\n\n\nspark_read_json()\nRead a JSON file into a Spark DataFrame\n\n\nspark_read_libsvm()\nRead libsvm file into a Spark DataFrame.\n\n\nspark_read_orc()\nRead a ORC file into a Spark DataFrame\n\n\nspark_read_parquet()\nRead a Parquet file into a Spark DataFrame\n\n\nspark_read_source()\nRead from a generic source into a Spark DataFrame.\n\n\nspark_read_table()\nReads from a Spark Table into a Spark DataFrame.\n\n\nspark_read_text()\nRead a Text file into a Spark DataFrame\n\n\nspark_write()\nWrite Spark DataFrame to file using a custom writer\n\n\nspark_write_avro()\nSerialize a Spark DataFrame into Apache Avro format\n\n\nspark_write_csv()\nWrite a Spark DataFrame to a CSV\n\n\nspark_write_delta()\nWrites a Spark DataFrame into Delta Lake\n\n\nspark_write_jdbc()\nWrites a Spark DataFrame into a JDBC table\n\n\nspark_write_json()\nWrite a Spark DataFrame to a JSON file\n\n\nspark_write_orc()\nWrite a Spark DataFrame to a ORC file\n\n\nspark_write_parquet()\nWrite a Spark DataFrame to a Parquet file\n\n\nspark_write_rds()\nWrite Spark DataFrame to RDS files\n\n\nspark_write_source()\nWrites a Spark DataFrame into a generic source\n\n\nspark_write_table()\nWrites a Spark DataFrame into a Spark table\n\n\nspark_write_text()\nWrite a Spark DataFrame to a Text file\n\n\nspark_insert_table()\nInserts a Spark DataFrame into a Spark table\n\n\nspark_save_table()\nSaves a Spark DataFrame as a Spark table\n\n\ncollect_from_rds()\nCollect Spark data serialized in RDS format into R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-tables",
    "href": "packages/sparklyr/latest/reference/index.html#spark-tables",
    "title": "sparklyr",
    "section": "Spark Tables",
    "text": "Spark Tables\n\n\n\nFunction(s)\nDescription\n\n\n\n\nsrc_databases()\nShow database list\n\n\ntbl_cache()\nCache a Spark Table\n\n\ntbl_change_db()\nUse specific database\n\n\ntbl_uncache()\nUncache a Spark Table"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-dataframes",
    "href": "packages/sparklyr/latest/reference/index.html#spark-dataframes",
    "title": "sparklyr",
    "section": "Spark DataFrames",
    "text": "Spark DataFrames\n\n\n\nFunction(s)\nDescription\n\n\n\n\ndplyr_hof\ndplyr wrappers for Apache Spark higher order functions\n\n\nsdf_save_table() sdf_load_table() sdf_save_parquet() sdf_load_parquet()\nSave / Load a Spark DataFrame\n\n\nsdf_predict() sdf_transform() sdf_fit() sdf_fit_and_transform()\nSpark ML – Transform, fit, and predict methods (sdf_ interface)\n\n\nsdf_along()\nCreate DataFrame for along Object\n\n\nsdf_bind_rows() sdf_bind_cols()\nBind multiple Spark DataFrames by row and column\n\n\nsdf_broadcast()\nBroadcast hint\n\n\nsdf_checkpoint()\nCheckpoint a Spark DataFrame\n\n\nsdf_coalesce()\nCoalesces a Spark DataFrame\n\n\nsdf_collect()\nCollect a Spark DataFrame into R.\n\n\nsdf_copy_to() sdf_import()\nCopy an Object into Spark\n\n\nsdf_crosstab()\nCross Tabulation\n\n\nsdf_debug_string()\nDebug Info for Spark DataFrame\n\n\nsdf_describe()\nCompute summary statistics for columns of a data frame\n\n\nsdf_dim() sdf_nrow() sdf_ncol()\nSupport for Dimension Operations\n\n\nsdf_distinct()\nInvoke distinct on a Spark DataFrame\n\n\nsdf_drop_duplicates()\nRemove duplicates from a Spark DataFrame\n\n\nsdf_expand_grid()\nCreate a Spark dataframe containing all combinations of inputs\n\n\nsdf_from_avro()\nConvert column(s) from avro format\n\n\nsdf_is_streaming()\nSpark DataFrame is Streaming\n\n\nsdf_last_index()\nReturns the last index of a Spark DataFrame\n\n\nsdf_len()\nCreate DataFrame for Length\n\n\nsdf_num_partitions()\nGets number of partitions of a Spark DataFrame\n\n\nsdf_partition_sizes()\nCompute the number of records within each partition of a Spark DataFrame\n\n\nsdf_persist()\nPersist a Spark DataFrame\n\n\nsdf_pivot()\nPivot a Spark DataFrame\n\n\nsdf_project()\nProject features onto principal components\n\n\nsdf_quantile()\nCompute (Approximate) Quantiles with a Spark DataFrame\n\n\nsdf_random_split() sdf_partition()\nPartition a Spark Dataframe\n\n\nsdf_rbeta()\nGenerate random samples from a Beta distribution\n\n\nsdf_rbinom()\nGenerate random samples from a binomial distribution\n\n\nsdf_rcauchy()\nGenerate random samples from a Cauchy distribution\n\n\nsdf_rchisq()\nGenerate random samples from a chi-squared distribution\n\n\nsdf_read_column()\nRead a Column from a Spark DataFrame\n\n\nsdf_register()\nRegister a Spark DataFrame\n\n\nsdf_repartition()\nRepartition a Spark DataFrame\n\n\nsdf_residuals()\nModel Residuals\n\n\nsdf_rexp()\nGenerate random samples from an exponential distribution\n\n\nsdf_rgamma()\nGenerate random samples from a Gamma distribution\n\n\nsdf_rgeom()\nGenerate random samples from a geometric distribution\n\n\nsdf_rhyper()\nGenerate random samples from a hypergeometric distribution\n\n\nsdf_rlnorm()\nGenerate random samples from a log normal distribution\n\n\nsdf_rnorm()\nGenerate random samples from the standard normal distribution\n\n\nsdf_rpois()\nGenerate random samples from a Poisson distribution\n\n\nsdf_rt()\nGenerate random samples from a t-distribution\n\n\nsdf_runif()\nGenerate random samples from the uniform distribution U(0, 1).\n\n\nsdf_rweibull()\nGenerate random samples from a Weibull distribution.\n\n\nsdf_sample()\nRandomly Sample Rows from a Spark DataFrame\n\n\nsdf_schema()\nRead the Schema of a Spark DataFrame\n\n\nsdf_separate_column()\nSeparate a Vector Column into Scalar Columns\n\n\nsdf_seq()\nCreate DataFrame for Range\n\n\nsdf_sort()\nSort a Spark DataFrame\n\n\nsdf_sql()\nSpark DataFrame from SQL\n\n\nsdf_to_avro()\nConvert column(s) to avro format\n\n\nsdf_unnest_longer()\nUnnest longer\n\n\nsdf_unnest_wider()\nUnnest wider\n\n\nsdf_weighted_sample()\nPerform Weighted Random Sampling on a Spark DataFrame\n\n\nsdf_with_sequential_id()\nAdd a Sequential ID Column to a Spark DataFrame\n\n\nsdf_with_unique_id()\nAdd a Unique ID Column to a Spark DataFrame\n\n\nhof_aggregate()\nApply Aggregate Function to Array Column\n\n\nhof_array_sort()\nSorts array using a custom comparator\n\n\nhof_exists()\nDetermine Whether Some Element Exists in an Array Column\n\n\nhof_filter()\nFilter Array Column\n\n\nhof_forall()\nChecks whether all elements in an array satisfy a predicate\n\n\nhof_map_filter()\nFilters a map\n\n\nhof_map_zip_with()\nMerges two maps into one\n\n\nhof_transform()\nTransform Array Column\n\n\nhof_transform_keys()\nTransforms keys of a map\n\n\nhof_transform_values()\nTransforms values of a map\n\n\nhof_zip_with()\nCombines 2 Array Columns\n\n\ntransform_sdf()\ntransform a subset of column(s) in a Spark Dataframe"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---regression",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---regression",
    "title": "sparklyr",
    "section": "Spark ML - Regression",
    "text": "Spark ML - Regression\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_linear_regression()\nSpark ML – Linear Regression\n\n\nml_aft_survival_regression() ml_survival_regression()\nSpark ML – Survival Regression\n\n\nml_isotonic_regression()\nSpark ML – Isotonic Regression\n\n\nml_aft_survival_regression() ml_survival_regression()\nSpark ML – Survival Regression\n\n\nml_generalized_linear_regression()\nSpark ML – Generalized Linear Regression"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---classification",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---classification",
    "title": "sparklyr",
    "section": "Spark ML - Classification",
    "text": "Spark ML - Classification\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_naive_bayes()\nSpark ML – Naive-Bayes\n\n\nml_one_vs_rest()\nSpark ML – OneVsRest\n\n\nml_logistic_regression()\nSpark ML – Logistic Regression\n\n\nml_multilayer_perceptron_classifier() ml_multilayer_perceptron()\nSpark ML – Multilayer Perceptron\n\n\nml_linear_svc()\nSpark ML – LinearSVC"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---tree",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---tree",
    "title": "sparklyr",
    "section": "Spark ML - Tree",
    "text": "Spark ML - Tree\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_decision_tree_classifier() ml_decision_tree() ml_decision_tree_regressor()\nSpark ML – Decision Trees\n\n\nml_decision_tree_classifier() ml_decision_tree() ml_decision_tree_regressor()\nSpark ML – Decision Trees\n\n\nml_decision_tree_classifier() ml_decision_tree() ml_decision_tree_regressor()\nSpark ML – Decision Trees\n\n\nml_gbt_classifier() ml_gradient_boosted_trees() ml_gbt_regressor()\nSpark ML – Gradient Boosted Trees\n\n\nml_gbt_classifier() ml_gradient_boosted_trees() ml_gbt_regressor()\nSpark ML – Gradient Boosted Trees\n\n\nml_gbt_classifier() ml_gradient_boosted_trees() ml_gbt_regressor()\nSpark ML – Gradient Boosted Trees\n\n\nml_random_forest_classifier() ml_random_forest() ml_random_forest_regressor()\nSpark ML – Random Forest\n\n\nml_random_forest_classifier() ml_random_forest() ml_random_forest_regressor()\nSpark ML – Random Forest\n\n\nml_random_forest_classifier() ml_random_forest() ml_random_forest_regressor()\nSpark ML – Random Forest\n\n\nml_feature_importances() ml_tree_feature_importance()\nSpark ML - Feature Importance for Tree Models\n\n\nml_feature_importances() ml_tree_feature_importance()\nSpark ML - Feature Importance for Tree Models"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---clustering",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---clustering",
    "title": "sparklyr",
    "section": "Spark ML - Clustering",
    "text": "Spark ML - Clustering\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_kmeans() ml_compute_cost() ml_compute_silhouette_measure()\nSpark ML – K-Means Clustering\n\n\nml_kmeans_cluster_eval\nEvaluate a K-mean clustering\n\n\nml_bisecting_kmeans()\nSpark ML – Bisecting K-Means Clustering\n\n\nml_gaussian_mixture()\nSpark ML – Gaussian Mixture clustering.\n\n\nml_kmeans() ml_compute_cost() ml_compute_silhouette_measure()\nSpark ML – K-Means Clustering\n\n\nml_power_iteration()\nSpark ML – Power Iteration Clustering"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---text",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---text",
    "title": "sparklyr",
    "section": "Spark ML - Text",
    "text": "Spark ML - Text\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_lda() ml_describe_topics() ml_log_likelihood() ml_log_perplexity() ml_topics_matrix()\nSpark ML – Latent Dirichlet Allocation\n\n\nml_chisquare_test()\nChi-square hypothesis testing for categorical data.\n\n\nml_default_stop_words()\nDefault stop words\n\n\nml_fpgrowth() ml_association_rules() ml_freq_itemsets()\nFrequent Pattern Mining – FPGrowth\n\n\nml_prefixspan() ml_freq_seq_patterns()\nFrequent Pattern Mining – PrefixSpan\n\n\nft_count_vectorizer() ml_vocabulary()\nFeature Transformation – CountVectorizer (Estimator)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---recommendations",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---recommendations",
    "title": "sparklyr",
    "section": "Spark ML - Recommendations",
    "text": "Spark ML - Recommendations\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_als() ml_recommend()\nSpark ML – ALS"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---hyper-parameter-tuning",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---hyper-parameter-tuning",
    "title": "sparklyr",
    "section": "Spark ML - Hyper-parameter tuning",
    "text": "Spark ML - Hyper-parameter tuning\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_sub_models() ml_validation_metrics() ml_cross_validator() ml_train_validation_split()\nSpark ML – Tuning"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---evaluation",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---evaluation",
    "title": "sparklyr",
    "section": "Spark ML - Evaluation",
    "text": "Spark ML - Evaluation\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_metrics_binary()\nExtracts metrics from a fitted table\n\n\nml_metrics_multiclass()\nExtracts metrics from a fitted table\n\n\nml_metrics_regression()\nExtracts metrics from a fitted table\n\n\nml_evaluate()\nEvaluate the Model on a Validation Set\n\n\nml_binary_classification_evaluator() ml_binary_classification_eval() ml_multiclass_classification_evaluator() ml_classification_eval() ml_regression_evaluator()\nSpark ML - Evaluators\n\n\nml_binary_classification_evaluator() ml_binary_classification_eval() ml_multiclass_classification_evaluator() ml_classification_eval() ml_regression_evaluator()\nSpark ML - Evaluators\n\n\nml_clustering_evaluator()\nSpark ML - Clustering Evaluator\n\n\nml_binary_classification_evaluator() ml_binary_classification_eval() ml_multiclass_classification_evaluator() ml_classification_eval() ml_regression_evaluator()\nSpark ML - Evaluators"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---operations",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---operations",
    "title": "sparklyr",
    "section": "Spark ML - Operations",
    "text": "Spark ML - Operations\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_model_data()\nExtracts data associated with a Spark ML model\n\n\nml_call_constructor()\nWrap a Spark ML JVM object\n\n\nml_corr()\nCompute correlation matrix\n\n\nis_ml_transformer() is_ml_estimator() ml_fit() ml_transform() ml_fit_and_transform() ml_predict()\nSpark ML – Transform, fit, and predict methods (ml_ interface)\n\n\nis_ml_transformer() is_ml_estimator() ml_fit() ml_transform() ml_fit_and_transform() ml_predict()\nSpark ML – Transform, fit, and predict methods (ml_ interface)\n\n\nft_string_indexer() ml_labels() ft_string_indexer_model()\nFeature Transformation – StringIndexer (Estimator)\n\n\nml_save() ml_load()\nSpark ML – Model Persistence\n\n\nml_is_set() ml_param_map() ml_param() ml_params()\nSpark ML – ML Params\n\n\nis_ml_transformer() is_ml_estimator() ml_fit() ml_transform() ml_fit_and_transform() ml_predict()\nSpark ML – Transform, fit, and predict methods (ml_ interface)\n\n\nml_save() ml_load()\nSpark ML – Model Persistence\n\n\nml_standardize_formula()\nStandardize Formula Input for ml_model\n\n\nml_summary()\nSpark ML – Extraction of summary metrics\n\n\nml_supervised_pipeline() ml_clustering_pipeline() ml_construct_model_supervised() ml_construct_model_clustering() new_ml_model_prediction() new_ml_model() new_ml_model_classification() new_ml_model_regression() new_ml_model_clustering()\nConstructors for ml_model Objects\n\n\nis_ml_transformer() is_ml_estimator() ml_fit() ml_transform() ml_fit_and_transform() ml_predict()\nSpark ML – Transform, fit, and predict methods (ml_ interface)\n\n\nml_uid()\nSpark ML – UID"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-pipelines",
    "href": "packages/sparklyr/latest/reference/index.html#spark-pipelines",
    "title": "sparklyr",
    "section": "Spark Pipelines",
    "text": "Spark Pipelines\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_pipeline()\nSpark ML – Pipelines\n\n\nml_stage() ml_stages()\nSpark ML – Pipeline stage extraction\n\n\nml_add_stage()\nAdd a Stage to a Pipeline"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-feature-transformers",
    "href": "packages/sparklyr/latest/reference/index.html#spark-feature-transformers",
    "title": "sparklyr",
    "section": "Spark Feature Transformers",
    "text": "Spark Feature Transformers\n\n\n\nFunction(s)\nDescription\n\n\n\n\nft_binarizer()\nFeature Transformation – Binarizer (Transformer)\n\n\nft_bucketizer()\nFeature Transformation – Bucketizer (Transformer)\n\n\nft_chisq_selector()\nFeature Transformation – ChiSqSelector (Estimator)\n\n\nft_count_vectorizer() ml_vocabulary()\nFeature Transformation – CountVectorizer (Estimator)\n\n\nft_dct() ft_discrete_cosine_transform()\nFeature Transformation – Discrete Cosine Transform (DCT) (Transformer)\n\n\nft_elementwise_product()\nFeature Transformation – ElementwiseProduct (Transformer)\n\n\nft_feature_hasher()\nFeature Transformation – FeatureHasher (Transformer)\n\n\nft_hashing_tf()\nFeature Transformation – HashingTF (Transformer)\n\n\nft_idf()\nFeature Transformation – IDF (Estimator)\n\n\nft_imputer()\nFeature Transformation – Imputer (Estimator)\n\n\nft_index_to_string()\nFeature Transformation – IndexToString (Transformer)\n\n\nft_interaction()\nFeature Transformation – Interaction (Transformer)\n\n\nft_bucketed_random_projection_lsh() ft_minhash_lsh()\nFeature Transformation – LSH (Estimator)\n\n\nml_approx_nearest_neighbors() ml_approx_similarity_join()\nUtility functions for LSH models\n\n\nft_max_abs_scaler()\nFeature Transformation – MaxAbsScaler (Estimator)\n\n\nft_min_max_scaler()\nFeature Transformation – MinMaxScaler (Estimator)\n\n\nft_ngram()\nFeature Transformation – NGram (Transformer)\n\n\nft_normalizer()\nFeature Transformation – Normalizer (Transformer)\n\n\nft_one_hot_encoder()\nFeature Transformation – OneHotEncoder (Transformer)\n\n\nft_one_hot_encoder_estimator()\nFeature Transformation – OneHotEncoderEstimator (Estimator)\n\n\nft_pca() ml_pca()\nFeature Transformation – PCA (Estimator)\n\n\nft_polynomial_expansion()\nFeature Transformation – PolynomialExpansion (Transformer)\n\n\nft_quantile_discretizer()\nFeature Transformation – QuantileDiscretizer (Estimator)\n\n\nft_r_formula()\nFeature Transformation – RFormula (Estimator)\n\n\nft_regex_tokenizer()\nFeature Transformation – RegexTokenizer (Transformer)\n\n\nft_robust_scaler()\nFeature Transformation – RobustScaler (Estimator)\n\n\nft_standard_scaler()\nFeature Transformation – StandardScaler (Estimator)\n\n\nft_stop_words_remover()\nFeature Transformation – StopWordsRemover (Transformer)\n\n\nft_string_indexer() ml_labels() ft_string_indexer_model()\nFeature Transformation – StringIndexer (Estimator)\n\n\nft_tokenizer()\nFeature Transformation – Tokenizer (Transformer)\n\n\nft_vector_assembler()\nFeature Transformation – VectorAssembler (Transformer)\n\n\nft_vector_indexer()\nFeature Transformation – VectorIndexer (Estimator)\n\n\nft_vector_slicer()\nFeature Transformation – VectorSlicer (Transformer)\n\n\nft_word2vec() ml_find_synonyms()\nFeature Transformation – Word2Vec (Estimator)\n\n\nft_sql_transformer() ft_dplyr_transformer()\nFeature Transformation – SQLTransformer\n\n\nft_pca() ml_pca()\nFeature Transformation – PCA (Estimator)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#extensions",
    "href": "packages/sparklyr/latest/reference/index.html#extensions",
    "title": "sparklyr",
    "section": "Extensions",
    "text": "Extensions\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_supervised_pipeline() ml_clustering_pipeline() ml_construct_model_supervised() ml_construct_model_clustering() new_ml_model_prediction() new_ml_model() new_ml_model_classification() new_ml_model_regression() new_ml_model_clustering()\nConstructors for ml_model Objects\n\n\ncompile_package_jars()\nCompile Scala sources into a Java Archive (jar)\n\n\nconnection_config()\nRead configuration values for a connection\n\n\ndownload_scalac()\nDownloads default Scala Compilers\n\n\nfind_scalac()\nDiscover the Scala Compiler\n\n\nspark_context() java_context() hive_context() spark_session()\nAccess the Spark API\n\n\nhive_context_config()\nRuntime configuration interface for Hive\n\n\ninvoke() invoke_static() invoke_new()\nInvoke a Method on a JVM Object\n\n\nj_invoke() j_invoke_static() j_invoke_new()\nInvoke a Java function.\n\n\njarray()\nInstantiate a Java array with a specific element type.\n\n\njfloat()\nInstantiate a Java float type.\n\n\njfloat_array()\nInstantiate an Array[Float].\n\n\nspark_context() java_context() hive_context() spark_session()\nAccess the Spark API\n\n\nregister_extension() registered_extensions()\nRegister a Package that Implements a Spark Extension\n\n\nspark_compilation_spec()\nDefine a Spark Compilation Specification\n\n\nspark_default_compilation_spec()\nDefault Compilation Specification for Spark Extensions\n\n\nspark_context() java_context() hive_context() spark_session()\nAccess the Spark API\n\n\nspark_context_config()\nRuntime configuration interface for the Spark Context.\n\n\nspark_dataframe()\nRetrieve a Spark DataFrame\n\n\nspark_dependency()\nDefine a Spark dependency\n\n\nspark_home_set()\nSet the SPARK_HOME environment variable\n\n\nspark_jobj()\nRetrieve a Spark JVM Object Reference\n\n\nspark_context() java_context() hive_context() spark_session()\nAccess the Spark API\n\n\nspark_version()\nGet the Spark Version Associated with a Spark Connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#distributed-computing",
    "href": "packages/sparklyr/latest/reference/index.html#distributed-computing",
    "title": "sparklyr",
    "section": "Distributed Computing",
    "text": "Distributed Computing\n\n\n\nFunction(s)\nDescription\n\n\n\n\nspark_apply()\nApply an R Function in Spark\n\n\nspark_apply_bundle()\nCreate Bundle for Spark Apply\n\n\nspark_apply_log()\nLog Writer for Spark Apply\n\n\nregisterDoSpark()\nRegister a Parallel Backend"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#livy",
    "href": "packages/sparklyr/latest/reference/index.html#livy",
    "title": "sparklyr",
    "section": "Livy",
    "text": "Livy\n\n\n\nFunction(s)\nDescription\n\n\n\n\nlivy_config()\nCreate a Spark Configuration for Livy\n\n\nlivy_service_start() livy_service_stop()\nStart Livy"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#streaming",
    "href": "packages/sparklyr/latest/reference/index.html#streaming",
    "title": "sparklyr",
    "section": "Streaming",
    "text": "Streaming\n\n\n\nFunction(s)\nDescription\n\n\n\n\nstream_find()\nFind Stream\n\n\nstream_generate_test()\nGenerate Test Stream\n\n\nstream_id()\nSpark Stream’s Identifier\n\n\nstream_lag()\nApply lag function to columns of a Spark Streaming DataFrame\n\n\nstream_name()\nSpark Stream’s Name\n\n\nstream_read_csv()\nRead CSV Stream\n\n\nstream_read_delta()\nRead Delta Stream\n\n\nstream_read_json()\nRead JSON Stream\n\n\nstream_read_kafka()\nRead Kafka Stream\n\n\nstream_read_orc()\nRead ORC Stream\n\n\nstream_read_parquet()\nRead Parquet Stream\n\n\nstream_read_socket()\nRead Socket Stream\n\n\nstream_read_text()\nRead Text Stream\n\n\nstream_render()\nRender Stream\n\n\nstream_stats()\nStream Statistics\n\n\nstream_stop()\nStops a Spark Stream\n\n\nstream_trigger_continuous()\nSpark Stream Continuous Trigger\n\n\nstream_trigger_interval()\nSpark Stream Interval Trigger\n\n\nstream_view()\nView Stream\n\n\nstream_watermark()\nWatermark Stream\n\n\nstream_write_console()\nWrite Console Stream\n\n\nstream_write_csv()\nWrite CSV Stream\n\n\nstream_write_delta()\nWrite Delta Stream\n\n\nstream_write_json()\nWrite JSON Stream\n\n\nstream_write_kafka()\nWrite Kafka Stream\n\n\nstream_write_memory()\nWrite Memory Stream\n\n\nstream_write_orc()\nWrite a ORC Stream\n\n\nstream_write_parquet()\nWrite Parquet Stream\n\n\nstream_write_text()\nWrite Text Stream\n\n\nreactiveSpark()\nReactive spark reader"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#dplyr-integration",
    "href": "packages/sparklyr/latest/reference/index.html#dplyr-integration",
    "title": "sparklyr",
    "section": "dplyr integration",
    "text": "dplyr integration\n\n\n\nFunction(s)\nDescription\n\n\n\n\ncopy_to()\nCopy an R Data Frame to Spark\n\n\ndistinct\nDistinct\n\n\nfilter\nFilter\n\n\nfull_join\nFull join\n\n\ninner_join\nInner join\n\n\ninner_join() left_join() right_join() full_join()\nJoin Spark tbls.\n\n\nleft_join\nLeft join\n\n\nmutate\nMutate\n\n\nright_join\nRight join\n\n\nselect\nSelect"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#tidyr-integration",
    "href": "packages/sparklyr/latest/reference/index.html#tidyr-integration",
    "title": "sparklyr",
    "section": "tidyr integration",
    "text": "tidyr integration\n\n\n\nFunction(s)\nDescription\n\n\n\n\npivot_longer\nPivot longer\n\n\npivot_wider\nPivot wider\n\n\nfill\nFill\n\n\nna.replace()\nReplace Missing Values in Objects\n\n\nnest\nNest\n\n\nreplace_na\nReplace NA\n\n\nseparate\nSeparate\n\n\nunite\nUnite\n\n\nunnest\nUnnest"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#tidymodels-integration",
    "href": "packages/sparklyr/latest/reference/index.html#tidymodels-integration",
    "title": "sparklyr",
    "section": "tidymodels integration",
    "text": "tidymodels integration\n\n\n\nFunction(s)\nDescription\n\n\n\n\ntidy() augment() glance()\nTidying methods for Spark ML ALS\n\n\nml_glm_tidiers tidy.ml_model_generalized_linear_regression tidy.ml_model_linear_regression augment.ml_model_generalized_linear_regression augment._ml_model_linear_regression augment.ml_model_linear_regression glance.ml_model_generalized_linear_regression glance.ml_model_linear_regression\nTidying methods for Spark ML linear models\n\n\ntidy() augment() glance()\nTidying methods for Spark ML Isotonic Regression\n\n\ntidy() augment() glance()\nTidying methods for Spark ML LDA models\n\n\ntidy() augment() glance()\nTidying methods for Spark ML linear svc\n\n\nml_logistic_regression_tidiers tidy.ml_model_logistic_regression augment.ml_model_logistic_regression augment._ml_model_logistic_regression glance.ml_model_logistic_regression\nTidying methods for Spark ML Logistic Regression\n\n\ntidy() augment() glance()\nTidying methods for Spark ML MLP\n\n\ntidy() augment() glance()\nTidying methods for Spark ML Naive Bayes\n\n\ntidy() augment() glance()\nTidying methods for Spark ML Principal Component Analysis\n\n\ntidy() augment() glance()\nTidying methods for Spark ML Survival Regression\n\n\nml_tree_tidiers tidy.ml_model_decision_tree_classification tidy.ml_model_decision_tree_regression augment.ml_model_decision_tree_classification augment._ml_model_decision_tree_classification augment.ml_model_decision_tree_regression augment._ml_model_decision_tree_regression glance.ml_model_decision_tree_classification glance.ml_model_decision_tree_regression tidy.ml_model_random_forest_classification tidy.ml_model_random_forest_regression augment.ml_model_random_forest_classification augment._ml_model_random_forest_classification augment.ml_model_random_forest_regression augment._ml_model_random_forest_regression glance.ml_model_random_forest_classification glance.ml_model_random_forest_regression tidy.ml_model_gbt_classification tidy.ml_model_gbt_regression augment.ml_model_gbt_classification augment._ml_model_gbt_classification augment.ml_model_gbt_regression augment._ml_model_gbt_regression glance.ml_model_gbt_classification glance.ml_model_gbt_regression\nTidying methods for Spark ML tree models\n\n\ntidy() augment() glance() tidy() augment() glance() tidy() augment() glance()\nTidying methods for Spark ML unsupervised models"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-operations",
    "href": "packages/sparklyr/latest/reference/index.html#spark-operations",
    "title": "sparklyr",
    "section": "Spark Operations",
    "text": "Spark Operations\n\n\n\nFunction(s)\nDescription\n\n\n\n\nget_spark_sql_catalog_implementation()\nRetrieve the Spark connection’s SQL catalog implementation property\n\n\nconnection_is_open()\nCheck whether the connection is open\n\n\nconnection_spark_shinyapp()\nA Shiny app that can be used to construct a spark_connect statement\n\n\nspark_session_config()\nRuntime configuration interface for the Spark Session\n\n\nspark_set_checkpoint_dir() spark_get_checkpoint_dir()\nSet/Get Spark checkpoint directory\n\n\nspark_connect() spark_connection_is_open() spark_disconnect() spark_disconnect_all() spark_submit()\nManage Spark Connections\n\n\nspark_table_name()\nGenerate a Table Name from Expression\n\n\nspark_install() spark_uninstall() spark_install_dir() spark_install_tar() spark_installed_versions() spark_available_versions()\nDownload and install various versions of Spark\n\n\nspark_version_from_home()\nGet the Spark Version Associated with a Spark Installation\n\n\nspark_versions()\nRetrieves a dataframe available Spark versions that van be installed.\n\n\nspark_config_kubernetes()\nKubernetes Configuration\n\n\nspark_config_settings()\nRetrieve Available Settings\n\n\nspark_connection_find()\nFind Spark Connection\n\n\nspark_dependency_fallback()\nFallback to Spark Dependency\n\n\nspark_extension()\nCreate Spark Extension\n\n\nspark_load_table()\nReads from a Spark Table into a Spark DataFrame.\n\n\nlist_sparklyr_jars()\nlist all sparklyr-*.jar files that have been built\n\n\nspark_config_packages()\nCreates Spark Configuration\n\n\nspark_connection()\nRetrieve the Spark Connection Associated with an R Object\n\n\nspark_adaptive_query_execution()\nRetrieves or sets status of Spark AQE\n\n\nspark_advisory_shuffle_partition_size()\nRetrieves or sets advisory size of the shuffle partition\n\n\nspark_auto_broadcast_join_threshold()\nRetrieves or sets the auto broadcast join threshold\n\n\nspark_coalesce_initial_num_partitions()\nRetrieves or sets initial number of shuffle partitions before coalescing\n\n\nspark_coalesce_min_num_partitions()\nRetrieves or sets the minimum number of shuffle partitions after coalescing\n\n\nspark_coalesce_shuffle_partitions()\nRetrieves or sets whether coalescing contiguous shuffle partitions is enabled\n\n\nspark_connection-class\nspark_connection class\n\n\nspark_jobj-class\nspark_jobj class\n\n\nsparklyr_get_backend_port()\nReturn the port number of a sparklyr backend."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#other",
    "href": "packages/sparklyr/latest/reference/index.html#other",
    "title": "sparklyr",
    "section": "Other",
    "text": "Other\n\n\n\nFunction(s)\nDescription\n\n\n\n\nspark_statistical_routines\nGenerate random samples from some distribution\n\n\nensure\nEnforce Specific Structure for R Objects\n\n\nrandom_string()\nRandom string generation\n\n\n%->%\nInfix operator for composing a lambda expression\n\n\n[()\nSubsetting operator for Spark dataframe\n\n\ngeneric_call_interface\nGeneric Call Interface"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install.html",
    "href": "packages/sparklyr/latest/reference/spark_install.html",
    "title": "Download and install various versions of Spark",
    "section": "",
    "text": "R/install_spark.R, R/install_spark_versions.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install.html#spark_install",
    "href": "packages/sparklyr/latest/reference/spark_install.html#spark_install",
    "title": "Download and install various versions of Spark",
    "section": "spark_install",
    "text": "spark_install"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install.html#description",
    "href": "packages/sparklyr/latest/reference/spark_install.html#description",
    "title": "Download and install various versions of Spark",
    "section": "Description",
    "text": "Description\nInstall versions of Spark for use with local Spark connections (i.e. spark_connect(master = \"local\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_install.html#usage",
    "title": "Download and install various versions of Spark",
    "section": "Usage",
    "text": "Usage\nspark_install( \n  version = NULL, \n  hadoop_version = NULL, \n  reset = TRUE, \n  logging = \"INFO\", \n  verbose = interactive() \n) \n\nspark_uninstall(version, hadoop_version) \n\nspark_install_dir() \n\nspark_install_tar(tarfile) \n\nspark_installed_versions() \n\nspark_available_versions( \n  show_hadoop = FALSE, \n  show_minor = FALSE, \n  show_future = FALSE \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_install.html#arguments",
    "title": "Download and install various versions of Spark",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nversion\nVersion of Spark to install. See spark_available_versions for a list of supported versions\n\n\nhadoop_version\nVersion of Hadoop to install. See spark_available_versions for a list of supported versions\n\n\nreset\nAttempts to reset settings to defaults.\n\n\nlogging\nLogging level to configure install. Supported options: “WARN”, “INFO”\n\n\nverbose\nReport information as Spark is downloaded / installed\n\n\ntarfile\nPath to TAR file conforming to the pattern spark-###-bin-(hadoop)?### where ### reference spark and hadoop versions respectively.\n\n\nshow_hadoop\nShow Hadoop distributions?\n\n\nshow_minor\nShow minor Spark versions?\n\n\nshow_future\nShould future versions which have not been released be shown?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install.html#value",
    "href": "packages/sparklyr/latest/reference/spark_install.html#value",
    "title": "Download and install various versions of Spark",
    "section": "Value",
    "text": "Value\nList with information about the installed version."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/download_scalac.html",
    "href": "packages/sparklyr/latest/reference/download_scalac.html",
    "title": "Downloads default Scala Compilers",
    "section": "",
    "text": "R/spark_compile.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/download_scalac.html#download_scalac",
    "href": "packages/sparklyr/latest/reference/download_scalac.html#download_scalac",
    "title": "Downloads default Scala Compilers",
    "section": "download_scalac",
    "text": "download_scalac"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/download_scalac.html#description",
    "href": "packages/sparklyr/latest/reference/download_scalac.html#description",
    "title": "Downloads default Scala Compilers",
    "section": "Description",
    "text": "Description\ncompile_package_jars requires several versions of the scala compiler to work, this is to match Spark scala versions. To help setup your environment, this function will download the required compilers under the default search path."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/download_scalac.html#usage",
    "href": "packages/sparklyr/latest/reference/download_scalac.html#usage",
    "title": "Downloads default Scala Compilers",
    "section": "Usage",
    "text": "Usage\ndownload_scalac(dest_path = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/download_scalac.html#arguments",
    "href": "packages/sparklyr/latest/reference/download_scalac.html#arguments",
    "title": "Downloads default Scala Compilers",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\ndest_path\nThe destination path where scalac will be downloaded to."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/download_scalac.html#details",
    "href": "packages/sparklyr/latest/reference/download_scalac.html#details",
    "title": "Downloads default Scala Compilers",
    "section": "Details",
    "text": "Details\nSee find_scalac for a list of paths searched and used by this function to install the required compilers."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_binary.html",
    "href": "packages/sparklyr/latest/reference/spark_read_binary.html",
    "title": "Read binary data into a Spark DataFrame.",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_binary.html#spark_read_binary",
    "href": "packages/sparklyr/latest/reference/spark_read_binary.html#spark_read_binary",
    "title": "Read binary data into a Spark DataFrame.",
    "section": "spark_read_binary",
    "text": "spark_read_binary"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_binary.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_binary.html#description",
    "title": "Read binary data into a Spark DataFrame.",
    "section": "Description",
    "text": "Description\nRead binary files within a directory and convert each file into a record within the resulting Spark dataframe. The output will be a Spark dataframe with the following columns and possibly partition columns:\n-path: StringType\n-modificationTime: TimestampType\n-length: LongType\n-content: BinaryType"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_binary.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_binary.html#usage",
    "title": "Read binary data into a Spark DataFrame.",
    "section": "Usage",
    "text": "Usage\nspark_read_binary( \n  sc, \n  name = NULL, \n  dir = name, \n  path_glob_filter = \"*\", \n  recursive_file_lookup = FALSE, \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_binary.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_binary.html#arguments",
    "title": "Read binary data into a Spark DataFrame.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\ndir\nDirectory to read binary files from.\n\n\npath_glob_filter\nGlob pattern of binary files to be loaded (e.g., “*.jpg”).\n\n\nrecursive_file_lookup\nIf FALSE (default), then partition discovery will be enabled (i.e., if a partition naming scheme is present, then partitions specified by subdirectory names such as “date=2019-07-01” will be created and files outside subdirectories following a partition naming scheme will be ignored). If TRUE, then all nested directories will be searched even if their names do not follow a partition naming scheme.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_binary.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_binary.html#see-also",
    "title": "Read binary data into a Spark DataFrame.",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pivot_wider.html",
    "href": "packages/sparklyr/latest/reference/pivot_wider.html",
    "title": "Pivot wider",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pivot_wider.html#pivot_wider",
    "href": "packages/sparklyr/latest/reference/pivot_wider.html#pivot_wider",
    "title": "Pivot wider",
    "section": "pivot_wider",
    "text": "pivot_wider"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pivot_wider.html#description",
    "href": "packages/sparklyr/latest/reference/pivot_wider.html#description",
    "title": "Pivot wider",
    "section": "Description",
    "text": "Description\nSee pivot_wider for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html",
    "title": "Read the Schema of a Spark DataFrame",
    "section": "",
    "text": "R/sdf_wrapper.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html#sdf_schema",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html#sdf_schema",
    "title": "Read the Schema of a Spark DataFrame",
    "section": "sdf_schema",
    "text": "sdf_schema"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html#description",
    "title": "Read the Schema of a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRead the schema of a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html#usage",
    "title": "Read the Schema of a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_schema(x, expand_nested_cols = FALSE, expand_struct_cols = FALSE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html#arguments",
    "title": "Read the Schema of a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nexpand_nested_cols\nWhether to expand columns containing nested array of structs (which are usually created by tidyr::nest on a Spark data frame)\n\n\nexpand_struct_cols\nWhether to expand columns containing structs"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html#details",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html#details",
    "title": "Read the Schema of a Spark DataFrame",
    "section": "Details",
    "text": "Details\nThe type column returned gives the string representation of the underlying Spark type for that column; for example, a vector of numeric values would be returned with the type \"DoubleType\". Please see the Spark Scala API Documentation\nfor information on what types are available and exposed by Spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html#value",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html#value",
    "title": "Read the Schema of a Spark DataFrame",
    "section": "Value",
    "text": "Value\nAn R list, with each list element describing the name and type of a column."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html",
    "href": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html",
    "title": "Default Compilation Specification for Spark Extensions",
    "section": "",
    "text": "R/spark_compile.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#spark_default_compilation_spec",
    "href": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#spark_default_compilation_spec",
    "title": "Default Compilation Specification for Spark Extensions",
    "section": "spark_default_compilation_spec",
    "text": "spark_default_compilation_spec"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#description",
    "href": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#description",
    "title": "Default Compilation Specification for Spark Extensions",
    "section": "Description",
    "text": "Description\nThis is the default compilation specification used for Spark extensions, when used with compile_package_jars."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#usage",
    "title": "Default Compilation Specification for Spark Extensions",
    "section": "Usage",
    "text": "Usage\nspark_default_compilation_spec( \n  pkg = infer_active_package_name(), \n  locations = NULL \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#arguments",
    "title": "Default Compilation Specification for Spark Extensions",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\npkg\nThe package containing Spark extensions to be compiled.\n\n\nlocations\nAdditional locations to scan. By default, the directories /opt/scala and /usr/local/scala will be scanned."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_exists.html",
    "href": "packages/sparklyr/latest/reference/hof_exists.html",
    "title": "Determine Whether Some Element Exists in an Array Column",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_exists.html#hof_exists",
    "href": "packages/sparklyr/latest/reference/hof_exists.html#hof_exists",
    "title": "Determine Whether Some Element Exists in an Array Column",
    "section": "hof_exists",
    "text": "hof_exists"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_exists.html#description",
    "href": "packages/sparklyr/latest/reference/hof_exists.html#description",
    "title": "Determine Whether Some Element Exists in an Array Column",
    "section": "Description",
    "text": "Description\nDetermines whether an element satisfying the given predicate exists in each array from an array column (this is essentially a dplyr wrapper for the exists(array<T>, function<T, Boolean>): Boolean built-in Spark SQL function)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_exists.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_exists.html#usage",
    "title": "Determine Whether Some Element Exists in an Array Column",
    "section": "Usage",
    "text": "Usage\nhof_exists(x, pred, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_exists.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_exists.html#arguments",
    "title": "Determine Whether Some Element Exists in an Array Column",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark data frame to search\n\n\npred\nA boolean predicate\n\n\nexpr\nThe array being searched (could be any SQL expression evaluating to an array)\n\n\ndest_col\nColumn to store the search result\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_version.html",
    "href": "packages/sparklyr/latest/reference/spark_default_version.html",
    "title": "determine the version that will be used by default if version is NULL",
    "section": "",
    "text": "R/install_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_version.html#spark_default_version",
    "href": "packages/sparklyr/latest/reference/spark_default_version.html#spark_default_version",
    "title": "determine the version that will be used by default if version is NULL",
    "section": "spark_default_version",
    "text": "spark_default_version"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_version.html#description",
    "href": "packages/sparklyr/latest/reference/spark_default_version.html#description",
    "title": "determine the version that will be used by default if version is NULL",
    "section": "Description",
    "text": "Description\ndetermine the version that will be used by default if version is NULL"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_version.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_default_version.html#usage",
    "title": "determine the version that will be used by default if version is NULL",
    "section": "Usage",
    "text": "Usage\nspark_default_version()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/distinct.html",
    "href": "packages/sparklyr/latest/reference/distinct.html",
    "title": "Distinct",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/distinct.html#distinct",
    "href": "packages/sparklyr/latest/reference/distinct.html#distinct",
    "title": "Distinct",
    "section": "distinct",
    "text": "distinct"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/distinct.html#description",
    "href": "packages/sparklyr/latest/reference/distinct.html#description",
    "title": "Distinct",
    "section": "Description",
    "text": "Description\nSee distinct for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-params.html",
    "href": "packages/sparklyr/latest/reference/ml-params.html",
    "title": "Spark ML – ML Params",
    "section": "",
    "text": "R/ml_param_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-params.html#ml-params",
    "href": "packages/sparklyr/latest/reference/ml-params.html#ml-params",
    "title": "Spark ML – ML Params",
    "section": "ml-params",
    "text": "ml-params"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-params.html#description",
    "href": "packages/sparklyr/latest/reference/ml-params.html#description",
    "title": "Spark ML – ML Params",
    "section": "Description",
    "text": "Description\nHelper methods for working with parameters for ML objects."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-params.html#usage",
    "href": "packages/sparklyr/latest/reference/ml-params.html#usage",
    "title": "Spark ML – ML Params",
    "section": "Usage",
    "text": "Usage\nml_is_set(x, param, ...) \n\nml_param_map(x, ...) \n\nml_param(x, param, allow_null = FALSE, ...) \n\nml_params(x, params = NULL, allow_null = FALSE, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-params.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml-params.html#arguments",
    "title": "Spark ML – ML Params",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark ML object, either a pipeline stage or an evaluator.\n\n\nparam\nThe parameter to extract or set.\n\n\n…\nOptional arguments; currently unused.\n\n\nallow_null\nWhether to allow NULL results when extracting parameters. If FALSE, an error will be thrown if the specified parameter is not found. Defaults to FALSE.\n\n\nparams\nA vector of parameters to extract."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html",
    "title": "Tidying methods for Spark ML unsupervised models",
    "section": "",
    "text": "R/tidiers_ml_unsupervised_models.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#ml_unsupervised_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#ml_unsupervised_tidiers",
    "title": "Tidying methods for Spark ML unsupervised models",
    "section": "ml_unsupervised_tidiers",
    "text": "ml_unsupervised_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#description",
    "title": "Tidying methods for Spark ML unsupervised models",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#usage",
    "title": "Tidying methods for Spark ML unsupervised models",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_kmeans'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_kmeans'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_kmeans'\nglance(x, ...) \n\n## S3 method for class 'ml_model_bisecting_kmeans'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_bisecting_kmeans'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_bisecting_kmeans'\nglance(x, ...) \n\n## S3 method for class 'ml_model_gaussian_mixture'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_gaussian_mixture'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_gaussian_mixture'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML unsupervised models",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_persist.html",
    "href": "packages/sparklyr/latest/reference/sdf_persist.html",
    "title": "Persist a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_persist.html#sdf_persist",
    "href": "packages/sparklyr/latest/reference/sdf_persist.html#sdf_persist",
    "title": "Persist a Spark DataFrame",
    "section": "sdf_persist",
    "text": "sdf_persist"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_persist.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_persist.html#description",
    "title": "Persist a Spark DataFrame",
    "section": "Description",
    "text": "Description\nPersist a Spark DataFrame, forcing any pending computations and (optionally) serializing the results to disk."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_persist.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_persist.html#usage",
    "title": "Persist a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_persist(x, storage.level = \"MEMORY_AND_DISK\", name = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_persist.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_persist.html#arguments",
    "title": "Persist a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nstorage.level\nThe storage level to be used. Please view the Spark Documentationfor information on what storage levels are accepted.\n\n\nname\nA name to assign this table. Passed to [sdf_register()]."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_persist.html#details",
    "href": "packages/sparklyr/latest/reference/sdf_persist.html#details",
    "title": "Persist a Spark DataFrame",
    "section": "Details",
    "text": "Details\nSpark DataFrames invoke their operations lazily – pending operations are deferred until their results are actually needed. Persisting a Spark DataFrame effectively ‘forces’ any pending computations, and then persists the generated Spark DataFrame as requested (to memory, to disk, or otherwise).\nUsers of Spark should be careful to persist the results of any computations which are non-deterministic – otherwise, one might see that the values within a column seem to ‘change’ as new operations are performed on that data set."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/quote_sql_name.html",
    "href": "packages/sparklyr/latest/reference/quote_sql_name.html",
    "title": "Translate input character vector or symbol to a SQL identifier",
    "section": "",
    "text": "R/sql_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/quote_sql_name.html#quote_sql_name",
    "href": "packages/sparklyr/latest/reference/quote_sql_name.html#quote_sql_name",
    "title": "Translate input character vector or symbol to a SQL identifier",
    "section": "quote_sql_name",
    "text": "quote_sql_name"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/quote_sql_name.html#description",
    "href": "packages/sparklyr/latest/reference/quote_sql_name.html#description",
    "title": "Translate input character vector or symbol to a SQL identifier",
    "section": "Description",
    "text": "Description\nCalls dbplyr::translate_sql_ on the input character vector or symbol to obtain the corresponding SQL identifier that is escaped and quoted properly"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/quote_sql_name.html#usage",
    "href": "packages/sparklyr/latest/reference/quote_sql_name.html#usage",
    "title": "Translate input character vector or symbol to a SQL identifier",
    "section": "Usage",
    "text": "Usage\nquote_sql_name(x, con = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_web.html",
    "href": "packages/sparklyr/latest/reference/spark_web.html",
    "title": "Open the Spark web interface",
    "section": "",
    "text": "R/spark_connection.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_web.html#spark_web",
    "href": "packages/sparklyr/latest/reference/spark_web.html#spark_web",
    "title": "Open the Spark web interface",
    "section": "spark_web",
    "text": "spark_web"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_web.html#description",
    "href": "packages/sparklyr/latest/reference/spark_web.html#description",
    "title": "Open the Spark web interface",
    "section": "Description",
    "text": "Description\nOpen the Spark web interface"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_web.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_web.html#usage",
    "title": "Open the Spark web interface",
    "section": "Usage",
    "text": "Usage\nspark_web(sc, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_web.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_web.html#arguments",
    "title": "Open the Spark web interface",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html",
    "title": "Tidying methods for Spark ML Survival Regression",
    "section": "",
    "text": "R/tidiers_ml_aft_survival_regression.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#ml_survival_regression_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#ml_survival_regression_tidiers",
    "title": "Tidying methods for Spark ML Survival Regression",
    "section": "ml_survival_regression_tidiers",
    "text": "ml_survival_regression_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#description",
    "title": "Tidying methods for Spark ML Survival Regression",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#usage",
    "title": "Tidying methods for Spark ML Survival Regression",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_aft_survival_regression'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_aft_survival_regression'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_aft_survival_regression'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML Survival Regression",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html",
    "title": "Retrieves or sets the minimum number of shuffle partitions after coalescing",
    "section": "",
    "text": "R/spark_context_config.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#spark_coalesce_min_num_partitions",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#spark_coalesce_min_num_partitions",
    "title": "Retrieves or sets the minimum number of shuffle partitions after coalescing",
    "section": "spark_coalesce_min_num_partitions",
    "text": "spark_coalesce_min_num_partitions"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#description",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#description",
    "title": "Retrieves or sets the minimum number of shuffle partitions after coalescing",
    "section": "Description",
    "text": "Description\nRetrieves or sets the minimum number of shuffle partitions after coalescing"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#usage",
    "title": "Retrieves or sets the minimum number of shuffle partitions after coalescing",
    "section": "Usage",
    "text": "Usage\nspark_coalesce_min_num_partitions(sc, num_partitions = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#arguments",
    "title": "Retrieves or sets the minimum number of shuffle partitions after coalescing",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nnum_partitions\nMinimum number of shuffle partitions after coalescing. Defaults to NULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#see-also",
    "title": "Retrieves or sets the minimum number of shuffle partitions after coalescing",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_adaptive_query_execution(), spark_advisory_shuffle_partition_size(), spark_auto_broadcast_join_threshold(), spark_coalesce_initial_num_partitions(), spark_coalesce_shuffle_partitions(), spark_session_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_lda_tidiers.html",
    "title": "Tidying methods for Spark ML LDA models",
    "section": "",
    "text": "R/tidiers_ml_lda.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#ml_lda_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#ml_lda_tidiers",
    "title": "Tidying methods for Spark ML LDA models",
    "section": "ml_lda_tidiers",
    "text": "ml_lda_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#description",
    "title": "Tidying methods for Spark ML LDA models",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#usage",
    "title": "Tidying methods for Spark ML LDA models",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_lda'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_lda'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_lda'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML LDA models",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rexp.html",
    "href": "packages/sparklyr/latest/reference/sdf_rexp.html",
    "title": "Generate random samples from an exponential distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rexp.html#sdf_rexp",
    "href": "packages/sparklyr/latest/reference/sdf_rexp.html#sdf_rexp",
    "title": "Generate random samples from an exponential distribution",
    "section": "sdf_rexp",
    "text": "sdf_rexp"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rexp.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rexp.html#description",
    "title": "Generate random samples from an exponential distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from an exponential distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rexp.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rexp.html#usage",
    "title": "Generate random samples from an exponential distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rexp(sc, n, rate = 1, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rexp.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rexp.html#arguments",
    "title": "Generate random samples from an exponential distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nrate\nRate of the exponential distribution (default: 1). The exponential distribution with rate lambda has mean 1 / lambda and density f(x) = lambda e ^- lambda x .\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rexp.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rexp.html#see-also",
    "title": "Generate random samples from an exponential distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ensure.html",
    "href": "packages/sparklyr/latest/reference/ensure.html",
    "title": "Enforce Specific Structure for R Objects",
    "section": "",
    "text": "R/precondition.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ensure.html#ensure",
    "href": "packages/sparklyr/latest/reference/ensure.html#ensure",
    "title": "Enforce Specific Structure for R Objects",
    "section": "ensure",
    "text": "ensure"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ensure.html#description",
    "href": "packages/sparklyr/latest/reference/ensure.html#description",
    "title": "Enforce Specific Structure for R Objects",
    "section": "Description",
    "text": "Description\nThese routines are useful when preparing to pass objects to a Spark routine, as it is often necessary to ensure certain parameters are scalar integers, or scalar doubles, and so on."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ensure.html#arguments",
    "href": "packages/sparklyr/latest/reference/ensure.html#arguments",
    "title": "Enforce Specific Structure for R Objects",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nobject\nAn R object.\n\n\nallow.na\nAre NA values permitted for this object?\n\n\nallow.null\nAre NULL values permitted for this object?\n\n\ndefault\nIf object is NULL, what value should be used in its place? If default is specified, allow.nullis ignored (and assumed to be TRUE)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect_from_rds.html",
    "href": "packages/sparklyr/latest/reference/collect_from_rds.html",
    "title": "Collect Spark data serialized in RDS format into R",
    "section": "",
    "text": "R/sdf_wrapper.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect_from_rds.html#collect_from_rds",
    "href": "packages/sparklyr/latest/reference/collect_from_rds.html#collect_from_rds",
    "title": "Collect Spark data serialized in RDS format into R",
    "section": "collect_from_rds",
    "text": "collect_from_rds"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect_from_rds.html#description",
    "href": "packages/sparklyr/latest/reference/collect_from_rds.html#description",
    "title": "Collect Spark data serialized in RDS format into R",
    "section": "Description",
    "text": "Description\nDeserialize Spark data that is serialized using spark_write_rds() into a R dataframe."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect_from_rds.html#usage",
    "href": "packages/sparklyr/latest/reference/collect_from_rds.html#usage",
    "title": "Collect Spark data serialized in RDS format into R",
    "section": "Usage",
    "text": "Usage\ncollect_from_rds(path)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect_from_rds.html#arguments",
    "href": "packages/sparklyr/latest/reference/collect_from_rds.html#arguments",
    "title": "Collect Spark data serialized in RDS format into R",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\npath\nPath to a local RDS file that is produced by spark_write_rds() (RDS files stored in HDFS will need to be downloaded to local filesystem first (e.g., by running hadoop fs -copyToLocal ... or similar)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect_from_rds.html#see-also",
    "href": "packages/sparklyr/latest/reference/collect_from_rds.html#see-also",
    "title": "Collect Spark data serialized in RDS format into R",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html",
    "href": "packages/sparklyr/latest/reference/sdf_register.html",
    "title": "Register a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#sdf_register",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#sdf_register",
    "title": "Register a Spark DataFrame",
    "section": "sdf_register",
    "text": "sdf_register"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#description",
    "title": "Register a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRegisters a Spark DataFrame (giving it a table name for the Spark SQL context), and returns a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#usage",
    "title": "Register a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_register(x, name = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#arguments",
    "title": "Register a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame.\n\n\nname\nA name to assign this table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#section",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#section",
    "title": "Register a Spark DataFrame",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#transforming-spark-dataframes",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#transforming-spark-dataframes",
    "title": "Register a Spark DataFrame",
    "section": "Transforming Spark DataFrames",
    "text": "Transforming Spark DataFrames\nThe family of functions prefixed with sdf_ generally access the Scala Spark DataFrame API directly, as opposed to the dplyr interface which uses Spark SQL. These functions will ‘force’ any pending SQL in a dplyr pipeline, such that the resulting tbl_spark object returned will no longer have the attached ‘lazy’ SQL operations. Note that the underlying Spark DataFrame does execute its operations lazily, so that even though the pending set of operations (currently) are not exposed at the R level, these operations will only be executed when you explicitly collect() the table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#see-also",
    "title": "Register a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_copy_to(), sdf_distinct(), sdf_random_split(), sdf_sample(), sdf_sort(), sdf_weighted_sample()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat.html",
    "href": "packages/sparklyr/latest/reference/jfloat.html",
    "title": "Instantiate a Java float type.",
    "section": "",
    "text": "R/utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat.html#jfloat",
    "href": "packages/sparklyr/latest/reference/jfloat.html#jfloat",
    "title": "Instantiate a Java float type.",
    "section": "jfloat",
    "text": "jfloat"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat.html#description",
    "href": "packages/sparklyr/latest/reference/jfloat.html#description",
    "title": "Instantiate a Java float type.",
    "section": "Description",
    "text": "Description\nInstantiate a java.lang.Float object with the value specified. NOTE: this method is useful when one has to invoke a Java/Scala method requiring a float (instead of double) type for at least one of its parameters."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat.html#usage",
    "href": "packages/sparklyr/latest/reference/jfloat.html#usage",
    "title": "Instantiate a Java float type.",
    "section": "Usage",
    "text": "Usage\njfloat(sc, x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat.html#arguments",
    "href": "packages/sparklyr/latest/reference/jfloat.html#arguments",
    "title": "Instantiate a Java float type.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nx\nA numeric value in R."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat.html#examples",
    "href": "packages/sparklyr/latest/reference/jfloat.html#examples",
    "title": "Instantiate a Java float type.",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"spark://HOST:PORT\") \njflt <- jfloat(sc, 1.23e-8) \n# jflt is now a reference to a java.lang.Float object"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_dim.html",
    "href": "packages/sparklyr/latest/reference/sdf_dim.html",
    "title": "Support for Dimension Operations",
    "section": "",
    "text": "R/sdf_dim.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_dim.html#sdf_dim",
    "href": "packages/sparklyr/latest/reference/sdf_dim.html#sdf_dim",
    "title": "Support for Dimension Operations",
    "section": "sdf_dim",
    "text": "sdf_dim"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_dim.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_dim.html#description",
    "title": "Support for Dimension Operations",
    "section": "Description",
    "text": "Description\nsdf_dim(), sdf_nrow() and sdf_ncol() provide similar functionality to dim(), nrow() and ncol()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_dim.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_dim.html#usage",
    "title": "Support for Dimension Operations",
    "section": "Usage",
    "text": "Usage\nsdf_dim(x) \n\nsdf_nrow(x) \n\nsdf_ncol(x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_dim.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_dim.html#arguments",
    "title": "Support for Dimension Operations",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object (usually a spark_tbl)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "",
    "text": "R/ml_feature_max_abs_scaler.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#ft_max_abs_scaler",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#ft_max_abs_scaler",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "ft_max_abs_scaler",
    "text": "ft_max_abs_scaler"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#description",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#description",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "Description",
    "text": "Description\nRescale each feature individually to range [-1, 1] by dividing through the largest maximum absolute value in each feature. It does not shift/center the data, and thus does not destroy any sparsity."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#usage",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_max_abs_scaler( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  uid = random_string(\"max_abs_scaler_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#arguments",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#details",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#details",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#value",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#value",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#examples",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#examples",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \nfeatures <- c(\"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\") \niris_tbl %>% \n  ft_vector_assembler( \n    input_col = features, \n    output_col = \"features_temp\" \n  ) %>% \n  ft_max_abs_scaler( \n    input_col = \"features_temp\", \n    output_col = \"features\" \n  ) \n#> # Source: spark<?> [?? x 7]\n#>    Sepal_L…¹ Sepal…² Petal…³ Petal…⁴ Species featu…⁵ featu…⁶\n#>        <dbl>   <dbl>   <dbl>   <dbl> <chr>   <list>  <list> \n#>  1       5.1     3.5     1.4     0.2 setosa  <dbl>   <dbl>  \n#>  2       4.9     3       1.4     0.2 setosa  <dbl>   <dbl>  \n#>  3       4.7     3.2     1.3     0.2 setosa  <dbl>   <dbl>  \n#>  4       4.6     3.1     1.5     0.2 setosa  <dbl>   <dbl>  \n#>  5       5       3.6     1.4     0.2 setosa  <dbl>   <dbl>  \n#>  6       5.4     3.9     1.7     0.4 setosa  <dbl>   <dbl>  \n#>  7       4.6     3.4     1.4     0.3 setosa  <dbl>   <dbl>  \n#>  8       5       3.4     1.5     0.2 setosa  <dbl>   <dbl>  \n#>  9       4.4     2.9     1.4     0.2 setosa  <dbl>   <dbl>  \n#> 10       4.9     3.1     1.5     0.1 setosa  <dbl>   <dbl>  \n#> # … with more rows, and abbreviated variable names\n#> #   ¹​Sepal_Length, ²​Sepal_Width, ³​Petal_Length,\n#> #   ⁴​Petal_Width, ⁵​features_temp, ⁶​features"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#see-also",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/src_databases.html",
    "href": "packages/sparklyr/latest/reference/src_databases.html",
    "title": "Show database list",
    "section": "",
    "text": "R/tables_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/src_databases.html#src_databases",
    "href": "packages/sparklyr/latest/reference/src_databases.html#src_databases",
    "title": "Show database list",
    "section": "src_databases",
    "text": "src_databases"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/src_databases.html#description",
    "href": "packages/sparklyr/latest/reference/src_databases.html#description",
    "title": "Show database list",
    "section": "Description",
    "text": "Description\nShow database list"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/src_databases.html#usage",
    "href": "packages/sparklyr/latest/reference/src_databases.html#usage",
    "title": "Show database list",
    "section": "Usage",
    "text": "Usage\nsrc_databases(sc, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/src_databases.html#arguments",
    "href": "packages/sparklyr/latest/reference/src_databases.html#arguments",
    "title": "Show database list",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_feature_importances.html",
    "href": "packages/sparklyr/latest/reference/ml_feature_importances.html",
    "title": "Spark ML - Feature Importance for Tree Models",
    "section": "",
    "text": "R/ml_helpers.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_feature_importances.html#ml_feature_importances",
    "href": "packages/sparklyr/latest/reference/ml_feature_importances.html#ml_feature_importances",
    "title": "Spark ML - Feature Importance for Tree Models",
    "section": "ml_feature_importances",
    "text": "ml_feature_importances"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_feature_importances.html#description",
    "href": "packages/sparklyr/latest/reference/ml_feature_importances.html#description",
    "title": "Spark ML - Feature Importance for Tree Models",
    "section": "Description",
    "text": "Description\nSpark ML - Feature Importance for Tree Models"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_feature_importances.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_feature_importances.html#usage",
    "title": "Spark ML - Feature Importance for Tree Models",
    "section": "Usage",
    "text": "Usage\nml_feature_importances(model, ...) \n\nml_tree_feature_importance(model, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_feature_importances.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_feature_importances.html#arguments",
    "title": "Spark ML - Feature Importance for Tree Models",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nmodel\nA decision tree-based model.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_feature_importances.html#value",
    "href": "packages/sparklyr/latest/reference/ml_feature_importances.html#value",
    "title": "Spark ML - Feature Importance for Tree Models",
    "section": "Value",
    "text": "Value\nFor ml_model, a sorted data frame with feature labels and their relative importance. For ml_prediction_model, a vector of relative importances."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_find.html",
    "href": "packages/sparklyr/latest/reference/spark_install_find.html",
    "title": "Find a given Spark installation by version.",
    "section": "",
    "text": "R/install_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_find.html#spark_install_find",
    "href": "packages/sparklyr/latest/reference/spark_install_find.html#spark_install_find",
    "title": "Find a given Spark installation by version.",
    "section": "spark_install_find",
    "text": "spark_install_find"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_find.html#description",
    "href": "packages/sparklyr/latest/reference/spark_install_find.html#description",
    "title": "Find a given Spark installation by version.",
    "section": "Description",
    "text": "Description\nFind a given Spark installation by version."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_find.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_install_find.html#usage",
    "title": "Find a given Spark installation by version.",
    "section": "Usage",
    "text": "Usage\nspark_install_find( \n  version = NULL, \n  hadoop_version = NULL, \n  installed_only = TRUE, \n  latest = FALSE, \n  hint = FALSE \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_find.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_install_find.html#arguments",
    "title": "Find a given Spark installation by version.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nversion\nVersion of Spark to install. See spark_available_versions for a list of supported versions\n\n\nhadoop_version\nVersion of Hadoop to install. See spark_available_versions for a list of supported versions\n\n\ninstalled_only\nSearch only the locally installed versions?\n\n\nlatest\nCheck for latest version?\n\n\nhint\nOn failure should the installation code be provided?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "",
    "text": "R/ml_regression_generalized_linear_regression.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#ml_generalized_linear_regression",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#ml_generalized_linear_regression",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "ml_generalized_linear_regression",
    "text": "ml_generalized_linear_regression"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#description",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#description",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "Description",
    "text": "Description\nPerform regression using Generalized Linear Model (GLM)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#usage",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "Usage",
    "text": "Usage\nml_generalized_linear_regression( \n  x, \n  formula = NULL, \n  family = \"gaussian\", \n  link = NULL, \n  fit_intercept = TRUE, \n  offset_col = NULL, \n  link_power = NULL, \n  link_prediction_col = NULL, \n  reg_param = 0, \n  max_iter = 25, \n  weight_col = NULL, \n  solver = \"irls\", \n  tol = 1e-06, \n  variance_power = 0, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"generalized_linear_regression_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#arguments",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nfamily\nName of family which is a description of the error distribution to be used in the model. Supported options: “gaussian”, “binomial”, “poisson”, “gamma” and “tweedie”. Default is “gaussian”.\n\n\nlink\nName of link function which provides the relationship between the linear predictor and the mean of the distribution function. See for supported link functions.\n\n\nfit_intercept\nBoolean; should the model be fit with an intercept term?\n\n\noffset_col\nOffset column name. If this is not set, we treat all instance offsets as 0.0. The feature specified as offset has a constant coefficient of 1.0.\n\n\nlink_power\nIndex in the power link function. Only applicable to the Tweedie family. Note that link power 0, 1, -1 or 0.5 corresponds to the Log, Identity, Inverse or Sqrt link, respectively. When not set, this value defaults to 1 - variancePower, which matches the R “statmod” package.\n\n\nlink_prediction_col\nLink prediction (linear predictor) column name. Default is not set, which means we do not output link prediction.\n\n\nreg_param\nRegularization parameter (aka lambda)\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nweight_col\nThe name of the column to use as weights for the model fit.\n\n\nsolver\nSolver algorithm for optimization.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nvariance_power\nPower in the variance function of the Tweedie distribution which provides the relationship between the variance and mean of the distribution. Only applicable to the Tweedie family. (see Tweedie Distribution (Wikipedia)) Supported values: 0 and [1, Inf). Note that variance power 0, 1, or 2 corresponds to the Gaussian, Poisson or Gamma family, respectively.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#details",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#details",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows.\nValid link functions for each family is listed below. The first link function of each family is the default one.\n\ngaussian: “identity”, “log”, “inverse”\nbinomial: “logit”, “probit”, “loglog”\npoisson: “log”, “identity”, “sqrt”\ngamma: “inverse”, “identity”, “log”\ntweedie: power link function specified through link_power. The default link power in the tweedie family is 1 - variance_power."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#value",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#value",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula\nis specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#examples",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\") \nmtcars_tbl <- sdf_copy_to(sc, mtcars, name = \"mtcars_tbl\", overwrite = TRUE) \npartitions <- mtcars_tbl %>% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \nmtcars_training <- partitions$training \nmtcars_test <- partitions$test \n# Specify the grid \nfamily <- c(\"gaussian\", \"gamma\", \"poisson\") \nlink <- c(\"identity\", \"log\") \nfamily_link <- expand.grid(family = family, link = link, stringsAsFactors = FALSE) \nfamily_link <- data.frame(family_link, rmse = 0) \n# Train the models \nfor (i in seq_len(nrow(family_link))) { \n  glm_model <- mtcars_training %>% \n    ml_generalized_linear_regression(mpg ~ ., \n      family = family_link[i, 1], \n      link = family_link[i, 2] \n    ) \n  pred <- ml_predict(glm_model, mtcars_test) \n  family_link[i, 3] <- ml_regression_evaluator(pred, label_col = \"mpg\") \n} \nfamily_link \n#>     family     link     rmse\n#> 1 gaussian identity 2.881163\n#> 2    gamma identity 2.954531\n#> 3  poisson identity 2.942684\n#> 4 gaussian      log 2.613220\n#> 5    gamma      log 2.721447\n#> 6  poisson      log 2.676343"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#see-also",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgamma.html",
    "href": "packages/sparklyr/latest/reference/sdf_rgamma.html",
    "title": "Generate random samples from a Gamma distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgamma.html#sdf_rgamma",
    "href": "packages/sparklyr/latest/reference/sdf_rgamma.html#sdf_rgamma",
    "title": "Generate random samples from a Gamma distribution",
    "section": "sdf_rgamma",
    "text": "sdf_rgamma"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgamma.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rgamma.html#description",
    "title": "Generate random samples from a Gamma distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Gamma distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgamma.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rgamma.html#usage",
    "title": "Generate random samples from a Gamma distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rgamma( \n  sc, \n  n, \n  shape, \n  rate = 1, \n  num_partitions = NULL, \n  seed = NULL, \n  output_col = \"x\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgamma.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rgamma.html#arguments",
    "title": "Generate random samples from a Gamma distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nshape\nShape parameter (greater than 0) for the Gamma distribution.\n\n\nrate\nRate parameter (greater than 0) for the Gamma distribution (scale is 1/rate).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgamma.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rgamma.html#see-also",
    "title": "Generate random samples from a Gamma distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/checkpoint_directory.html",
    "href": "packages/sparklyr/latest/reference/checkpoint_directory.html",
    "title": "Set/Get Spark checkpoint directory",
    "section": "",
    "text": "R/spark_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/checkpoint_directory.html#checkpoint_directory",
    "href": "packages/sparklyr/latest/reference/checkpoint_directory.html#checkpoint_directory",
    "title": "Set/Get Spark checkpoint directory",
    "section": "checkpoint_directory",
    "text": "checkpoint_directory"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/checkpoint_directory.html#description",
    "href": "packages/sparklyr/latest/reference/checkpoint_directory.html#description",
    "title": "Set/Get Spark checkpoint directory",
    "section": "Description",
    "text": "Description\nSet/Get Spark checkpoint directory"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/checkpoint_directory.html#usage",
    "href": "packages/sparklyr/latest/reference/checkpoint_directory.html#usage",
    "title": "Set/Get Spark checkpoint directory",
    "section": "Usage",
    "text": "Usage\nspark_set_checkpoint_dir(sc, dir) \n\nspark_get_checkpoint_dir(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/checkpoint_directory.html#arguments",
    "href": "packages/sparklyr/latest/reference/checkpoint_directory.html#arguments",
    "title": "Set/Get Spark checkpoint directory",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\ndir\ncheckpoint directory, must be HDFS path of running on cluster"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html",
    "title": "Spark ML – Transform, fit, and predict methods (ml_ interface)",
    "section": "",
    "text": "R/ml_transformation_methods.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html#ml-transform-methods",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html#ml-transform-methods",
    "title": "Spark ML – Transform, fit, and predict methods (ml_ interface)",
    "section": "ml-transform-methods",
    "text": "ml-transform-methods"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html#description",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html#description",
    "title": "Spark ML – Transform, fit, and predict methods (ml_ interface)",
    "section": "Description",
    "text": "Description\nMethods for transformation, fit, and prediction. These are mirrors of the corresponding sdf-transform-methods."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html#usage",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html#usage",
    "title": "Spark ML – Transform, fit, and predict methods (ml_ interface)",
    "section": "Usage",
    "text": "Usage\nis_ml_transformer(x) \n\nis_ml_estimator(x) \n\nml_fit(x, dataset, ...) \n\nml_transform(x, dataset, ...) \n\nml_fit_and_transform(x, dataset, ...) \n\nml_predict(x, dataset, ...) \n\n## S3 method for class 'ml_model_classification'\nml_predict(x, dataset, probability_prefix = \"probability_\", ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html#arguments",
    "title": "Spark ML – Transform, fit, and predict methods (ml_ interface)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA ml_estimator, ml_transformer (or a list thereof), or ml_model object.\n\n\ndataset\nA tbl_spark.\n\n\n…\nOptional arguments; currently unused.\n\n\nprobability_prefix\nString used to prepend the class probability output columns."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html#details",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html#details",
    "title": "Spark ML – Transform, fit, and predict methods (ml_ interface)",
    "section": "Details",
    "text": "Details\nThese methods are"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html#value",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html#value",
    "title": "Spark ML – Transform, fit, and predict methods (ml_ interface)",
    "section": "Value",
    "text": "Value\nWhen x is an estimator, ml_fit() returns a transformer whereas ml_fit_and_transform() returns a transformed dataset. When x is a transformer, ml_transform() and ml_predict() return a transformed dataset. When ml_predict() is called on a ml_model object, additional columns (e.g. probabilities in case of classification models) are appended to the transformed output for the user’s convenience."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html",
    "title": "Spark ML - Evaluators",
    "section": "",
    "text": "R/ml_evaluation_prediction.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#ml_evaluator",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#ml_evaluator",
    "title": "Spark ML - Evaluators",
    "section": "ml_evaluator",
    "text": "ml_evaluator"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#description",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#description",
    "title": "Spark ML - Evaluators",
    "section": "Description",
    "text": "Description\nA set of functions to calculate performance metrics for prediction models. Also see the Spark ML Documentation https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.evaluation.package"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#usage",
    "title": "Spark ML - Evaluators",
    "section": "Usage",
    "text": "Usage\nml_binary_classification_evaluator( \n  x, \n  label_col = \"label\", \n  raw_prediction_col = \"rawPrediction\", \n  metric_name = \"areaUnderROC\", \n  uid = random_string(\"binary_classification_evaluator_\"), \n  ... \n) \n\nml_binary_classification_eval( \n  x, \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  metric_name = \"areaUnderROC\" \n) \n\nml_multiclass_classification_evaluator( \n  x, \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  metric_name = \"f1\", \n  uid = random_string(\"multiclass_classification_evaluator_\"), \n  ... \n) \n\nml_classification_eval( \n  x, \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  metric_name = \"f1\" \n) \n\nml_regression_evaluator( \n  x, \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  metric_name = \"rmse\", \n  uid = random_string(\"regression_evaluator_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#arguments",
    "title": "Spark ML - Evaluators",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection object or a tbl_spark containing label and prediction columns. The latter should be the output of sdf_predict.\n\n\nlabel_col\nName of column string specifying which column contains the true labels or values.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nmetric_name\nThe performance metric. See details.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; currently unused.\n\n\nprediction_col\nName of the column that contains the predicted label or value NOT the scored probability. Column should be of type Double."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#details",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#details",
    "title": "Spark ML - Evaluators",
    "section": "Details",
    "text": "Details\nThe following metrics are supported\n\nBinary Classification: areaUnderROC (default) or areaUnderPR (not available in Spark 2.X.)\nMulticlass Classification: f1 (default), precision, recall, weightedPrecision, weightedRecall or accuracy; for Spark 2.X: f1 (default), weightedPrecision, weightedRecall or accuracy.\nRegression: rmse (root mean squared error, default), mse (mean squared error), r2, or mae (mean absolute error.)\n\nml_binary_classification_eval() is an alias for ml_binary_classification_evaluator() for backwards compatibility.\nml_classification_eval() is an alias for ml_multiclass_classification_evaluator() for backwards compatibility."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#value",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#value",
    "title": "Spark ML - Evaluators",
    "section": "Value",
    "text": "Value\nThe calculated performance metric"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#examples",
    "title": "Spark ML - Evaluators",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \nmtcars_tbl <- sdf_copy_to(sc, mtcars, name = \"mtcars_tbl\", overwrite = TRUE) \npartitions <- mtcars_tbl %>% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \nmtcars_training <- partitions$training \nmtcars_test <- partitions$test \n# for multiclass classification \nrf_model <- mtcars_training %>% \n  ml_random_forest(cyl ~ ., type = \"classification\") \npred <- ml_predict(rf_model, mtcars_test) \nml_multiclass_classification_evaluator(pred) \n#> [1] 1\n# for regression \nrf_model <- mtcars_training %>% \n  ml_random_forest(cyl ~ ., type = \"regression\") \npred <- ml_predict(rf_model, mtcars_test) \nml_regression_evaluator(pred, label_col = \"cyl\") \n#> [1] 0.4444097\n# for binary classification \nrf_model <- mtcars_training %>% \n  ml_random_forest(am ~ gear + carb, type = \"classification\") \npred <- ml_predict(rf_model, mtcars_test) \nml_binary_classification_evaluator(pred) \n#> [1] 0.96875"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html",
    "href": "packages/sparklyr/latest/reference/spark-connections.html",
    "title": "Manage Spark Connections",
    "section": "",
    "text": "R/connection_spark.R, R/spark_submit.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html#spark-connections",
    "href": "packages/sparklyr/latest/reference/spark-connections.html#spark-connections",
    "title": "Manage Spark Connections",
    "section": "spark-connections",
    "text": "spark-connections"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html#description",
    "href": "packages/sparklyr/latest/reference/spark-connections.html#description",
    "title": "Manage Spark Connections",
    "section": "Description",
    "text": "Description\nThese routines allow you to manage your connections to Spark.\nCall spark_disconnect() on each open Spark connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html#usage",
    "href": "packages/sparklyr/latest/reference/spark-connections.html#usage",
    "title": "Manage Spark Connections",
    "section": "Usage",
    "text": "Usage\nspark_connect( \n  master, \n  spark_home = Sys.getenv(\"SPARK_HOME\"), \n  method = c(\"shell\", \"livy\", \"databricks\", \"test\", \"qubole\"), \n  app_name = \"sparklyr\", \n  version = NULL, \n  config = spark_config(), \n  extensions = sparklyr::registered_extensions(), \n  packages = NULL, \n  scala_version = NULL, \n  ... \n) \n\nspark_connection_is_open(sc) \n\nspark_disconnect(sc, ...) \n\nspark_disconnect_all(...) \n\nspark_submit( \n  master, \n  file, \n  spark_home = Sys.getenv(\"SPARK_HOME\"), \n  app_name = \"sparklyr\", \n  version = NULL, \n  config = spark_config(), \n  extensions = sparklyr::registered_extensions(), \n  scala_version = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark-connections.html#arguments",
    "title": "Manage Spark Connections",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nmaster\nSpark cluster url to connect to. Use \"local\" to connect to a local instance of Spark installed via spark_install.\n\n\nspark_home\nThe path to a Spark installation. Defaults to the path provided by the SPARK_HOME environment variable. If SPARK_HOME is defined, it will always be used unless the version parameter is specified to force the use of a locally installed version.\n\n\nmethod\nThe method used to connect to Spark. Default connection method is \"shell\" to connect using spark-submit, use \"livy\" to perform remote connections using HTTP, or \"databricks\" when using a Databricks clusters.\n\n\napp_name\nThe application name to be used while running in the Spark cluster.\n\n\nversion\nThe version of Spark to use. Required for \"local\" Spark connections, optional otherwise.\n\n\nconfig\nCustom configuration for the generated Spark connection. See spark_config for details.\n\n\nextensions\nExtension R packages to enable for this connection. By default, all packages enabled through the use of sparklyr::register_extension will be passed here.\n\n\npackages\nA list of Spark packages to load. For example, \"delta\" or \"kafka\" to enable Delta Lake or Kafka. Also supports full versions like \"io.delta:delta-core_2.11:0.4.0\". This is similar to adding packages into the sparklyr.shell.packages configuration option. Notice that the versionparameter is used to choose the correct package, otherwise assumes the latest version is being used.\n\n\nscala_version\nLoad the sparklyr jar file that is built with the version of Scala specified (this currently only makes sense for Spark 2.4, where sparklyr will by default assume Spark 2.4 on current host is built with Scala 2.11, and therefore scala_version = '2.12' is needed if sparklyr is connecting to Spark 2.4 built with Scala 2.12)\n\n\n…\nAdditional params to be passed to each spark_disconnect() call (e.g., terminate = TRUE)\n\n\nsc\nA spark_connection.\n\n\nfile\nPath to R source file to submit for batch execution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html#details",
    "href": "packages/sparklyr/latest/reference/spark-connections.html#details",
    "title": "Manage Spark Connections",
    "section": "Details",
    "text": "Details\nBy default, when using method = \"livy\", jars are downloaded from GitHub. But an alternative path (local to Livy server or on HDFS or HTTP(s)) to sparklyr\nJAR can also be specified through the sparklyr.livy.jar setting."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html#examples",
    "href": "packages/sparklyr/latest/reference/spark-connections.html#examples",
    "title": "Manage Spark Connections",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nconf <- spark_config() \nconf$`sparklyr.shell.conf` <- c( \n  \"spark.executor.extraJavaOptions=-Duser.timezone='UTC'\", \n  \"spark.driver.extraJavaOptions=-Duser.timezone='UTC'\", \n  \"spark.sql.session.timeZone='UTC'\" \n) \nsc <- spark_connect( \n  master = \"spark://HOST:PORT\", config = conf \n) \nconnection_is_open(sc) \nspark_disconnect(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.html",
    "href": "packages/sparklyr/latest/reference/copy_to.html",
    "title": "Copy To",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.html#copy_to",
    "href": "packages/sparklyr/latest/reference/copy_to.html#copy_to",
    "title": "Copy To",
    "section": "copy_to",
    "text": "copy_to"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.html#description",
    "href": "packages/sparklyr/latest/reference/copy_to.html#description",
    "title": "Copy To",
    "section": "Description",
    "text": "Description\nSee copy_to for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rweibull.html",
    "href": "packages/sparklyr/latest/reference/sdf_rweibull.html",
    "title": "Generate random samples from a Weibull distribution.",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rweibull.html#sdf_rweibull",
    "href": "packages/sparklyr/latest/reference/sdf_rweibull.html#sdf_rweibull",
    "title": "Generate random samples from a Weibull distribution.",
    "section": "sdf_rweibull",
    "text": "sdf_rweibull"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rweibull.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rweibull.html#description",
    "title": "Generate random samples from a Weibull distribution.",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Weibull distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rweibull.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rweibull.html#usage",
    "title": "Generate random samples from a Weibull distribution.",
    "section": "Usage",
    "text": "Usage\nsdf_rweibull( \n  sc, \n  n, \n  shape, \n  scale = 1, \n  num_partitions = NULL, \n  seed = NULL, \n  output_col = \"x\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rweibull.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rweibull.html#arguments",
    "title": "Generate random samples from a Weibull distribution.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nshape\nThe shape of the Weibull distribution.\n\n\nscale\nThe scale of the Weibull distribution (default: 1).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rweibull.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rweibull.html#see-also",
    "title": "Generate random samples from a Weibull distribution.",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html",
    "title": "Default stop words",
    "section": "",
    "text": "R/ml_feature_stop_words_remover.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#ml_default_stop_words",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#ml_default_stop_words",
    "title": "Default stop words",
    "section": "ml_default_stop_words",
    "text": "ml_default_stop_words"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#description",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#description",
    "title": "Default stop words",
    "section": "Description",
    "text": "Description\nLoads the default stop words for the given language."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#usage",
    "title": "Default stop words",
    "section": "Usage",
    "text": "Usage\nml_default_stop_words( \n  sc, \n  language = c(\"english\", \"danish\", \"dutch\", \"finnish\", \"french\", \"german\", \"hungarian\", \n    \"italian\", \"norwegian\", \"portuguese\", \"russian\", \"spanish\", \"swedish\", \"turkish\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#arguments",
    "title": "Default stop words",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection\n\n\nlanguage\nA character string.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#details",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#details",
    "title": "Default stop words",
    "section": "Details",
    "text": "Details\nSupported languages: danish, dutch, english, finnish, french, german, hungarian, italian, norwegian, portuguese, russian, spanish, swedish, turkish. Defaults to English. See https://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/\nfor more details"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#value",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#value",
    "title": "Default stop words",
    "section": "Value",
    "text": "Value\nA list of stop words."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#see-also",
    "title": "Default stop words",
    "section": "See Also",
    "text": "See Also\nft_stop_words_remover"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/mutate.html",
    "href": "packages/sparklyr/latest/reference/mutate.html",
    "title": "Mutate",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/mutate.html#mutate",
    "href": "packages/sparklyr/latest/reference/mutate.html#mutate",
    "title": "Mutate",
    "section": "mutate",
    "text": "mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/mutate.html#description",
    "href": "packages/sparklyr/latest/reference/mutate.html#description",
    "title": "Mutate",
    "section": "Description",
    "text": "Description\nSee mutate for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/full_join.html",
    "href": "packages/sparklyr/latest/reference/full_join.html",
    "title": "Full join",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/full_join.html#full_join",
    "href": "packages/sparklyr/latest/reference/full_join.html#full_join",
    "title": "Full join",
    "section": "full_join",
    "text": "full_join"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/full_join.html#description",
    "href": "packages/sparklyr/latest/reference/full_join.html#description",
    "title": "Full join",
    "section": "Description",
    "text": "Description\nSee full_join for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "",
    "text": "R/ml_feature_standard_scaler.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#ft_standard_scaler",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#ft_standard_scaler",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "ft_standard_scaler",
    "text": "ft_standard_scaler"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#description",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#description",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "Description",
    "text": "Description\nStandardizes features by removing the mean and scaling to unit variance using column summary statistics on the samples in the training set. The “unit std” is computed using the corrected sample standard deviation, which is computed as the square root of the unbiased sample variance."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#usage",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_standard_scaler( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  with_mean = FALSE, \n  with_std = TRUE, \n  uid = random_string(\"standard_scaler_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#arguments",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nwith_mean\nWhether to center the data with mean before scaling. It will build a dense output, so take care when applying to sparse input. Default: FALSE\n\n\nwith_std\nWhether to scale the data to unit standard deviation. Default: TRUE\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#details",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#details",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#value",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#value",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#examples",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#examples",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \nfeatures <- c(\"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\") \niris_tbl %>% \n  ft_vector_assembler( \n    input_col = features, \n    output_col = \"features_temp\" \n  ) %>% \n  ft_standard_scaler( \n    input_col = \"features_temp\", \n    output_col = \"features\", \n    with_mean = TRUE \n  ) \n#> # Source: spark<?> [?? x 7]\n#>    Sepal_L…¹ Sepal…² Petal…³ Petal…⁴ Species featu…⁵ featu…⁶\n#>        <dbl>   <dbl>   <dbl>   <dbl> <chr>   <list>  <list> \n#>  1       5.1     3.5     1.4     0.2 setosa  <dbl>   <dbl>  \n#>  2       4.9     3       1.4     0.2 setosa  <dbl>   <dbl>  \n#>  3       4.7     3.2     1.3     0.2 setosa  <dbl>   <dbl>  \n#>  4       4.6     3.1     1.5     0.2 setosa  <dbl>   <dbl>  \n#>  5       5       3.6     1.4     0.2 setosa  <dbl>   <dbl>  \n#>  6       5.4     3.9     1.7     0.4 setosa  <dbl>   <dbl>  \n#>  7       4.6     3.4     1.4     0.3 setosa  <dbl>   <dbl>  \n#>  8       5       3.4     1.5     0.2 setosa  <dbl>   <dbl>  \n#>  9       4.4     2.9     1.4     0.2 setosa  <dbl>   <dbl>  \n#> 10       4.9     3.1     1.5     0.1 setosa  <dbl>   <dbl>  \n#> # … with more rows, and abbreviated variable names\n#> #   ¹​Sepal_Length, ²​Sepal_Width, ³​Petal_Length,\n#> #   ⁴​Petal_Width, ⁵​features_temp, ⁶​features"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#see-also",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-transform-methods.html",
    "href": "packages/sparklyr/latest/reference/sdf-transform-methods.html",
    "title": "Spark ML – Transform, fit, and predict methods (sdf_ interface)",
    "section": "",
    "text": "R/ml_transformation_methods.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-transform-methods.html#sdf-transform-methods",
    "href": "packages/sparklyr/latest/reference/sdf-transform-methods.html#sdf-transform-methods",
    "title": "Spark ML – Transform, fit, and predict methods (sdf_ interface)",
    "section": "sdf-transform-methods",
    "text": "sdf-transform-methods"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-transform-methods.html#description",
    "href": "packages/sparklyr/latest/reference/sdf-transform-methods.html#description",
    "title": "Spark ML – Transform, fit, and predict methods (sdf_ interface)",
    "section": "Description",
    "text": "Description\nDeprecated methods for transformation, fit, and prediction. These are mirrors of the corresponding ml-transform-methods."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-transform-methods.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf-transform-methods.html#usage",
    "title": "Spark ML – Transform, fit, and predict methods (sdf_ interface)",
    "section": "Usage",
    "text": "Usage\nsdf_predict(x, model, ...) \n\nsdf_transform(x, transformer, ...) \n\nsdf_fit(x, estimator, ...) \n\nsdf_fit_and_transform(x, estimator, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-transform-methods.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf-transform-methods.html#arguments",
    "title": "Spark ML – Transform, fit, and predict methods (sdf_ interface)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA tbl_spark.\n\n\nmodel\nA ml_transformer or a ml_model object.\n\n\n…\nOptional arguments passed to the corresponding ml_ methods.\n\n\ntransformer\nA ml_transformer object.\n\n\nestimator\nA ml_estimator object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-transform-methods.html#value",
    "href": "packages/sparklyr/latest/reference/sdf-transform-methods.html#value",
    "title": "Spark ML – Transform, fit, and predict methods (sdf_ interface)",
    "section": "Value",
    "text": "Value\nsdf_predict(), sdf_transform(), and sdf_fit_and_transform() return a transformed dataframe whereas sdf_fit() returns a ml_transformer."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/unite.html",
    "href": "packages/sparklyr/latest/reference/unite.html",
    "title": "Unite",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/unite.html#unite",
    "href": "packages/sparklyr/latest/reference/unite.html#unite",
    "title": "Unite",
    "section": "unite",
    "text": "unite"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/unite.html#description",
    "href": "packages/sparklyr/latest/reference/unite.html#description",
    "title": "Unite",
    "section": "Description",
    "text": "Description\nSee unite for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html",
    "href": "packages/sparklyr/latest/reference/ml_corr.html",
    "title": "Compute correlation matrix",
    "section": "",
    "text": "R/ml_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html#ml_corr",
    "href": "packages/sparklyr/latest/reference/ml_corr.html#ml_corr",
    "title": "Compute correlation matrix",
    "section": "ml_corr",
    "text": "ml_corr"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html#description",
    "href": "packages/sparklyr/latest/reference/ml_corr.html#description",
    "title": "Compute correlation matrix",
    "section": "Description",
    "text": "Description\nCompute correlation matrix"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_corr.html#usage",
    "title": "Compute correlation matrix",
    "section": "Usage",
    "text": "Usage\nml_corr(x, columns = NULL, method = c(\"pearson\", \"spearman\"))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_corr.html#arguments",
    "title": "Compute correlation matrix",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA tbl_spark.\n\n\ncolumns\nThe names of the columns to calculate correlations of. If only one column is specified, it must be a vector column (for example, assembled using ft_vector_assember()).\n\n\nmethod\nThe method to use, either \"pearson\" or \"spearman\"."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html#value",
    "href": "packages/sparklyr/latest/reference/ml_corr.html#value",
    "title": "Compute correlation matrix",
    "section": "Value",
    "text": "Value\nA correlation matrix organized as a data frame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_corr.html#examples",
    "title": "Compute correlation matrix",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \nfeatures <- c(\"Petal_Width\", \"Petal_Length\", \"Sepal_Length\", \"Sepal_Width\") \nml_corr(iris_tbl, columns = features, method = \"pearson\") \n#> New names:\n#> • `` -> `...1`\n#> • `` -> `...2`\n#> • `` -> `...3`\n#> • `` -> `...4`\n#> # A tibble: 4 × 4\n#>   Petal_Width Petal_Length Sepal_Length Sepal_Width\n#>         <dbl>        <dbl>        <dbl>       <dbl>\n#> 1       1            0.963        0.818      -0.366\n#> 2       0.963        1            0.872      -0.428\n#> 3       0.818        0.872        1          -0.118\n#> 4      -0.366       -0.428       -0.118       1"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html",
    "title": "Feature Transformation – IndexToString (Transformer)",
    "section": "",
    "text": "R/ml_feature_index_to_string.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html#ft_index_to_string",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html#ft_index_to_string",
    "title": "Feature Transformation – IndexToString (Transformer)",
    "section": "ft_index_to_string",
    "text": "ft_index_to_string"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html#description",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html#description",
    "title": "Feature Transformation – IndexToString (Transformer)",
    "section": "Description",
    "text": "Description\nA Transformer that maps a column of indices back to a new column of corresponding string values. The index-string mapping is either from the ML attributes of the input column, or from user-supplied labels (which take precedence over ML attributes). This function is the inverse of ft_string_indexer."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html#usage",
    "title": "Feature Transformation – IndexToString (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_index_to_string( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  labels = NULL, \n  uid = random_string(\"index_to_string_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html#arguments",
    "title": "Feature Transformation – IndexToString (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nlabels\nOptional param for array of labels specifying index-string mapping.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html#value",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html#value",
    "title": "Feature Transformation – IndexToString (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html#see-also",
    "title": "Feature Transformation – IndexToString (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nft_string_indexer\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rcauchy.html",
    "href": "packages/sparklyr/latest/reference/sdf_rcauchy.html",
    "title": "Generate random samples from a Cauchy distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rcauchy.html#sdf_rcauchy",
    "href": "packages/sparklyr/latest/reference/sdf_rcauchy.html#sdf_rcauchy",
    "title": "Generate random samples from a Cauchy distribution",
    "section": "sdf_rcauchy",
    "text": "sdf_rcauchy"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rcauchy.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rcauchy.html#description",
    "title": "Generate random samples from a Cauchy distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Cauchy distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rcauchy.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rcauchy.html#usage",
    "title": "Generate random samples from a Cauchy distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rcauchy( \n  sc, \n  n, \n  location = 0, \n  scale = 1, \n  num_partitions = NULL, \n  seed = NULL, \n  output_col = \"x\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rcauchy.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rcauchy.html#arguments",
    "title": "Generate random samples from a Cauchy distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nlocation\nLocation parameter of the distribution.\n\n\nscale\nScale parameter of the distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rcauchy.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rcauchy.html#see-also",
    "title": "Generate random samples from a Cauchy distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_save_table.html",
    "href": "packages/sparklyr/latest/reference/spark_save_table.html",
    "title": "Saves a Spark DataFrame as a Spark table",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_save_table.html#spark_save_table",
    "href": "packages/sparklyr/latest/reference/spark_save_table.html#spark_save_table",
    "title": "Saves a Spark DataFrame as a Spark table",
    "section": "spark_save_table",
    "text": "spark_save_table"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_save_table.html#description",
    "href": "packages/sparklyr/latest/reference/spark_save_table.html#description",
    "title": "Saves a Spark DataFrame as a Spark table",
    "section": "Description",
    "text": "Description\nSaves a Spark DataFrame and as a Spark table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_save_table.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_save_table.html#usage",
    "title": "Saves a Spark DataFrame as a Spark table",
    "section": "Usage",
    "text": "Usage\nspark_save_table(x, path, mode = NULL, options = list())"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_save_table.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_save_table.html#arguments",
    "title": "Saves a Spark DataFrame as a Spark table",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_save_table.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_save_table.html#see-also",
    "title": "Saves a Spark DataFrame as a Spark table",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/transform_sdf.html",
    "href": "packages/sparklyr/latest/reference/transform_sdf.html",
    "title": "transform a subset of column(s) in a Spark Dataframe",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/transform_sdf.html#transform_sdf",
    "href": "packages/sparklyr/latest/reference/transform_sdf.html#transform_sdf",
    "title": "transform a subset of column(s) in a Spark Dataframe",
    "section": "transform_sdf",
    "text": "transform_sdf"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/transform_sdf.html#description",
    "href": "packages/sparklyr/latest/reference/transform_sdf.html#description",
    "title": "transform a subset of column(s) in a Spark Dataframe",
    "section": "Description",
    "text": "Description\ntransform a subset of column(s) in a Spark Dataframe"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/transform_sdf.html#usage",
    "href": "packages/sparklyr/latest/reference/transform_sdf.html#usage",
    "title": "transform a subset of column(s) in a Spark Dataframe",
    "section": "Usage",
    "text": "Usage\ntransform_sdf(x, cols, fn)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/transform_sdf.html#arguments",
    "href": "packages/sparklyr/latest/reference/transform_sdf.html#arguments",
    "title": "transform a subset of column(s) in a Spark Dataframe",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercible to a Spark DataFrame\n\n\ncols\nSubset of columns to apply transformation to\n\n\nfn\nTransformation function taking column name as the 1st parameter, the corresponding org.apache.spark.sql.Column object as the 2nd parameter, and returning a transformed org.apache.spark.sql.Column object"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_values.html",
    "href": "packages/sparklyr/latest/reference/hof_transform_values.html",
    "title": "Transforms values of a map",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_values.html#hof_transform_values",
    "href": "packages/sparklyr/latest/reference/hof_transform_values.html#hof_transform_values",
    "title": "Transforms values of a map",
    "section": "hof_transform_values",
    "text": "hof_transform_values"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_values.html#description",
    "href": "packages/sparklyr/latest/reference/hof_transform_values.html#description",
    "title": "Transforms values of a map",
    "section": "Description",
    "text": "Description\nApplies the transformation function specified to all values of a map (this is essentially a dplyr wrapper to the transform_values(expr, func) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_values.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_transform_values.html#usage",
    "title": "Transforms values of a map",
    "section": "Usage",
    "text": "Usage\nhof_transform_values(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_values.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_transform_values.html#arguments",
    "title": "Transforms values of a map",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\nfunc\nThe transformation function to apply (it should take (key, value) as arguments and return a transformed value)\n\n\nexpr\nThe map being transformed, could be any SQL expression evaluating to a map (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the transformed result (default: expr)\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_values.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_transform_values.html#examples",
    "title": "Transforms values of a map",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\", version = \"3.0.0\") \nsdf <- sdf_len(sc, 1) %>% dplyr::mutate(m = map(\"a\", 0L, \"b\", 2L, \"c\", -1L)) \ntransformed_sdf <- sdf %>% hof_transform_values(~ CONCAT(.x, \" == \", .y))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pipe.html",
    "href": "packages/sparklyr/latest/reference/pipe.html",
    "title": "Pipe operator",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pipe.html#section",
    "href": "packages/sparklyr/latest/reference/pipe.html#section",
    "title": "Pipe operator",
    "section": "%>%",
    "text": "%>%"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pipe.html#description",
    "href": "packages/sparklyr/latest/reference/pipe.html#description",
    "title": "Pipe operator",
    "section": "Description",
    "text": "Description\nSee %>% for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jarray.html",
    "href": "packages/sparklyr/latest/reference/jarray.html",
    "title": "Instantiate a Java array with a specific element type.",
    "section": "",
    "text": "R/utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jarray.html#jarray",
    "href": "packages/sparklyr/latest/reference/jarray.html#jarray",
    "title": "Instantiate a Java array with a specific element type.",
    "section": "jarray",
    "text": "jarray"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jarray.html#description",
    "href": "packages/sparklyr/latest/reference/jarray.html#description",
    "title": "Instantiate a Java array with a specific element type.",
    "section": "Description",
    "text": "Description\nGiven a list of Java object references, instantiate an Array[T]\ncontaining the same list of references, where T is a non-primitive type that is more specific than java.lang.Object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jarray.html#usage",
    "href": "packages/sparklyr/latest/reference/jarray.html#usage",
    "title": "Instantiate a Java array with a specific element type.",
    "section": "Usage",
    "text": "Usage\njarray(sc, x, element_type)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jarray.html#arguments",
    "href": "packages/sparklyr/latest/reference/jarray.html#arguments",
    "title": "Instantiate a Java array with a specific element type.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nx\nA list of Java object references.\n\n\nelement_type\nA valid Java class name representing the generic type parameter of the Java array to be instantiated. Each element of xmust refer to a Java object that is assignable to element_type."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jarray.html#examples",
    "href": "packages/sparklyr/latest/reference/jarray.html#examples",
    "title": "Instantiate a Java array with a specific element type.",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"spark://HOST:PORT\") \nstring_arr <- jarray(sc, letters, element_type = \"java.lang.String\") \n# string_arr is now a reference to an array of type String[]"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_delta.html",
    "href": "packages/sparklyr/latest/reference/spark_read_delta.html",
    "title": "Read from Delta Lake into a Spark DataFrame.",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_delta.html#spark_read_delta",
    "href": "packages/sparklyr/latest/reference/spark_read_delta.html#spark_read_delta",
    "title": "Read from Delta Lake into a Spark DataFrame.",
    "section": "spark_read_delta",
    "text": "spark_read_delta"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_delta.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_delta.html#description",
    "title": "Read from Delta Lake into a Spark DataFrame.",
    "section": "Description",
    "text": "Description\nRead from Delta Lake into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_delta.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_delta.html#usage",
    "title": "Read from Delta Lake into a Spark DataFrame.",
    "section": "Usage",
    "text": "Usage\nspark_read_delta( \n  sc, \n  path, \n  name = NULL, \n  version = NULL, \n  timestamp = NULL, \n  options = list(), \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_delta.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_delta.html#arguments",
    "title": "Read from Delta Lake into a Spark DataFrame.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nname\nThe name to assign to the newly generated table.\n\n\nversion\nThe version of the delta table to read.\n\n\ntimestamp\nThe timestamp of the delta table to read. For example, \"2019-01-01\" or \"2019-01-01'T'00:00:00.000Z\".\n\n\noptions\nA list of strings with additional options.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_delta.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_delta.html#see-also",
    "title": "Read from Delta Lake into a Spark DataFrame.",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html",
    "title": "Feature Transformation – Tokenizer (Transformer)",
    "section": "",
    "text": "R/ml_feature_tokenizer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html#ft_tokenizer",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html#ft_tokenizer",
    "title": "Feature Transformation – Tokenizer (Transformer)",
    "section": "ft_tokenizer",
    "text": "ft_tokenizer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html#description",
    "title": "Feature Transformation – Tokenizer (Transformer)",
    "section": "Description",
    "text": "Description\nA tokenizer that converts the input string to lowercase and then splits it by white spaces."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html#usage",
    "title": "Feature Transformation – Tokenizer (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_tokenizer( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  uid = random_string(\"tokenizer_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html#arguments",
    "title": "Feature Transformation – Tokenizer (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html#value",
    "title": "Feature Transformation – Tokenizer (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html#see-also",
    "title": "Feature Transformation – Tokenizer (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_unique_id.html",
    "href": "packages/sparklyr/latest/reference/sdf_with_unique_id.html",
    "title": "Add a Unique ID Column to a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#sdf_with_unique_id",
    "href": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#sdf_with_unique_id",
    "title": "Add a Unique ID Column to a Spark DataFrame",
    "section": "sdf_with_unique_id",
    "text": "sdf_with_unique_id"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#description",
    "title": "Add a Unique ID Column to a Spark DataFrame",
    "section": "Description",
    "text": "Description\nAdd a unique ID column to a Spark DataFrame. The Spark monotonicallyIncreasingId function is used to produce these and is guaranteed to produce unique, monotonically increasing ids; however, there is no guarantee that these IDs will be sequential. The table is persisted immediately after the column is generated, to ensure that the column is stable – otherwise, it can differ across new computations."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#usage",
    "title": "Add a Unique ID Column to a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_with_unique_id(x, id = \"id\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#arguments",
    "title": "Add a Unique ID Column to a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nid\nThe name of the column to host the generated IDs."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html",
    "title": "Read Text Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html#stream_read_text",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html#stream_read_text",
    "title": "Read Text Stream",
    "section": "stream_read_text",
    "text": "stream_read_text"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html#description",
    "title": "Read Text Stream",
    "section": "Description",
    "text": "Description\nReads a text stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html#usage",
    "title": "Read Text Stream",
    "section": "Usage",
    "text": "Usage\nstream_read_text(sc, path, name = NULL, options = list(), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html#arguments",
    "title": "Read Text Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html#examples",
    "title": "Read Text Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \ndir.create(\"text-in\") \nwriteLines(\"A text entry\", \"text-in/text.txt\") \ntext_path <- file.path(\"file://\", getwd(), \"text-in\") \nstream <- stream_read_text(sc, text_path) %>% stream_write_text(\"text-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html#see-also",
    "title": "Read Text Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html",
    "title": "Write CSV Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html#stream_write_csv",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html#stream_write_csv",
    "title": "Write CSV Stream",
    "section": "stream_write_csv",
    "text": "stream_write_csv"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html#description",
    "title": "Write CSV Stream",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into a tabular (typically, comma-separated) stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html#usage",
    "title": "Write CSV Stream",
    "section": "Usage",
    "text": "Usage\nstream_write_csv( \n  x, \n  path, \n  mode = c(\"append\", \"complete\", \"update\"), \n  trigger = stream_trigger_interval(), \n  checkpoint = file.path(path, \"checkpoint\"), \n  header = TRUE, \n  delimiter = \",\", \n  quote = \"\\\"\", \n  escape = \"\\\\\", \n  charset = \"UTF-8\", \n  null_value = NULL, \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html#arguments",
    "title": "Write CSV Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\".\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous.\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\nheader\nShould the first row of data be used as a header? Defaults to TRUE.\n\n\ndelimiter\nThe character used to delimit each column, defaults to ,.\n\n\nquote\nThe character used as a quote. Defaults to '\"'.\n\n\nescape\nThe character used to escape other characters, defaults to \\.\n\n\ncharset\nThe character set, defaults to \"UTF-8\".\n\n\nnull_value\nThe character to use for default values, defaults to NULL.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html#examples",
    "title": "Write CSV Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \ndir.create(\"csv-in\") \nwrite.csv(iris, \"csv-in/data.csv\", row.names = FALSE) \ncsv_path <- file.path(\"file://\", getwd(), \"csv-in\") \nstream <- stream_read_csv(sc, csv_path) %>% stream_write_csv(\"csv-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html#see-also",
    "title": "Write CSV Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html",
    "title": "Feature Transformation – RegexTokenizer (Transformer)",
    "section": "",
    "text": "R/ml_feature_regex_tokenizer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#ft_regex_tokenizer",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#ft_regex_tokenizer",
    "title": "Feature Transformation – RegexTokenizer (Transformer)",
    "section": "ft_regex_tokenizer",
    "text": "ft_regex_tokenizer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#description",
    "title": "Feature Transformation – RegexTokenizer (Transformer)",
    "section": "Description",
    "text": "Description\nA regex based tokenizer that extracts tokens either by using the provided regex pattern to split the text (default) or repeatedly matching the regex (if gaps is false). Optional parameters also allow filtering tokens using a minimal length. It returns an array of strings that can be empty."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#usage",
    "title": "Feature Transformation – RegexTokenizer (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_regex_tokenizer( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  gaps = TRUE, \n  min_token_length = 1, \n  pattern = \"\\\\s+\", \n  to_lower_case = TRUE, \n  uid = random_string(\"regex_tokenizer_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#arguments",
    "title": "Feature Transformation – RegexTokenizer (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\ngaps\nIndicates whether regex splits on gaps (TRUE) or matches tokens (FALSE).\n\n\nmin_token_length\nMinimum token length, greater than or equal to 0.\n\n\npattern\nThe regular expression pattern to be used.\n\n\nto_lower_case\nIndicates whether to convert all characters to lowercase before tokenizing.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#value",
    "title": "Feature Transformation – RegexTokenizer (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#see-also",
    "title": "Feature Transformation – RegexTokenizer (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html",
    "title": "Spark ML – Logistic Regression",
    "section": "",
    "text": "R/ml_classification_logistic_regression.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#ml_logistic_regression",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#ml_logistic_regression",
    "title": "Spark ML – Logistic Regression",
    "section": "ml_logistic_regression",
    "text": "ml_logistic_regression"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#description",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#description",
    "title": "Spark ML – Logistic Regression",
    "section": "Description",
    "text": "Description\nPerform classification using logistic regression."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#usage",
    "title": "Spark ML – Logistic Regression",
    "section": "Usage",
    "text": "Usage\nml_logistic_regression( \n  x, \n  formula = NULL, \n  fit_intercept = TRUE, \n  elastic_net_param = 0, \n  reg_param = 0, \n  max_iter = 100, \n  threshold = 0.5, \n  thresholds = NULL, \n  tol = 1e-06, \n  weight_col = NULL, \n  aggregation_depth = 2, \n  lower_bounds_on_coefficients = NULL, \n  lower_bounds_on_intercepts = NULL, \n  upper_bounds_on_coefficients = NULL, \n  upper_bounds_on_intercepts = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  family = \"auto\", \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  uid = random_string(\"logistic_regression_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#arguments",
    "title": "Spark ML – Logistic Regression",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nfit_intercept\nBoolean; should the model be fit with an intercept term?\n\n\nelastic_net_param\nElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n\n\nreg_param\nRegularization parameter (aka lambda)\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nthreshold\nin binary classification prediction, in range [0, 1].\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nweight_col\nThe name of the column to use as weights for the model fit.\n\n\naggregation_depth\n(Spark 2.1.0+) Suggested depth for treeAggregate (>= 2).\n\n\nlower_bounds_on_coefficients\n(Spark 2.2.0+) Lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression.\n\n\nlower_bounds_on_intercepts\n(Spark 2.2.0+) Lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression.\n\n\nupper_bounds_on_coefficients\n(Spark 2.2.0+) Upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression.\n\n\nupper_bounds_on_intercepts\n(Spark 2.2.0+) Upper bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nfamily\n(Spark 2.1.0+) Param for the name of family which is a description of the label distribution to be used in the model. Supported options: “auto”, “binomial”, and “multinomial.”\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#details",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#details",
    "title": "Spark ML – Logistic Regression",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#value",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#value",
    "title": "Spark ML – Logistic Regression",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula\nis specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#examples",
    "title": "Spark ML – Logistic Regression",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \nmtcars_tbl <- sdf_copy_to(sc, mtcars, name = \"mtcars_tbl\", overwrite = TRUE) \npartitions <- mtcars_tbl %>% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \nmtcars_training <- partitions$training \nmtcars_test <- partitions$test \nlr_model <- mtcars_training %>% \n  ml_logistic_regression(am ~ gear + carb) \npred <- ml_predict(lr_model, mtcars_test) \nml_binary_classification_evaluator(pred) \n#> [1] 0.96875"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#see-also",
    "title": "Spark ML – Logistic Regression",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_aggregate.html",
    "href": "packages/sparklyr/latest/reference/hof_aggregate.html",
    "title": "Apply Aggregate Function to Array Column",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_aggregate.html#hof_aggregate",
    "href": "packages/sparklyr/latest/reference/hof_aggregate.html#hof_aggregate",
    "title": "Apply Aggregate Function to Array Column",
    "section": "hof_aggregate",
    "text": "hof_aggregate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_aggregate.html#description",
    "href": "packages/sparklyr/latest/reference/hof_aggregate.html#description",
    "title": "Apply Aggregate Function to Array Column",
    "section": "Description",
    "text": "Description\nApply an element-wise aggregation function to an array column (this is essentially a dplyr wrapper for the aggregate(array<T>, A, function<A, T, A>[, function<A, R>]): R\nbuilt-in Spark SQL functions)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_aggregate.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_aggregate.html#usage",
    "title": "Apply Aggregate Function to Array Column",
    "section": "Usage",
    "text": "Usage\nhof_aggregate( \n  x, \n  start, \n  merge, \n  finish = NULL, \n  expr = NULL, \n  dest_col = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_aggregate.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_aggregate.html#arguments",
    "title": "Apply Aggregate Function to Array Column",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark data frame to run aggregation on\n\n\nstart\nThe starting value of the aggregation\n\n\nmerge\nThe aggregation function\n\n\nfinish\nOptional param specifying a transformation to apply on the final value of the aggregation\n\n\nexpr\nThe array being aggregated, could be any SQL expression evaluating to an array (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the aggregated result (default: expr)\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_aggregate.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_aggregate.html#examples",
    "title": "Apply Aggregate Function to Array Column",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\") \n# concatenates all numbers of each array in `array_column` and add parentheses \n# around the resulting string \ncopy_to(sc, tibble::tibble(array_column = list(1:5, 21:25))) %>% \n  hof_aggregate( \n    start = \"\", \n    merge = ~ CONCAT(.y, .x), \n    finish = ~ CONCAT(\"(\", .x, \")\") \n  ) \n#> # Source: spark<?> [?? x 1]\n#>   array_column\n#>   <chr>       \n#> 1 (54321)     \n#> 2 (2524232221)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_zip_with.html",
    "href": "packages/sparklyr/latest/reference/hof_zip_with.html",
    "title": "Combines 2 Array Columns",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_zip_with.html#hof_zip_with",
    "href": "packages/sparklyr/latest/reference/hof_zip_with.html#hof_zip_with",
    "title": "Combines 2 Array Columns",
    "section": "hof_zip_with",
    "text": "hof_zip_with"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_zip_with.html#description",
    "href": "packages/sparklyr/latest/reference/hof_zip_with.html#description",
    "title": "Combines 2 Array Columns",
    "section": "Description",
    "text": "Description\nApplies an element-wise function to combine elements from 2 array columns (this is essentially a dplyr wrapper for the zip_with(array<T>, array<U>, function<T, U, R>): array<R>\nbuilt-in function in Spark SQL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_zip_with.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_zip_with.html#usage",
    "title": "Combines 2 Array Columns",
    "section": "Usage",
    "text": "Usage\nhof_zip_with(x, func, dest_col = NULL, left = NULL, right = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_zip_with.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_zip_with.html#arguments",
    "title": "Combines 2 Array Columns",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark data frame to process\n\n\nfunc\nElement-wise combining function to be applied\n\n\ndest_col\nColumn to store the query result (default: the last column of the Spark data frame)\n\n\nleft\nAny expression evaluating to an array (default: the first column of the Spark data frame)\n\n\nright\nAny expression evaluating to an array (default: the second column of the Spark data frame)\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_zip_with.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_zip_with.html#examples",
    "title": "Combines 2 Array Columns",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\") \n# compute element-wise products of 2 arrays from each row of `left` and `right` \n# and store the resuling array in `res` \ncopy_to( \n  sc, \n  tibble::tibble( \n    left = list(1:5, 21:25), \n    right = list(6:10, 16:20), \n    res = c(0, 0) \n  ) \n) %>% \n  hof_zip_with(~ .x * .y) \n#> # Source: spark<?> [?? x 3]\n#>   left      right     res      \n#>   <list>    <list>    <list>   \n#> 1 <dbl [5]> <dbl [5]> <dbl [5]>\n#> 2 <dbl [5]> <dbl [5]> <dbl [5]>"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html",
    "title": "Feature Transformation – PolynomialExpansion (Transformer)",
    "section": "",
    "text": "R/ml_feature_polynomial_expansion.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#ft_polynomial_expansion",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#ft_polynomial_expansion",
    "title": "Feature Transformation – PolynomialExpansion (Transformer)",
    "section": "ft_polynomial_expansion",
    "text": "ft_polynomial_expansion"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#description",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#description",
    "title": "Feature Transformation – PolynomialExpansion (Transformer)",
    "section": "Description",
    "text": "Description\nPerform feature expansion in a polynomial space. E.g. take a 2-variable feature vector as an example: (x, y), if we want to expand it with degree 2, then we get (x, x * x, y, x * y, y * y)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#usage",
    "title": "Feature Transformation – PolynomialExpansion (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_polynomial_expansion( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  degree = 2, \n  uid = random_string(\"polynomial_expansion_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#arguments",
    "title": "Feature Transformation – PolynomialExpansion (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\ndegree\nThe polynomial degree to expand, which should be greater than equal to 1. A value of 1 means no expansion. Default: 2\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#value",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#value",
    "title": "Feature Transformation – PolynomialExpansion (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#see-also",
    "title": "Feature Transformation – PolynomialExpansion (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html",
    "href": "packages/sparklyr/latest/reference/ft_dct.html",
    "title": "Feature Transformation – Discrete Cosine Transform (DCT) (Transformer)",
    "section": "",
    "text": "R/ml_feature_dct.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#ft_dct",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#ft_dct",
    "title": "Feature Transformation – Discrete Cosine Transform (DCT) (Transformer)",
    "section": "ft_dct",
    "text": "ft_dct"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#description",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#description",
    "title": "Feature Transformation – Discrete Cosine Transform (DCT) (Transformer)",
    "section": "Description",
    "text": "Description\nA feature transformer that takes the 1D discrete cosine transform of a real vector. No zero padding is performed on the input vector. It returns a real vector of the same length representing the DCT. The return vector is scaled such that the transform matrix is unitary (aka scaled DCT-II)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#usage",
    "title": "Feature Transformation – Discrete Cosine Transform (DCT) (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_dct( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  inverse = FALSE, \n  uid = random_string(\"dct_\"), \n  ... \n) \n\nft_discrete_cosine_transform( \n  x, \n  input_col, \n  output_col, \n  inverse = FALSE, \n  uid = random_string(\"dct_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#arguments",
    "title": "Feature Transformation – Discrete Cosine Transform (DCT) (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\ninverse\nIndicates whether to perform the inverse DCT (TRUE) or forward DCT (FALSE).\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#details",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#details",
    "title": "Feature Transformation – Discrete Cosine Transform (DCT) (Transformer)",
    "section": "Details",
    "text": "Details\nft_discrete_cosine_transform() is an alias for ft_dct for backwards compatibility."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#value",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#value",
    "title": "Feature Transformation – Discrete Cosine Transform (DCT) (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#see-also",
    "title": "Feature Transformation – Discrete Cosine Transform (DCT) (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/find_scalac.html",
    "href": "packages/sparklyr/latest/reference/find_scalac.html",
    "title": "Discover the Scala Compiler",
    "section": "",
    "text": "R/spark_compile.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/find_scalac.html#find_scalac",
    "href": "packages/sparklyr/latest/reference/find_scalac.html#find_scalac",
    "title": "Discover the Scala Compiler",
    "section": "find_scalac",
    "text": "find_scalac"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/find_scalac.html#description",
    "href": "packages/sparklyr/latest/reference/find_scalac.html#description",
    "title": "Discover the Scala Compiler",
    "section": "Description",
    "text": "Description\nFind the scalac compiler for a particular version of scala, by scanning some common directories containing scala installations."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/find_scalac.html#usage",
    "href": "packages/sparklyr/latest/reference/find_scalac.html#usage",
    "title": "Discover the Scala Compiler",
    "section": "Usage",
    "text": "Usage\nfind_scalac(version, locations = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/find_scalac.html#arguments",
    "href": "packages/sparklyr/latest/reference/find_scalac.html#arguments",
    "title": "Discover the Scala Compiler",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nversion\nThe scala version to search for. Versions of the form major.minor will be matched against the scalac installation with version major.minor.patch; if multiple compilers are discovered the most recent one will be used.\n\n\nlocations\nAdditional locations to scan. By default, the directories /opt/scala and /usr/local/scala will be scanned."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/select.html",
    "href": "packages/sparklyr/latest/reference/select.html",
    "title": "Select",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/select.html#select",
    "href": "packages/sparklyr/latest/reference/select.html#select",
    "title": "Select",
    "section": "select",
    "text": "select"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/select.html#description",
    "href": "packages/sparklyr/latest/reference/select.html#description",
    "title": "Select",
    "section": "Description",
    "text": "Description\nSee select for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html",
    "title": "Feature Transformation – QuantileDiscretizer (Estimator)",
    "section": "",
    "text": "R/ml_feature_quantile_discretizer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#ft_quantile_discretizer",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#ft_quantile_discretizer",
    "title": "Feature Transformation – QuantileDiscretizer (Estimator)",
    "section": "ft_quantile_discretizer",
    "text": "ft_quantile_discretizer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#description",
    "title": "Feature Transformation – QuantileDiscretizer (Estimator)",
    "section": "Description",
    "text": "Description\nft_quantile_discretizer takes a column with continuous features and outputs a column with binned categorical features. The number of bins can be set using the num_buckets parameter. It is possible that the number of buckets used will be smaller than this value, for example, if there are too few distinct values of the input to create enough distinct quantiles."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#usage",
    "title": "Feature Transformation – QuantileDiscretizer (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_quantile_discretizer( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  num_buckets = 2, \n  input_cols = NULL, \n  output_cols = NULL, \n  num_buckets_array = NULL, \n  handle_invalid = \"error\", \n  relative_error = 0.001, \n  uid = random_string(\"quantile_discretizer_\"), \n  weight_column = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#arguments",
    "title": "Feature Transformation – QuantileDiscretizer (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nnum_buckets\nNumber of buckets (quantiles, or categories) into which data points are grouped. Must be greater than or equal to 2.\n\n\ninput_cols\nNames of input columns.\n\n\noutput_cols\nNames of output columns.\n\n\nnum_buckets_array\nArray of number of buckets (quantiles, or categories) into which data points are grouped. Each value must be greater than or equal to 2.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error”\n\n\nrelative_error\n(Spark 2.0.0+) Relative error (see documentation for org.apache.spark.sql.DataFrameStatFunctions.approxQuantile herefor description). Must be in the range [0, 1]. default: 0.001\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\nweight_column\nIf not NULL, then a generalized version of the Greenwald-Khanna algorithm will be run to compute weighted percentiles, with each input having a relative weight specified by the corresponding value in weight_column. The weights can be considered as relative frequencies of sample inputs.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#details",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#details",
    "title": "Feature Transformation – QuantileDiscretizer (Estimator)",
    "section": "Details",
    "text": "Details\nNaN handling: null and NaN values will be ignored from the column during QuantileDiscretizer fitting. This will produce a Bucketizer\nmodel for making predictions. During the transformation, Bucketizer\nwill raise an error when it finds NaN values in the dataset, but the user can also choose to either keep or remove NaN values within the dataset by setting handle_invalid If the user chooses to keep NaN values, they will be handled specially and placed into their own bucket, for example, if 4 buckets are used, then non-NaN data will be put into buckets[0-3], but NaNs will be counted in a special bucket[4].\nAlgorithm: The bin ranges are chosen using an approximate algorithm (see the documentation for org.apache.spark.sql.DataFrameStatFunctions.approxQuantile here for a detailed description). The precision of the approximation can be controlled with the relative_error parameter. The lower and upper bin bounds will be -Infinity and +Infinity, covering all real values.\nNote that the result may be different every time you run it, since the sample strategy behind it is non-deterministic.\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#value",
    "title": "Feature Transformation – QuantileDiscretizer (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#see-also",
    "title": "Feature Transformation – QuantileDiscretizer (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nft_bucketizer\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html",
    "title": "Tidying methods for Spark ML linear svc",
    "section": "",
    "text": "R/tidiers_ml_svc_models.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#ml_linear_svc_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#ml_linear_svc_tidiers",
    "title": "Tidying methods for Spark ML linear svc",
    "section": "ml_linear_svc_tidiers",
    "text": "ml_linear_svc_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#description",
    "title": "Tidying methods for Spark ML linear svc",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#usage",
    "title": "Tidying methods for Spark ML linear svc",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_linear_svc'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_linear_svc'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_linear_svc'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML linear svc",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_pivot.html",
    "href": "packages/sparklyr/latest/reference/sdf_pivot.html",
    "title": "Pivot a Spark DataFrame",
    "section": "",
    "text": "R/sdf_wrapper.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_pivot.html#sdf_pivot",
    "href": "packages/sparklyr/latest/reference/sdf_pivot.html#sdf_pivot",
    "title": "Pivot a Spark DataFrame",
    "section": "sdf_pivot",
    "text": "sdf_pivot"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_pivot.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_pivot.html#description",
    "title": "Pivot a Spark DataFrame",
    "section": "Description",
    "text": "Description\nConstruct a pivot table over a Spark Dataframe, using a syntax similar to that from reshape2::dcast."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_pivot.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_pivot.html#usage",
    "title": "Pivot a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_pivot(x, formula, fun.aggregate = \"count\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_pivot.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_pivot.html#arguments",
    "title": "Pivot a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nA two-sided R formula of the form x_1 + x_2 + ... ~ y_1. The left-hand side of the formula indicates which variables are used for grouping, and the right-hand side indicates which variable is used for pivoting. Currently, only a single pivot column is supported.\n\n\nfun.aggregate\nHow should the grouped dataset be aggregated? Can be a length-one character vector, giving the name of a Spark aggregation function to be called; a named R list mapping column names to an aggregation method, or an R function that is invoked on the grouped dataset."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_pivot.html#examples",
    "href": "packages/sparklyr/latest/reference/sdf_pivot.html#examples",
    "title": "Pivot a Spark DataFrame",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nlibrary(dplyr) \nsc <- spark_connect(master = \"local\") \niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n# aggregating by mean \niris_tbl %>% \n  mutate(Petal_Width = ifelse(Petal_Width > 1.5, \"High\", \"Low\")) %>% \n  sdf_pivot(Petal_Width ~ Species, \n    fun.aggregate = list(Petal_Length = \"mean\") \n  ) \n#> # Source: spark<?> [?? x 4]\n#>   Petal_Width setosa versicolor virginica\n#>   <chr>        <dbl>      <dbl>     <dbl>\n#> 1 Low           1.46       4.20      5.23\n#> 2 High         NA          4.82      5.57\n# aggregating all observations in a list \niris_tbl %>% \n  mutate(Petal_Width = ifelse(Petal_Width > 1.5, \"High\", \"Low\")) %>% \n  sdf_pivot(Petal_Width ~ Species, \n    fun.aggregate = list(Petal_Length = \"collect_list\") \n  ) \n#> # Source: spark<?> [?? x 4]\n#>   Petal_Width setosa     versicolor virginica \n#>   <chr>       <list>     <list>     <list>    \n#> 1 Low         <dbl [50]> <dbl [45]> <dbl [3]> \n#> 2 High        <dbl [0]>  <dbl [5]>  <dbl [47]>"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html",
    "title": "Set the SPARK_HOME environment variable",
    "section": "",
    "text": "R/spark_home.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html#spark_home_set",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html#spark_home_set",
    "title": "Set the SPARK_HOME environment variable",
    "section": "spark_home_set",
    "text": "spark_home_set"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html#description",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html#description",
    "title": "Set the SPARK_HOME environment variable",
    "section": "Description",
    "text": "Description\nSet the SPARK_HOME environment variable. This slightly speeds up some operations, including the connection time."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html#usage",
    "title": "Set the SPARK_HOME environment variable",
    "section": "Usage",
    "text": "Usage\nspark_home_set(path = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html#arguments",
    "title": "Set the SPARK_HOME environment variable",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\npath\nA string containing the path to the installation location of Spark. If NULL, the path to the most latest Spark/Hadoop versions is used.\n\n\n…\nAdditional parameters not currently used."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html#value",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html#value",
    "title": "Set the SPARK_HOME environment variable",
    "section": "Value",
    "text": "Value\nThe function is mostly invoked for the side-effect of setting the SPARK_HOME environment variable. It also returns TRUE if the environment was successfully set, and FALSE otherwise."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html#examples",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html#examples",
    "title": "Set the SPARK_HOME environment variable",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n# Not run due to side-effects \nspark_home_set() \n#> Setting SPARK_HOME environment variable to /Users/edgar/spark/spark-3.0.0-bin-hadoop3.2"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_get_java.html",
    "href": "packages/sparklyr/latest/reference/spark_get_java.html",
    "title": "Find path to Java",
    "section": "",
    "text": "R/java.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_get_java.html#spark_get_java",
    "href": "packages/sparklyr/latest/reference/spark_get_java.html#spark_get_java",
    "title": "Find path to Java",
    "section": "spark_get_java",
    "text": "spark_get_java"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_get_java.html#description",
    "href": "packages/sparklyr/latest/reference/spark_get_java.html#description",
    "title": "Find path to Java",
    "section": "Description",
    "text": "Description\nFinds the path to JAVA_HOME."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_get_java.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_get_java.html#usage",
    "title": "Find path to Java",
    "section": "Usage",
    "text": "Usage\nspark_get_java(throws = FALSE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_get_java.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_get_java.html#arguments",
    "title": "Find path to Java",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nthrows\nThrow an error when path not found?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_collect.html",
    "href": "packages/sparklyr/latest/reference/sdf_collect.html",
    "title": "Collect a Spark DataFrame into R.",
    "section": "",
    "text": "R/sdf_wrapper.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_collect.html#sdf_collect",
    "href": "packages/sparklyr/latest/reference/sdf_collect.html#sdf_collect",
    "title": "Collect a Spark DataFrame into R.",
    "section": "sdf_collect",
    "text": "sdf_collect"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_collect.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_collect.html#description",
    "title": "Collect a Spark DataFrame into R.",
    "section": "Description",
    "text": "Description\nCollects a Spark dataframe into R."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_collect.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_collect.html#usage",
    "title": "Collect a Spark DataFrame into R.",
    "section": "Usage",
    "text": "Usage\nsdf_collect(object, impl = c(\"row-wise\", \"row-wise-iter\", \"column-wise\"), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_collect.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_collect.html#arguments",
    "title": "Collect a Spark DataFrame into R.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nobject\nSpark dataframe to collect\n\n\nimpl\nWhich implementation to use while collecting Spark dataframe - row-wise: fetch the entire dataframe into memory and then process it row-by-row - row-wise-iter: iterate through the dataframe using RDD local iterator, processing one row at a time (hence reducing memory footprint) - column-wise: fetch the entire dataframe into memory and then process it column-by-column NOTE: (1) this will not apply to streaming or arrow use cases (2) this parameter will only affect implementation detail, and will not affect result of sdf_collect, and should only be set if performance profiling indicates any particular choice will be significantly better than the default choice (“row-wise”)\n\n\n…\nAdditional options."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rhyper.html",
    "href": "packages/sparklyr/latest/reference/sdf_rhyper.html",
    "title": "Generate random samples from a hypergeometric distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rhyper.html#sdf_rhyper",
    "href": "packages/sparklyr/latest/reference/sdf_rhyper.html#sdf_rhyper",
    "title": "Generate random samples from a hypergeometric distribution",
    "section": "sdf_rhyper",
    "text": "sdf_rhyper"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rhyper.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rhyper.html#description",
    "title": "Generate random samples from a hypergeometric distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a hypergeometric distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rhyper.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rhyper.html#usage",
    "title": "Generate random samples from a hypergeometric distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rhyper( \n  sc, \n  nn, \n  m, \n  n, \n  k, \n  num_partitions = NULL, \n  seed = NULL, \n  output_col = \"x\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rhyper.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rhyper.html#arguments",
    "title": "Generate random samples from a hypergeometric distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nnn\nSample Size.\n\n\nm\nThe number of successes among the population.\n\n\nn\nThe number of failures among the population.\n\n\nk\nThe number of draws.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rhyper.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rhyper.html#see-also",
    "title": "Generate random samples from a hypergeometric distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_libsvm.html",
    "href": "packages/sparklyr/latest/reference/spark_read_libsvm.html",
    "title": "Read libsvm file into a Spark DataFrame.",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_libsvm.html#spark_read_libsvm",
    "href": "packages/sparklyr/latest/reference/spark_read_libsvm.html#spark_read_libsvm",
    "title": "Read libsvm file into a Spark DataFrame.",
    "section": "spark_read_libsvm",
    "text": "spark_read_libsvm"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_libsvm.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_libsvm.html#description",
    "title": "Read libsvm file into a Spark DataFrame.",
    "section": "Description",
    "text": "Description\nRead libsvm file into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_libsvm.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_libsvm.html#usage",
    "title": "Read libsvm file into a Spark DataFrame.",
    "section": "Usage",
    "text": "Usage\nspark_read_libsvm( \n  sc, \n  name = NULL, \n  path = name, \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE, \n  options = list(), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_libsvm.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_libsvm.html#arguments",
    "title": "Read libsvm file into a Spark DataFrame.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_libsvm.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_libsvm.html#see-also",
    "title": "Read libsvm file into a Spark DataFrame.",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_cache.html",
    "href": "packages/sparklyr/latest/reference/tbl_cache.html",
    "title": "Cache a Spark Table",
    "section": "",
    "text": "R/tables_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_cache.html#tbl_cache",
    "href": "packages/sparklyr/latest/reference/tbl_cache.html#tbl_cache",
    "title": "Cache a Spark Table",
    "section": "tbl_cache",
    "text": "tbl_cache"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_cache.html#description",
    "href": "packages/sparklyr/latest/reference/tbl_cache.html#description",
    "title": "Cache a Spark Table",
    "section": "Description",
    "text": "Description\nForce a Spark table with name name to be loaded into memory. Operations on cached tables should normally (although not always) be more performant than the same operation performed on an uncached table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_cache.html#usage",
    "href": "packages/sparklyr/latest/reference/tbl_cache.html#usage",
    "title": "Cache a Spark Table",
    "section": "Usage",
    "text": "Usage\ntbl_cache(sc, name, force = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_cache.html#arguments",
    "href": "packages/sparklyr/latest/reference/tbl_cache.html#arguments",
    "title": "Cache a Spark Table",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe table name.\n\n\nforce\nForce the data to be loaded into memory? This is accomplished by calling the count API on the associated Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_extension.html",
    "href": "packages/sparklyr/latest/reference/spark_extension.html",
    "title": "Create Spark Extension",
    "section": "",
    "text": "R/project_template.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_extension.html#spark_extension",
    "href": "packages/sparklyr/latest/reference/spark_extension.html#spark_extension",
    "title": "Create Spark Extension",
    "section": "spark_extension",
    "text": "spark_extension"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_extension.html#description",
    "href": "packages/sparklyr/latest/reference/spark_extension.html#description",
    "title": "Create Spark Extension",
    "section": "Description",
    "text": "Description\nCreates an R package ready to be used as an Spark extension."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_extension.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_extension.html#usage",
    "title": "Create Spark Extension",
    "section": "Usage",
    "text": "Usage\nspark_extension(path)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_extension.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_extension.html#arguments",
    "title": "Create Spark Extension",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\npath\nLocation where the extension will be created."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_kubernetes.html",
    "href": "packages/sparklyr/latest/reference/spark_config_kubernetes.html",
    "title": "Kubernetes Configuration",
    "section": "",
    "text": "R/kubernetes_config.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#spark_config_kubernetes",
    "href": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#spark_config_kubernetes",
    "title": "Kubernetes Configuration",
    "section": "spark_config_kubernetes",
    "text": "spark_config_kubernetes"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#description",
    "href": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#description",
    "title": "Kubernetes Configuration",
    "section": "Description",
    "text": "Description\nConvenience function to initialize a Kubernetes configuration instead of spark_config(), exposes common properties to set in Kubernetes clusters."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#usage",
    "title": "Kubernetes Configuration",
    "section": "Usage",
    "text": "Usage\nspark_config_kubernetes( \n  master, \n  version = \"2.3.2\", \n  image = \"spark:sparklyr\", \n  driver = random_string(\"sparklyr-\"), \n  account = \"spark\", \n  jars = \"local:///opt/sparklyr\", \n  forward = TRUE, \n  executors = NULL, \n  conf = NULL, \n  timeout = 120, \n  ports = c(8880, 8881, 4040), \n  fix_config = identical(.Platform$OS.type, \"windows\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#arguments",
    "title": "Kubernetes Configuration",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nmaster\nKubernetes url to connect to, found by running kubectl cluster-info.\n\n\nversion\nThe version of Spark being used.\n\n\nimage\nContainer image to use to launch Spark and sparklyr. Also known as spark.kubernetes.container.image.\n\n\ndriver\nName of the driver pod. If not set, the driver pod name is set to “sparklyr” suffixed by id to avoid name conflicts. Also known as spark.kubernetes.driver.pod.name.\n\n\naccount\nService account that is used when running the driver pod. The driver pod uses this service account when requesting executor pods from the API server. Also known as spark.kubernetes.authenticate.driver.serviceAccountName.\n\n\njars\nPath to the sparklyr jars; either, a local path inside the container image with the sparklyr jars copied when the image was created or, a path accesible by the container where the sparklyr jars were copied. You can find a path to the sparklyr jars by running system.file(\"java/\", package = \"sparklyr\").\n\n\nforward\nShould ports used in sparklyr be forwarded automatically through Kubernetes? Default to TRUE which runs kubectl port-forward and pkill kubectlon disconnection.\n\n\nexecutors\nNumber of executors to request while connecting.\n\n\nconf\nA named list of additional entries to add to sparklyr.shell.conf.\n\n\ntimeout\nTotal seconds to wait before giving up on connection.\n\n\nports\nPorts to forward using kubectl.\n\n\nfix_config\nShould the spark-defaults.conf get fixed? TRUE for Windows.\n\n\n…\nAdditional parameters, currently not in use."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html",
    "href": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html",
    "title": "Return the port number of a sparklyr backend.",
    "section": "",
    "text": "R/connection_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#sparklyr_get_backend_port",
    "href": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#sparklyr_get_backend_port",
    "title": "Return the port number of a sparklyr backend.",
    "section": "sparklyr_get_backend_port",
    "text": "sparklyr_get_backend_port"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#description",
    "href": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#description",
    "title": "Return the port number of a sparklyr backend.",
    "section": "Description",
    "text": "Description\nRetrieve the port number of the sparklyr backend associated with a Spark connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#usage",
    "href": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#usage",
    "title": "Return the port number of a sparklyr backend.",
    "section": "Usage",
    "text": "Usage\nsparklyr_get_backend_port(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#arguments",
    "href": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#arguments",
    "title": "Return the port number of a sparklyr backend.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#value",
    "href": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#value",
    "title": "Return the port number of a sparklyr backend.",
    "section": "Value",
    "text": "Value\nThe port number of the sparklyr backend associated with sc."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html",
    "title": "Feature Transformation – VectorSlicer (Transformer)",
    "section": "",
    "text": "R/ml_feature_vector_slicer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html#ft_vector_slicer",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html#ft_vector_slicer",
    "title": "Feature Transformation – VectorSlicer (Transformer)",
    "section": "ft_vector_slicer",
    "text": "ft_vector_slicer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html#description",
    "title": "Feature Transformation – VectorSlicer (Transformer)",
    "section": "Description",
    "text": "Description\nTakes a feature vector and outputs a new feature vector with a subarray of the original features."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html#usage",
    "title": "Feature Transformation – VectorSlicer (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_vector_slicer( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  indices = NULL, \n  uid = random_string(\"vector_slicer_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html#arguments",
    "title": "Feature Transformation – VectorSlicer (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nindices\nAn vector of indices to select features from a vector column. Note that the indices are 0-based.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html#value",
    "title": "Feature Transformation – VectorSlicer (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html#see-also",
    "title": "Feature Transformation – VectorSlicer (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version_from_home.html",
    "href": "packages/sparklyr/latest/reference/spark_version_from_home.html",
    "title": "Get the Spark Version Associated with a Spark Installation",
    "section": "",
    "text": "R/spark_version.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version_from_home.html#spark_version_from_home",
    "href": "packages/sparklyr/latest/reference/spark_version_from_home.html#spark_version_from_home",
    "title": "Get the Spark Version Associated with a Spark Installation",
    "section": "spark_version_from_home",
    "text": "spark_version_from_home"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version_from_home.html#description",
    "href": "packages/sparklyr/latest/reference/spark_version_from_home.html#description",
    "title": "Get the Spark Version Associated with a Spark Installation",
    "section": "Description",
    "text": "Description\nRetrieve the version of Spark associated with a Spark installation."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version_from_home.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_version_from_home.html#usage",
    "title": "Get the Spark Version Associated with a Spark Installation",
    "section": "Usage",
    "text": "Usage\nspark_version_from_home(spark_home, default = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version_from_home.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_version_from_home.html#arguments",
    "title": "Get the Spark Version Associated with a Spark Installation",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nspark_home\nThe path to a Spark installation.\n\n\ndefault\nThe default version to be inferred, in case version lookup failed, e.g. no Spark installation was found at spark_home."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_text.html",
    "href": "packages/sparklyr/latest/reference/spark_write_text.html",
    "title": "Write a Spark DataFrame to a Text file",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_text.html#spark_write_text",
    "href": "packages/sparklyr/latest/reference/spark_write_text.html#spark_write_text",
    "title": "Write a Spark DataFrame to a Text file",
    "section": "spark_write_text",
    "text": "spark_write_text"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_text.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_text.html#description",
    "title": "Write a Spark DataFrame to a Text file",
    "section": "Description",
    "text": "Description\nSerialize a Spark DataFrame to the plain text format."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_text.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_text.html#usage",
    "title": "Write a Spark DataFrame to a Text file",
    "section": "Usage",
    "text": "Usage\nspark_write_text( \n  x, \n  path, \n  mode = NULL, \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_text.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_text.html#arguments",
    "title": "Write a Spark DataFrame to a Text file",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_text.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_text.html#see-also",
    "title": "Write a Spark DataFrame to a Text file",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_load_table.html",
    "href": "packages/sparklyr/latest/reference/spark_load_table.html",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_load_table.html#spark_load_table",
    "href": "packages/sparklyr/latest/reference/spark_load_table.html#spark_load_table",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "spark_load_table",
    "text": "spark_load_table"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_load_table.html#description",
    "href": "packages/sparklyr/latest/reference/spark_load_table.html#description",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "Description",
    "text": "Description\nReads from a Spark Table into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_load_table.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_load_table.html#usage",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "Usage",
    "text": "Usage\nspark_load_table( \n  sc, \n  name, \n  path, \n  options = list(), \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_load_table.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_load_table.html#arguments",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\noptions\nA list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_load_table.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_load_table.html#see-also",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html",
    "href": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html",
    "title": "Extracts metrics from a fitted table",
    "section": "",
    "text": "R/ml_metrics.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#ml_metrics_multiclass",
    "href": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#ml_metrics_multiclass",
    "title": "Extracts metrics from a fitted table",
    "section": "ml_metrics_multiclass",
    "text": "ml_metrics_multiclass"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#description",
    "href": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#description",
    "title": "Extracts metrics from a fitted table",
    "section": "Description",
    "text": "Description\nThe function works best when passed a tbl_spark created by ml_predict(). The output tbl_spark will contain the correct variable types and format that the given Spark model “evaluator” expects."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#usage",
    "title": "Extracts metrics from a fitted table",
    "section": "Usage",
    "text": "Usage\nml_metrics_multiclass( \n  x, \n  truth = label, \n  estimate = prediction, \n  metrics = c(\"accuracy\"), \n  beta = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#arguments",
    "title": "Extracts metrics from a fitted table",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA tbl_spark containing the estimate (prediction) and the truth (value of what actually happened)\n\n\ntruth\nThe name of the column from x with an integer field containing an the indexed value for each outcome . The ml_predict() function will create a new field named label which contains the expected type and values. truth defaults to label.\n\n\nestimate\nThe name of the column from x that contains the prediction. Defaults to prediction, since its type and indexed values will match truth.\n\n\nmetrics\nA character vector with the metrics to calculate. For multiclass models the possible values are: acurracy, f_meas (F-score), recall and precision. This function translates the argument into an acceptable Spark parameter. If no translation is found, then the raw value of the argument is passed to Spark. This makes it possible to request a metric that is not listed here but, depending on version, it is available in Spark. Other metrics form multi-class models are: weightedTruePositiveRate, weightedFalsePositiveRate, weightedFMeasure, truePositiveRateByLabel, falsePositiveRateByLabel, precisionByLabel, recallByLabel, fMeasureByLabel, logLoss, hammingLoss\n\n\nbeta\nNumerical value used for precision and recall. Defaults to NULL, but if the Spark session’s verion is 3.0 and above, then NULL is changed to 1, unless something different is supplied in this argument.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#details",
    "href": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#details",
    "title": "Extracts metrics from a fitted table",
    "section": "Details",
    "text": "Details\nThe ml_metrics family of functions implement Spark’s evaluate closer to how the yardstick package works. The functions expect a table containing the truth and estimate, and return a tibble with the results. The tibble has the same format and variable names as the output of the yardstick functions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#examples",
    "title": "Extracts metrics from a fitted table",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(\"local\") \ntbl_iris <- copy_to(sc, iris) \niris_split <- sdf_random_split(tbl_iris, training = 0.5, test = 0.5) \nmodel <- ml_random_forest(iris_split$training, \"Species ~ .\") \ntbl_predictions <- ml_predict(model, iris_split$test) \nml_metrics_multiclass(tbl_predictions) \n#> # A tibble: 1 × 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy multiclass     0.949\n# Request different metrics \nml_metrics_multiclass(tbl_predictions, metrics = c(\"recall\", \"precision\")) \n#> # A tibble: 2 × 3\n#>   .metric   .estimator .estimate\n#>   <chr>     <chr>          <dbl>\n#> 1 recall    multiclass     0.949\n#> 2 precision multiclass     0.949\n# Request metrics not translated by the function, but valid in Spark \nml_metrics_multiclass(tbl_predictions, metrics = c(\"logLoss\", \"hammingLoss\")) \n#> # A tibble: 2 × 3\n#>   .metric     .estimator .estimate\n#>   <chr>       <chr>          <dbl>\n#> 1 logLoss     multiclass    0.143 \n#> 2 hammingLoss multiclass    0.0506"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "",
    "text": "R/ml_classification_gbt_classifier.R,"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#ml_gbt_classifier",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#ml_gbt_classifier",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "ml_gbt_classifier",
    "text": "ml_gbt_classifier"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#description",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#description",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "Description",
    "text": "Description\nPerform binary classification and regression using gradient boosted trees. Multiclass classification is not supported yet."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#usage",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "Usage",
    "text": "Usage\nml_gbt_classifier( \n  x, \n  formula = NULL, \n  max_iter = 20, \n  max_depth = 5, \n  step_size = 0.1, \n  subsampling_rate = 1, \n  feature_subset_strategy = \"auto\", \n  min_instances_per_node = 1L, \n  max_bins = 32, \n  min_info_gain = 0, \n  loss_type = \"logistic\", \n  seed = NULL, \n  thresholds = NULL, \n  checkpoint_interval = 10, \n  cache_node_ids = FALSE, \n  max_memory_in_mb = 256, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  uid = random_string(\"gbt_classifier_\"), \n  ... \n) \n\nml_gradient_boosted_trees( \n  x, \n  formula = NULL, \n  type = c(\"auto\", \"regression\", \"classification\"), \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  checkpoint_interval = 10, \n  loss_type = c(\"auto\", \"logistic\", \"squared\", \"absolute\"), \n  max_bins = 32, \n  max_depth = 5, \n  max_iter = 20L, \n  min_info_gain = 0, \n  min_instances_per_node = 1, \n  step_size = 0.1, \n  subsampling_rate = 1, \n  feature_subset_strategy = \"auto\", \n  seed = NULL, \n  thresholds = NULL, \n  cache_node_ids = FALSE, \n  max_memory_in_mb = 256, \n  uid = random_string(\"gradient_boosted_trees_\"), \n  response = NULL, \n  features = NULL, \n  ... \n) \n\nml_gbt_regressor( \n  x, \n  formula = NULL, \n  max_iter = 20, \n  max_depth = 5, \n  step_size = 0.1, \n  subsampling_rate = 1, \n  feature_subset_strategy = \"auto\", \n  min_instances_per_node = 1, \n  max_bins = 32, \n  min_info_gain = 0, \n  loss_type = \"squared\", \n  seed = NULL, \n  checkpoint_interval = 10, \n  cache_node_ids = FALSE, \n  max_memory_in_mb = 256, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"gbt_regressor_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#arguments",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nmax_iter\nMaxmimum number of iterations.\n\n\nmax_depth\nMaximum depth of the tree (>= 0); that is, the maximum number of nodes separating any leaves from the root of the tree.\n\n\nstep_size\nStep size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default = 0.1)\n\n\nsubsampling_rate\nFraction of the training data used for learning each decision tree, in range (0, 1]. (default = 1.0)\n\n\nfeature_subset_strategy\nThe number of features to consider for splits at each tree node. See details for options.\n\n\nmin_instances_per_node\nMinimum number of instances each child must have after split.\n\n\nmax_bins\nThe maximum number of bins used for discretizing continuous features and for choosing how to split on features at each node. More bins give higher granularity.\n\n\nmin_info_gain\nMinimum information gain for a split to be considered at a tree node. Should be >= 0, defaults to 0.\n\n\nloss_type\nLoss function which GBT tries to minimize. Supported: \"squared\" (L2) and \"absolute\" (L1) (default = squared) for regression and \"logistic\" (default) for classification. For ml_gradient_boosted_trees, setting \"auto\"will default to the appropriate loss type based on model type.\n\n\nseed\nSeed for random numbers.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\ncheckpoint_interval\nSet checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.\n\n\ncache_node_ids\nIf FALSE, the algorithm will pass trees to executors to match instances with nodes. If TRUE, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Defaults to FALSE.\n\n\nmax_memory_in_mb\nMaximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. Defaults to 256.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details.\n\n\ntype\nThe type of model to fit. \"regression\" treats the response as a continuous variable, while \"classification\" treats the response as a categorical variable. When \"auto\" is used, the model type is inferred based on the response variable type – if it is a numeric type, then regression is used; classification otherwise.\n\n\nresponse\n(Deprecated) The name of the response column (as a length-one character vector.)\n\n\nfeatures\n(Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#details",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#details",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows.\nThe supported options for feature_subset_strategy are\n\n\"auto\": Choose automatically for task: If num_trees == 1, set to \"all\". If num_trees > 1 (forest), set to \"sqrt\" for classification and to \"onethird\" for regression.\n\"all\": use all features\n\"onethird\": use 1/3 of the features\n\"sqrt\": use use sqrt(number of features)\n\"log2\": use log2(number of features)\n\"n\": when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features. (default = \"auto\")\n\nml_gradient_boosted_trees is a wrapper around ml_gbt_regressor.tbl_spark and ml_gbt_classifier.tbl_spark and calls the appropriate method based on model type."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#value",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#value",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula\nis specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#examples",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \npartitions <- iris_tbl %>% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \niris_training <- partitions$training \niris_test <- partitions$test \ngbt_model <- iris_training %>% \n  ml_gradient_boosted_trees(Sepal_Length ~ Petal_Length + Petal_Width) \npred <- ml_predict(gbt_model, iris_test) \nml_regression_evaluator(pred, label_col = \"Sepal_Length\") \n#> [1] 0.4036941"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#see-also",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency.html",
    "href": "packages/sparklyr/latest/reference/spark_dependency.html",
    "title": "Define a Spark dependency",
    "section": "",
    "text": "R/spark_extensions.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency.html#spark_dependency",
    "href": "packages/sparklyr/latest/reference/spark_dependency.html#spark_dependency",
    "title": "Define a Spark dependency",
    "section": "spark_dependency",
    "text": "spark_dependency"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency.html#description",
    "href": "packages/sparklyr/latest/reference/spark_dependency.html#description",
    "title": "Define a Spark dependency",
    "section": "Description",
    "text": "Description\nDefine a Spark dependency consisting of a set of custom JARs, Spark packages, and customized dbplyr SQL translation env."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_dependency.html#usage",
    "title": "Define a Spark dependency",
    "section": "Usage",
    "text": "Usage\nspark_dependency( \n  jars = NULL, \n  packages = NULL, \n  initializer = NULL, \n  catalog = NULL, \n  repositories = NULL, \n  dbplyr_sql_variant = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_dependency.html#arguments",
    "title": "Define a Spark dependency",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\njars\nCharacter vector of full paths to JAR files.\n\n\npackages\nCharacter vector of Spark packages names.\n\n\ninitializer\nOptional callback function called when initializing a connection.\n\n\ncatalog\nOptional location where extension JAR files can be downloaded for Livy.\n\n\nrepositories\nCharacter vector of Spark package repositories.\n\n\ndbplyr_sql_variant\nCustomization of dbplyr SQL translation env. Must be a named list of the following form: <<<<<<<<<<<<<<<<<<<<<<<<<  list(     scalar = list(scalar_fn1 = ..., scalar_fn2 = ..., <etc>),     aggregate = list(agg_fn1 = ..., agg_fn2 = ..., <etc>),     window = list(wnd_fn1 = ..., wnd_fn2 = ..., <etc>)   )See sql_variant for details.\n\n\n…\nAdditional optional arguments."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency.html#value",
    "href": "packages/sparklyr/latest/reference/spark_dependency.html#value",
    "title": "Define a Spark dependency",
    "section": "Value",
    "text": "Value\nAn object of type spark_dependency"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html",
    "title": "Read Kafka Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#stream_read_kafka",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#stream_read_kafka",
    "title": "Read Kafka Stream",
    "section": "stream_read_kafka",
    "text": "stream_read_kafka"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#description",
    "title": "Read Kafka Stream",
    "section": "Description",
    "text": "Description\nReads a Kafka stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#usage",
    "title": "Read Kafka Stream",
    "section": "Usage",
    "text": "Usage\nstream_read_kafka(sc, name = NULL, options = list(), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#arguments",
    "title": "Read Kafka Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#details",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#details",
    "title": "Read Kafka Stream",
    "section": "Details",
    "text": "Details\nPlease note that Kafka requires installing the appropriate package by setting the packages parameter to \"kafka\" in spark_connect()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#examples",
    "title": "Read Kafka Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\", version = \"2.3\", packages = \"kafka\") \nread_options <- list(kafka.bootstrap.servers = \"localhost:9092\", subscribe = \"topic1\") \nwrite_options <- list(kafka.bootstrap.servers = \"localhost:9092\", topic = \"topic2\") \nstream <- stream_read_kafka(sc, options = read_options) %>% \n  stream_write_kafka(options = write_options) \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#see-also",
    "title": "Read Kafka Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_fpgrowth.html",
    "href": "packages/sparklyr/latest/reference/ml_fpgrowth.html",
    "title": "Frequent Pattern Mining – FPGrowth",
    "section": "",
    "text": "R/ml_fpm_fpgrowth.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_fpgrowth.html#ml_fpgrowth",
    "href": "packages/sparklyr/latest/reference/ml_fpgrowth.html#ml_fpgrowth",
    "title": "Frequent Pattern Mining – FPGrowth",
    "section": "ml_fpgrowth",
    "text": "ml_fpgrowth"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_fpgrowth.html#description",
    "href": "packages/sparklyr/latest/reference/ml_fpgrowth.html#description",
    "title": "Frequent Pattern Mining – FPGrowth",
    "section": "Description",
    "text": "Description\nA parallel FP-growth algorithm to mine frequent itemsets."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_fpgrowth.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_fpgrowth.html#usage",
    "title": "Frequent Pattern Mining – FPGrowth",
    "section": "Usage",
    "text": "Usage\nml_fpgrowth( \n  x, \n  items_col = \"items\", \n  min_confidence = 0.8, \n  min_support = 0.3, \n  prediction_col = \"prediction\", \n  uid = random_string(\"fpgrowth_\"), \n  ... \n) \n\nml_association_rules(model) \n\nml_freq_itemsets(model)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_fpgrowth.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_fpgrowth.html#arguments",
    "title": "Frequent Pattern Mining – FPGrowth",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nitems_col\nItems column name. Default: “items”\n\n\nmin_confidence\nMinimal confidence for generating Association Rule. min_confidence will not affect the mining for frequent itemsets, but will affect the association rules generation. Default: 0.8\n\n\nmin_support\nMinimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears more than (min_support * size-of-the-dataset) times will be output in the frequent itemsets. Default: 0.3\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; currently unused.\n\n\nmodel\nA fitted FPGrowth model returned by ml_fpgrowth()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_glm_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_glm_tidiers.html",
    "title": "Tidying methods for Spark ML linear models",
    "section": "",
    "text": "R/tidiers_ml_linear_models.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#ml_glm_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#ml_glm_tidiers",
    "title": "Tidying methods for Spark ML linear models",
    "section": "ml_glm_tidiers",
    "text": "ml_glm_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#description",
    "title": "Tidying methods for Spark ML linear models",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#usage",
    "title": "Tidying methods for Spark ML linear models",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_generalized_linear_regression'\ntidy(x, exponentiate = FALSE, ...) \n\n## S3 method for class 'ml_model_linear_regression'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_generalized_linear_regression'\naugment( \n  x, \n  newdata = NULL, \n  type.residuals = c(\"working\", \"deviance\", \"pearson\", \"response\"), \n  ... \n) \n\n## S3 method for class '`_ml_model_linear_regression`'\naugment( \n  x, \n  new_data = NULL, \n  type.residuals = c(\"working\", \"deviance\", \"pearson\", \"response\"), \n  ... \n) \n\n## S3 method for class 'ml_model_linear_regression'\naugment( \n  x, \n  newdata = NULL, \n  type.residuals = c(\"working\", \"deviance\", \"pearson\", \"response\"), \n  ... \n) \n\n## S3 method for class 'ml_model_generalized_linear_regression'\nglance(x, ...) \n\n## S3 method for class 'ml_model_linear_regression'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML linear models",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\nexponentiate\nFor GLM, whether to exponentiate the coefficient estimates (typical for logistic regression.)\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction.\n\n\ntype.residuals\ntype of residuals, defaults to \"working\". Must be set to \"working\" when newdata is supplied.\n\n\nnew_data\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#details",
    "href": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#details",
    "title": "Tidying methods for Spark ML linear models",
    "section": "Details",
    "text": "Details\nThe residuals attached by augment are of type “working” by default, which is different from the default of “deviance” for residuals() or sdf_residuals()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_name.html",
    "href": "packages/sparklyr/latest/reference/stream_name.html",
    "title": "Spark Stream’s Name",
    "section": "",
    "text": "R/stream_operations.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_name.html#stream_name",
    "href": "packages/sparklyr/latest/reference/stream_name.html#stream_name",
    "title": "Spark Stream’s Name",
    "section": "stream_name",
    "text": "stream_name"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_name.html#description",
    "href": "packages/sparklyr/latest/reference/stream_name.html#description",
    "title": "Spark Stream’s Name",
    "section": "Description",
    "text": "Description\nRetrieves the name of the Spark stream if available."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_name.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_name.html#usage",
    "title": "Spark Stream’s Name",
    "section": "Usage",
    "text": "Usage\nstream_name(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_name.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_name.html#arguments",
    "title": "Spark Stream’s Name",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nstream\nThe spark stream object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_table.html",
    "href": "packages/sparklyr/latest/reference/spark_write_table.html",
    "title": "Writes a Spark DataFrame into a Spark table",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_table.html#spark_write_table",
    "href": "packages/sparklyr/latest/reference/spark_write_table.html#spark_write_table",
    "title": "Writes a Spark DataFrame into a Spark table",
    "section": "spark_write_table",
    "text": "spark_write_table"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_table.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_table.html#description",
    "title": "Writes a Spark DataFrame into a Spark table",
    "section": "Description",
    "text": "Description\nWrites a Spark DataFrame into a Spark table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_table.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_table.html#usage",
    "title": "Writes a Spark DataFrame into a Spark table",
    "section": "Usage",
    "text": "Usage\nspark_write_table( \n  x, \n  name, \n  mode = NULL, \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_table.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_table.html#arguments",
    "title": "Writes a Spark DataFrame into a Spark table",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nname\nThe name to assign to the newly generated table.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_table.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_table.html#see-also",
    "title": "Writes a Spark DataFrame into a Spark table",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_table_name.html",
    "href": "packages/sparklyr/latest/reference/spark_table_name.html",
    "title": "Generate a Table Name from Expression",
    "section": "",
    "text": "R/spark_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_table_name.html#spark_table_name",
    "href": "packages/sparklyr/latest/reference/spark_table_name.html#spark_table_name",
    "title": "Generate a Table Name from Expression",
    "section": "spark_table_name",
    "text": "spark_table_name"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_table_name.html#description",
    "href": "packages/sparklyr/latest/reference/spark_table_name.html#description",
    "title": "Generate a Table Name from Expression",
    "section": "Description",
    "text": "Description\nAttempts to generate a table name from an expression; otherwise, assigns an auto-generated generic name with “sparklyr_” prefix."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_table_name.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_table_name.html#usage",
    "title": "Generate a Table Name from Expression",
    "section": "Usage",
    "text": "Usage\nspark_table_name(expr)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_table_name.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_table_name.html#arguments",
    "title": "Generate a Table Name from Expression",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nexpr\nThe expression to attempt to use as name"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html",
    "title": "Tidying methods for Spark ML Naive Bayes",
    "section": "",
    "text": "R/tidiers_ml_naive_bayes.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#ml_naive_bayes_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#ml_naive_bayes_tidiers",
    "title": "Tidying methods for Spark ML Naive Bayes",
    "section": "ml_naive_bayes_tidiers",
    "text": "ml_naive_bayes_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#description",
    "title": "Tidying methods for Spark ML Naive Bayes",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#usage",
    "title": "Tidying methods for Spark ML Naive Bayes",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_naive_bayes'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_naive_bayes'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_naive_bayes'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML Naive Bayes",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_runif.html",
    "href": "packages/sparklyr/latest/reference/sdf_runif.html",
    "title": "Generate random samples from the uniform distribution U(0, 1).",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_runif.html#sdf_runif",
    "href": "packages/sparklyr/latest/reference/sdf_runif.html#sdf_runif",
    "title": "Generate random samples from the uniform distribution U(0, 1).",
    "section": "sdf_runif",
    "text": "sdf_runif"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_runif.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_runif.html#description",
    "title": "Generate random samples from the uniform distribution U(0, 1).",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from the uniform distribution U(0, 1)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_runif.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_runif.html#usage",
    "title": "Generate random samples from the uniform distribution U(0, 1).",
    "section": "Usage",
    "text": "Usage\nsdf_runif( \n  sc, \n  n, \n  min = 0, \n  max = 1, \n  num_partitions = NULL, \n  seed = NULL, \n  output_col = \"x\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_runif.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_runif.html#arguments",
    "title": "Generate random samples from the uniform distribution U(0, 1).",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nmin\nThe lower limit of the distribution.\n\n\nmax\nThe upper limit of the distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_runif.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_runif.html#see-also",
    "title": "Generate random samples from the uniform distribution U(0, 1).",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html",
    "title": "Spark ML – Power Iteration Clustering",
    "section": "",
    "text": "R/ml_clustering_power_iteration.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html#ml_power_iteration",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html#ml_power_iteration",
    "title": "Spark ML – Power Iteration Clustering",
    "section": "ml_power_iteration",
    "text": "ml_power_iteration"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html#description",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html#description",
    "title": "Spark ML – Power Iteration Clustering",
    "section": "Description",
    "text": "Description\nPower iteration clustering (PIC) is a scalable and efficient algorithm for clustering vertices of a graph given pairwise similarities as edge properties, described in the paper “Power Iteration Clustering” by Frank Lin and William W. Cohen. It computes a pseudo-eigenvector of the normalized affinity matrix of the graph via power iteration and uses it to cluster vertices. spark.mllib includes an implementation of PIC using GraphX as its backend. It takes an RDD of (srcId, dstId, similarity) tuples and outputs a model with the clustering assignments. The similarities must be nonnegative. PIC assumes that the similarity measure is symmetric. A pair (srcId, dstId) regardless of the ordering should appear at most once in the input data. If a pair is missing from input, their similarity is treated as zero."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html#usage",
    "title": "Spark ML – Power Iteration Clustering",
    "section": "Usage",
    "text": "Usage\nml_power_iteration( \n  x, \n  k = 4, \n  max_iter = 20, \n  init_mode = \"random\", \n  src_col = \"src\", \n  dst_col = \"dst\", \n  weight_col = \"weight\", \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html#arguments",
    "title": "Spark ML – Power Iteration Clustering",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA ‘spark_connection’ or a ‘tbl_spark’.\n\n\nk\nThe number of clusters to create.\n\n\nmax_iter\nThe maximum number of iterations to run.\n\n\ninit_mode\nThis can be either “random”, which is the default, to use a random vector as vertex properties, or “degree” to use normalized sum similarities.\n\n\nsrc_col\nColumn in the input Spark dataframe containing 0-based indexes of all source vertices in the affinity matrix described in the PIC paper.\n\n\ndst_col\nColumn in the input Spark dataframe containing 0-based indexes of all destination vertices in the affinity matrix described in the PIC paper.\n\n\nweight_col\nColumn in the input Spark dataframe containing non-negative edge weights in the affinity matrix described in the PIC paper.\n\n\n…\nOptional arguments. Currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html#value",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html#value",
    "title": "Spark ML – Power Iteration Clustering",
    "section": "Value",
    "text": "Value\nA 2-column R dataframe with columns named “id” and “cluster” describing the resulting cluster assignments"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html#examples",
    "title": "Spark ML – Power Iteration Clustering",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\") \nr1 <- 1 \nn1 <- 80L \nr2 <- 4 \nn2 <- 80L \ngen_circle <- function(radius, num_pts) { \n  # generate evenly distributed points on a circle centered at the origin \n  seq(0, num_pts - 1) %>% \n    lapply( \n      function(pt) { \n        theta <- 2 * pi * pt / num_pts \n        radius * c(cos(theta), sin(theta)) \n      } \n    ) \n} \nguassian_similarity <- function(pt1, pt2) { \n  dist2 <- sum((pt2 - pt1)^2) \n  exp(-dist2 / 2) \n} \ngen_pic_data <- function() { \n  # generate points on 2 concentric circle centered at the origin and then \n  # compute pairwise Gaussian similarity values of all unordered pair of \n  # points \n  n <- n1 + n2 \n  pts <- append(gen_circle(r1, n1), gen_circle(r2, n2)) \n  num_unordered_pairs <- n * (n - 1) / 2 \n  src <- rep(0L, num_unordered_pairs) \n  dst <- rep(0L, num_unordered_pairs) \n  sim <- rep(0, num_unordered_pairs) \n  idx <- 1 \n  for (i in seq(2, n)) { \n    for (j in seq(i - 1)) { \n      src[[idx]] <- i - 1L \n      dst[[idx]] <- j - 1L \n      sim[[idx]] <- guassian_similarity(pts[[i]], pts[[j]]) \n      idx <- idx + 1 \n    } \n  } \n  tibble::tibble(src = src, dst = dst, sim = sim) \n} \npic_data <- copy_to(sc, gen_pic_data()) \nclusters <- ml_power_iteration( \n  pic_data, \n  src_col = \"src\", dst_col = \"dst\", weight_col = \"sim\", k = 2, max_iter = 40 \n) \nprint(clusters) \n#> # A tibble: 160 × 2\n#>       id cluster\n#>    <dbl>   <int>\n#>  1     0       1\n#>  2     1       1\n#>  3     2       1\n#>  4     3       1\n#>  5     4       1\n#>  6     5       1\n#>  7     6       1\n#>  8     7       1\n#>  9     8       1\n#> 10     9       1\n#> # … with 150 more rows"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html",
    "title": "Feature Transformation – VectorAssembler (Transformer)",
    "section": "",
    "text": "R/ml_feature_vector_assembler.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html#ft_vector_assembler",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html#ft_vector_assembler",
    "title": "Feature Transformation – VectorAssembler (Transformer)",
    "section": "ft_vector_assembler",
    "text": "ft_vector_assembler"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html#description",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html#description",
    "title": "Feature Transformation – VectorAssembler (Transformer)",
    "section": "Description",
    "text": "Description\nCombine multiple vectors into a single row-vector; that is, where each row element of the newly generated column is a vector formed by concatenating each row element from the specified input columns."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html#usage",
    "title": "Feature Transformation – VectorAssembler (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_vector_assembler( \n  x, \n  input_cols = NULL, \n  output_col = NULL, \n  uid = random_string(\"vector_assembler_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html#arguments",
    "title": "Feature Transformation – VectorAssembler (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_cols\nThe names of the input columns\n\n\noutput_col\nThe name of the output column.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html#value",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html#value",
    "title": "Feature Transformation – VectorAssembler (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html#see-also",
    "title": "Feature Transformation – VectorAssembler (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_seq.html",
    "href": "packages/sparklyr/latest/reference/sdf_seq.html",
    "title": "Create DataFrame for Range",
    "section": "",
    "text": "R/sdf_sequence.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_seq.html#sdf_seq",
    "href": "packages/sparklyr/latest/reference/sdf_seq.html#sdf_seq",
    "title": "Create DataFrame for Range",
    "section": "sdf_seq",
    "text": "sdf_seq"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_seq.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_seq.html#description",
    "title": "Create DataFrame for Range",
    "section": "Description",
    "text": "Description\nCreates a DataFrame for the given range"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_seq.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_seq.html#usage",
    "title": "Create DataFrame for Range",
    "section": "Usage",
    "text": "Usage\nsdf_seq( \n  sc, \n  from = 1L, \n  to = 1L, \n  by = 1L, \n  repartition = NULL, \n  type = c(\"integer\", \"integer64\") \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_seq.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_seq.html#arguments",
    "title": "Create DataFrame for Range",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nfrom, to\nThe start and end to use as a range\n\n\nby\nThe increment of the sequence.\n\n\nrepartition\nThe number of partitions to use when distributing the data across the Spark cluster. Defaults to the minimum number of partitions.\n\n\ntype\nThe data type to use for the index, either \"integer\" or \"integer64\"."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html",
    "title": "Read a ORC file into a Spark DataFrame",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html#spark_read_orc",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html#spark_read_orc",
    "title": "Read a ORC file into a Spark DataFrame",
    "section": "spark_read_orc",
    "text": "spark_read_orc"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html#description",
    "title": "Read a ORC file into a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRead a ORC file into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html#usage",
    "title": "Read a ORC file into a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nspark_read_orc( \n  sc, \n  name = NULL, \n  path = name, \n  options = list(), \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE, \n  columns = NULL, \n  schema = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html#arguments",
    "title": "Read a ORC file into a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\noptions\nA list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\nschema\nA (java) read schema. Useful for optimizing read operation on nested data.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html#details",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html#details",
    "title": "Read a ORC file into a Spark DataFrame",
    "section": "Details",
    "text": "Details\nYou can read data from HDFS (hdfs://), S3 (s3a://), as well as the local file system (file://)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html#see-also",
    "title": "Read a ORC file into a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_sync.html",
    "href": "packages/sparklyr/latest/reference/spark_install_sync.html",
    "title": "helper function to sync sparkinstall project to sparklyr",
    "section": "",
    "text": "R/install_tools.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_sync.html#spark_install_sync",
    "href": "packages/sparklyr/latest/reference/spark_install_sync.html#spark_install_sync",
    "title": "helper function to sync sparkinstall project to sparklyr",
    "section": "spark_install_sync",
    "text": "spark_install_sync"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_sync.html#description",
    "href": "packages/sparklyr/latest/reference/spark_install_sync.html#description",
    "title": "helper function to sync sparkinstall project to sparklyr",
    "section": "Description",
    "text": "Description\nSee: https://github.com/rstudio/spark-install"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_sync.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_install_sync.html#usage",
    "title": "helper function to sync sparkinstall project to sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_install_sync(project_path)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_sync.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_install_sync.html#arguments",
    "title": "helper function to sync sparkinstall project to sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nproject_path\nThe path to the sparkinstall project"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_along.html",
    "href": "packages/sparklyr/latest/reference/sdf_along.html",
    "title": "Create DataFrame for along Object",
    "section": "",
    "text": "R/sdf_sequence.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_along.html#sdf_along",
    "href": "packages/sparklyr/latest/reference/sdf_along.html#sdf_along",
    "title": "Create DataFrame for along Object",
    "section": "sdf_along",
    "text": "sdf_along"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_along.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_along.html#description",
    "title": "Create DataFrame for along Object",
    "section": "Description",
    "text": "Description\nCreates a DataFrame along the given object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_along.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_along.html#usage",
    "title": "Create DataFrame for along Object",
    "section": "Usage",
    "text": "Usage\nsdf_along(sc, along, repartition = NULL, type = c(\"integer\", \"integer64\"))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_along.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_along.html#arguments",
    "title": "Create DataFrame for along Object",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nalong\nTakes the length from the length of this argument.\n\n\nrepartition\nThe number of partitions to use when distributing the data across the Spark cluster.\n\n\ntype\nThe data type to use for the index, either \"integer\" or \"integer64\"."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html",
    "href": "packages/sparklyr/latest/reference/spark_read.html",
    "title": "Read file(s) into a Spark DataFrame using a custom reader",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html#spark_read",
    "href": "packages/sparklyr/latest/reference/spark_read.html#spark_read",
    "title": "Read file(s) into a Spark DataFrame using a custom reader",
    "section": "spark_read",
    "text": "spark_read"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read.html#description",
    "title": "Read file(s) into a Spark DataFrame using a custom reader",
    "section": "Description",
    "text": "Description\nRun a custom R function on Spark workers to ingest data from one or more files into a Spark DataFrame, assuming all files follow the same schema."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read.html#usage",
    "title": "Read file(s) into a Spark DataFrame using a custom reader",
    "section": "Usage",
    "text": "Usage\nspark_read(sc, paths, reader, columns, packages = TRUE, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read.html#arguments",
    "title": "Read file(s) into a Spark DataFrame using a custom reader",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npaths\nA character vector of one or more file URIs (e.g., c(“hdfs://localhost:9000/file.txt”, “hdfs://localhost:9000/file2.txt”))\n\n\nreader\nA self-contained R function that takes a single file URI as argument and returns the data read from that file as a data frame.\n\n\ncolumns\na named list of column names and column types of the resulting data frame (e.g., list(column_1 = “integer”, column_2 = “character”)), or a list of column names only if column types should be inferred from the data (e.g., list(“column_1”, “column_2”), or NULL if column types should be inferred and resulting data frame can have arbitrary column names\n\n\npackages\nA list of R packages to distribute to Spark workers\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html#examples",
    "href": "packages/sparklyr/latest/reference/spark_read.html#examples",
    "title": "Read file(s) into a Spark DataFrame using a custom reader",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc <- spark_connect( \n  master = \"yarn\", \n  spark_home = \"~/spark/spark-2.4.5-bin-hadoop2.7\" \n) \n# This is a contrived example to show reader tasks will be distributed across \n# all Spark worker nodes \nspark_read( \n  sc, \n  rep(\"/dev/null\", 10), \n  reader = function(path) system(\"hostname\", intern = TRUE), \n  columns = c(hostname = \"string\") \n) %>% sdf_collect()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read.html#see-also",
    "title": "Read file(s) into a Spark DataFrame using a custom reader",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_log.html",
    "href": "packages/sparklyr/latest/reference/spark_apply_log.html",
    "title": "Log Writer for Spark Apply",
    "section": "",
    "text": "R/spark_apply.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_log.html#spark_apply_log",
    "href": "packages/sparklyr/latest/reference/spark_apply_log.html#spark_apply_log",
    "title": "Log Writer for Spark Apply",
    "section": "spark_apply_log",
    "text": "spark_apply_log"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_log.html#description",
    "href": "packages/sparklyr/latest/reference/spark_apply_log.html#description",
    "title": "Log Writer for Spark Apply",
    "section": "Description",
    "text": "Description\nWrites data to log under spark_apply()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_log.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_apply_log.html#usage",
    "title": "Log Writer for Spark Apply",
    "section": "Usage",
    "text": "Usage\nspark_apply_log(..., level = \"INFO\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_log.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_apply_log.html#arguments",
    "title": "Log Writer for Spark Apply",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\n…\nArguments to write to log.\n\n\nlevel\nSeverity level for this entry; recommended values: INFO, ERROR or WARN."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_broadcast.html",
    "href": "packages/sparklyr/latest/reference/sdf_broadcast.html",
    "title": "Broadcast hint",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_broadcast.html#sdf_broadcast",
    "href": "packages/sparklyr/latest/reference/sdf_broadcast.html#sdf_broadcast",
    "title": "Broadcast hint",
    "section": "sdf_broadcast",
    "text": "sdf_broadcast"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_broadcast.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_broadcast.html#description",
    "title": "Broadcast hint",
    "section": "Description",
    "text": "Description\nUsed to force broadcast hash joins."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_broadcast.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_broadcast.html#usage",
    "title": "Broadcast hint",
    "section": "Usage",
    "text": "Usage\nsdf_broadcast(x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_broadcast.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_broadcast.html#arguments",
    "title": "Broadcast hint",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_lag.html",
    "href": "packages/sparklyr/latest/reference/stream_lag.html",
    "title": "Apply lag function to columns of a Spark Streaming DataFrame",
    "section": "",
    "text": "R/stream_operations.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_lag.html#stream_lag",
    "href": "packages/sparklyr/latest/reference/stream_lag.html#stream_lag",
    "title": "Apply lag function to columns of a Spark Streaming DataFrame",
    "section": "stream_lag",
    "text": "stream_lag"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_lag.html#description",
    "href": "packages/sparklyr/latest/reference/stream_lag.html#description",
    "title": "Apply lag function to columns of a Spark Streaming DataFrame",
    "section": "Description",
    "text": "Description\nGiven a streaming Spark dataframe as input, this function will return another streaming dataframe that contains all columns in the input and column(s) that are shifted behind by the offset(s) specified in ... (see example)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_lag.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_lag.html#usage",
    "title": "Apply lag function to columns of a Spark Streaming DataFrame",
    "section": "Usage",
    "text": "Usage\nstream_lag(x, cols, thresholds = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_lag.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_lag.html#arguments",
    "title": "Apply lag function to columns of a Spark Streaming DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a Spark Streaming DataFrame.\n\n\ncols\nA list of expressions for a single or multiple variables to create that will contain the value of a previous entry.\n\n\nthresholds\nOptional named list of timestamp column(s) and corresponding time duration(s) for deterimining whether a previous record is sufficiently recent relative to the current record. If the any of the time difference(s) between the current and a previous record is greater than the maximal duration allowed, then the previous record is discarded and will not be part of the query result. The durations can be specified with numeric types (which will be interpreted as max difference allowed in number of milliseconds between 2 UNIX timestamps) or time duration strings such as “5s”, “5sec”, “5min”, “5hour”, etc. Any timestamp column in x that is not of timestamp of date Spark SQL types will be interepreted as number of milliseconds since the UNIX epoch."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_lag.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_lag.html#examples",
    "title": "Apply lag function to columns of a Spark Streaming DataFrame",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\", version = \"2.2.0\") \nstreaming_path <- tempfile(\"days_df_\") \ndays_df <- tibble::tibble( \n  today = weekdays(as.Date(seq(7), origin = \"1970-01-01\")) \n) \nnum_iters <- 7 \nstream_generate_test( \n  df = days_df, \n  path = streaming_path, \n  distribution = rep(nrow(days_df), num_iters), \n  iterations = num_iters \n) \nstream_read_csv(sc, streaming_path) %>% \n  stream_lag(cols = c(yesterday = today ~ 1, two_days_ago = today ~ 2)) %>% \n  collect() %>% \n  print(n = 10L)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html",
    "href": "packages/sparklyr/latest/reference/registerdospark.html",
    "title": "Register a Parallel Backend",
    "section": "",
    "text": "R/do_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html#registerdospark",
    "href": "packages/sparklyr/latest/reference/registerdospark.html#registerdospark",
    "title": "Register a Parallel Backend",
    "section": "registerDoSpark",
    "text": "registerDoSpark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html#description",
    "href": "packages/sparklyr/latest/reference/registerdospark.html#description",
    "title": "Register a Parallel Backend",
    "section": "Description",
    "text": "Description\nRegisters a parallel backend using the foreach package."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html#usage",
    "href": "packages/sparklyr/latest/reference/registerdospark.html#usage",
    "title": "Register a Parallel Backend",
    "section": "Usage",
    "text": "Usage\nregisterDoSpark(spark_conn, parallelism = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html#arguments",
    "href": "packages/sparklyr/latest/reference/registerdospark.html#arguments",
    "title": "Register a Parallel Backend",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nspark_conn\nSpark connection to use\n\n\nparallelism\nLevel of parallelism to use for task execution (if unspecified, then it will take the value of SparkContext.defaultParallelism() which by default is the number of cores available to the sparklyr application)\n\n\n…\nadditional options for sparklyr parallel backend (currently only the only valid option is nocompile = T, F )"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html#value",
    "href": "packages/sparklyr/latest/reference/registerdospark.html#value",
    "title": "Register a Parallel Backend",
    "section": "Value",
    "text": "Value\nNone"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html#examples",
    "href": "packages/sparklyr/latest/reference/registerdospark.html#examples",
    "title": "Register a Parallel Backend",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \nregisterDoSpark(sc, nocompile = FALSE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_prefixspan.html",
    "href": "packages/sparklyr/latest/reference/ml_prefixspan.html",
    "title": "Frequent Pattern Mining – PrefixSpan",
    "section": "",
    "text": "R/ml_fpm_prefixspan.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_prefixspan.html#ml_prefixspan",
    "href": "packages/sparklyr/latest/reference/ml_prefixspan.html#ml_prefixspan",
    "title": "Frequent Pattern Mining – PrefixSpan",
    "section": "ml_prefixspan",
    "text": "ml_prefixspan"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_prefixspan.html#description",
    "href": "packages/sparklyr/latest/reference/ml_prefixspan.html#description",
    "title": "Frequent Pattern Mining – PrefixSpan",
    "section": "Description",
    "text": "Description\nPrefixSpan algorithm for mining frequent itemsets."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_prefixspan.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_prefixspan.html#usage",
    "title": "Frequent Pattern Mining – PrefixSpan",
    "section": "Usage",
    "text": "Usage\nml_prefixspan( \n  x, \n  seq_col = \"sequence\", \n  min_support = 0.1, \n  max_pattern_length = 10, \n  max_local_proj_db_size = 3.2e+07, \n  uid = random_string(\"prefixspan_\"), \n  ... \n) \n\nml_freq_seq_patterns(model)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_prefixspan.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_prefixspan.html#arguments",
    "title": "Frequent Pattern Mining – PrefixSpan",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nseq_col\nThe name of the sequence column in dataset (default “sequence”). Rows with nulls in this column are ignored.\n\n\nmin_support\nThe minimum support required to be considered a frequent sequential pattern.\n\n\nmax_pattern_length\nThe maximum length of a frequent sequential pattern. Any frequent pattern exceeding this length will not be included in the results.\n\n\nmax_local_proj_db_size\nThe maximum number of items allowed in a prefix-projected database before local iterative processing of the projected database begins. This parameter should be tuned with respect to the size of your executors.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; currently unused.\n\n\nmodel\nA Prefix Span model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_prefixspan.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_prefixspan.html#examples",
    "title": "Frequent Pattern Mining – PrefixSpan",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\", version = \"2.4.0\") \nitems_df <- tibble::tibble( \n  seq = list( \n    list(list(1, 2), list(3)), \n    list(list(1), list(3, 2), list(1, 2)), \n    list(list(1, 2), list(5)), \n    list(list(6)) \n  ) \n) \nitems_sdf <- copy_to(sc, items_df, overwrite = TRUE) \nprefix_span_model <- ml_prefixspan( \n  sc, \n  seq_col = \"seq\", \n  min_support = 0.5, \n  max_pattern_length = 5, \n  max_local_proj_db_size = 32000000 \n) \nfrequent_items <- prefix_span_model$frequent_sequential_patterns(items_sdf) %>% collect()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_separate_column.html",
    "href": "packages/sparklyr/latest/reference/sdf_separate_column.html",
    "title": "Separate a Vector Column into Scalar Columns",
    "section": "",
    "text": "R/sdf_wrapper.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_separate_column.html#sdf_separate_column",
    "href": "packages/sparklyr/latest/reference/sdf_separate_column.html#sdf_separate_column",
    "title": "Separate a Vector Column into Scalar Columns",
    "section": "sdf_separate_column",
    "text": "sdf_separate_column"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_separate_column.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_separate_column.html#description",
    "title": "Separate a Vector Column into Scalar Columns",
    "section": "Description",
    "text": "Description\nGiven a vector column in a Spark DataFrame, split that into n separate columns, each column made up of the different elements in the column column."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_separate_column.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_separate_column.html#usage",
    "title": "Separate a Vector Column into Scalar Columns",
    "section": "Usage",
    "text": "Usage\nsdf_separate_column(x, column, into = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_separate_column.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_separate_column.html#arguments",
    "title": "Separate a Vector Column into Scalar Columns",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ncolumn\nThe name of a (vector-typed) column.\n\n\ninto\nA specification of the columns that should be generated from column. This can either be a vector of column names, or an R list mapping column names to the (1-based) index at which a particular vector element should be extracted."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_is_open.html",
    "href": "packages/sparklyr/latest/reference/connection_is_open.html",
    "title": "Check whether the connection is open",
    "section": "",
    "text": "R/core_connection.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_is_open.html#connection_is_open",
    "href": "packages/sparklyr/latest/reference/connection_is_open.html#connection_is_open",
    "title": "Check whether the connection is open",
    "section": "connection_is_open",
    "text": "connection_is_open"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_is_open.html#description",
    "href": "packages/sparklyr/latest/reference/connection_is_open.html#description",
    "title": "Check whether the connection is open",
    "section": "Description",
    "text": "Description\nCheck whether the connection is open"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_is_open.html#usage",
    "href": "packages/sparklyr/latest/reference/connection_is_open.html#usage",
    "title": "Check whether the connection is open",
    "section": "Usage",
    "text": "Usage\nconnection_is_open(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_is_open.html#arguments",
    "href": "packages/sparklyr/latest/reference/connection_is_open.html#arguments",
    "title": "Check whether the connection is open",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nspark_connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_settings.html",
    "href": "packages/sparklyr/latest/reference/spark_config_settings.html",
    "title": "Retrieve Available Settings",
    "section": "",
    "text": "R/config_settings.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_settings.html#spark_config_settings",
    "href": "packages/sparklyr/latest/reference/spark_config_settings.html#spark_config_settings",
    "title": "Retrieve Available Settings",
    "section": "spark_config_settings",
    "text": "spark_config_settings"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_settings.html#description",
    "href": "packages/sparklyr/latest/reference/spark_config_settings.html#description",
    "title": "Retrieve Available Settings",
    "section": "Description",
    "text": "Description\nRetrieves available sparklyr settings that can be used in configuration files or spark_config()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_settings.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_config_settings.html#usage",
    "title": "Retrieve Available Settings",
    "section": "Usage",
    "text": "Usage\nspark_config_settings()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html",
    "title": "Sort a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#sdf_sort",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#sdf_sort",
    "title": "Sort a Spark DataFrame",
    "section": "sdf_sort",
    "text": "sdf_sort"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#description",
    "title": "Sort a Spark DataFrame",
    "section": "Description",
    "text": "Description\nSort a Spark DataFrame by one or more columns, with each column sorted in ascending order."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#usage",
    "title": "Sort a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_sort(x, columns)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#arguments",
    "title": "Sort a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\ncolumns\nThe column(s) to sort by."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#section",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#section",
    "title": "Sort a Spark DataFrame",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#transforming-spark-dataframes",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#transforming-spark-dataframes",
    "title": "Sort a Spark DataFrame",
    "section": "Transforming Spark DataFrames",
    "text": "Transforming Spark DataFrames\nThe family of functions prefixed with sdf_ generally access the Scala Spark DataFrame API directly, as opposed to the dplyr interface which uses Spark SQL. These functions will ‘force’ any pending SQL in a dplyr pipeline, such that the resulting tbl_spark object returned will no longer have the attached ‘lazy’ SQL operations. Note that the underlying Spark DataFrame does execute its operations lazily, so that even though the pending set of operations (currently) are not exposed at the R level, these operations will only be executed when you explicitly collect() the table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#see-also",
    "title": "Sort a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_copy_to(), sdf_distinct(), sdf_random_split(), sdf_register(), sdf_sample(), sdf_weighted_sample()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_delta.html",
    "href": "packages/sparklyr/latest/reference/spark_write_delta.html",
    "title": "Writes a Spark DataFrame into Delta Lake",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_delta.html#spark_write_delta",
    "href": "packages/sparklyr/latest/reference/spark_write_delta.html#spark_write_delta",
    "title": "Writes a Spark DataFrame into Delta Lake",
    "section": "spark_write_delta",
    "text": "spark_write_delta"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_delta.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_delta.html#description",
    "title": "Writes a Spark DataFrame into Delta Lake",
    "section": "Description",
    "text": "Description\nWrites a Spark DataFrame into Delta Lake."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_delta.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_delta.html#usage",
    "title": "Writes a Spark DataFrame into Delta Lake",
    "section": "Usage",
    "text": "Usage\nspark_write_delta( \n  x, \n  path, \n  mode = NULL, \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_delta.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_delta.html#arguments",
    "title": "Writes a Spark DataFrame into Delta Lake",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_delta.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_delta.html#see-also",
    "title": "Writes a Spark DataFrame into Delta Lake",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-model-constructors.html",
    "href": "packages/sparklyr/latest/reference/ml-model-constructors.html",
    "title": "Constructors for ml_model Objects",
    "section": "",
    "text": "R/ml_model_helpers.R, R/ml_model_constructors.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-model-constructors.html#ml_supervised_pipeline",
    "href": "packages/sparklyr/latest/reference/ml-model-constructors.html#ml_supervised_pipeline",
    "title": "Constructors for ml_model Objects",
    "section": "ml_supervised_pipeline",
    "text": "ml_supervised_pipeline"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-model-constructors.html#description",
    "href": "packages/sparklyr/latest/reference/ml-model-constructors.html#description",
    "title": "Constructors for ml_model Objects",
    "section": "Description",
    "text": "Description\nFunctions for developers writing extensions for Spark ML. These functions are constructors for ml_model objects that are returned when using the formula interface."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-model-constructors.html#usage",
    "href": "packages/sparklyr/latest/reference/ml-model-constructors.html#usage",
    "title": "Constructors for ml_model Objects",
    "section": "Usage",
    "text": "Usage\nml_supervised_pipeline(predictor, dataset, formula, features_col, label_col) \n\nml_clustering_pipeline(predictor, dataset, formula, features_col) \n\nml_construct_model_supervised( \n  constructor, \n  predictor, \n  formula, \n  dataset, \n  features_col, \n  label_col, \n  ... \n) \n\nml_construct_model_clustering( \n  constructor, \n  predictor, \n  formula, \n  dataset, \n  features_col, \n  ... \n) \n\nnew_ml_model_prediction( \n  pipeline_model, \n  formula, \n  dataset, \n  label_col, \n  features_col, \n  ..., \n  class = character() \n) \n\nnew_ml_model(pipeline_model, formula, dataset, ..., class = character()) \n\nnew_ml_model_classification( \n  pipeline_model, \n  formula, \n  dataset, \n  label_col, \n  features_col, \n  predicted_label_col, \n  ..., \n  class = character() \n) \n\nnew_ml_model_regression( \n  pipeline_model, \n  formula, \n  dataset, \n  label_col, \n  features_col, \n  ..., \n  class = character() \n) \n\nnew_ml_model_clustering( \n  pipeline_model, \n  formula, \n  dataset, \n  features_col, \n  ..., \n  class = character() \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-model-constructors.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml-model-constructors.html#arguments",
    "title": "Constructors for ml_model Objects",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\npredictor\nThe pipeline stage corresponding to the ML algorithm.\n\n\ndataset\nThe training dataset.\n\n\nformula\nThe formula used for data preprocessing\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nconstructor\nThe constructor function for the ml_model.\n\n\npipeline_model\nThe pipeline model object returned by ml_supervised_pipeline().\n\n\nclass\nName of the subclass."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html",
    "title": "Spark ML – Naive-Bayes",
    "section": "",
    "text": "R/ml_classification_naive_bayes.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#ml_naive_bayes",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#ml_naive_bayes",
    "title": "Spark ML – Naive-Bayes",
    "section": "ml_naive_bayes",
    "text": "ml_naive_bayes"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#description",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#description",
    "title": "Spark ML – Naive-Bayes",
    "section": "Description",
    "text": "Description\nNaive Bayes Classifiers. It supports Multinomial NB (see here) which can handle finitely supported discrete data. For example, by converting documents into TF-IDF vectors, it can be used for document classification. By making every vector a binary (0/1) data, it can also be used as Bernoulli NB (see here). The input feature values must be nonnegative."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#usage",
    "title": "Spark ML – Naive-Bayes",
    "section": "Usage",
    "text": "Usage\nml_naive_bayes( \n  x, \n  formula = NULL, \n  model_type = \"multinomial\", \n  smoothing = 1, \n  thresholds = NULL, \n  weight_col = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  uid = random_string(\"naive_bayes_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#arguments",
    "title": "Spark ML – Naive-Bayes",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nmodel_type\nThe model type. Supported options: \"multinomial\"and \"bernoulli\". (default = multinomial)\n\n\nsmoothing\nThe (Laplace) smoothing parameter. Defaults to 1.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\nweight_col\n(Spark 2.1.0+) Weight column name. If this is not set or empty, we treat all instance weights as 1.0.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#details",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#details",
    "title": "Spark ML – Naive-Bayes",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#value",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#value",
    "title": "Spark ML – Naive-Bayes",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula\nis specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#examples",
    "title": "Spark ML – Naive-Bayes",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \npartitions <- iris_tbl %>% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \niris_training <- partitions$training \niris_test <- partitions$test \nnb_model <- iris_training %>% \n  ml_naive_bayes(Species ~ .) \npred <- ml_predict(nb_model, iris_test) \nml_multiclass_classification_evaluator(pred) \n#> [1] 0.9393939"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#see-also",
    "title": "Spark ML – Naive-Bayes",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/list_sparklyr_jars.html",
    "href": "packages/sparklyr/latest/reference/list_sparklyr_jars.html",
    "title": "list all sparklyr-*.jar files that have been built",
    "section": "",
    "text": "R/spark_compile.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/list_sparklyr_jars.html#list_sparklyr_jars",
    "href": "packages/sparklyr/latest/reference/list_sparklyr_jars.html#list_sparklyr_jars",
    "title": "list all sparklyr-*.jar files that have been built",
    "section": "list_sparklyr_jars",
    "text": "list_sparklyr_jars"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/list_sparklyr_jars.html#description",
    "href": "packages/sparklyr/latest/reference/list_sparklyr_jars.html#description",
    "title": "list all sparklyr-*.jar files that have been built",
    "section": "Description",
    "text": "Description\nlist all sparklyr-*.jar files that have been built"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/list_sparklyr_jars.html#usage",
    "href": "packages/sparklyr/latest/reference/list_sparklyr_jars.html#usage",
    "title": "list all sparklyr-*.jar files that have been built",
    "section": "Usage",
    "text": "Usage\nlist_sparklyr_jars()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/arrow_enabled_object.html",
    "href": "packages/sparklyr/latest/reference/arrow_enabled_object.html",
    "title": "Determine whether arrow is able to serialize the given R object",
    "section": "",
    "text": "R/arrow_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/arrow_enabled_object.html#arrow_enabled_object",
    "href": "packages/sparklyr/latest/reference/arrow_enabled_object.html#arrow_enabled_object",
    "title": "Determine whether arrow is able to serialize the given R object",
    "section": "arrow_enabled_object",
    "text": "arrow_enabled_object"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/arrow_enabled_object.html#description",
    "href": "packages/sparklyr/latest/reference/arrow_enabled_object.html#description",
    "title": "Determine whether arrow is able to serialize the given R object",
    "section": "Description",
    "text": "Description\nIf the given R object is not serializable by arrow due to some known limitations of arrow, then return FALSE, otherwise return TRUE"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/arrow_enabled_object.html#usage",
    "href": "packages/sparklyr/latest/reference/arrow_enabled_object.html#usage",
    "title": "Determine whether arrow is able to serialize the given R object",
    "section": "Usage",
    "text": "Usage\narrow_enabled_object(object)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/arrow_enabled_object.html#arguments",
    "href": "packages/sparklyr/latest/reference/arrow_enabled_object.html#arguments",
    "title": "Determine whether arrow is able to serialize the given R object",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nobject\nThe object to be serialized"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/arrow_enabled_object.html#examples",
    "href": "packages/sparklyr/latest/reference/arrow_enabled_object.html#examples",
    "title": "Determine whether arrow is able to serialize the given R object",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\ndf <- tibble::tibble(x = seq(5)) \narrow_enabled_object(df) \n#> [1] TRUE"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_summary.html",
    "href": "packages/sparklyr/latest/reference/ml_summary.html",
    "title": "Spark ML – Extraction of summary metrics",
    "section": "",
    "text": "R/ml_pipeline_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_summary.html#ml_summary",
    "href": "packages/sparklyr/latest/reference/ml_summary.html#ml_summary",
    "title": "Spark ML – Extraction of summary metrics",
    "section": "ml_summary",
    "text": "ml_summary"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_summary.html#description",
    "href": "packages/sparklyr/latest/reference/ml_summary.html#description",
    "title": "Spark ML – Extraction of summary metrics",
    "section": "Description",
    "text": "Description\nExtracts a metric from the summary object of a Spark ML model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_summary.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_summary.html#usage",
    "title": "Spark ML – Extraction of summary metrics",
    "section": "Usage",
    "text": "Usage\nml_summary(x, metric = NULL, allow_null = FALSE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_summary.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_summary.html#arguments",
    "title": "Spark ML – Extraction of summary metrics",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark ML model that has a summary.\n\n\nmetric\nThe name of the metric to extract. If not set, returns the summary object.\n\n\nallow_null\nWhether null results are allowed when the metric is not found in the summary."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "",
    "text": "R/ml_feature_min_max_scaler.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#ft_min_max_scaler",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#ft_min_max_scaler",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "ft_min_max_scaler",
    "text": "ft_min_max_scaler"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#description",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#description",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "Description",
    "text": "Description\nRescale each feature individually to a common range [min, max] linearly using column summary statistics, which is also known as min-max normalization or Rescaling"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#usage",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_min_max_scaler( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  min = 0, \n  max = 1, \n  uid = random_string(\"min_max_scaler_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#arguments",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nmin\nLower bound after transformation, shared by all features Default: 0.0\n\n\nmax\nUpper bound after transformation, shared by all features Default: 1.0\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#details",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#details",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#value",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#value",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#examples",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#examples",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \nfeatures <- c(\"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\") \niris_tbl %>% \n  ft_vector_assembler( \n    input_col = features, \n    output_col = \"features_temp\" \n  ) %>% \n  ft_min_max_scaler( \n    input_col = \"features_temp\", \n    output_col = \"features\" \n  ) \n#> # Source: spark<?> [?? x 7]\n#>    Sepal_L…¹ Sepal…² Petal…³ Petal…⁴ Species featu…⁵ featu…⁶\n#>        <dbl>   <dbl>   <dbl>   <dbl> <chr>   <list>  <list> \n#>  1       5.1     3.5     1.4     0.2 setosa  <dbl>   <dbl>  \n#>  2       4.9     3       1.4     0.2 setosa  <dbl>   <dbl>  \n#>  3       4.7     3.2     1.3     0.2 setosa  <dbl>   <dbl>  \n#>  4       4.6     3.1     1.5     0.2 setosa  <dbl>   <dbl>  \n#>  5       5       3.6     1.4     0.2 setosa  <dbl>   <dbl>  \n#>  6       5.4     3.9     1.7     0.4 setosa  <dbl>   <dbl>  \n#>  7       4.6     3.4     1.4     0.3 setosa  <dbl>   <dbl>  \n#>  8       5       3.4     1.5     0.2 setosa  <dbl>   <dbl>  \n#>  9       4.4     2.9     1.4     0.2 setosa  <dbl>   <dbl>  \n#> 10       4.9     3.1     1.5     0.1 setosa  <dbl>   <dbl>  \n#> # … with more rows, and abbreviated variable names\n#> #   ¹​Sepal_Length, ²​Sepal_Width, ³​Petal_Length,\n#> #   ⁴​Petal_Width, ⁵​features_temp, ⁶​features"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#see-also",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_als_tidiers.html",
    "title": "Tidying methods for Spark ML ALS",
    "section": "",
    "text": "R/tidiers_ml_als.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als_tidiers.html#ml_als_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_als_tidiers.html#ml_als_tidiers",
    "title": "Tidying methods for Spark ML ALS",
    "section": "ml_als_tidiers",
    "text": "ml_als_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_als_tidiers.html#description",
    "title": "Tidying methods for Spark ML ALS",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_als_tidiers.html#usage",
    "title": "Tidying methods for Spark ML ALS",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_als'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_als'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_als'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_als_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML ALS",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_pipeline_stage.html",
    "href": "packages/sparklyr/latest/reference/spark_pipeline_stage.html",
    "title": "Create a Pipeline Stage Object",
    "section": "",
    "text": "R/ml_pipeline_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#spark_pipeline_stage",
    "href": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#spark_pipeline_stage",
    "title": "Create a Pipeline Stage Object",
    "section": "spark_pipeline_stage",
    "text": "spark_pipeline_stage"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#description",
    "href": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#description",
    "title": "Create a Pipeline Stage Object",
    "section": "Description",
    "text": "Description\nHelper function to create pipeline stage objects with common parameter setters."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#usage",
    "title": "Create a Pipeline Stage Object",
    "section": "Usage",
    "text": "Usage\nspark_pipeline_stage( \n  sc, \n  class, \n  uid, \n  features_col = NULL, \n  label_col = NULL, \n  prediction_col = NULL, \n  probability_col = NULL, \n  raw_prediction_col = NULL, \n  k = NULL, \n  max_iter = NULL, \n  seed = NULL, \n  input_col = NULL, \n  input_cols = NULL, \n  output_col = NULL, \n  output_cols = NULL \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#arguments",
    "title": "Create a Pipeline Stage Object",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection object.\n\n\nclass\nClass name for the pipeline stage.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nk\nThe number of clusters to create\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\ninput_col\nThe name of the input column.\n\n\ninput_cols\nNames of output columns.\n\n\noutput_col\nThe name of the output column.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html",
    "title": "Tidying methods for Spark ML MLP",
    "section": "",
    "text": "R/tidiers_ml_multilayer_perceptron.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#ml_multilayer_perceptron_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#ml_multilayer_perceptron_tidiers",
    "title": "Tidying methods for Spark ML MLP",
    "section": "ml_multilayer_perceptron_tidiers",
    "text": "ml_multilayer_perceptron_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#description",
    "title": "Tidying methods for Spark ML MLP",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#usage",
    "title": "Tidying methods for Spark ML MLP",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_multilayer_perceptron_classification'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_multilayer_perceptron_classification'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_multilayer_perceptron_classification'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML MLP",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_model_data.html",
    "href": "packages/sparklyr/latest/reference/ml_model_data.html",
    "title": "Extracts data associated with a Spark ML model",
    "section": "",
    "text": "R/ml_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_model_data.html#ml_model_data",
    "href": "packages/sparklyr/latest/reference/ml_model_data.html#ml_model_data",
    "title": "Extracts data associated with a Spark ML model",
    "section": "ml_model_data",
    "text": "ml_model_data"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_model_data.html#description",
    "href": "packages/sparklyr/latest/reference/ml_model_data.html#description",
    "title": "Extracts data associated with a Spark ML model",
    "section": "Description",
    "text": "Description\nExtracts data associated with a Spark ML model"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_model_data.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_model_data.html#usage",
    "title": "Extracts data associated with a Spark ML model",
    "section": "Usage",
    "text": "Usage\nml_model_data(object)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_model_data.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_model_data.html#arguments",
    "title": "Extracts data associated with a Spark ML model",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nobject\na Spark ML model"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_model_data.html#value",
    "href": "packages/sparklyr/latest/reference/ml_model_data.html#value",
    "title": "Extracts data associated with a Spark ML model",
    "section": "Value",
    "text": "Value\nA tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html",
    "href": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html",
    "title": "Fast cbind for Spark DataFrames",
    "section": "",
    "text": "R/sdf_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#sdf_fast_bind_cols",
    "href": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#sdf_fast_bind_cols",
    "title": "Fast cbind for Spark DataFrames",
    "section": "sdf_fast_bind_cols",
    "text": "sdf_fast_bind_cols"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#description",
    "title": "Fast cbind for Spark DataFrames",
    "section": "Description",
    "text": "Description\nThis is a version of sdf_bind_cols that works by zipping RDDs. From the API docs: “Assumes that the two RDDs have the same number of partitions and the same number of elements in each partition (e.g. one was made through a map on the other).”"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#usage",
    "title": "Fast cbind for Spark DataFrames",
    "section": "Usage",
    "text": "Usage\nsdf_fast_bind_cols(...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#arguments",
    "title": "Fast cbind for Spark DataFrames",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\n…\nSpark DataFrames to cbind"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html",
    "href": "packages/sparklyr/latest/reference/spark_version.html",
    "title": "Get the Spark Version Associated with a Spark Connection",
    "section": "",
    "text": "R/spark_version.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html#spark_version",
    "href": "packages/sparklyr/latest/reference/spark_version.html#spark_version",
    "title": "Get the Spark Version Associated with a Spark Connection",
    "section": "spark_version",
    "text": "spark_version"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html#description",
    "href": "packages/sparklyr/latest/reference/spark_version.html#description",
    "title": "Get the Spark Version Associated with a Spark Connection",
    "section": "Description",
    "text": "Description\nRetrieve the version of Spark associated with a Spark connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_version.html#usage",
    "title": "Get the Spark Version Associated with a Spark Connection",
    "section": "Usage",
    "text": "Usage\nspark_version(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_version.html#arguments",
    "title": "Get the Spark Version Associated with a Spark Connection",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html#details",
    "href": "packages/sparklyr/latest/reference/spark_version.html#details",
    "title": "Get the Spark Version Associated with a Spark Connection",
    "section": "Details",
    "text": "Details\nSuffixes for e.g. preview versions, or snapshotted versions, are trimmed – if you require the full Spark version, you can retrieve it with invoke(spark_context(sc), \"version\")."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html#value",
    "href": "packages/sparklyr/latest/reference/spark_version.html#value",
    "title": "Get the Spark Version Associated with a Spark Connection",
    "section": "Value",
    "text": "Value\nThe Spark version as a numeric_version."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html",
    "title": "Spark ML - Clustering Evaluator",
    "section": "",
    "text": "R/ml_evaluation_clustering.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#ml_clustering_evaluator",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#ml_clustering_evaluator",
    "title": "Spark ML - Clustering Evaluator",
    "section": "ml_clustering_evaluator",
    "text": "ml_clustering_evaluator"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#description",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#description",
    "title": "Spark ML - Clustering Evaluator",
    "section": "Description",
    "text": "Description\nEvaluator for clustering results. The metric computes the Silhouette measure using the squared Euclidean distance. The Silhouette is a measure for the validation of the consistency within clusters. It ranges between 1 and -1, where a value close to 1 means that the points in a cluster are close to the other points in the same cluster and far from the points of the other clusters."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#usage",
    "title": "Spark ML - Clustering Evaluator",
    "section": "Usage",
    "text": "Usage\nml_clustering_evaluator( \n  x, \n  features_col = \"features\", \n  prediction_col = \"prediction\", \n  metric_name = \"silhouette\", \n  uid = random_string(\"clustering_evaluator_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#arguments",
    "title": "Spark ML - Clustering Evaluator",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection object or a tbl_spark containing label and prediction columns. The latter should be the output of sdf_predict.\n\n\nfeatures_col\nName of features column.\n\n\nprediction_col\nName of the prediction column.\n\n\nmetric_name\nThe performance metric. Currently supports “silhouette”.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#value",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#value",
    "title": "Spark ML - Clustering Evaluator",
    "section": "Value",
    "text": "Value\nThe calculated performance metric"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#examples",
    "title": "Spark ML - Clustering Evaluator",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \npartitions <- iris_tbl %>% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \niris_training <- partitions$training \niris_test <- partitions$test \nformula <- Species ~ . \n# Train the models \nkmeans_model <- ml_kmeans(iris_training, formula = formula) \nb_kmeans_model <- ml_bisecting_kmeans(iris_training, formula = formula) \ngmm_model <- ml_gaussian_mixture(iris_training, formula = formula) \n# Predict \npred_kmeans <- ml_predict(kmeans_model, iris_test) \npred_b_kmeans <- ml_predict(b_kmeans_model, iris_test) \npred_gmm <- ml_predict(gmm_model, iris_test) \n# Evaluate \nml_clustering_evaluator(pred_kmeans) \n#> [1] 0.8736088\nml_clustering_evaluator(pred_b_kmeans) \n#> [1] 0.4433206\nml_clustering_evaluator(pred_gmm) \n#> [1] 0.8677525"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html",
    "title": "Write Parquet Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html#stream_write_parquet",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html#stream_write_parquet",
    "title": "Write Parquet Stream",
    "section": "stream_write_parquet",
    "text": "stream_write_parquet"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html#description",
    "title": "Write Parquet Stream",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into a parquet stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html#usage",
    "title": "Write Parquet Stream",
    "section": "Usage",
    "text": "Usage\nstream_write_parquet( \n  x, \n  path, \n  mode = c(\"append\", \"complete\", \"update\"), \n  trigger = stream_trigger_interval(), \n  checkpoint = file.path(path, \"checkpoints\", random_string(\"\")), \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html#arguments",
    "title": "Write Parquet Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe destination path. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\".\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous.\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html#examples",
    "title": "Write Parquet Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \nsdf_len(sc, 10) %>% spark_write_parquet(\"parquet-in\") \nstream <- stream_read_parquet(sc, \"parquet-in\") %>% stream_write_parquet(\"parquet-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html#see-also",
    "title": "Write Parquet Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html",
    "title": "Write Text Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html#stream_write_text",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html#stream_write_text",
    "title": "Write Text Stream",
    "section": "stream_write_text",
    "text": "stream_write_text"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html#description",
    "title": "Write Text Stream",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into a text stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html#usage",
    "title": "Write Text Stream",
    "section": "Usage",
    "text": "Usage\nstream_write_text( \n  x, \n  path, \n  mode = c(\"append\", \"complete\", \"update\"), \n  trigger = stream_trigger_interval(), \n  checkpoint = file.path(path, \"checkpoints\", random_string(\"\")), \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html#arguments",
    "title": "Write Text Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe destination path. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\".\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous.\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html#examples",
    "title": "Write Text Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \ndir.create(\"text-in\") \nwriteLines(\"A text entry\", \"text-in/text.txt\") \ntext_path <- file.path(\"file://\", getwd(), \"text-in\") \nstream <- stream_read_text(sc, text_path) %>% stream_write_text(\"text-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html#see-also",
    "title": "Write Text Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_repartition.html",
    "href": "packages/sparklyr/latest/reference/sdf_repartition.html",
    "title": "Repartition a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_repartition.html#sdf_repartition",
    "href": "packages/sparklyr/latest/reference/sdf_repartition.html#sdf_repartition",
    "title": "Repartition a Spark DataFrame",
    "section": "sdf_repartition",
    "text": "sdf_repartition"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_repartition.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_repartition.html#description",
    "title": "Repartition a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRepartition a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_repartition.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_repartition.html#usage",
    "title": "Repartition a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_repartition(x, partitions = NULL, partition_by = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_repartition.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_repartition.html#arguments",
    "title": "Repartition a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\npartitions\nnumber of partitions\n\n\npartition_by\nvector of column names used for partitioning, only supported for Spark 2.0+"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pca_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_pca_tidiers.html",
    "title": "Tidying methods for Spark ML Principal Component Analysis",
    "section": "",
    "text": "R/tidiers_pca.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#ml_pca_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#ml_pca_tidiers",
    "title": "Tidying methods for Spark ML Principal Component Analysis",
    "section": "ml_pca_tidiers",
    "text": "ml_pca_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#description",
    "title": "Tidying methods for Spark ML Principal Component Analysis",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#usage",
    "title": "Tidying methods for Spark ML Principal Component Analysis",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_pca'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_pca'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_pca'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML Principal Component Analysis",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/fill.html",
    "href": "packages/sparklyr/latest/reference/fill.html",
    "title": "Fill",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/fill.html#fill",
    "href": "packages/sparklyr/latest/reference/fill.html#fill",
    "title": "Fill",
    "section": "fill",
    "text": "fill"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/fill.html#description",
    "href": "packages/sparklyr/latest/reference/fill.html#description",
    "title": "Fill",
    "section": "Description",
    "text": "Description\nSee fill for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_id.html",
    "href": "packages/sparklyr/latest/reference/stream_id.html",
    "title": "Spark Stream’s Identifier",
    "section": "",
    "text": "R/stream_operations.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_id.html#stream_id",
    "href": "packages/sparklyr/latest/reference/stream_id.html#stream_id",
    "title": "Spark Stream’s Identifier",
    "section": "stream_id",
    "text": "stream_id"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_id.html#description",
    "href": "packages/sparklyr/latest/reference/stream_id.html#description",
    "title": "Spark Stream’s Identifier",
    "section": "Description",
    "text": "Description\nRetrieves the identifier of the Spark stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_id.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_id.html#usage",
    "title": "Spark Stream’s Identifier",
    "section": "Usage",
    "text": "Usage\nstream_id(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_id.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_id.html#arguments",
    "title": "Spark Stream’s Identifier",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nstream\nThe spark stream object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dataframe.html",
    "href": "packages/sparklyr/latest/reference/spark_dataframe.html",
    "title": "Retrieve a Spark DataFrame",
    "section": "",
    "text": "R/spark_dataframe.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dataframe.html#spark_dataframe",
    "href": "packages/sparklyr/latest/reference/spark_dataframe.html#spark_dataframe",
    "title": "Retrieve a Spark DataFrame",
    "section": "spark_dataframe",
    "text": "spark_dataframe"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dataframe.html#description",
    "href": "packages/sparklyr/latest/reference/spark_dataframe.html#description",
    "title": "Retrieve a Spark DataFrame",
    "section": "Description",
    "text": "Description\nThis S3 generic is used to access a Spark DataFrame object (as a Java object reference) from an R object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dataframe.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_dataframe.html#usage",
    "title": "Retrieve a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nspark_dataframe(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dataframe.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_dataframe.html#arguments",
    "title": "Retrieve a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn R object wrapping, or containing, a Spark DataFrame.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dataframe.html#value",
    "href": "packages/sparklyr/latest/reference/spark_dataframe.html#value",
    "title": "Retrieve a Spark DataFrame",
    "section": "Value",
    "text": "Value\nA spark_jobj representing a Java object reference to a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_csv.html",
    "href": "packages/sparklyr/latest/reference/spark_write_csv.html",
    "title": "Write a Spark DataFrame to a CSV",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_csv.html#spark_write_csv",
    "href": "packages/sparklyr/latest/reference/spark_write_csv.html#spark_write_csv",
    "title": "Write a Spark DataFrame to a CSV",
    "section": "spark_write_csv",
    "text": "spark_write_csv"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_csv.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_csv.html#description",
    "title": "Write a Spark DataFrame to a CSV",
    "section": "Description",
    "text": "Description\nWrite a Spark DataFrame to a tabular (typically, comma-separated) file."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_csv.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_csv.html#usage",
    "title": "Write a Spark DataFrame to a CSV",
    "section": "Usage",
    "text": "Usage\nspark_write_csv( \n  x, \n  path, \n  header = TRUE, \n  delimiter = \",\", \n  quote = \"\\\"\", \n  escape = \"\\\\\", \n  charset = \"UTF-8\", \n  null_value = NULL, \n  options = list(), \n  mode = NULL, \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_csv.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_csv.html#arguments",
    "title": "Write a Spark DataFrame to a CSV",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nheader\nShould the first row of data be used as a header? Defaults to TRUE.\n\n\ndelimiter\nThe character used to delimit each column, defaults to ,.\n\n\nquote\nThe character used as a quote. Defaults to '\"'.\n\n\nescape\nThe character used to escape other characters, defaults to \\.\n\n\ncharset\nThe character set, defaults to \"UTF-8\".\n\n\nnull_value\nThe character to use for default values, defaults to NULL.\n\n\noptions\nA list of strings with additional options.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_csv.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_csv.html#see-also",
    "title": "Write a Spark DataFrame to a CSV",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html",
    "title": "Write Console Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html#stream_write_console",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html#stream_write_console",
    "title": "Write Console Stream",
    "section": "stream_write_console",
    "text": "stream_write_console"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html#description",
    "title": "Write Console Stream",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into console logs."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html#usage",
    "title": "Write Console Stream",
    "section": "Usage",
    "text": "Usage\nstream_write_console( \n  x, \n  mode = c(\"append\", \"complete\", \"update\"), \n  options = list(), \n  trigger = stream_trigger_interval(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html#arguments",
    "title": "Write Console Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\".\n\n\noptions\nA list of strings with additional options.\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html#examples",
    "title": "Write Console Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \nsdf_len(sc, 10) %>% \n  dplyr::transmute(text = as.character(id)) %>% \n  spark_write_text(\"text-in\") \nstream <- stream_read_text(sc, \"text-in\") %>% stream_write_console() \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html#see-also",
    "title": "Write Console Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html",
    "title": "Perform Weighted Random Sampling on a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#sdf_weighted_sample",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#sdf_weighted_sample",
    "title": "Perform Weighted Random Sampling on a Spark DataFrame",
    "section": "sdf_weighted_sample",
    "text": "sdf_weighted_sample"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#description",
    "title": "Perform Weighted Random Sampling on a Spark DataFrame",
    "section": "Description",
    "text": "Description\nDraw a random sample of rows (with or without replacement) from a Spark DataFrame If the sampling is done without replacement, then it will be conceptually equivalent to an iterative process such that in each step the probability of adding a row to the sample set is equal to its weight divided by summation of weights of all rows that are not in the sample set yet in that step."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#usage",
    "title": "Perform Weighted Random Sampling on a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_weighted_sample(x, weight_col, k, replacement = TRUE, seed = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#arguments",
    "title": "Perform Weighted Random Sampling on a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\nweight_col\nName of the weight column\n\n\nk\nSample set size\n\n\nreplacement\nWhether to sample with replacement\n\n\nseed\nAn (optional) integer seed"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#section",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#section",
    "title": "Perform Weighted Random Sampling on a Spark DataFrame",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#transforming-spark-dataframes",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#transforming-spark-dataframes",
    "title": "Perform Weighted Random Sampling on a Spark DataFrame",
    "section": "Transforming Spark DataFrames",
    "text": "Transforming Spark DataFrames\nThe family of functions prefixed with sdf_ generally access the Scala Spark DataFrame API directly, as opposed to the dplyr interface which uses Spark SQL. These functions will ‘force’ any pending SQL in a dplyr pipeline, such that the resulting tbl_spark object returned will no longer have the attached ‘lazy’ SQL operations. Note that the underlying Spark DataFrame does execute its operations lazily, so that even though the pending set of operations (currently) are not exposed at the R level, these operations will only be executed when you explicitly collect() the table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#see-also",
    "title": "Perform Weighted Random Sampling on a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_copy_to(), sdf_distinct(), sdf_random_split(), sdf_register(), sdf_sample(), sdf_sort()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/print_jobj.html",
    "href": "packages/sparklyr/latest/reference/print_jobj.html",
    "title": "Generic method for print jobj for a connection type",
    "section": "",
    "text": "R/core_jobj.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/print_jobj.html#print_jobj",
    "href": "packages/sparklyr/latest/reference/print_jobj.html#print_jobj",
    "title": "Generic method for print jobj for a connection type",
    "section": "print_jobj",
    "text": "print_jobj"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/print_jobj.html#description",
    "href": "packages/sparklyr/latest/reference/print_jobj.html#description",
    "title": "Generic method for print jobj for a connection type",
    "section": "Description",
    "text": "Description\nGeneric method for print jobj for a connection type"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/print_jobj.html#usage",
    "href": "packages/sparklyr/latest/reference/print_jobj.html#usage",
    "title": "Generic method for print jobj for a connection type",
    "section": "Usage",
    "text": "Usage\nprint_jobj(sc, jobj, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/print_jobj.html#arguments",
    "href": "packages/sparklyr/latest/reference/print_jobj.html#arguments",
    "title": "Generic method for print jobj for a connection type",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nspark_connection (used for type dispatch)\n\n\njobj\nObject to print"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_versions.html",
    "href": "packages/sparklyr/latest/reference/spark_versions.html",
    "title": "Retrieves a dataframe available Spark versions that van be installed.",
    "section": "",
    "text": "R/install_spark_versions.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_versions.html#spark_versions",
    "href": "packages/sparklyr/latest/reference/spark_versions.html#spark_versions",
    "title": "Retrieves a dataframe available Spark versions that van be installed.",
    "section": "spark_versions",
    "text": "spark_versions"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_versions.html#description",
    "href": "packages/sparklyr/latest/reference/spark_versions.html#description",
    "title": "Retrieves a dataframe available Spark versions that van be installed.",
    "section": "Description",
    "text": "Description\nRetrieves a dataframe available Spark versions that van be installed."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_versions.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_versions.html#usage",
    "title": "Retrieves a dataframe available Spark versions that van be installed.",
    "section": "Usage",
    "text": "Usage\nspark_versions(latest = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_versions.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_versions.html#arguments",
    "title": "Retrieves a dataframe available Spark versions that van be installed.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nlatest\nCheck for latest version?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rchisq.html",
    "href": "packages/sparklyr/latest/reference/sdf_rchisq.html",
    "title": "Generate random samples from a chi-squared distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rchisq.html#sdf_rchisq",
    "href": "packages/sparklyr/latest/reference/sdf_rchisq.html#sdf_rchisq",
    "title": "Generate random samples from a chi-squared distribution",
    "section": "sdf_rchisq",
    "text": "sdf_rchisq"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rchisq.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rchisq.html#description",
    "title": "Generate random samples from a chi-squared distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a chi-squared distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rchisq.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rchisq.html#usage",
    "title": "Generate random samples from a chi-squared distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rchisq(sc, n, df, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rchisq.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rchisq.html#arguments",
    "title": "Generate random samples from a chi-squared distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\ndf\nDegrees of freedom (non-negative, but can be non-integer).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rchisq.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rchisq.html#see-also",
    "title": "Generate random samples from a chi-squared distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html",
    "title": "Read a Text file into a Spark DataFrame",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html#spark_read_text",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html#spark_read_text",
    "title": "Read a Text file into a Spark DataFrame",
    "section": "spark_read_text",
    "text": "spark_read_text"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html#description",
    "title": "Read a Text file into a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRead a text file into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html#usage",
    "title": "Read a Text file into a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nspark_read_text( \n  sc, \n  name = NULL, \n  path = name, \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE, \n  options = list(), \n  whole = FALSE, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html#arguments",
    "title": "Read a Text file into a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\noptions\nA list of strings with additional options.\n\n\nwhole\nRead the entire text file as a single entry? Defaults to FALSE.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html#details",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html#details",
    "title": "Read a Text file into a Spark DataFrame",
    "section": "Details",
    "text": "Details\nYou can read data from HDFS (hdfs://), S3 (s3a://), as well as the local file system (file://).\nIf you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf spark.hadoop.fs.s3a.access.key, spark.hadoop.fs.s3a.secret.key or any of the methods outlined in the aws-sdk documentation Working with AWS credentials\nIn order to work with the newer s3a:// protocol also set the values for spark.hadoop.fs.s3a.impl and spark.hadoop.fs.s3a.endpoint. In addition, to support v4 of the S3 api be sure to pass the -Dcom.amazonaws.services.s3.enableV4 driver options for the config key spark.driver.extraJavaOptions\nFor instructions on how to configure s3n:// check the hadoop documentation: s3n authentication properties"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html#see-also",
    "title": "Read a Text file into a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_view.html",
    "href": "packages/sparklyr/latest/reference/stream_view.html",
    "title": "View Stream",
    "section": "",
    "text": "R/stream_view.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_view.html#stream_view",
    "href": "packages/sparklyr/latest/reference/stream_view.html#stream_view",
    "title": "View Stream",
    "section": "stream_view",
    "text": "stream_view"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_view.html#description",
    "href": "packages/sparklyr/latest/reference/stream_view.html#description",
    "title": "View Stream",
    "section": "Description",
    "text": "Description\nOpens a Shiny gadget to visualize the given stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_view.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_view.html#usage",
    "title": "View Stream",
    "section": "Usage",
    "text": "Usage\nstream_view(stream, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_view.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_view.html#arguments",
    "title": "View Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nstream\nThe stream to visualize.\n\n\n…\nAdditional optional arguments."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_view.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_view.html#examples",
    "title": "View Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\") \ndir.create(\"iris-in\") \nwrite.csv(iris, \"iris-in/iris.csv\", row.names = FALSE) \nstream_read_csv(sc, \"iris-in/\") %>% \n  stream_write_csv(\"iris-out/\") %>% \n  stream_view() %>% \n  stream_stop()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write.html",
    "href": "packages/sparklyr/latest/reference/spark_write.html",
    "title": "Write Spark DataFrame to file using a custom writer",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write.html#spark_write",
    "href": "packages/sparklyr/latest/reference/spark_write.html#spark_write",
    "title": "Write Spark DataFrame to file using a custom writer",
    "section": "spark_write",
    "text": "spark_write"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write.html#description",
    "title": "Write Spark DataFrame to file using a custom writer",
    "section": "Description",
    "text": "Description\nRun a custom R function on Spark worker to write a Spark DataFrame into file(s). If Spark’s speculative execution feature is enabled (i.e., spark.speculation is true), then each write task may be executed more than once and the user-defined writer function will need to ensure no concurrent writes happen to the same file path (e.g., by appending UUID to each file name)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write.html#usage",
    "title": "Write Spark DataFrame to file using a custom writer",
    "section": "Usage",
    "text": "Usage\nspark_write(x, writer, paths, packages = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write.html#arguments",
    "title": "Write Spark DataFrame to file using a custom writer",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark Dataframe to be saved into file(s)\n\n\nwriter\nA writer function with the signature function(partition, path) where partition is a R dataframe containing all rows from one partition of the original Spark Dataframe x and path is a string specifying the file to write partition to\n\n\npaths\nA single destination path or a list of destination paths, each one specifying a location for a partition from x to be written to. If number of partition(s) in x is not equal to length(paths) then x will be re-partitioned to contain length(paths) partition(s)\n\n\npackages\nBoolean to distribute .libPaths() packages to each node, a list of packages to distribute, or a package bundle created with"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write.html#examples",
    "href": "packages/sparklyr/latest/reference/spark_write.html#examples",
    "title": "Write Spark DataFrame to file using a custom writer",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local[3]\") \n# copy some test data into a Spark Dataframe \nsdf <- sdf_copy_to(sc, iris, overwrite = TRUE) \n# create a writer function \nwriter <- function(df, path) { \n  write.csv(df, path) \n} \nspark_write( \n  sdf, \n  writer, \n  # re-partition sdf into 3 partitions and write them to 3 separate files \n  paths = list(\"file:///tmp/file1\", \"file:///tmp/file2\", \"file:///tmp/file3\"), \n) \n#> list()\nspark_write( \n  sdf, \n  writer, \n  # save all rows into a single file \n  paths = list(\"file:///tmp/all_rows\") \n) \n#> list()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_debug_string.html",
    "href": "packages/sparklyr/latest/reference/sdf_debug_string.html",
    "title": "Debug Info for Spark DataFrame",
    "section": "",
    "text": "R/sdf_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_debug_string.html#sdf_debug_string",
    "href": "packages/sparklyr/latest/reference/sdf_debug_string.html#sdf_debug_string",
    "title": "Debug Info for Spark DataFrame",
    "section": "sdf_debug_string",
    "text": "sdf_debug_string"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_debug_string.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_debug_string.html#description",
    "title": "Debug Info for Spark DataFrame",
    "section": "Description",
    "text": "Description\nPrints plan of execution to generate x. This plan will, among other things, show the number of partitions in parenthesis at the far left and indicate stages using indentation."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_debug_string.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_debug_string.html#usage",
    "title": "Debug Info for Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_debug_string(x, print = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_debug_string.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_debug_string.html#arguments",
    "title": "Debug Info for Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn R object wrapping, or containing, a Spark DataFrame.\n\n\nprint\nPrint debug information?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html",
    "title": "Read Socket Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html#stream_read_socket",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html#stream_read_socket",
    "title": "Read Socket Stream",
    "section": "stream_read_socket",
    "text": "stream_read_socket"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html#description",
    "title": "Read Socket Stream",
    "section": "Description",
    "text": "Description\nReads a Socket stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html#usage",
    "title": "Read Socket Stream",
    "section": "Usage",
    "text": "Usage\nstream_read_socket(sc, name = NULL, columns = NULL, options = list(), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html#arguments",
    "title": "Read Socket Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html#examples",
    "title": "Read Socket Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \n# Start socket server from terminal, example: nc -lk 9999 \nstream <- stream_read_socket(sc, options = list(host = \"localhost\", port = 9999)) \nstream"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html#see-also",
    "title": "Read Socket Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html",
    "title": "Feature Transformation – Word2Vec (Estimator)",
    "section": "",
    "text": "R/ml_feature_word2vec.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#ft_word2vec",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#ft_word2vec",
    "title": "Feature Transformation – Word2Vec (Estimator)",
    "section": "ft_word2vec",
    "text": "ft_word2vec"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#description",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#description",
    "title": "Feature Transformation – Word2Vec (Estimator)",
    "section": "Description",
    "text": "Description\nWord2Vec transforms a word into a code for further natural language processing or machine learning process."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#usage",
    "title": "Feature Transformation – Word2Vec (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_word2vec( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  vector_size = 100, \n  min_count = 5, \n  max_sentence_length = 1000, \n  num_partitions = 1, \n  step_size = 0.025, \n  max_iter = 1, \n  seed = NULL, \n  uid = random_string(\"word2vec_\"), \n  ... \n) \n\nml_find_synonyms(model, word, num)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#arguments",
    "title": "Feature Transformation – Word2Vec (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nvector_size\nThe dimension of the code that you want to transform from words. Default: 100\n\n\nmin_count\nThe minimum number of times a token must appear to be included in the word2vec model’s vocabulary. Default: 5\n\n\nmax_sentence_length\n(Spark 2.0.0+) Sets the maximum length (in words) of each sentence in the input data. Any sentence longer than this threshold will be divided into chunks of up to max_sentence_length size. Default: 1000\n\n\nnum_partitions\nNumber of partitions for sentences of words. Default: 1\n\n\nstep_size\nParam for Step size to be used for each iteration of optimization (> 0).\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused.\n\n\nmodel\nA fitted Word2Vec model, returned by ft_word2vec().\n\n\nword\nA word, as a length-one character vector.\n\n\nnum\nNumber of words closest in similarity to the given word to find."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#details",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#details",
    "title": "Feature Transformation – Word2Vec (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#value",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#value",
    "title": "Feature Transformation – Word2Vec (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark\n\nml_find_synonyms() returns a DataFrame of synonyms and cosine similarities"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#see-also",
    "title": "Feature Transformation – Word2Vec (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html",
    "title": "Partition a Spark Dataframe",
    "section": "",
    "text": "R/sdf_ml.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#sdf_random_split",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#sdf_random_split",
    "title": "Partition a Spark Dataframe",
    "section": "sdf_random_split",
    "text": "sdf_random_split"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#description",
    "title": "Partition a Spark Dataframe",
    "section": "Description",
    "text": "Description\nPartition a Spark DataFrame into multiple groups. This routine is useful for splitting a DataFrame into, for example, training and test datasets."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#usage",
    "title": "Partition a Spark Dataframe",
    "section": "Usage",
    "text": "Usage\nsdf_random_split( \n  x, \n  ..., \n  weights = NULL, \n  seed = sample(.Machine$integer.max, 1) \n) \n\nsdf_partition(x, ..., weights = NULL, seed = sample(.Machine$integer.max, 1))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#arguments",
    "title": "Partition a Spark Dataframe",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\n…\nNamed parameters, mapping table names to weights. The weights will be normalized such that they sum to 1.\n\n\nweights\nAn alternate mechanism for supplying weights – when specified, this takes precedence over the ... arguments.\n\n\nseed\nRandom seed to use for randomly partitioning the dataset. Set this if you want your partitioning to be reproducible on repeated runs."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#details",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#details",
    "title": "Partition a Spark Dataframe",
    "section": "Details",
    "text": "Details\nThe sampling weights define the probability that a particular observation will be assigned to a particular partition, not the resulting size of the partition. This implies that partitioning a DataFrame with, for example,\nsdf_random_split(x, training = 0.5, test = 0.5)\nis not guaranteed to produce training and test partitions of equal size."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#section",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#section",
    "title": "Partition a Spark Dataframe",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#transforming-spark-dataframes",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#transforming-spark-dataframes",
    "title": "Partition a Spark Dataframe",
    "section": "Transforming Spark DataFrames",
    "text": "Transforming Spark DataFrames\nThe family of functions prefixed with sdf_ generally access the Scala Spark DataFrame API directly, as opposed to the dplyr interface which uses Spark SQL. These functions will ‘force’ any pending SQL in a dplyr pipeline, such that the resulting tbl_spark object returned will no longer have the attached ‘lazy’ SQL operations. Note that the underlying Spark DataFrame does execute its operations lazily, so that even though the pending set of operations (currently) are not exposed at the R level, these operations will only be executed when you explicitly collect() the table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#value",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#value",
    "title": "Partition a Spark Dataframe",
    "section": "Value",
    "text": "Value\nAn R list of tbl_sparks."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#examples",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#examples",
    "title": "Partition a Spark Dataframe",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n# randomly partition data into a 'training' and 'test' \n# dataset, with 60% of the observations assigned to the \n# 'training' dataset, and 40% assigned to the 'test' dataset \ndata(diamonds, package = \"ggplot2\") \ndiamonds_tbl <- copy_to(sc, diamonds, \"diamonds\") \npartitions <- diamonds_tbl %>% \n  sdf_random_split(training = 0.6, test = 0.4) \nprint(partitions) \n# alternate way of specifying weights \nweights <- c(training = 0.6, test = 0.4) \ndiamonds_tbl %>% sdf_random_split(weights = weights)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#see-also",
    "title": "Partition a Spark Dataframe",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_copy_to(), sdf_distinct(), sdf_register(), sdf_sample(), sdf_sort(), sdf_weighted_sample()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html",
    "title": "Tidying methods for Spark ML Logistic Regression",
    "section": "",
    "text": "R/tidiers_ml_logistic_regression.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#ml_logistic_regression_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#ml_logistic_regression_tidiers",
    "title": "Tidying methods for Spark ML Logistic Regression",
    "section": "ml_logistic_regression_tidiers",
    "text": "ml_logistic_regression_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#description",
    "title": "Tidying methods for Spark ML Logistic Regression",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#usage",
    "title": "Tidying methods for Spark ML Logistic Regression",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_logistic_regression'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_logistic_regression'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class '`_ml_model_logistic_regression`'\naugment(x, new_data = NULL, ...) \n\n## S3 method for class 'ml_model_logistic_regression'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML Logistic Regression",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction.\n\n\nnew_data\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.spark_connection.html",
    "href": "packages/sparklyr/latest/reference/copy_to.spark_connection.html",
    "title": "Copy an R Data Frame to Spark",
    "section": "",
    "text": "R/dplyr_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#copy_to.spark_connection",
    "href": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#copy_to.spark_connection",
    "title": "Copy an R Data Frame to Spark",
    "section": "copy_to.spark_connection",
    "text": "copy_to.spark_connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#description",
    "href": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#description",
    "title": "Copy an R Data Frame to Spark",
    "section": "Description",
    "text": "Description\nCopy an R data.frame to Spark, and return a reference to the generated Spark DataFrame as a tbl_spark. The returned object will act as a dplyr-compatible interface to the underlying Spark table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#usage",
    "href": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#usage",
    "title": "Copy an R Data Frame to Spark",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'spark_connection'\ncopy_to( \n  dest, \n  df, \n  name = spark_table_name(substitute(df)), \n  overwrite = FALSE, \n  memory = TRUE, \n  repartition = 0L, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#arguments",
    "href": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#arguments",
    "title": "Copy an R Data Frame to Spark",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\ndest\nA spark_connection.\n\n\ndf\nAn R data.frame.\n\n\nname\nThe name to assign to the copied table in Spark.\n\n\noverwrite\nBoolean; overwrite a pre-existing table with the name nameif one already exists?\n\n\nmemory\nBoolean; should the table be cached into memory?\n\n\nrepartition\nThe number of partitions to use when distributing the table across the Spark cluster. The default (0) can be used to avoid partitioning.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#value",
    "href": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#value",
    "title": "Copy an R Data Frame to Spark",
    "section": "Value",
    "text": "Value\nA tbl_spark, representing a dplyr-compatible interface to a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/na.replace.html",
    "href": "packages/sparklyr/latest/reference/na.replace.html",
    "title": "Replace Missing Values in Objects",
    "section": "",
    "text": "R/na_actions.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/na.replace.html#na.replace",
    "href": "packages/sparklyr/latest/reference/na.replace.html#na.replace",
    "title": "Replace Missing Values in Objects",
    "section": "na.replace",
    "text": "na.replace"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/na.replace.html#description",
    "href": "packages/sparklyr/latest/reference/na.replace.html#description",
    "title": "Replace Missing Values in Objects",
    "section": "Description",
    "text": "Description\nThis S3 generic provides an interface for replacing NA values within an object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/na.replace.html#usage",
    "href": "packages/sparklyr/latest/reference/na.replace.html#usage",
    "title": "Replace Missing Values in Objects",
    "section": "Usage",
    "text": "Usage\nna.replace(object, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/na.replace.html#arguments",
    "href": "packages/sparklyr/latest/reference/na.replace.html#arguments",
    "title": "Replace Missing Values in Objects",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nobject\nAn R object.\n\n\n…\nArguments passed along to implementing methods."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html",
    "title": "Spark ML – Tuning",
    "section": "",
    "text": "R/ml_tuning.R, R/ml_tuning_cross_validator.R,"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#ml-tuning",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#ml-tuning",
    "title": "Spark ML – Tuning",
    "section": "ml-tuning",
    "text": "ml-tuning"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#description",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#description",
    "title": "Spark ML – Tuning",
    "section": "Description",
    "text": "Description\nPerform hyper-parameter tuning using either K-fold cross validation or train-validation split."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#usage",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#usage",
    "title": "Spark ML – Tuning",
    "section": "Usage",
    "text": "Usage\nml_sub_models(model) \n\nml_validation_metrics(model) \n\nml_cross_validator( \n  x, \n  estimator = NULL, \n  estimator_param_maps = NULL, \n  evaluator = NULL, \n  num_folds = 3, \n  collect_sub_models = FALSE, \n  parallelism = 1, \n  seed = NULL, \n  uid = random_string(\"cross_validator_\"), \n  ... \n) \n\nml_train_validation_split( \n  x, \n  estimator = NULL, \n  estimator_param_maps = NULL, \n  evaluator = NULL, \n  train_ratio = 0.75, \n  collect_sub_models = FALSE, \n  parallelism = 1, \n  seed = NULL, \n  uid = random_string(\"train_validation_split_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#arguments",
    "title": "Spark ML – Tuning",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nmodel\nA cross validation or train-validation-split model.\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nestimator\nA ml_estimator object.\n\n\nestimator_param_maps\nA named list of stages and hyper-parameter sets to tune. See details.\n\n\nevaluator\nA ml_evaluator object, see ml_evaluator.\n\n\nnum_folds\nNumber of folds for cross validation. Must be >= 2. Default: 3\n\n\ncollect_sub_models\nWhether to collect a list of sub-models trained during tuning. If set to FALSE, then only the single best sub-model will be available after fitting. If set to true, then all sub-models will be available. Warning: For large models, collecting all sub-models can cause OOMs on the Spark driver.\n\n\nparallelism\nThe number of threads to use when running parallel algorithms. Default is 1 for serial execution.\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; currently unused.\n\n\ntrain_ratio\nRatio between train and validation data. Must be between 0 and 1. Default: 0.75"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#details",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#details",
    "title": "Spark ML – Tuning",
    "section": "Details",
    "text": "Details\nml_cross_validator() performs k-fold cross validation while ml_train_validation_split() performs tuning on one pair of train and validation datasets."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#value",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#value",
    "title": "Spark ML – Tuning",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_cross_validator or ml_traing_validation_split object.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the tuning estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a tuning estimator is constructed then immediately fit with the input tbl_spark, returning a ml_cross_validation_model or a ml_train_validation_split_model object.\n\nFor cross validation, ml_sub_models() returns a nested list of models, where the first layer represents fold indices and the second layer represents param maps. For train-validation split, ml_sub_models() returns a list of models, corresponding to the order of the estimator param maps.\nml_validation_metrics() returns a data frame of performance metrics and hyperparameter combinations."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#examples",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#examples",
    "title": "Spark ML – Tuning",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n# Create a pipeline \npipeline <- ml_pipeline(sc) %>% \n  ft_r_formula(Species ~ .) %>% \n  ml_random_forest_classifier() \n# Specify hyperparameter grid \ngrid <- list( \n  random_forest = list( \n    num_trees = c(5, 10), \n    max_depth = c(5, 10), \n    impurity = c(\"entropy\", \"gini\") \n  ) \n) \n# Create the cross validator object \ncv <- ml_cross_validator( \n  sc, \n  estimator = pipeline, estimator_param_maps = grid, \n  evaluator = ml_multiclass_classification_evaluator(sc), \n  num_folds = 3, \n  parallelism = 4 \n) \n# Train the models \ncv_model <- ml_fit(cv, iris_tbl) \n# Print the metrics \nml_validation_metrics(cv_model) \n#>          f1 impurity_1 num_trees_1 max_depth_1\n#> 1 0.9397887    entropy           5           5\n#> 2 0.9056526    entropy          10           5\n#> 3 0.9397887    entropy           5          10\n#> 4 0.9056526    entropy          10          10\n#> 5 0.9397887       gini           5           5\n#> 6 0.9127092       gini          10           5\n#> 7 0.9397887       gini           5          10\n#> 8 0.9127092       gini          10          10"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html",
    "title": "Retrieves or sets initial number of shuffle partitions before coalescing",
    "section": "",
    "text": "R/spark_context_config.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#spark_coalesce_initial_num_partitions",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#spark_coalesce_initial_num_partitions",
    "title": "Retrieves or sets initial number of shuffle partitions before coalescing",
    "section": "spark_coalesce_initial_num_partitions",
    "text": "spark_coalesce_initial_num_partitions"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#description",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#description",
    "title": "Retrieves or sets initial number of shuffle partitions before coalescing",
    "section": "Description",
    "text": "Description\nRetrieves or sets initial number of shuffle partitions before coalescing"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#usage",
    "title": "Retrieves or sets initial number of shuffle partitions before coalescing",
    "section": "Usage",
    "text": "Usage\nspark_coalesce_initial_num_partitions(sc, num_partitions = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#arguments",
    "title": "Retrieves or sets initial number of shuffle partitions before coalescing",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nnum_partitions\nInitial number of shuffle partitions before coalescing. Defaults to NULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#see-also",
    "title": "Retrieves or sets initial number of shuffle partitions before coalescing",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_adaptive_query_execution(), spark_advisory_shuffle_partition_size(), spark_auto_broadcast_join_threshold(), spark_coalesce_min_num_partitions(), spark_coalesce_shuffle_partitions(), spark_session_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html",
    "href": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html",
    "title": "Retrieves or sets advisory size of the shuffle partition",
    "section": "",
    "text": "R/spark_context_config.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#spark_advisory_shuffle_partition_size",
    "href": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#spark_advisory_shuffle_partition_size",
    "title": "Retrieves or sets advisory size of the shuffle partition",
    "section": "spark_advisory_shuffle_partition_size",
    "text": "spark_advisory_shuffle_partition_size"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#description",
    "href": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#description",
    "title": "Retrieves or sets advisory size of the shuffle partition",
    "section": "Description",
    "text": "Description\nRetrieves or sets advisory size in bytes of the shuffle partition during adaptive optimization"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#usage",
    "title": "Retrieves or sets advisory size of the shuffle partition",
    "section": "Usage",
    "text": "Usage\nspark_advisory_shuffle_partition_size(sc, size = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#arguments",
    "title": "Retrieves or sets advisory size of the shuffle partition",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nsize\nAdvisory size in bytes of the shuffle partition. Defaults to NULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#see-also",
    "title": "Retrieves or sets advisory size of the shuffle partition",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_adaptive_query_execution(), spark_auto_broadcast_join_threshold(), spark_coalesce_initial_num_partitions(), spark_coalesce_min_num_partitions(), spark_coalesce_shuffle_partitions(), spark_session_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sub-.tbl_spark.html",
    "href": "packages/sparklyr/latest/reference/sub-.tbl_spark.html",
    "title": "Subsetting operator for Spark dataframe",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#tbl_spark",
    "href": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#tbl_spark",
    "title": "Subsetting operator for Spark dataframe",
    "section": "[.tbl_spark",
    "text": "[.tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#description",
    "href": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#description",
    "title": "Subsetting operator for Spark dataframe",
    "section": "Description",
    "text": "Description\nSusetting operator for Spark dataframe allowing a subset of column(s) to be selected using syntaxes similar to those supported by R dataframes"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#usage",
    "href": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#usage",
    "title": "Subsetting operator for Spark dataframe",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'tbl_spark'\n[(x, i)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#arguments",
    "href": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#arguments",
    "title": "Subsetting operator for Spark dataframe",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark dataframe\n\n\ni\nExpression specifying subset of column(s) to include or exclude from the result (e.g., [\"col1\"], [c(\"col1\", \"col2\")], [1:10], [-1], [NULL], or [])"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#examples",
    "href": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#examples",
    "title": "Subsetting operator for Spark dataframe",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc <- spark_connect(master = \"spark://HOST:PORT\") \nexample_sdf <- copy_to(sc, tibble::tibble(a = 1, b = 2)) \nexample_sdf[\"a\"] %>% print() \n#> # A tibble: 1 × 1\n#>       a\n#>   <dbl>\n#> 1     1"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgeom.html",
    "href": "packages/sparklyr/latest/reference/sdf_rgeom.html",
    "title": "Generate random samples from a geometric distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgeom.html#sdf_rgeom",
    "href": "packages/sparklyr/latest/reference/sdf_rgeom.html#sdf_rgeom",
    "title": "Generate random samples from a geometric distribution",
    "section": "sdf_rgeom",
    "text": "sdf_rgeom"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgeom.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rgeom.html#description",
    "title": "Generate random samples from a geometric distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a geometric distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgeom.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rgeom.html#usage",
    "title": "Generate random samples from a geometric distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rgeom(sc, n, prob, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgeom.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rgeom.html#arguments",
    "title": "Generate random samples from a geometric distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nprob\nProbability of success in each trial.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgeom.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rgeom.html#see-also",
    "title": "Generate random samples from a geometric distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/separate.html",
    "href": "packages/sparklyr/latest/reference/separate.html",
    "title": "Separate",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/separate.html#separate",
    "href": "packages/sparklyr/latest/reference/separate.html#separate",
    "title": "Separate",
    "section": "separate",
    "text": "separate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/separate.html#description",
    "href": "packages/sparklyr/latest/reference/separate.html#description",
    "title": "Separate",
    "section": "Description",
    "text": "Description\nSee separate for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_tree_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_tree_tidiers.html",
    "title": "Tidying methods for Spark ML tree models",
    "section": "",
    "text": "R/tidiers_ml_tree_models.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#ml_tree_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#ml_tree_tidiers",
    "title": "Tidying methods for Spark ML tree models",
    "section": "ml_tree_tidiers",
    "text": "ml_tree_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#description",
    "title": "Tidying methods for Spark ML tree models",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#usage",
    "title": "Tidying methods for Spark ML tree models",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_decision_tree_classification'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_decision_tree_regression'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_decision_tree_classification'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class '`_ml_model_decision_tree_classification`'\naugment(x, new_data = NULL, ...) \n\n## S3 method for class 'ml_model_decision_tree_regression'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class '`_ml_model_decision_tree_regression`'\naugment(x, new_data = NULL, ...) \n\n## S3 method for class 'ml_model_decision_tree_classification'\nglance(x, ...) \n\n## S3 method for class 'ml_model_decision_tree_regression'\nglance(x, ...) \n\n## S3 method for class 'ml_model_random_forest_classification'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_random_forest_regression'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_random_forest_classification'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class '`_ml_model_random_forest_classification`'\naugment(x, new_data = NULL, ...) \n\n## S3 method for class 'ml_model_random_forest_regression'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class '`_ml_model_random_forest_regression`'\naugment(x, new_data = NULL, ...) \n\n## S3 method for class 'ml_model_random_forest_classification'\nglance(x, ...) \n\n## S3 method for class 'ml_model_random_forest_regression'\nglance(x, ...) \n\n## S3 method for class 'ml_model_gbt_classification'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_gbt_regression'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_gbt_classification'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class '`_ml_model_gbt_classification`'\naugment(x, new_data = NULL, ...) \n\n## S3 method for class 'ml_model_gbt_regression'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class '`_ml_model_gbt_regression`'\naugment(x, new_data = NULL, ...) \n\n## S3 method for class 'ml_model_gbt_classification'\nglance(x, ...) \n\n## S3 method for class 'ml_model_gbt_regression'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML tree models",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction.\n\n\nnew_data\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pivot_longer.html",
    "href": "packages/sparklyr/latest/reference/pivot_longer.html",
    "title": "Pivot longer",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pivot_longer.html#pivot_longer",
    "href": "packages/sparklyr/latest/reference/pivot_longer.html#pivot_longer",
    "title": "Pivot longer",
    "section": "pivot_longer",
    "text": "pivot_longer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pivot_longer.html#description",
    "href": "packages/sparklyr/latest/reference/pivot_longer.html#description",
    "title": "Pivot longer",
    "section": "Description",
    "text": "Description\nSee pivot_longer for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html",
    "title": "Spark ML – Decision Trees",
    "section": "",
    "text": "R/ml_classification_decision_tree_classifier.R,"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#ml_decision_tree_classifier",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#ml_decision_tree_classifier",
    "title": "Spark ML – Decision Trees",
    "section": "ml_decision_tree_classifier",
    "text": "ml_decision_tree_classifier"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#description",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#description",
    "title": "Spark ML – Decision Trees",
    "section": "Description",
    "text": "Description\nPerform classification and regression using decision trees."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#usage",
    "title": "Spark ML – Decision Trees",
    "section": "Usage",
    "text": "Usage\nml_decision_tree_classifier( \n  x, \n  formula = NULL, \n  max_depth = 5, \n  max_bins = 32, \n  min_instances_per_node = 1, \n  min_info_gain = 0, \n  impurity = \"gini\", \n  seed = NULL, \n  thresholds = NULL, \n  cache_node_ids = FALSE, \n  checkpoint_interval = 10, \n  max_memory_in_mb = 256, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  uid = random_string(\"decision_tree_classifier_\"), \n  ... \n) \n\nml_decision_tree( \n  x, \n  formula = NULL, \n  type = c(\"auto\", \"regression\", \"classification\"), \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  variance_col = NULL, \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  checkpoint_interval = 10L, \n  impurity = \"auto\", \n  max_bins = 32L, \n  max_depth = 5L, \n  min_info_gain = 0, \n  min_instances_per_node = 1L, \n  seed = NULL, \n  thresholds = NULL, \n  cache_node_ids = FALSE, \n  max_memory_in_mb = 256L, \n  uid = random_string(\"decision_tree_\"), \n  response = NULL, \n  features = NULL, \n  ... \n) \n\nml_decision_tree_regressor( \n  x, \n  formula = NULL, \n  max_depth = 5, \n  max_bins = 32, \n  min_instances_per_node = 1, \n  min_info_gain = 0, \n  impurity = \"variance\", \n  seed = NULL, \n  cache_node_ids = FALSE, \n  checkpoint_interval = 10, \n  max_memory_in_mb = 256, \n  variance_col = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"decision_tree_regressor_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#arguments",
    "title": "Spark ML – Decision Trees",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nmax_depth\nMaximum depth of the tree (>= 0); that is, the maximum number of nodes separating any leaves from the root of the tree.\n\n\nmax_bins\nThe maximum number of bins used for discretizing continuous features and for choosing how to split on features at each node. More bins give higher granularity.\n\n\nmin_instances_per_node\nMinimum number of instances each child must have after split.\n\n\nmin_info_gain\nMinimum information gain for a split to be considered at a tree node. Should be >= 0, defaults to 0.\n\n\nimpurity\nCriterion used for information gain calculation. Supported: “entropy” and “gini” (default) for classification and “variance” (default) for regression. For ml_decision_tree, setting \"auto\" will default to the appropriate criterion based on model type.\n\n\nseed\nSeed for random numbers.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\ncache_node_ids\nIf FALSE, the algorithm will pass trees to executors to match instances with nodes. If TRUE, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Defaults to FALSE.\n\n\ncheckpoint_interval\nSet checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.\n\n\nmax_memory_in_mb\nMaximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. Defaults to 256.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details.\n\n\ntype\nThe type of model to fit. \"regression\" treats the response as a continuous variable, while \"classification\" treats the response as a categorical variable. When \"auto\" is used, the model type is inferred based on the response variable type – if it is a numeric type, then regression is used; classification otherwise.\n\n\nvariance_col\n(Optional) Column name for the biased sample variance of prediction.\n\n\nresponse\n(Deprecated) The name of the response column (as a length-one character vector.)\n\n\nfeatures\n(Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#details",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#details",
    "title": "Spark ML – Decision Trees",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows.\nml_decision_tree is a wrapper around ml_decision_tree_regressor.tbl_spark and ml_decision_tree_classifier.tbl_spark and calls the appropriate method based on model type."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#value",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#value",
    "title": "Spark ML – Decision Trees",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula\nis specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#examples",
    "title": "Spark ML – Decision Trees",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \npartitions <- iris_tbl %>% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \niris_training <- partitions$training \niris_test <- partitions$test \ndt_model <- iris_training %>% \n  ml_decision_tree(Species ~ .) \npred <- ml_predict(dt_model, iris_test) \nml_multiclass_classification_evaluator(pred) \n#> [1] 0.9393939"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#see-also",
    "title": "Spark ML – Decision Trees",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html",
    "title": "Spark ML – OneVsRest",
    "section": "",
    "text": "R/ml_classification_one_vs_rest.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#ml_one_vs_rest",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#ml_one_vs_rest",
    "title": "Spark ML – OneVsRest",
    "section": "ml_one_vs_rest",
    "text": "ml_one_vs_rest"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#description",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#description",
    "title": "Spark ML – OneVsRest",
    "section": "Description",
    "text": "Description\nReduction of Multiclass Classification to Binary Classification. Performs reduction using one against all strategy. For a multiclass classification with k classes, train k models (one per class). Each example is scored against all k models and the model with highest score is picked to label the example."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#usage",
    "title": "Spark ML – OneVsRest",
    "section": "Usage",
    "text": "Usage\nml_one_vs_rest( \n  x, \n  formula = NULL, \n  classifier = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"one_vs_rest_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#arguments",
    "title": "Spark ML – OneVsRest",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nclassifier\nObject of class ml_estimator. Base binary classifier that we reduce multiclass classification into.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#details",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#details",
    "title": "Spark ML – OneVsRest",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#value",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#value",
    "title": "Spark ML – OneVsRest",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula\nis specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#see-also",
    "title": "Spark ML – OneVsRest",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html",
    "title": "Feature Transformation – Normalizer (Transformer)",
    "section": "",
    "text": "R/ml_feature_normalizer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html#ft_normalizer",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html#ft_normalizer",
    "title": "Feature Transformation – Normalizer (Transformer)",
    "section": "ft_normalizer",
    "text": "ft_normalizer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html#description",
    "title": "Feature Transformation – Normalizer (Transformer)",
    "section": "Description",
    "text": "Description\nNormalize a vector to have unit norm using the given p-norm."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html#usage",
    "title": "Feature Transformation – Normalizer (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_normalizer( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  p = 2, \n  uid = random_string(\"normalizer_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html#arguments",
    "title": "Feature Transformation – Normalizer (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\np\nNormalization in L^p space. Must be >= 1. Defaults to 2.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html#value",
    "title": "Feature Transformation – Normalizer (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html#see-also",
    "title": "Feature Transformation – Normalizer (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_regression.html",
    "href": "packages/sparklyr/latest/reference/ml_metrics_regression.html",
    "title": "Extracts metrics from a fitted table",
    "section": "",
    "text": "R/ml_metrics.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_regression.html#ml_metrics_regression",
    "href": "packages/sparklyr/latest/reference/ml_metrics_regression.html#ml_metrics_regression",
    "title": "Extracts metrics from a fitted table",
    "section": "ml_metrics_regression",
    "text": "ml_metrics_regression"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_regression.html#description",
    "href": "packages/sparklyr/latest/reference/ml_metrics_regression.html#description",
    "title": "Extracts metrics from a fitted table",
    "section": "Description",
    "text": "Description\nThe function works best when passed a tbl_spark created by ml_predict(). The output tbl_spark will contain the correct variable types and format that the given Spark model “evaluator” expects."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_regression.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_metrics_regression.html#usage",
    "title": "Extracts metrics from a fitted table",
    "section": "Usage",
    "text": "Usage\nml_metrics_regression( \n  x, \n  truth, \n  estimate = prediction, \n  metrics = c(\"rmse\", \"rsq\", \"mae\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_regression.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_metrics_regression.html#arguments",
    "title": "Extracts metrics from a fitted table",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA tbl_spark containing the estimate (prediction) and the truth (value of what actually happened)\n\n\ntruth\nThe name of the column from x that contains the value of what actually happened\n\n\nestimate\nThe name of the column from x that contains the prediction. Defaults to prediction, since it is the default that ml_predict() uses.\n\n\nmetrics\nA character vector with the metrics to calculate. For regression models the possible values are: rmse (Root mean squared error), mse (Mean squared error),rsq (R squared), mae (Mean absolute error), and var (Explained variance). Defaults to: rmse, rsq, mae\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_regression.html#details",
    "href": "packages/sparklyr/latest/reference/ml_metrics_regression.html#details",
    "title": "Extracts metrics from a fitted table",
    "section": "Details",
    "text": "Details\nThe ml_metrics family of functions implement Spark’s evaluate closer to how the yardstick package works. The functions expect a table containing the truth and estimate, and return a tibble with the results. The tibble has the same format and variable names as the output of the yardstick functions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_regression.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_metrics_regression.html#examples",
    "title": "Extracts metrics from a fitted table",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(\"local\") \ntbl_iris <- copy_to(sc, iris) \niris_split <- sdf_random_split(tbl_iris, training = 0.5, test = 0.5) \ntraining <- iris_split$training \nreg_formula <- \"Sepal_Length ~ Sepal_Width + Petal_Length + Petal_Width\" \nmodel <- ml_generalized_linear_regression(training, reg_formula) \ntbl_predictions <- ml_predict(model, iris_split$test) \ntbl_predictions %>% \n  ml_metrics_regression(Sepal_Length) \n#> # A tibble: 3 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard       0.335\n#> 2 rsq     standard       0.825\n#> 3 mae     standard       0.273"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_uid.html",
    "href": "packages/sparklyr/latest/reference/ml_uid.html",
    "title": "Spark ML – UID",
    "section": "",
    "text": "R/ml_pipeline_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_uid.html#ml_uid",
    "href": "packages/sparklyr/latest/reference/ml_uid.html#ml_uid",
    "title": "Spark ML – UID",
    "section": "ml_uid",
    "text": "ml_uid"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_uid.html#description",
    "href": "packages/sparklyr/latest/reference/ml_uid.html#description",
    "title": "Spark ML – UID",
    "section": "Description",
    "text": "Description\nExtracts the UID of an ML object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_uid.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_uid.html#usage",
    "title": "Spark ML – UID",
    "section": "Usage",
    "text": "Usage\nml_uid(x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_uid.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_uid.html#arguments",
    "title": "Spark ML – UID",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark ML object"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection.html",
    "href": "packages/sparklyr/latest/reference/spark_connection.html",
    "title": "Retrieve the Spark Connection Associated with an R Object",
    "section": "",
    "text": "R/spark_connection.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection.html#spark_connection",
    "href": "packages/sparklyr/latest/reference/spark_connection.html#spark_connection",
    "title": "Retrieve the Spark Connection Associated with an R Object",
    "section": "spark_connection",
    "text": "spark_connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection.html#description",
    "href": "packages/sparklyr/latest/reference/spark_connection.html#description",
    "title": "Retrieve the Spark Connection Associated with an R Object",
    "section": "Description",
    "text": "Description\nRetrieve the spark_connection associated with an R object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_connection.html#usage",
    "title": "Retrieve the Spark Connection Associated with an R Object",
    "section": "Usage",
    "text": "Usage\nspark_connection(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_connection.html#arguments",
    "title": "Retrieve the Spark Connection Associated with an R Object",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn R object from which a spark_connection can be obtained.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html",
    "href": "packages/sparklyr/latest/reference/ml_lda.html",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "",
    "text": "R/ml_clustering_lda.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#ml_lda",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#ml_lda",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "ml_lda",
    "text": "ml_lda"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#description",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#description",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "Description",
    "text": "Description\nLatent Dirichlet Allocation (LDA), a topic model designed for text documents."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#usage",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "Usage",
    "text": "Usage\nml_lda( \n  x, \n  formula = NULL, \n  k = 10, \n  max_iter = 20, \n  doc_concentration = NULL, \n  topic_concentration = NULL, \n  subsampling_rate = 0.05, \n  optimizer = \"online\", \n  checkpoint_interval = 10, \n  keep_last_checkpoint = TRUE, \n  learning_decay = 0.51, \n  learning_offset = 1024, \n  optimize_doc_concentration = TRUE, \n  seed = NULL, \n  features_col = \"features\", \n  topic_distribution_col = \"topicDistribution\", \n  uid = random_string(\"lda_\"), \n  ... \n) \n\nml_describe_topics(model, max_terms_per_topic = 10) \n\nml_log_likelihood(model, dataset) \n\nml_log_perplexity(model, dataset) \n\nml_topics_matrix(model)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#arguments",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nk\nThe number of clusters to create\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\ndoc_concentration\nConcentration parameter (commonly named “alpha”) for the prior placed on documents’ distributions over topics (“theta”). See details.\n\n\ntopic_concentration\nConcentration parameter (commonly named “beta” or “eta”) for the prior placed on topics’ distributions over terms.\n\n\nsubsampling_rate\n(For Online optimizer only) Fraction of the corpus to be sampled and used in each iteration of mini-batch gradient descent, in range (0, 1]. Note that this should be adjusted in synch with max_iter so the entire corpus is used. Specifically, set both so that maxIterations * miniBatchFraction greater than or equal to 1.\n\n\noptimizer\nOptimizer or inference algorithm used to estimate the LDA model. Supported: “online” for Online Variational Bayes (default) and “em” for Expectation-Maximization.\n\n\ncheckpoint_interval\nSet checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.\n\n\nkeep_last_checkpoint\n(Spark 2.0.0+) (For EM optimizer only) If using checkpointing, this indicates whether to keep the last checkpoint. If FALSE, then the checkpoint will be deleted. Deleting the checkpoint can cause failures if a data partition is lost, so set this bit with care. Note that checkpoints will be cleaned up via reference counting, regardless.\n\n\nlearning_decay\n(For Online optimizer only) Learning rate, set as an exponential decay rate. This should be between (0.5, 1.0] to guarantee asymptotic convergence. This is called “kappa” in the Online LDA paper (Hoffman et al., 2010). Default: 0.51, based on Hoffman et al.\n\n\nlearning_offset\n(For Online optimizer only) A (positive) learning parameter that downweights early iterations. Larger values make early iterations count less. This is called “tau0” in the Online LDA paper (Hoffman et al., 2010) Default: 1024, following Hoffman et al.\n\n\noptimize_doc_concentration\n(For Online optimizer only) Indicates whether the doc_concentration (Dirichlet parameter for document-topic distribution) will be optimized during training. Setting this to true will make the model more expressive and fit the training data better. Default: FALSE\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\ntopic_distribution_col\nOutput column with estimates of the topic mixture distribution for each document (often called “theta” in the literature). Returns a vector of zeros for an empty document.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments, see Details.\n\n\nmodel\nA fitted LDA model returned by ml_lda().\n\n\nmax_terms_per_topic\nMaximum number of terms to collect for each topic. Default value of 10.\n\n\ndataset\ntest corpus to use for calculating log likelihood or log perplexity"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#details",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#details",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "Details",
    "text": "Details\nFor ml_lda.tbl_spark with the formula interface, you can specify named arguments in ... that will be passed ft_regex_tokenizer(), ft_stop_words_remover(), and ft_count_vectorizer(). For example, to increase the default min_token_length, you can use ml_lda(dataset, ~ text, min_token_length = 4).\nTerminology for LDA:\n\n“term” = “word”: an element of the vocabulary\n“token”: instance of a term appearing in a document\n“topic”: multinomial distribution over terms representing some concept\n“document”: one piece of text, corresponding to one row in the input data\n\nOriginal LDA paper (journal version): Blei, Ng, and Jordan. “Latent Dirichlet Allocation.” JMLR, 2003.\nInput data (features_col): LDA is given a collection of documents as input data, via the features_col parameter. Each document is specified as a Vector of length vocab_size, where each entry is the count for the corresponding term (word) in the document. Feature transformers such as ft_tokenizer and ft_count_vectorizer can be useful for converting text to word count vectors"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#section",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#section",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#parameter-details",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#parameter-details",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "Parameter details",
    "text": "Parameter details\ndoc_concentration\nThis is the parameter to a Dirichlet distribution, where larger values mean more smoothing (more regularization). If not set by the user, then doc_concentration is set automatically. If set to singleton vector [alpha], then alpha is replicated to a vector of length k in fitting. Otherwise, the doc_concentration vector must be length k. (default = automatic)\nOptimizer-specific parameter settings:\nEM\n\nCurrently only supports symmetric distributions, so all values in the vector should be the same.\nValues should be greater than 1.0\ndefault = uniformly (50 / k) + 1, where 50/k is common in LDA libraries and +1 follows from Asuncion et al. (2009), who recommend a +1 adjustment for EM.\n\nOnline\n\nValues should be greater than or equal to 0\ndefault = uniformly (1.0 / k), following the implementation from here\n\ntopic_concentration\nThis is the parameter to a symmetric Dirichlet distribution.\nNote: The topics’ distributions over terms are called “beta” in the original LDA paper by Blei et al., but are called “phi” in many later papers such as Asuncion et al., 2009.\nIf not set by the user, then topic_concentration is set automatically. (default = automatic)\nOptimizer-specific parameter settings:\nEM\n\nValue should be greater than 1.0\ndefault = 0.1 + 1, where 0.1 gives a small amount of smoothing and +1 follows Asuncion et al. (2009), who recommend a +1 adjustment for EM.\n\nOnline\n\nValue should be greater than or equal to 0\ndefault = (1.0 / k), following the implementation from here.\n\ntopic_distribution_col\nThis uses a variational approximation following Hoffman et al. (2010), where the approximate distribution is called “gamma.” Technically, this method returns this approximation “gamma” for each document."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#value",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#value",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the clustering estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, an estimator is constructed then immediately fit with the input tbl_spark, returning a clustering model.\ntbl_spark, with formula or features specified: When formula\nis specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the estimator. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model. This signature does not apply to ml_lda().\n\nml_describe_topics returns a DataFrame with topics and their top-weighted terms.\nml_log_likelihood calculates a lower bound on the log likelihood of the entire corpus"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#examples",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nlibrary(janeaustenr) \nlibrary(dplyr) \nsc <- spark_connect(master = \"local\") \nlines_tbl <- sdf_copy_to(sc, \n  austen_books()[c(1:30), ], \n  name = \"lines_tbl\", \n  overwrite = TRUE \n) \n# transform the data in a tidy form \nlines_tbl_tidy <- lines_tbl %>% \n  ft_tokenizer( \n    input_col = \"text\", \n    output_col = \"word_list\" \n  ) %>% \n  ft_stop_words_remover( \n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\" \n  ) %>% \n  mutate(text = explode(wo_stop_words)) %>% \n  filter(text != \"\") %>% \n  select(text, book) \nlda_model <- lines_tbl_tidy %>% \n  ml_lda(~text, k = 4) \n# vocabulary and topics \ntidy(lda_model) \n#> New names:\n#> • `` -> `...1`\n#> • `` -> `...2`\n#> • `` -> `...3`\n#> • `` -> `...4`\n#> # A tibble: 388 × 3\n#>    topic term      beta\n#>    <int> <chr>    <dbl>\n#>  1     0 norland  0.607\n#>  2     0 years    0.663\n#>  3     0 lived    2.07 \n#>  4     0 estate   0.615\n#>  5     0 constant 0.703\n#>  6     0 henry    1.96 \n#>  7     0 many     0.600\n#>  8     0 family   1.54 \n#>  9     0 dashwood 0.585\n#> 10     0 nephew   0.721\n#> # … with 378 more rows"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#see-also",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-clustering.html for more information on the set of clustering algorithms.\nOther ml clustering algorithms: ml_bisecting_kmeans(), ml_gaussian_mixture(), ml_kmeans()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_binary.html",
    "href": "packages/sparklyr/latest/reference/ml_metrics_binary.html",
    "title": "Extracts metrics from a fitted table",
    "section": "",
    "text": "R/ml_metrics.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_binary.html#ml_metrics_binary",
    "href": "packages/sparklyr/latest/reference/ml_metrics_binary.html#ml_metrics_binary",
    "title": "Extracts metrics from a fitted table",
    "section": "ml_metrics_binary",
    "text": "ml_metrics_binary"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_binary.html#description",
    "href": "packages/sparklyr/latest/reference/ml_metrics_binary.html#description",
    "title": "Extracts metrics from a fitted table",
    "section": "Description",
    "text": "Description\nThe function works best when passed a tbl_spark created by ml_predict(). The output tbl_spark will contain the correct variable types and format that the given Spark model “evaluator” expects."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_binary.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_metrics_binary.html#usage",
    "title": "Extracts metrics from a fitted table",
    "section": "Usage",
    "text": "Usage\nml_metrics_binary( \n  x, \n  truth = label, \n  estimate = rawPrediction, \n  metrics = c(\"roc_auc\", \"pr_auc\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_binary.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_metrics_binary.html#arguments",
    "title": "Extracts metrics from a fitted table",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA tbl_spark containing the estimate (prediction) and the truth (value of what actually happened)\n\n\ntruth\nThe name of the column from x with an integer field containing the binary response (0 or 1). The ml_predict() function will create a new field named label which contains the expected type and values. truth defaults to label.\n\n\nestimate\nThe name of the column from x that contains the prediction. Defaults to rawPrediction, since its type and expected values will match truth.\n\n\nmetrics\nA character vector with the metrics to calculate. For binary models the possible values are: roc_auc (Area under the Receiver Operator curve), pr_auc (Area under the Precesion Recall curve). Defaults to: roc_auc, pr_auc\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_binary.html#details",
    "href": "packages/sparklyr/latest/reference/ml_metrics_binary.html#details",
    "title": "Extracts metrics from a fitted table",
    "section": "Details",
    "text": "Details\nThe ml_metrics family of functions implement Spark’s evaluate closer to how the yardstick package works. The functions expect a table containing the truth and estimate, and return a tibble with the results. The tibble has the same format and variable names as the output of the yardstick functions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_binary.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_metrics_binary.html#examples",
    "title": "Extracts metrics from a fitted table",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(\"local\") \ntbl_iris <- copy_to(sc, iris) \nprep_iris <- tbl_iris %>% \n  mutate(is_setosa = ifelse(Species == \"setosa\", 1, 0)) \niris_split <- sdf_random_split(prep_iris, training = 0.5, test = 0.5) \nmodel <- ml_logistic_regression(iris_split$training, \"is_setosa ~ Sepal_Length\") \ntbl_predictions <- ml_predict(model, iris_split$test) \nml_metrics_binary(tbl_predictions) \n#> # A tibble: 2 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.983\n#> 2 pr_auc  binary         0.967"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html",
    "title": "Feature Transformation – StringIndexer (Estimator)",
    "section": "",
    "text": "R/ml_feature_string_indexer.R,"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#ft_string_indexer",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#ft_string_indexer",
    "title": "Feature Transformation – StringIndexer (Estimator)",
    "section": "ft_string_indexer",
    "text": "ft_string_indexer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#description",
    "title": "Feature Transformation – StringIndexer (Estimator)",
    "section": "Description",
    "text": "Description\nA label indexer that maps a string column of labels to an ML column of label indices. If the input column is numeric, we cast it to string and index the string values. The indices are in [0, numLabels), ordered by label frequencies. So the most frequent label gets index 0. This function is the inverse of ft_index_to_string."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#usage",
    "title": "Feature Transformation – StringIndexer (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_string_indexer( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  handle_invalid = \"error\", \n  string_order_type = \"frequencyDesc\", \n  uid = random_string(\"string_indexer_\"), \n  ... \n) \n\nml_labels(model) \n\nft_string_indexer_model( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  labels, \n  handle_invalid = \"error\", \n  uid = random_string(\"string_indexer_model_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#arguments",
    "title": "Feature Transformation – StringIndexer (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error”\n\n\nstring_order_type\n(Spark 2.3+)How to order labels of string column. The first label after ordering is assigned an index of 0. Options are \"frequencyDesc\", \"frequencyAsc\", \"alphabetDesc\", and \"alphabetAsc\". Defaults to \"frequencyDesc\".\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused.\n\n\nmodel\nA fitted StringIndexer model returned by ft_string_indexer()\n\n\nlabels\nVector of labels, corresponding to indices to be assigned."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#details",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#details",
    "title": "Feature Transformation – StringIndexer (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#value",
    "title": "Feature Transformation – StringIndexer (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark\n\nml_labels() returns a vector of labels, corresponding to indices to be assigned."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#see-also",
    "title": "Feature Transformation – StringIndexer (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nft_index_to_string\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_image.html",
    "href": "packages/sparklyr/latest/reference/spark_read_image.html",
    "title": "Read image data into a Spark DataFrame.",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_image.html#spark_read_image",
    "href": "packages/sparklyr/latest/reference/spark_read_image.html#spark_read_image",
    "title": "Read image data into a Spark DataFrame.",
    "section": "spark_read_image",
    "text": "spark_read_image"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_image.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_image.html#description",
    "title": "Read image data into a Spark DataFrame.",
    "section": "Description",
    "text": "Description\nRead image files within a directory and convert each file into a record within the resulting Spark dataframe. The output will be a Spark dataframe consisting of struct types containing the following attributes:\n-origin: StringType\n-height: IntegerType\n-width: IntegerType\n-nChannels: IntegerType\n-mode: IntegerType\n-data: BinaryType"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_image.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_image.html#usage",
    "title": "Read image data into a Spark DataFrame.",
    "section": "Usage",
    "text": "Usage\nspark_read_image( \n  sc, \n  name = NULL, \n  dir = name, \n  drop_invalid = TRUE, \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_image.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_image.html#arguments",
    "title": "Read image data into a Spark DataFrame.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\ndir\nDirectory to read binary files from.\n\n\ndrop_invalid\nWhether to drop files that are not valid images from the result (default: TRUE).\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_image.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_image.html#see-also",
    "title": "Read image data into a Spark DataFrame.",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_call_constructor.html",
    "href": "packages/sparklyr/latest/reference/ml_call_constructor.html",
    "title": "Wrap a Spark ML JVM object",
    "section": "",
    "text": "R/ml_constructor_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_call_constructor.html#ml_call_constructor",
    "href": "packages/sparklyr/latest/reference/ml_call_constructor.html#ml_call_constructor",
    "title": "Wrap a Spark ML JVM object",
    "section": "ml_call_constructor",
    "text": "ml_call_constructor"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_call_constructor.html#description",
    "href": "packages/sparklyr/latest/reference/ml_call_constructor.html#description",
    "title": "Wrap a Spark ML JVM object",
    "section": "Description",
    "text": "Description\nIdentifies the associated sparklyr ML constructor for the JVM object by inspecting its class and performing a lookup. The lookup table is specified by the sparkml/class_mapping.json files of sparklyr and the loaded extensions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_call_constructor.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_call_constructor.html#usage",
    "title": "Wrap a Spark ML JVM object",
    "section": "Usage",
    "text": "Usage\nml_call_constructor(jobj)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_call_constructor.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_call_constructor.html#arguments",
    "title": "Wrap a Spark ML JVM object",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\njobj\nThe jobj for the pipeline stage."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html",
    "title": "Invoke distinct on a Spark DataFrame",
    "section": "",
    "text": "R/sdf_distinct.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#sdf_distinct",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#sdf_distinct",
    "title": "Invoke distinct on a Spark DataFrame",
    "section": "sdf_distinct",
    "text": "sdf_distinct"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#description",
    "title": "Invoke distinct on a Spark DataFrame",
    "section": "Description",
    "text": "Description\nInvoke distinct on a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#usage",
    "title": "Invoke distinct on a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_distinct(x, ..., name)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#arguments",
    "title": "Invoke distinct on a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame.\n\n\n…\nOptional variables to use when determining uniqueness. If there are multiple rows for a given combination of inputs, only the first row will be preserved. If omitted, will use all variables.\n\n\nname\nA name to assign this table. Passed to [sdf_register()]."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#section",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#section",
    "title": "Invoke distinct on a Spark DataFrame",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#transforming-spark-dataframes",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#transforming-spark-dataframes",
    "title": "Invoke distinct on a Spark DataFrame",
    "section": "Transforming Spark DataFrames",
    "text": "Transforming Spark DataFrames\nThe family of functions prefixed with sdf_ generally access the Scala Spark DataFrame API directly, as opposed to the dplyr interface which uses Spark SQL. These functions will ‘force’ any pending SQL in a dplyr pipeline, such that the resulting tbl_spark object returned will no longer have the attached ‘lazy’ SQL operations. Note that the underlying Spark DataFrame does execute its operations lazily, so that even though the pending set of operations (currently) are not exposed at the R level, these operations will only be executed when you explicitly collect() the table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#see-also",
    "title": "Invoke distinct on a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_copy_to(), sdf_random_split(), sdf_register(), sdf_sample(), sdf_sort(), sdf_weighted_sample()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_checkpoint.html",
    "href": "packages/sparklyr/latest/reference/sdf_checkpoint.html",
    "title": "Checkpoint a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_checkpoint.html#sdf_checkpoint",
    "href": "packages/sparklyr/latest/reference/sdf_checkpoint.html#sdf_checkpoint",
    "title": "Checkpoint a Spark DataFrame",
    "section": "sdf_checkpoint",
    "text": "sdf_checkpoint"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_checkpoint.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_checkpoint.html#description",
    "title": "Checkpoint a Spark DataFrame",
    "section": "Description",
    "text": "Description\nCheckpoint a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_checkpoint.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_checkpoint.html#usage",
    "title": "Checkpoint a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_checkpoint(x, eager = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_checkpoint.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_checkpoint.html#arguments",
    "title": "Checkpoint a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nan object coercible to a Spark DataFrame\n\n\neager\nwhether to truncate the lineage of the DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html",
    "title": "Read Delta Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#stream_read_delta",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#stream_read_delta",
    "title": "Read Delta Stream",
    "section": "stream_read_delta",
    "text": "stream_read_delta"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#description",
    "title": "Read Delta Stream",
    "section": "Description",
    "text": "Description\nReads a Delta Lake table as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#usage",
    "title": "Read Delta Stream",
    "section": "Usage",
    "text": "Usage\nstream_read_delta(sc, path, name = NULL, options = list(), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#arguments",
    "title": "Read Delta Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#details",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#details",
    "title": "Read Delta Stream",
    "section": "Details",
    "text": "Details\nPlease note that Delta Lake requires installing the appropriate package by setting the packages parameter to \"delta\" in spark_connect()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#examples",
    "title": "Read Delta Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\", version = \"2.4.0\", packages = \"delta\") \nsdf_len(sc, 5) %>% spark_write_delta(path = \"delta-test\") \nstream <- stream_read_delta(sc, \"delta-test\") %>% \n  stream_write_json(\"json-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#see-also",
    "title": "Read Delta Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection-class.html",
    "href": "packages/sparklyr/latest/reference/spark_connection-class.html",
    "title": "spark_connection class",
    "section": "",
    "text": "R/connection_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection-class.html#spark_connection-class",
    "href": "packages/sparklyr/latest/reference/spark_connection-class.html#spark_connection-class",
    "title": "spark_connection class",
    "section": "spark_connection-class",
    "text": "spark_connection-class"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection-class.html#description",
    "href": "packages/sparklyr/latest/reference/spark_connection-class.html#description",
    "title": "spark_connection class",
    "section": "Description",
    "text": "Description\nspark_connection class"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/nest.html",
    "href": "packages/sparklyr/latest/reference/nest.html",
    "title": "Nest",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/nest.html#nest",
    "href": "packages/sparklyr/latest/reference/nest.html#nest",
    "title": "Nest",
    "section": "nest",
    "text": "nest"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/nest.html#description",
    "href": "packages/sparklyr/latest/reference/nest.html#description",
    "title": "Nest",
    "section": "Description",
    "text": "Description\nSee nest for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compilation_spec.html",
    "href": "packages/sparklyr/latest/reference/spark_compilation_spec.html",
    "title": "Define a Spark Compilation Specification",
    "section": "",
    "text": "R/spark_compile.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compilation_spec.html#spark_compilation_spec",
    "href": "packages/sparklyr/latest/reference/spark_compilation_spec.html#spark_compilation_spec",
    "title": "Define a Spark Compilation Specification",
    "section": "spark_compilation_spec",
    "text": "spark_compilation_spec"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compilation_spec.html#description",
    "href": "packages/sparklyr/latest/reference/spark_compilation_spec.html#description",
    "title": "Define a Spark Compilation Specification",
    "section": "Description",
    "text": "Description\nFor use with compile_package_jars. The Spark compilation specification is used when compiling Spark extension Java Archives, and defines which versions of Spark, as well as which versions of Scala, should be used for compilation."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compilation_spec.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_compilation_spec.html#usage",
    "title": "Define a Spark Compilation Specification",
    "section": "Usage",
    "text": "Usage\nspark_compilation_spec( \n  spark_version = NULL, \n  spark_home = NULL, \n  scalac_path = NULL, \n  scala_filter = NULL, \n  jar_name = NULL, \n  jar_path = NULL, \n  jar_dep = NULL, \n  embedded_srcs = \"embedded_sources.R\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compilation_spec.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_compilation_spec.html#arguments",
    "title": "Define a Spark Compilation Specification",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nspark_version\nThe Spark version to build against. This can be left unset if the path to a suitable Spark home is supplied.\n\n\nspark_home\nThe path to a Spark home installation. This can be left unset if spark_version is supplied; in such a case, sparklyr will attempt to discover the associated Spark installation using spark_home_dir.\n\n\nscalac_path\nThe path to the scalac compiler to be used during compilation of your Spark extension. Note that you should ensure the version of scalac selected matches the version of scalac used with the version of Spark you are compiling against.\n\n\nscala_filter\nAn optional R function that can be used to filter which scala files are used during compilation. This can be useful if you have auxiliary files that should only be included with certain versions of Spark.\n\n\njar_name\nThe name to be assigned to the generated jar.\n\n\njar_path\nThe path to the jar tool to be used during compilation of your Spark extension.\n\n\njar_dep\nAn optional list of additional jar dependencies.\n\n\nembedded_srcs\nEmbedded source file(s) under <R package root>/java to be included in the root of the resulting jar file as resources"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compilation_spec.html#details",
    "href": "packages/sparklyr/latest/reference/spark_compilation_spec.html#details",
    "title": "Define a Spark Compilation Specification",
    "section": "Details",
    "text": "Details\nMost Spark extensions won’t need to define their own compilation specification, and can instead rely on the default behavior of compile_package_jars."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html",
    "title": "Read CSV Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html#stream_read_csv",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html#stream_read_csv",
    "title": "Read CSV Stream",
    "section": "stream_read_csv",
    "text": "stream_read_csv"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html#description",
    "title": "Read CSV Stream",
    "section": "Description",
    "text": "Description\nReads a CSV stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html#usage",
    "title": "Read CSV Stream",
    "section": "Usage",
    "text": "Usage\nstream_read_csv( \n  sc, \n  path, \n  name = NULL, \n  header = TRUE, \n  columns = NULL, \n  delimiter = \",\", \n  quote = \"\\\"\", \n  escape = \"\\\\\", \n  charset = \"UTF-8\", \n  null_value = NULL, \n  options = list(), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html#arguments",
    "title": "Read CSV Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\nheader\nBoolean; should the first row of data be used as a header? Defaults to TRUE.\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\ndelimiter\nThe character used to delimit each column. Defaults to ','.\n\n\nquote\nThe character used as a quote. Defaults to '\"'.\n\n\nescape\nThe character used to escape other characters. Defaults to '\\'.\n\n\ncharset\nThe character set. Defaults to \"UTF-8\".\n\n\nnull_value\nThe character to use for null, or missing, values. Defaults to NULL.\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html#examples",
    "title": "Read CSV Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \ndir.create(\"csv-in\") \nwrite.csv(iris, \"csv-in/data.csv\", row.names = FALSE) \ncsv_path <- file.path(\"file://\", getwd(), \"csv-in\") \nstream <- stream_read_csv(sc, csv_path) %>% stream_write_csv(\"csv-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html#see-also",
    "title": "Read CSV Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/generic_call_interface.html",
    "href": "packages/sparklyr/latest/reference/generic_call_interface.html",
    "title": "Generic Call Interface",
    "section": "",
    "text": "R/spark_invoke.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/generic_call_interface.html#generic_call_interface",
    "href": "packages/sparklyr/latest/reference/generic_call_interface.html#generic_call_interface",
    "title": "Generic Call Interface",
    "section": "generic_call_interface",
    "text": "generic_call_interface"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/generic_call_interface.html#description",
    "href": "packages/sparklyr/latest/reference/generic_call_interface.html#description",
    "title": "Generic Call Interface",
    "section": "Description",
    "text": "Description\nGeneric Call Interface"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/generic_call_interface.html#arguments",
    "href": "packages/sparklyr/latest/reference/generic_call_interface.html#arguments",
    "title": "Generic Call Interface",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nspark_connection\n\n\nstatic\nIs this a static method call (including a constructor). If so then the object parameter should be the name of a class (otherwise it should be a spark_jobj instance).\n\n\nobject\nObject instance or name of class (for static)\n\n\nmethod\nName of method\n\n\n…\nCall parameters"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html",
    "href": "packages/sparklyr/latest/reference/ml_als.html",
    "title": "Spark ML – ALS",
    "section": "",
    "text": "R/ml_recommendation_als.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#ml_als",
    "href": "packages/sparklyr/latest/reference/ml_als.html#ml_als",
    "title": "Spark ML – ALS",
    "section": "ml_als",
    "text": "ml_als"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#description",
    "href": "packages/sparklyr/latest/reference/ml_als.html#description",
    "title": "Spark ML – ALS",
    "section": "Description",
    "text": "Description\nPerform recommendation using Alternating Least Squares (ALS) matrix factorization."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_als.html#usage",
    "title": "Spark ML – ALS",
    "section": "Usage",
    "text": "Usage\nml_als( \n  x, \n  formula = NULL, \n  rating_col = \"rating\", \n  user_col = \"user\", \n  item_col = \"item\", \n  rank = 10, \n  reg_param = 0.1, \n  implicit_prefs = FALSE, \n  alpha = 1, \n  nonnegative = FALSE, \n  max_iter = 10, \n  num_user_blocks = 10, \n  num_item_blocks = 10, \n  checkpoint_interval = 10, \n  cold_start_strategy = \"nan\", \n  intermediate_storage_level = \"MEMORY_AND_DISK\", \n  final_storage_level = \"MEMORY_AND_DISK\", \n  uid = random_string(\"als_\"), \n  ... \n) \n\nml_recommend(model, type = c(\"items\", \"users\"), n = 1)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_als.html#arguments",
    "title": "Spark ML – ALS",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details. The ALS model requires a specific formula format, please use rating_col ~ user_col + item_col.\n\n\nrating_col\nColumn name for ratings. Default: “rating”\n\n\nuser_col\nColumn name for user ids. Ids must be integers. Other numeric types are supported for this column, but will be cast to integers as long as they fall within the integer value range. Default: “user”\n\n\nitem_col\nColumn name for item ids. Ids must be integers. Other numeric types are supported for this column, but will be cast to integers as long as they fall within the integer value range. Default: “item”\n\n\nrank\nRank of the matrix factorization (positive). Default: 10\n\n\nreg_param\nRegularization parameter.\n\n\nimplicit_prefs\nWhether to use implicit preference. Default: FALSE.\n\n\nalpha\nAlpha parameter in the implicit preference formulation (nonnegative).\n\n\nnonnegative\nWhether to apply nonnegativity constraints. Default: FALSE.\n\n\nmax_iter\nMaximum number of iterations.\n\n\nnum_user_blocks\nNumber of user blocks (positive). Default: 10\n\n\nnum_item_blocks\nNumber of item blocks (positive). Default: 10\n\n\ncheckpoint_interval\nSet checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.\n\n\ncold_start_strategy\n(Spark 2.2.0+) Strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: - “nan”: predicted value for unknown ids will be NaN. - “drop”: rows in the input DataFrame containing unknown ids will be dropped from the output DataFrame containing predictions. Default: “nan”.\n\n\nintermediate_storage_level\n(Spark 2.0.0+) StorageLevel for intermediate datasets. Pass in a string representation of StorageLevel. Cannot be “NONE”. Default: “MEMORY_AND_DISK”.\n\n\nfinal_storage_level\n(Spark 2.0.0+) StorageLevel for ALS model factors. Pass in a string representation of StorageLevel. Default: “MEMORY_AND_DISK”.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; currently unused.\n\n\nmodel\nAn ALS model object\n\n\ntype\nWhat to recommend, one of items or users\n\n\nn\nMaximum number of recommendations to return"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#details",
    "href": "packages/sparklyr/latest/reference/ml_als.html#details",
    "title": "Spark ML – ALS",
    "section": "Details",
    "text": "Details\nml_recommend() returns the top n users/items recommended for each item/user, for all items/users. The output has been transformed (exploded and separated) from the default Spark outputs to be more user friendly."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#value",
    "href": "packages/sparklyr/latest/reference/ml_als.html#value",
    "title": "Spark ML – ALS",
    "section": "Value",
    "text": "Value\nALS attempts to estimate the ratings matrix R as the product of two lower-rank matrices, X and Y, i.e. X * Yt = R. Typically these approximations are called ‘factor’ matrices. The general approach is iterative. During each iteration, one of the factor matrices is held constant, while the other is solved for using least squares. The newly-solved factor matrix is then held constant while solving for the other factor matrix.\nThis is a blocked implementation of the ALS factorization algorithm that groups the two sets of factors (referred to as “users” and “products”) into blocks and reduces communication by only sending one copy of each user vector to each product block on each iteration, and only for the product blocks that need that user’s feature vector. This is achieved by pre-computing some information about the ratings matrix to determine the “out-links” of each user (which blocks of products it will contribute to) and “in-link” information for each product (which of the feature vectors it receives from each user block it will depend on). This allows us to send only an array of feature vectors between each user block and product block, and have the product block find the users’ ratings and update the products based on these messages.\nFor implicit preference data, the algorithm used is based on “Collaborative Filtering for Implicit Feedback Datasets”, available at 10.1109/ICDM.2008.22, adapted for the blocked approach used here.\nEssentially instead of finding the low-rank approximations to the rating matrix R, this finds the approximations for a preference matrix P where the elements of P are 1 if r is greater than 0 and 0 if r is less than or equal to 0. The ratings then act as ‘confidence’ values related to strength of indicated user preferences rather than explicit ratings given to items.\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_als recommender object, which is an Estimator.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the recommender appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a recommender estimator is constructed then immediately fit with the input tbl_spark, returning a recommendation model, i.e. ml_als_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_als.html#examples",
    "title": "Spark ML – ALS",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\") \nmovies <- data.frame( \n  user   = c(1, 2, 0, 1, 2, 0), \n  item   = c(1, 1, 1, 2, 2, 0), \n  rating = c(3, 1, 2, 4, 5, 4) \n) \nmovies_tbl <- sdf_copy_to(sc, movies) \nmodel <- ml_als(movies_tbl, rating ~ user + item) \nml_predict(model, movies_tbl) \n#> # Source: spark<?> [?? x 6]\n#>    user  item rating features  label prediction\n#>   <dbl> <dbl>  <dbl> <list>    <dbl>      <dbl>\n#> 1     1     1      3 <dbl [2]>     3       2.80\n#> 2     2     1      1 <dbl [2]>     1       1.08\n#> 3     0     1      2 <dbl [2]>     2       2.00\n#> 4     1     2      4 <dbl [2]>     4       3.98\n#> 5     2     2      5 <dbl [2]>     5       4.86\n#> 6     0     0      4 <dbl [2]>     4       3.88\nml_recommend(model, type = \"item\", 1) \n#> # Source: spark<?> [?? x 4]\n#>    user recommendations   item rating\n#>   <int> <list>           <int>  <dbl>\n#> 1     1 <named list [2]>     2   3.98\n#> 2     2 <named list [2]>     2   4.86\n#> 3     0 <named list [2]>     0   3.88"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pipeline.html",
    "href": "packages/sparklyr/latest/reference/ml_pipeline.html",
    "title": "Spark ML – Pipelines",
    "section": "",
    "text": "R/ml_pipeline.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pipeline.html#ml_pipeline",
    "href": "packages/sparklyr/latest/reference/ml_pipeline.html#ml_pipeline",
    "title": "Spark ML – Pipelines",
    "section": "ml_pipeline",
    "text": "ml_pipeline"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pipeline.html#description",
    "href": "packages/sparklyr/latest/reference/ml_pipeline.html#description",
    "title": "Spark ML – Pipelines",
    "section": "Description",
    "text": "Description\nCreate Spark ML Pipelines"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pipeline.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_pipeline.html#usage",
    "title": "Spark ML – Pipelines",
    "section": "Usage",
    "text": "Usage\nml_pipeline(x, ..., uid = random_string(\"pipeline_\"))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pipeline.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_pipeline.html#arguments",
    "title": "Spark ML – Pipelines",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nEither a spark_connection or ml_pipeline_stage objects\n\n\n…\nml_pipeline_stage objects.\n\n\nuid\nA character string used to uniquely identify the ML estimator."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pipeline.html#value",
    "href": "packages/sparklyr/latest/reference/ml_pipeline.html#value",
    "title": "Spark ML – Pipelines",
    "section": "Value",
    "text": "Value\nWhen x is a spark_connection, ml_pipeline() returns an empty pipeline object. When x is a ml_pipeline_stage, ml_pipeline() returns an ml_pipeline with the stages set to x and any transformers or estimators given in ...."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_value.html",
    "href": "packages/sparklyr/latest/reference/spark_config_value.html",
    "title": "A helper function to retrieve values from spark_config()",
    "section": "",
    "text": "R/core_config.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_value.html#spark_config_value",
    "href": "packages/sparklyr/latest/reference/spark_config_value.html#spark_config_value",
    "title": "A helper function to retrieve values from spark_config()",
    "section": "spark_config_value",
    "text": "spark_config_value"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_value.html#description",
    "href": "packages/sparklyr/latest/reference/spark_config_value.html#description",
    "title": "A helper function to retrieve values from spark_config()",
    "section": "Description",
    "text": "Description\nA helper function to retrieve values from spark_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_value.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_config_value.html#usage",
    "title": "A helper function to retrieve values from spark_config()",
    "section": "Usage",
    "text": "Usage\nspark_config_value(config, name, default = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_value.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_config_value.html#arguments",
    "title": "A helper function to retrieve values from spark_config()",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nconfig\nThe configuration list from spark_config()\n\n\nname\nThe name of the configuration entry\n\n\ndefault\nThe default value to use when entry is not present"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/inner_join.html",
    "href": "packages/sparklyr/latest/reference/inner_join.html",
    "title": "Inner join",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/inner_join.html#inner_join",
    "href": "packages/sparklyr/latest/reference/inner_join.html#inner_join",
    "title": "Inner join",
    "section": "inner_join",
    "text": "inner_join"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/inner_join.html#description",
    "href": "packages/sparklyr/latest/reference/inner_join.html#description",
    "title": "Inner join",
    "section": "Description",
    "text": "Description\nSee inner_join for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/filter.html",
    "href": "packages/sparklyr/latest/reference/filter.html",
    "title": "Filter",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/filter.html#filter",
    "href": "packages/sparklyr/latest/reference/filter.html#filter",
    "title": "Filter",
    "section": "filter",
    "text": "filter"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/filter.html#description",
    "href": "packages/sparklyr/latest/reference/filter.html#description",
    "title": "Filter",
    "section": "Description",
    "text": "Description\nSee filter for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/compile_package_jars.html",
    "href": "packages/sparklyr/latest/reference/compile_package_jars.html",
    "title": "Compile Scala sources into a Java Archive (jar)",
    "section": "",
    "text": "R/spark_compile.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/compile_package_jars.html#compile_package_jars",
    "href": "packages/sparklyr/latest/reference/compile_package_jars.html#compile_package_jars",
    "title": "Compile Scala sources into a Java Archive (jar)",
    "section": "compile_package_jars",
    "text": "compile_package_jars"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/compile_package_jars.html#description",
    "href": "packages/sparklyr/latest/reference/compile_package_jars.html#description",
    "title": "Compile Scala sources into a Java Archive (jar)",
    "section": "Description",
    "text": "Description\nCompile the scala source files contained within an R package into a Java Archive (jar) file that can be loaded and used within a Spark environment."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/compile_package_jars.html#usage",
    "href": "packages/sparklyr/latest/reference/compile_package_jars.html#usage",
    "title": "Compile Scala sources into a Java Archive (jar)",
    "section": "Usage",
    "text": "Usage\ncompile_package_jars(..., spec = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/compile_package_jars.html#arguments",
    "href": "packages/sparklyr/latest/reference/compile_package_jars.html#arguments",
    "title": "Compile Scala sources into a Java Archive (jar)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\n…\nOptional compilation specifications, as generated by spark_compilation_spec. When no arguments are passed, spark_default_compilation_spec is used instead.\n\n\nspec\nAn optional list of compilation specifications. When set, this option takes precedence over arguments passed to ...."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html",
    "title": "Bind multiple Spark DataFrames by row and column",
    "section": "",
    "text": "R/mutation.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html#sdf_bind",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html#sdf_bind",
    "title": "Bind multiple Spark DataFrames by row and column",
    "section": "sdf_bind",
    "text": "sdf_bind"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html#description",
    "title": "Bind multiple Spark DataFrames by row and column",
    "section": "Description",
    "text": "Description\nsdf_bind_rows() and sdf_bind_cols() are implementation of the common pattern of do.call(rbind, sdfs) or do.call(cbind, sdfs) for binding many Spark DataFrames into one."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html#usage",
    "title": "Bind multiple Spark DataFrames by row and column",
    "section": "Usage",
    "text": "Usage\nsdf_bind_rows(..., id = NULL) \n\nsdf_bind_cols(...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html#arguments",
    "title": "Bind multiple Spark DataFrames by row and column",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\n…\nSpark tbls to combine.  Each argument can either be a Spark DataFrame or a list of Spark DataFrames  When row-binding, columns are matched by name, and any missing columns with be filled with NA.  When column-binding, rows are matched by position, so all data frames must have the same number of rows.\n\n\nid\nData frame identifier.  When id is supplied, a new column of identifiers is created to link each row to its original Spark DataFrame. The labels are taken from the named arguments to sdf_bind_rows(). When a list of Spark DataFrames is supplied, the labels are taken from the names of the list. If no names are found a numeric sequence is used instead."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html#details",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html#details",
    "title": "Bind multiple Spark DataFrames by row and column",
    "section": "Details",
    "text": "Details\nThe output of sdf_bind_rows() will contain a column if that column appears in any of the inputs."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html#value",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html#value",
    "title": "Bind multiple Spark DataFrames by row and column",
    "section": "Value",
    "text": "Value\nsdf_bind_rows() and sdf_bind_cols() return tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_last_index.html",
    "href": "packages/sparklyr/latest/reference/sdf_last_index.html",
    "title": "Returns the last index of a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_last_index.html#sdf_last_index",
    "href": "packages/sparklyr/latest/reference/sdf_last_index.html#sdf_last_index",
    "title": "Returns the last index of a Spark DataFrame",
    "section": "sdf_last_index",
    "text": "sdf_last_index"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_last_index.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_last_index.html#description",
    "title": "Returns the last index of a Spark DataFrame",
    "section": "Description",
    "text": "Description\nReturns the last index of a Spark DataFrame. The Spark mapPartitionsWithIndex function is used to iterate through the last nonempty partition of the RDD to find the last record."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_last_index.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_last_index.html#usage",
    "title": "Returns the last index of a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_last_index(x, id = \"id\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_last_index.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_last_index.html#arguments",
    "title": "Returns the last index of a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nid\nThe name of the index column."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_table.html",
    "href": "packages/sparklyr/latest/reference/spark_read_table.html",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_table.html#spark_read_table",
    "href": "packages/sparklyr/latest/reference/spark_read_table.html#spark_read_table",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "spark_read_table",
    "text": "spark_read_table"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_table.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_table.html#description",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "Description",
    "text": "Description\nReads from a Spark Table into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_table.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_table.html#usage",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "Usage",
    "text": "Usage\nspark_read_table( \n  sc, \n  name, \n  options = list(), \n  repartition = 0, \n  memory = TRUE, \n  columns = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_table.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_table.html#arguments",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\noptions\nA list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_table.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_table.html#see-also",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/reexports.html",
    "href": "packages/sparklyr/latest/reference/reexports.html",
    "title": "Objects exported from other packages",
    "section": "",
    "text": "R/imports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/reexports.html#reexports",
    "href": "packages/sparklyr/latest/reference/reexports.html#reexports",
    "title": "Objects exported from other packages",
    "section": "reexports",
    "text": "reexports"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/reexports.html#description",
    "href": "packages/sparklyr/latest/reference/reexports.html#description",
    "title": "Objects exported from other packages",
    "section": "Description",
    "text": "Description\nThese objects are imported from other packages. Follow the links below to see their documentation.\ngenerics\naugment"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke.html",
    "href": "packages/sparklyr/latest/reference/j_invoke.html",
    "title": "Invoke a Java function.",
    "section": "",
    "text": "R/spark_invoke.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke.html#j_invoke",
    "href": "packages/sparklyr/latest/reference/j_invoke.html#j_invoke",
    "title": "Invoke a Java function.",
    "section": "j_invoke",
    "text": "j_invoke"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke.html#description",
    "href": "packages/sparklyr/latest/reference/j_invoke.html#description",
    "title": "Invoke a Java function.",
    "section": "Description",
    "text": "Description\nInvoke a Java function and force return value of the call to be retrieved as a Java object reference."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke.html#usage",
    "href": "packages/sparklyr/latest/reference/j_invoke.html#usage",
    "title": "Invoke a Java function.",
    "section": "Usage",
    "text": "Usage\nj_invoke(jobj, method, ...) \n\nj_invoke_static(sc, class, method, ...) \n\nj_invoke_new(sc, class, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke.html#arguments",
    "href": "packages/sparklyr/latest/reference/j_invoke.html#arguments",
    "title": "Invoke a Java function.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\njobj\nAn R object acting as a Java object reference (typically, a spark_jobj).\n\n\nmethod\nThe name of the method to be invoked.\n\n\n…\nOptional arguments, currently unused.\n\n\nsc\nA spark_connection.\n\n\nclass\nThe name of the Java class whose methods should be invoked."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_partition_sizes.html",
    "href": "packages/sparklyr/latest/reference/sdf_partition_sizes.html",
    "title": "Compute the number of records within each partition of a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#sdf_partition_sizes",
    "href": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#sdf_partition_sizes",
    "title": "Compute the number of records within each partition of a Spark DataFrame",
    "section": "sdf_partition_sizes",
    "text": "sdf_partition_sizes"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#description",
    "title": "Compute the number of records within each partition of a Spark DataFrame",
    "section": "Description",
    "text": "Description\nCompute the number of records within each partition of a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#usage",
    "title": "Compute the number of records within each partition of a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_partition_sizes(x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#arguments",
    "title": "Compute the number of records within each partition of a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#examples",
    "href": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#examples",
    "title": "Compute the number of records within each partition of a Spark DataFrame",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc <- spark_connect(master = \"spark://HOST:PORT\") \nexample_sdf <- sdf_len(sc, 100L, repartition = 10L) \nexample_sdf %>% \n  sdf_partition_sizes() %>% \n  print()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "",
    "text": "NULL"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#ml_multilayer_perceptron_classifier",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#ml_multilayer_perceptron_classifier",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "ml_multilayer_perceptron_classifier",
    "text": "ml_multilayer_perceptron_classifier"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#description",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#description",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "Description",
    "text": "Description\nClassification model based on the Multilayer Perceptron. Each layer has sigmoid activation function, output layer has softmax."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#usage",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "Usage",
    "text": "Usage\nml_multilayer_perceptron_classifier( \n  x, \n  formula = NULL, \n  layers = NULL, \n  max_iter = 100, \n  step_size = 0.03, \n  tol = 1e-06, \n  block_size = 128, \n  solver = \"l-bfgs\", \n  seed = NULL, \n  initial_weights = NULL, \n  thresholds = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  uid = random_string(\"multilayer_perceptron_classifier_\"), \n  ... \n) \n\nml_multilayer_perceptron( \n  x, \n  formula = NULL, \n  layers, \n  max_iter = 100, \n  step_size = 0.03, \n  tol = 1e-06, \n  block_size = 128, \n  solver = \"l-bfgs\", \n  seed = NULL, \n  initial_weights = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  thresholds = NULL, \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  uid = random_string(\"multilayer_perceptron_classifier_\"), \n  response = NULL, \n  features = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#arguments",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nlayers\nA numeric vector describing the layers – each element in the vector gives the size of a layer. For example, c(4, 5, 2) would imply three layers, with an input (feature) layer of size 4, an intermediate layer of size 5, and an output (class) layer of size 2.\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nstep_size\nStep size to be used for each iteration of optimization (> 0).\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nblock_size\nBlock size for stacking input data in matrices to speed up the computation. Data is stacked within partitions. If block size is more than remaining data in a partition then it is adjusted to the size of this data. Recommended size is between 10 and 1000. Default: 128\n\n\nsolver\nThe solver algorithm for optimization. Supported options: “gd” (minibatch gradient descent) or “l-bfgs”. Default: “l-bfgs”\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\ninitial_weights\nThe initial weights of the model.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details.\n\n\nresponse\n(Deprecated) The name of the response column (as a length-one character vector.)\n\n\nfeatures\n(Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#details",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#details",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows.\nml_multilayer_perceptron() is an alias for ml_multilayer_perceptron_classifier() for backwards compatibility."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#value",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#value",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula\nis specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#examples",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \npartitions <- iris_tbl %>% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \niris_training <- partitions$training \niris_test <- partitions$test \nmlp_model <- iris_training %>% \n  ml_multilayer_perceptron_classifier(Species ~ ., layers = c(4, 3, 3)) \npred <- ml_predict(mlp_model, iris_test) \nml_multiclass_classification_evaluator(pred) \n#> [1] 0.5227273"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#see-also",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_crosstab.html",
    "href": "packages/sparklyr/latest/reference/sdf_crosstab.html",
    "title": "Cross Tabulation",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_crosstab.html#sdf_crosstab",
    "href": "packages/sparklyr/latest/reference/sdf_crosstab.html#sdf_crosstab",
    "title": "Cross Tabulation",
    "section": "sdf_crosstab",
    "text": "sdf_crosstab"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_crosstab.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_crosstab.html#description",
    "title": "Cross Tabulation",
    "section": "Description",
    "text": "Description\nBuilds a contingency table at each combination of factor levels."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_crosstab.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_crosstab.html#usage",
    "title": "Cross Tabulation",
    "section": "Usage",
    "text": "Usage\nsdf_crosstab(x, col1, col2)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_crosstab.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_crosstab.html#arguments",
    "title": "Cross Tabulation",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame\n\n\ncol1\nThe name of the first column. Distinct items will make the first item of each row.\n\n\ncol2\nThe name of the second column. Distinct items will make the column names of the DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_crosstab.html#value",
    "href": "packages/sparklyr/latest/reference/sdf_crosstab.html#value",
    "title": "Cross Tabulation",
    "section": "Value",
    "text": "Value\nA DataFrame containing the contingency table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_class.html",
    "href": "packages/sparklyr/latest/reference/jobj_class.html",
    "title": "Superclasses of object",
    "section": "",
    "text": "R/spark_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_class.html#jobj_class",
    "href": "packages/sparklyr/latest/reference/jobj_class.html#jobj_class",
    "title": "Superclasses of object",
    "section": "jobj_class",
    "text": "jobj_class"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_class.html#description",
    "href": "packages/sparklyr/latest/reference/jobj_class.html#description",
    "title": "Superclasses of object",
    "section": "Description",
    "text": "Description\nExtract the classes that a Java object inherits from. This is the jobj equivalent of class()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_class.html#usage",
    "href": "packages/sparklyr/latest/reference/jobj_class.html#usage",
    "title": "Superclasses of object",
    "section": "Usage",
    "text": "Usage\njobj_class(jobj, simple_name = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_class.html#arguments",
    "href": "packages/sparklyr/latest/reference/jobj_class.html#arguments",
    "title": "Superclasses of object",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\njobj\nA spark_jobj\n\n\nsimple_name\nWhether to return simple names, defaults to TRUE"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html",
    "title": "Feature Transformation – Bucketizer (Transformer)",
    "section": "",
    "text": "R/ml_feature_bucketizer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#ft_bucketizer",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#ft_bucketizer",
    "title": "Feature Transformation – Bucketizer (Transformer)",
    "section": "ft_bucketizer",
    "text": "ft_bucketizer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#description",
    "title": "Feature Transformation – Bucketizer (Transformer)",
    "section": "Description",
    "text": "Description\nSimilar to R’s cut function, this transforms a numeric column into a discretized column, with breaks specified through the splits\nparameter."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#usage",
    "title": "Feature Transformation – Bucketizer (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_bucketizer( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  splits = NULL, \n  input_cols = NULL, \n  output_cols = NULL, \n  splits_array = NULL, \n  handle_invalid = \"error\", \n  uid = random_string(\"bucketizer_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#arguments",
    "title": "Feature Transformation – Bucketizer (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nsplits\nA numeric vector of cutpoints, indicating the bucket boundaries.\n\n\ninput_cols\nNames of input columns.\n\n\noutput_cols\nNames of output columns.\n\n\nsplits_array\nParameter for specifying multiple splits parameters. Each element in this array can be used to map continuous features into buckets.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error”\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#value",
    "title": "Feature Transformation – Bucketizer (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#examples",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#examples",
    "title": "Feature Transformation – Bucketizer (Transformer)",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nlibrary(dplyr) \nsc <- spark_connect(master = \"local\") \niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \niris_tbl %>% \n  ft_bucketizer( \n    input_col = \"Sepal_Length\", \n    output_col = \"Sepal_Length_bucket\", \n    splits = c(0, 4.5, 5, 8) \n  ) %>% \n  select(Sepal_Length, Sepal_Length_bucket, Species) \n#> # Source: spark<?> [?? x 3]\n#>    Sepal_Length Sepal_Length_bucket Species\n#>           <dbl>               <dbl> <chr>  \n#>  1          5.1                   2 setosa \n#>  2          4.9                   1 setosa \n#>  3          4.7                   1 setosa \n#>  4          4.6                   1 setosa \n#>  5          5                     2 setosa \n#>  6          5.4                   2 setosa \n#>  7          4.6                   1 setosa \n#>  8          5                     2 setosa \n#>  9          4.4                   0 setosa \n#> 10          4.9                   1 setosa \n#> # … with more rows"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#see-also",
    "title": "Feature Transformation – Bucketizer (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html",
    "title": "Read Parquet Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html#stream_read_parquet",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html#stream_read_parquet",
    "title": "Read Parquet Stream",
    "section": "stream_read_parquet",
    "text": "stream_read_parquet"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html#description",
    "title": "Read Parquet Stream",
    "section": "Description",
    "text": "Description\nReads a parquet stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html#usage",
    "title": "Read Parquet Stream",
    "section": "Usage",
    "text": "Usage\nstream_read_parquet( \n  sc, \n  path, \n  name = NULL, \n  columns = NULL, \n  options = list(), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html#arguments",
    "title": "Read Parquet Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html#examples",
    "title": "Read Parquet Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \nsdf_len(sc, 10) %>% spark_write_parquet(\"parquet-in\") \nstream <- stream_read_parquet(sc, \"parquet-in\") %>% stream_write_parquet(\"parquet-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html#see-also",
    "title": "Read Parquet Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_find.html",
    "href": "packages/sparklyr/latest/reference/stream_find.html",
    "title": "Find Stream",
    "section": "",
    "text": "R/stream_operations.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_find.html#stream_find",
    "href": "packages/sparklyr/latest/reference/stream_find.html#stream_find",
    "title": "Find Stream",
    "section": "stream_find",
    "text": "stream_find"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_find.html#description",
    "href": "packages/sparklyr/latest/reference/stream_find.html#description",
    "title": "Find Stream",
    "section": "Description",
    "text": "Description\nFinds and returns a stream based on the stream’s identifier."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_find.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_find.html#usage",
    "title": "Find Stream",
    "section": "Usage",
    "text": "Usage\nstream_find(sc, id)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_find.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_find.html#arguments",
    "title": "Find Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nid\nThe stream identifier to find."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_find.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_find.html#examples",
    "title": "Find Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\") \nsdf_len(sc, 10) %>% \n  spark_write_parquet(path = \"parquet-in\") \nstream <- stream_read_parquet(sc, \"parquet-in\") %>% \n  stream_write_parquet(\"parquet-out\") \nstream_id <- stream_id(stream) \nstream_find(sc, stream_id)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rlnorm.html",
    "href": "packages/sparklyr/latest/reference/sdf_rlnorm.html",
    "title": "Generate random samples from a log normal distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rlnorm.html#sdf_rlnorm",
    "href": "packages/sparklyr/latest/reference/sdf_rlnorm.html#sdf_rlnorm",
    "title": "Generate random samples from a log normal distribution",
    "section": "sdf_rlnorm",
    "text": "sdf_rlnorm"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rlnorm.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rlnorm.html#description",
    "title": "Generate random samples from a log normal distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a log normal distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rlnorm.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rlnorm.html#usage",
    "title": "Generate random samples from a log normal distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rlnorm( \n  sc, \n  n, \n  meanlog = 0, \n  sdlog = 1, \n  num_partitions = NULL, \n  seed = NULL, \n  output_col = \"x\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rlnorm.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rlnorm.html#arguments",
    "title": "Generate random samples from a log normal distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nmeanlog\nThe mean of the normally distributed natural logarithm of this distribution.\n\n\nsdlog\nThe Standard deviation of the normally distributed natural logarithm of this distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rlnorm.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rlnorm.html#see-also",
    "title": "Generate random samples from a log normal distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_rds.html",
    "href": "packages/sparklyr/latest/reference/spark_write_rds.html",
    "title": "Write Spark DataFrame to RDS files",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_rds.html#spark_write_rds",
    "href": "packages/sparklyr/latest/reference/spark_write_rds.html#spark_write_rds",
    "title": "Write Spark DataFrame to RDS files",
    "section": "spark_write_rds",
    "text": "spark_write_rds"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_rds.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_rds.html#description",
    "title": "Write Spark DataFrame to RDS files",
    "section": "Description",
    "text": "Description\nWrite Spark dataframe to RDS files. Each partition of the dataframe will be exported to a separate RDS file so that all partitions can be processed in parallel."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_rds.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_rds.html#usage",
    "title": "Write Spark DataFrame to RDS files",
    "section": "Usage",
    "text": "Usage\nspark_write_rds(x, dest_uri)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_rds.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_rds.html#arguments",
    "title": "Write Spark DataFrame to RDS files",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame to be exported\n\n\ndest_uri\nCan be a URI template containing “partitionId” (e.g., \"hdfs://my_data_part_{partitionId}.rds\") where “partitionId” will be substituted with ID of each partition using glue, or a list of URIs to be assigned to RDS output from all partitions (e.g., \"hdfs://my_data_part_0.rds\", \"hdfs://my_data_part_1.rds\", and so on) If working with a Spark instance running locally, then all URIs should be in \"file://<local file path>\" form. Otherwise the scheme of the URI should reflect the underlying file system the Spark instance is working with (e.g., “hdfs://”). If the resulting list of URI(s) does not contain unique values, then it will be post-processed with make.unique() to ensure uniqueness."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_rds.html#value",
    "href": "packages/sparklyr/latest/reference/spark_write_rds.html#value",
    "title": "Write Spark DataFrame to RDS files",
    "section": "Value",
    "text": "Value\nA tibble containing partition ID and RDS file location for each partition of the input Spark dataframe."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html",
    "title": "Feature Transformation – OneHotEncoder (Transformer)",
    "section": "",
    "text": "R/ml_feature_one_hot_encoder.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#ft_one_hot_encoder",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#ft_one_hot_encoder",
    "title": "Feature Transformation – OneHotEncoder (Transformer)",
    "section": "ft_one_hot_encoder",
    "text": "ft_one_hot_encoder"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#description",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#description",
    "title": "Feature Transformation – OneHotEncoder (Transformer)",
    "section": "Description",
    "text": "Description\nOne-hot encoding maps a column of label indices to a column of binary vectors, with at most a single one-value. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features. Typically, used with ft_string_indexer() to index a column first."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#usage",
    "title": "Feature Transformation – OneHotEncoder (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_one_hot_encoder( \n  x, \n  input_cols = NULL, \n  output_cols = NULL, \n  handle_invalid = NULL, \n  drop_last = TRUE, \n  uid = random_string(\"one_hot_encoder_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#arguments",
    "title": "Feature Transformation – OneHotEncoder (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_cols\nThe name of the input columns.\n\n\noutput_cols\nThe name of the output columns.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error”\n\n\ndrop_last\nWhether to drop the last category. Defaults to TRUE.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#value",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#value",
    "title": "Feature Transformation – OneHotEncoder (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#see-also",
    "title": "Feature Transformation – OneHotEncoder (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html",
    "title": "Read a Parquet file into a Spark DataFrame",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html#spark_read_parquet",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html#spark_read_parquet",
    "title": "Read a Parquet file into a Spark DataFrame",
    "section": "spark_read_parquet",
    "text": "spark_read_parquet"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html#description",
    "title": "Read a Parquet file into a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRead a Parquet file into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html#usage",
    "title": "Read a Parquet file into a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nspark_read_parquet( \n  sc, \n  name = NULL, \n  path = name, \n  options = list(), \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE, \n  columns = NULL, \n  schema = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html#arguments",
    "title": "Read a Parquet file into a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\noptions\nA list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\nschema\nA (java) read schema. Useful for optimizing read operation on nested data.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html#details",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html#details",
    "title": "Read a Parquet file into a Spark DataFrame",
    "section": "Details",
    "text": "Details\nYou can read data from HDFS (hdfs://), S3 (s3a://), as well as the local file system (file://).\nIf you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf spark.hadoop.fs.s3a.access.key, spark.hadoop.fs.s3a.secret.key or any of the methods outlined in the aws-sdk documentation Working with AWS credentials\nIn order to work with the newer s3a:// protocol also set the values for spark.hadoop.fs.s3a.impl and spark.hadoop.fs.s3a.endpoint. In addition, to support v4 of the S3 api be sure to pass the -Dcom.amazonaws.services.s3.enableV4 driver options for the config key spark.driver.extraJavaOptions\nFor instructions on how to configure s3n:// check the hadoop documentation: s3n authentication properties"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html#see-also",
    "title": "Read a Parquet file into a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparktf/latest/news.html",
    "href": "packages/sparktf/latest/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "sparkdf is a sparklyr extension for reading and writing TensorFlow TFRecord files via Apache Spark."
  },
  {
    "objectID": "packages/sparktf/latest/index.html",
    "href": "packages/sparktf/latest/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "sparktf is a sparklyr extension that allows writing of Spark DataFrames to TFRecord, the recommended format for persisting data to be used in training with TensorFlow.\n\n\n\nYou can install the development version of sparktf from GitHub with:\ndevtools::install_github(\"rstudio/sparktf\")\n\n\n\nWe first attach the required packages and establish a Spark connection.\nlibrary(sparktf)\nlibrary(sparklyr)\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution()\nlibrary(tfdatasets)\n\nsc <- spark_connect(master = \"local\")\nCopied a sample dataset to Spark then write it to disk via spark_write_tfrecord().\ndata_path <- file.path(tempdir(), \"iris\")\niris_tbl <- sdf_copy_to(sc, iris)\n\niris_tbl %>%\n  ft_string_indexer_model(\n    \"Species\", \"label\",\n    labels = c(\"setosa\", \"versicolor\", \"virginica\")\n  ) %>%\n  spark_write_tfrecord(\n    path = data_path,\n    write_locality = \"local\"\n  )\nWe now read the saved TFRecord file and parse the contents to create a dataset object. For details, refer to the package website for tfdatasets.\ndataset <- tfrecord_dataset(list.files(data_path, full.names = TRUE)) %>%\n  dataset_map(function(example_proto) {\n    features <- list(\n      label = tf$FixedLenFeature(shape(), tf$float32),\n      Sepal_Length = tf$FixedLenFeature(shape(), tf$float32),\n      Sepal_Width = tf$FixedLenFeature(shape(), tf$float32),\n      Petal_Length = tf$FixedLenFeature(shape(), tf$float32),\n      Petal_Width = tf$FixedLenFeature(shape(), tf$float32)\n    )\n\n    features <- tf$parse_single_example(example_proto, features)\n    x <- list(\n      features$Sepal_Length, features$Sepal_Width,\n      features$Petal_Length, features$Petal_Width\n      )\n    y <- tf$one_hot(tf$cast(features$label, tf$int32), 3L)\n    list(x, y)\n  }) %>%\n  dataset_shuffle(150) %>%\n  dataset_batch(16)\nNow, we can define a Keras model using the keras package and fit it by feeding the dataset object defined above.\nmodel <- keras_model_sequential() %>%\n  layer_dense(32, activation = \"relu\", input_shape = 4) %>%\n  layer_dense(3, activation = \"softmax\")\n\nmodel %>%\n  compile(loss = \"categorical_crossentropy\", optimizer = tf$train$AdamOptimizer())\n\nhistory <- model %>%\n  fit(dataset, epochs = 100, verbose = 0)\nFinally, we can use the trained model to make some predictions.\nnew_data <- tf$constant(c(4.9, 3.2, 1.4, 0.2), shape = c(1, 4))\nmodel(new_data)\n#> tf.Tensor([[0.76382965 0.19407341 0.04209692]], shape=(1, 3), dtype=float32)"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_read_tfrecord.html",
    "href": "packages/sparktf/latest/reference/spark_read_tfrecord.html",
    "title": "Read a TFRecord File",
    "section": "",
    "text": "R/loader.R"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_read_tfrecord.html#spark_read_tfrecord",
    "href": "packages/sparktf/latest/reference/spark_read_tfrecord.html#spark_read_tfrecord",
    "title": "Read a TFRecord File",
    "section": "spark_read_tfrecord",
    "text": "spark_read_tfrecord"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_read_tfrecord.html#description",
    "href": "packages/sparktf/latest/reference/spark_read_tfrecord.html#description",
    "title": "Read a TFRecord File",
    "section": "Description",
    "text": "Description\nRead a TFRecord file as a Spark DataFrame."
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_read_tfrecord.html#usage",
    "href": "packages/sparktf/latest/reference/spark_read_tfrecord.html#usage",
    "title": "Read a TFRecord File",
    "section": "Usage",
    "text": "Usage\nspark_read_tfrecord(sc, name, path, schema = NULL, \n  record_type = c(\"Example\", \"SequenceExample\"), overwrite = TRUE)"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_read_tfrecord.html#arguments",
    "href": "packages/sparktf/latest/reference/spark_read_tfrecord.html#arguments",
    "title": "Read a TFRecord File",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark conneciton.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://”, “s3a://” and “file://” protocols.\n\n\nschema\n(Currently unsupported.) Schema of TensorFlow records. If not provided, the schema is inferred from TensorFlow records.\n\n\nrecord_type\nInput format of TensorFlow records. By default it is Example.\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?"
  },
  {
    "objectID": "packages/sparktf/latest/reference/index.html",
    "href": "packages/sparktf/latest/reference/index.html",
    "title": "sparktf",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\nspark_read_tfrecord()\nRead a TFRecord File\n\n\nspark_write_tfrecord()\nWrite a Spark DataFrame to a TFRecord file"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_write_tfrecord.html",
    "href": "packages/sparktf/latest/reference/spark_write_tfrecord.html",
    "title": "Write a Spark DataFrame to a TFRecord file",
    "section": "",
    "text": "R/writer.R"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_write_tfrecord.html#spark_write_tfrecord",
    "href": "packages/sparktf/latest/reference/spark_write_tfrecord.html#spark_write_tfrecord",
    "title": "Write a Spark DataFrame to a TFRecord file",
    "section": "spark_write_tfrecord",
    "text": "spark_write_tfrecord"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_write_tfrecord.html#description",
    "href": "packages/sparktf/latest/reference/spark_write_tfrecord.html#description",
    "title": "Write a Spark DataFrame to a TFRecord file",
    "section": "Description",
    "text": "Description\nSerialize a Spark DataFrame to the TensorFlow TFRecord format for training or inference."
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_write_tfrecord.html#usage",
    "href": "packages/sparktf/latest/reference/spark_write_tfrecord.html#usage",
    "title": "Write a Spark DataFrame to a TFRecord file",
    "section": "Usage",
    "text": "Usage\nspark_write_tfrecord(x, path, record_type = c(\"Example\", \n  \"SequenceExample\"), write_locality = c(\"distributed\", \"local\"), \n  mode = NULL)"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_write_tfrecord.html#arguments",
    "href": "packages/sparktf/latest/reference/spark_write_tfrecord.html#arguments",
    "title": "Write a Spark DataFrame to a TFRecord file",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://”, “s3a://”, and “file://” protocols.\n\n\nrecord_type\nOutput format of TensorFlow records. One of \"Example\" and \"SequenceExample\".\n\n\nwrite_locality\nDetermines whether the TensorFlow records are written locally on the workers or on a distributed file system. One of \"distributed\" and \"local\". See Details for more information.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ‘ignore’. Notice that ‘overwrite’ will also change the column structure.  For more details see also http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark."
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_write_tfrecord.html#details",
    "href": "packages/sparktf/latest/reference/spark_write_tfrecord.html#details",
    "title": "Write a Spark DataFrame to a TFRecord file",
    "section": "Details",
    "text": "Details\nFor write_locality = local, each of the workers stores on the local disk a subset of the data. The subset that is stored on each worker is determined by the partitioning of the DataFrame. Each of the partitions is coalesced into a single TFRecord file and written on the node where the partition lives. This is useful in the context of distributed training, in which each of the workers gets a subset of the data to work on. When this mode is activated, the path provided to the writer is interpreted as a base path that is created on each of the worker nodes, and that will be populated with data from the DataFrame."
  },
  {
    "objectID": "packages/index.html",
    "href": "packages/index.html",
    "title": "Packages",
    "section": "",
    "text": "Package\nDescription\n\n\n\n\nmleap\nA sparklyr extension that provides an interface to MLeap, which allows us to take Spark pipelines to production.\n\n\ngraphframes\nEnables graph analysis in Spark via GraphFrames"
  },
  {
    "objectID": "packages/index.html#on-github",
    "href": "packages/index.html#on-github",
    "title": "Packages",
    "section": "On Github",
    "text": "On Github\n\n\n\nPackage\nDescription\n\n\n\n\nsparktf\nAllows writing of Spark DataFrame’s to TFRecord, the recommended format for persisting data to be used in training with TensorFlow\n\n\nsparkxgb\nA sparklyr extension that provides an interface to XGBoost on Spark."
  },
  {
    "objectID": "packages/graphframes/latest/news.html",
    "href": "packages/graphframes/latest/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "Updated dependency to graphframes 0.6.0, with support for Spark 2.3."
  },
  {
    "objectID": "packages/graphframes/latest/index.html",
    "href": "packages/graphframes/latest/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Support for GraphFrames which aims to provide the functionality of GraphX.\nPerform graph algorithms like: PageRank, ShortestPaths and many others.\nDesigned to work with sparklyr and the sparklyr extensions.\n\n\n\nFor those already using sparklyr simply run:\ninstall.packages(\"graphframes\")\n# or, for the development version,\n# devtools::install_github(\"rstudio/graphframes\")\nOtherwise, install first sparklyr from CRAN using:\ninstall.packages(\"sparklyr\")\nThe examples make use of the highschool dataset from the ggplot package.\n\n\n\nWe will calculate PageRank over the built-in “friends” dataset as follows.\nlibrary(graphframes)\nlibrary(sparklyr)\nlibrary(dplyr)\n\n# connect to spark using sparklyr\nsc <- spark_connect(master = \"local\", version = \"2.3.0\")\n\n# obtain the example graph\ng <- gf_friends(sc)\n\n# compute PageRank\nresults <- gf_pagerank(g, tol = 0.01, reset_probability = 0.15)\nresults\n## GraphFrame\n## Vertices:\n##   $ id       <chr> \"f\", \"b\", \"g\", \"a\", \"d\", \"c\", \"e\"\n##   $ name     <chr> \"Fanny\", \"Bob\", \"Gabby\", \"Alice\", \"David\", \"Charlie\",...\n##   $ age      <int> 36, 36, 60, 34, 29, 30, 32\n##   $ pagerank <dbl> 0.3283607, 2.6555078, 0.1799821, 0.4491063, 0.3283607...\n## Edges:\n##   $ src          <chr> \"b\", \"c\", \"d\", \"e\", \"a\", \"a\", \"e\", \"f\"\n##   $ dst          <chr> \"c\", \"b\", \"a\", \"f\", \"e\", \"b\", \"d\", \"c\"\n##   $ relationship <chr> \"follow\", \"follow\", \"friend\", \"follow\", \"friend\",...\n##   $ weight       <dbl> 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0\nWe can then visualize the results by collecting the results to R:\nlibrary(tidygraph)\nlibrary(ggraph)\n\nvertices <- results %>%\n  gf_vertices() %>%\n  collect()\n\nedges <- results %>%\n  gf_edges() %>%\n  collect()\n\nedges %>%\n  as_tbl_graph() %>%\n  activate(nodes) %>%\n  left_join(vertices, by = c(name = \"id\")) %>%\n  ggraph(layout = \"nicely\") +\n  geom_node_label(aes(label = name.y, color = pagerank)) +\n  geom_edge_link(\n    aes(\n      alpha = weight,\n      start_cap = label_rect(node1.name.y),\n      end_cap = label_rect(node2.name.y)\n    ),\n    arrow = arrow(length = unit(4, \"mm\"))\n  ) +\n  theme_graph(fg_text_colour = 'white')\n\n\n\n\nAppart from calculating PageRank using gf_pagerank, many other functions are available, including:\n\ngf_bfs(): Breadth-first search (BFS).\ngf_connected_components(): Connected components.\ngf_shortest_paths(): Shortest paths algorithm.\ngf_scc(): Strongly connected components.\ngf_triangle_count(): Computes the number of triangles passing through each vertex and others.\ngf_degrees(): Degrees of vertices\n\nFor instance, one can calculate the degrees of vertices using gf_degrees as follows:\ngf_friends(sc) %>% gf_degrees()\n## # Source: spark<?> [?? x 2]\n##   id    degree\n## * <chr>  <int>\n## 1 f          2\n## 2 b          3\n## 3 a          3\n## 4 c          3\n## 5 e          3\n## 6 d          2\nFinally, we disconnect from Spark:\nspark_disconnect(sc)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertex_columns.html",
    "href": "packages/graphframes/latest/reference/gf_vertex_columns.html",
    "title": "Vertices column names",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertex_columns.html#gf_vertex_columns",
    "href": "packages/graphframes/latest/reference/gf_vertex_columns.html#gf_vertex_columns",
    "title": "Vertices column names",
    "section": "gf_vertex_columns",
    "text": "gf_vertex_columns"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertex_columns.html#description",
    "href": "packages/graphframes/latest/reference/gf_vertex_columns.html#description",
    "title": "Vertices column names",
    "section": "Description",
    "text": "Description\nVertices column names"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertex_columns.html#usage",
    "href": "packages/graphframes/latest/reference/gf_vertex_columns.html#usage",
    "title": "Vertices column names",
    "section": "Usage",
    "text": "Usage\ngf_vertex_columns(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertex_columns.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_vertex_columns.html#arguments",
    "title": "Vertices column names",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_degrees.html",
    "href": "packages/graphframes/latest/reference/gf_degrees.html",
    "title": "Degrees of vertices",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_degrees.html#gf_degrees",
    "href": "packages/graphframes/latest/reference/gf_degrees.html#gf_degrees",
    "title": "Degrees of vertices",
    "section": "gf_degrees",
    "text": "gf_degrees"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_degrees.html#description",
    "href": "packages/graphframes/latest/reference/gf_degrees.html#description",
    "title": "Degrees of vertices",
    "section": "Description",
    "text": "Description\nDegrees of vertices"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_degrees.html#usage",
    "href": "packages/graphframes/latest/reference/gf_degrees.html#usage",
    "title": "Degrees of vertices",
    "section": "Usage",
    "text": "Usage\ngf_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_degrees.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_degrees.html#arguments",
    "title": "Degrees of vertices",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_register.html",
    "href": "packages/graphframes/latest/reference/gf_register.html",
    "title": "Register a GraphFrame object",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_register.html#gf_register",
    "href": "packages/graphframes/latest/reference/gf_register.html#gf_register",
    "title": "Register a GraphFrame object",
    "section": "gf_register",
    "text": "gf_register"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_register.html#description",
    "href": "packages/graphframes/latest/reference/gf_register.html#description",
    "title": "Register a GraphFrame object",
    "section": "Description",
    "text": "Description\nRegister a GraphFrame object"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_register.html#usage",
    "href": "packages/graphframes/latest/reference/gf_register.html#usage",
    "title": "Register a GraphFrame object",
    "section": "Usage",
    "text": "Usage\ngf_register(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_register.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_register.html#arguments",
    "title": "Register a GraphFrame object",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html",
    "title": "Create a new GraphFrame",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#gf_graphframe",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#gf_graphframe",
    "title": "Create a new GraphFrame",
    "section": "gf_graphframe",
    "text": "gf_graphframe"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#description",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#description",
    "title": "Create a new GraphFrame",
    "section": "Description",
    "text": "Description\nCreate a new GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#usage",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#usage",
    "title": "Create a new GraphFrame",
    "section": "Usage",
    "text": "Usage\ngf_graphframe(vertices = NULL, edges)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#arguments",
    "title": "Create a new GraphFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nvertices\nA tbl_spark representing vertices.\n\n\nedges\nA tbl_psark representing edges."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#examples",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#examples",
    "title": "Create a new GraphFrame",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\", version = \"2.3.0\") \nv_tbl <- sdf_copy_to( \n  sc, data.frame(id = 1:3, name = LETTERS[1:3]) \n) \ne_tbl <- sdf_copy_to( \n  sc, data.frame(src = c(1, 2, 2), dst = c(2, 1, 3), \n                 action = c(\"love\", \"hate\", \"follow\")) \n) \ngf_graphframe(v_tbl, e_tbl) \ngf_graphframe(edges = e_tbl)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_in_degrees.html",
    "href": "packages/graphframes/latest/reference/gf_in_degrees.html",
    "title": "In-degrees of vertices",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_in_degrees.html#gf_in_degrees",
    "href": "packages/graphframes/latest/reference/gf_in_degrees.html#gf_in_degrees",
    "title": "In-degrees of vertices",
    "section": "gf_in_degrees",
    "text": "gf_in_degrees"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_in_degrees.html#description",
    "href": "packages/graphframes/latest/reference/gf_in_degrees.html#description",
    "title": "In-degrees of vertices",
    "section": "Description",
    "text": "Description\nIn-degrees of vertices"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_in_degrees.html#usage",
    "href": "packages/graphframes/latest/reference/gf_in_degrees.html#usage",
    "title": "In-degrees of vertices",
    "section": "Usage",
    "text": "Usage\ngf_in_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_in_degrees.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_in_degrees.html#arguments",
    "title": "In-degrees of vertices",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html",
    "title": "Connected components",
    "section": "",
    "text": "R/gf_connected_components.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#gf_connected_components",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#gf_connected_components",
    "title": "Connected components",
    "section": "gf_connected_components",
    "text": "gf_connected_components"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#description",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#description",
    "title": "Connected components",
    "section": "Description",
    "text": "Description\nComputes the connected component membership of each vertex and returns a DataFrame of vertex information with each vertex assigned a component ID."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#usage",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#usage",
    "title": "Connected components",
    "section": "Usage",
    "text": "Usage\ngf_connected_components(x, broadcast_threshold = 1000000L, \n  algorithm = c(\"graphframes\", \"graphx\"), checkpoint_interval = 2L, \n  ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#arguments",
    "title": "Connected components",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\nbroadcast_threshold\nBroadcast threshold in propagating component assignments.\n\n\nalgorithm\nOne of ‘graphframes’ or ‘graphx’.\n\n\ncheckpoint_interval\nCheckpoint interval in terms of number of iterations.\n\n\n…\nOptional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#examples",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#examples",
    "title": "Connected components",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\n# checkpoint directory is required for gf_connected_components() \nspark_set_checkpoint_dir(sc, tempdir()) \ng <- gf_friends(sc) \ngf_connected_components(g)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertices.html",
    "href": "packages/graphframes/latest/reference/gf_vertices.html",
    "title": "Extract vertices DataFrame",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertices.html#gf_vertices",
    "href": "packages/graphframes/latest/reference/gf_vertices.html#gf_vertices",
    "title": "Extract vertices DataFrame",
    "section": "gf_vertices",
    "text": "gf_vertices"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertices.html#description",
    "href": "packages/graphframes/latest/reference/gf_vertices.html#description",
    "title": "Extract vertices DataFrame",
    "section": "Description",
    "text": "Description\nExtract vertices DataFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertices.html#usage",
    "href": "packages/graphframes/latest/reference/gf_vertices.html#usage",
    "title": "Extract vertices DataFrame",
    "section": "Usage",
    "text": "Usage\ngf_vertices(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertices.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_vertices.html#arguments",
    "title": "Extract vertices DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html",
    "title": "Shortest paths",
    "section": "",
    "text": "R/gf_shortest_paths.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#gf_shortest_paths",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#gf_shortest_paths",
    "title": "Shortest paths",
    "section": "gf_shortest_paths",
    "text": "gf_shortest_paths"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#description",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#description",
    "title": "Shortest paths",
    "section": "Description",
    "text": "Description\nComputes shortest paths from every vertex to the given set of landmark vertices. Note that this takes edge direction into account."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#usage",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#usage",
    "title": "Shortest paths",
    "section": "Usage",
    "text": "Usage\ngf_shortest_paths(x, landmarks, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#arguments",
    "title": "Shortest paths",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\nlandmarks\nIDs of landmark vertices.\n\n\n…\nOptional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#examples",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#examples",
    "title": "Shortest paths",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ng <- gf_friends(sc) \ngf_shortest_paths(g, landmarks = c(\"a\", \"d\"))"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_out_degrees.html",
    "href": "packages/graphframes/latest/reference/gf_out_degrees.html",
    "title": "Out-degrees of vertices",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_out_degrees.html#gf_out_degrees",
    "href": "packages/graphframes/latest/reference/gf_out_degrees.html#gf_out_degrees",
    "title": "Out-degrees of vertices",
    "section": "gf_out_degrees",
    "text": "gf_out_degrees"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_out_degrees.html#description",
    "href": "packages/graphframes/latest/reference/gf_out_degrees.html#description",
    "title": "Out-degrees of vertices",
    "section": "Description",
    "text": "Description\nOut-degrees of vertices"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_out_degrees.html#usage",
    "href": "packages/graphframes/latest/reference/gf_out_degrees.html#usage",
    "title": "Out-degrees of vertices",
    "section": "Usage",
    "text": "Usage\ngf_out_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_out_degrees.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_out_degrees.html#arguments",
    "title": "Out-degrees of vertices",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_unpersist.html",
    "href": "packages/graphframes/latest/reference/gf_unpersist.html",
    "title": "Unpersist the GraphFrame",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_unpersist.html#gf_unpersist",
    "href": "packages/graphframes/latest/reference/gf_unpersist.html#gf_unpersist",
    "title": "Unpersist the GraphFrame",
    "section": "gf_unpersist",
    "text": "gf_unpersist"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_unpersist.html#description",
    "href": "packages/graphframes/latest/reference/gf_unpersist.html#description",
    "title": "Unpersist the GraphFrame",
    "section": "Description",
    "text": "Description\nUnpersist the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_unpersist.html#usage",
    "href": "packages/graphframes/latest/reference/gf_unpersist.html#usage",
    "title": "Unpersist the GraphFrame",
    "section": "Usage",
    "text": "Usage\ngf_unpersist(x, blocking = FALSE)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_unpersist.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_unpersist.html#arguments",
    "title": "Unpersist the GraphFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\nblocking\nwhether to block until all blocks are deleted"
  },
  {
    "objectID": "packages/graphframes/latest/reference/index.html",
    "href": "packages/graphframes/latest/reference/index.html",
    "title": "graphframes",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ngf_bfs()\nBreadth-first search (BFS)\n\n\ngf_cache()\nCache the GraphFrame\n\n\ngf_chain()\nChain graph\n\n\ngf_connected_components()\nConnected components\n\n\ngf_degrees()\nDegrees of vertices\n\n\ngf_edge_columns()\nEdges column names\n\n\ngf_edges()\nExtract edges DataFrame\n\n\ngf_find()\nMotif finding: Searching the graph for structural patterns\n\n\ngf_friends()\nGraph of friends in a social network.\n\n\ngf_graphframe()\nCreate a new GraphFrame\n\n\ngf_grid_ising_model()\nGenerate a grid Ising model with random parameters\n\n\ngf_in_degrees()\nIn-degrees of vertices\n\n\ngf_lpa()\nLabel propagation algorithm (LPA)\n\n\ngf_out_degrees()\nOut-degrees of vertices\n\n\ngf_pagerank()\nPageRank\n\n\ngf_persist()\nPersist the GraphFrame\n\n\ngf_register()\nRegister a GraphFrame object\n\n\ngf_scc()\nStrongly connected components\n\n\ngf_shortest_paths()\nShortest paths\n\n\ngf_star()\nGenerate a star graph\n\n\ngf_triangle_count()\nComputes the number of triangles passing through each vertex.\n\n\ngf_triplets()\nTriplets of graph\n\n\ngf_two_blobs()\nGenerate two blobs\n\n\ngf_unpersist()\nUnpersist the GraphFrame\n\n\ngf_vertex_columns()\nVertices column names\n\n\ngf_vertices()\nExtract vertices DataFrame\n\n\nspark_graphframe() spark_graphframe()\nRetrieve a GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html",
    "href": "packages/graphframes/latest/reference/gf_lpa.html",
    "title": "Label propagation algorithm (LPA)",
    "section": "",
    "text": "R/gf_lpa.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#gf_lpa",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#gf_lpa",
    "title": "Label propagation algorithm (LPA)",
    "section": "gf_lpa",
    "text": "gf_lpa"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#description",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#description",
    "title": "Label propagation algorithm (LPA)",
    "section": "Description",
    "text": "Description\nRun static Label Propagation for detecting communities in networks. Each node in the network is initially assigned to its own community. At every iteration, nodes send their community affiliation to all neighbors and update their state to the mode community affiliation of incoming messages. LPA is a standard community detection algorithm for graphs. It is very inexpensive computationally, although (1) convergence is not guaranteed and (2) one can end up with trivial solutions (all nodes are identified into a single community)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#usage",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#usage",
    "title": "Label propagation algorithm (LPA)",
    "section": "Usage",
    "text": "Usage\ngf_lpa(x, max_iter, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#arguments",
    "title": "Label propagation algorithm (LPA)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\nmax_iter\nMaximum number of iterations.\n\n\n…\nOptional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#examples",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#examples",
    "title": "Label propagation algorithm (LPA)",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ng <- gf_friends(sc) \ngf_lpa(g, max_iter = 5)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html",
    "title": "Generate a grid Ising model with random parameters",
    "section": "",
    "text": "R/gf_examples.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#gf_grid_ising_model",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#gf_grid_ising_model",
    "title": "Generate a grid Ising model with random parameters",
    "section": "gf_grid_ising_model",
    "text": "gf_grid_ising_model"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#description",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#description",
    "title": "Generate a grid Ising model with random parameters",
    "section": "Description",
    "text": "Description\nGenerate a grid Ising model with random parameters"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#usage",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#usage",
    "title": "Generate a grid Ising model with random parameters",
    "section": "Usage",
    "text": "Usage\ngf_grid_ising_model(sc, n, v_std = 1, e_std = 1)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#arguments",
    "title": "Generate a grid Ising model with random parameters",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nLength of one side of the grid. The grid will be of size n x n.\n\n\nv_std\nStandard deviation of normal distribution used to generate vertex factors “a”. Default of 1.0.\n\n\ne_std\nStandard deviation of normal distribution used to generate edge factors “b”. Default of 1.0."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#details",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#details",
    "title": "Generate a grid Ising model with random parameters",
    "section": "Details",
    "text": "Details\nThis method generates a grid Ising model with random parameters. Ising models are probabilistic graphical models over binary variables xi. Each binary variable xi corresponds to one vertex, and it may take values -1 or +1. The probability distribution P(X) (over all xi) is parameterized by vertex factors ai and edge factors bij:\n\\(P(X) = (1/Z) * exp[ \\sum_i a_i x_i + \\sum_{ij} b_{ij} x_i x_j ]\\)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#value",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#value",
    "title": "Generate a grid Ising model with random parameters",
    "section": "Value",
    "text": "Value\nGraphFrame. Vertices have columns “id” and “a”. Edges have columns “src”, “dst”, and “b”. Edges are directed, but they should be treated as undirected in any algorithms run on this model. Vertex IDs are of the form “i,j”. E.g., vertex “1,3” is in the second row and fourth column of the grid."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#examples",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#examples",
    "title": "Generate a grid Ising model with random parameters",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ngf_grid_ising_model(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html",
    "href": "packages/graphframes/latest/reference/gf_friends.html",
    "title": "Graph of friends in a social network.",
    "section": "",
    "text": "R/gf_examples.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#gf_friends",
    "href": "packages/graphframes/latest/reference/gf_friends.html#gf_friends",
    "title": "Graph of friends in a social network.",
    "section": "gf_friends",
    "text": "gf_friends"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#description",
    "href": "packages/graphframes/latest/reference/gf_friends.html#description",
    "title": "Graph of friends in a social network.",
    "section": "Description",
    "text": "Description\nGraph of friends in a social network."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#usage",
    "href": "packages/graphframes/latest/reference/gf_friends.html#usage",
    "title": "Graph of friends in a social network.",
    "section": "Usage",
    "text": "Usage\ngf_friends(sc)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_friends.html#arguments",
    "title": "Graph of friends in a social network.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#examples",
    "href": "packages/graphframes/latest/reference/gf_friends.html#examples",
    "title": "Graph of friends in a social network.",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\") \ngf_friends(sc)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html",
    "href": "packages/graphframes/latest/reference/gf_find.html",
    "title": "Motif finding: Searching the graph for structural patterns",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#gf_find",
    "href": "packages/graphframes/latest/reference/gf_find.html#gf_find",
    "title": "Motif finding: Searching the graph for structural patterns",
    "section": "gf_find",
    "text": "gf_find"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#description",
    "href": "packages/graphframes/latest/reference/gf_find.html#description",
    "title": "Motif finding: Searching the graph for structural patterns",
    "section": "Description",
    "text": "Description\nMotif finding uses a simple Domain-Specific Language (DSL) for expressing structural queries. For example, gf_find(g, “(a)-[e]->(b); (b)-[e2]->(a)”) will search for pairs of vertices a,b connected by edges in both directions. It will return a DataFrame of all such structures in the graph, with columns for each of the named elements (vertices or edges) in the motif. In this case, the returned columns will be in order of the pattern: “a, e, b, e2.”"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#usage",
    "href": "packages/graphframes/latest/reference/gf_find.html#usage",
    "title": "Motif finding: Searching the graph for structural patterns",
    "section": "Usage",
    "text": "Usage\ngf_find(x, pattern)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_find.html#arguments",
    "title": "Motif finding: Searching the graph for structural patterns",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\npattern\npattern specifying a motif to search for"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#examples",
    "href": "packages/graphframes/latest/reference/gf_find.html#examples",
    "title": "Motif finding: Searching the graph for structural patterns",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ngf_friends(sc) %>% \n  gf_find(\"(a)-[e]->(b); (b)-[e2]->(a)\")"
  },
  {
    "objectID": "packages/graphframes/latest/reference/spark_graphframe.html",
    "href": "packages/graphframes/latest/reference/spark_graphframe.html",
    "title": "Retrieve a GraphFrame",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/spark_graphframe.html#spark_graphframe",
    "href": "packages/graphframes/latest/reference/spark_graphframe.html#spark_graphframe",
    "title": "Retrieve a GraphFrame",
    "section": "spark_graphframe",
    "text": "spark_graphframe"
  },
  {
    "objectID": "packages/graphframes/latest/reference/spark_graphframe.html#description",
    "href": "packages/graphframes/latest/reference/spark_graphframe.html#description",
    "title": "Retrieve a GraphFrame",
    "section": "Description",
    "text": "Description\nRetrieve a GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/spark_graphframe.html#usage",
    "href": "packages/graphframes/latest/reference/spark_graphframe.html#usage",
    "title": "Retrieve a GraphFrame",
    "section": "Usage",
    "text": "Usage\nspark_graphframe(x, ...) \n\nspark_graphframe(x, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/spark_graphframe.html#arguments",
    "href": "packages/graphframes/latest/reference/spark_graphframe.html#arguments",
    "title": "Retrieve a GraphFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\n…\nadditional arguments, not used"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edges.html",
    "href": "packages/graphframes/latest/reference/gf_edges.html",
    "title": "Extract edges DataFrame",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edges.html#gf_edges",
    "href": "packages/graphframes/latest/reference/gf_edges.html#gf_edges",
    "title": "Extract edges DataFrame",
    "section": "gf_edges",
    "text": "gf_edges"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edges.html#description",
    "href": "packages/graphframes/latest/reference/gf_edges.html#description",
    "title": "Extract edges DataFrame",
    "section": "Description",
    "text": "Description\nExtract edges DataFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edges.html#usage",
    "href": "packages/graphframes/latest/reference/gf_edges.html#usage",
    "title": "Extract edges DataFrame",
    "section": "Usage",
    "text": "Usage\ngf_edges(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edges.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_edges.html#arguments",
    "title": "Extract edges DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edge_columns.html",
    "href": "packages/graphframes/latest/reference/gf_edge_columns.html",
    "title": "Edges column names",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edge_columns.html#gf_edge_columns",
    "href": "packages/graphframes/latest/reference/gf_edge_columns.html#gf_edge_columns",
    "title": "Edges column names",
    "section": "gf_edge_columns",
    "text": "gf_edge_columns"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edge_columns.html#description",
    "href": "packages/graphframes/latest/reference/gf_edge_columns.html#description",
    "title": "Edges column names",
    "section": "Description",
    "text": "Description\nEdges column names"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edge_columns.html#usage",
    "href": "packages/graphframes/latest/reference/gf_edge_columns.html#usage",
    "title": "Edges column names",
    "section": "Usage",
    "text": "Usage\ngf_edge_columns(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edge_columns.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_edge_columns.html#arguments",
    "title": "Edges column names",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html",
    "href": "packages/graphframes/latest/reference/gf_bfs.html",
    "title": "Breadth-first search (BFS)",
    "section": "",
    "text": "R/gf_bfs.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#gf_bfs",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#gf_bfs",
    "title": "Breadth-first search (BFS)",
    "section": "gf_bfs",
    "text": "gf_bfs"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#description",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#description",
    "title": "Breadth-first search (BFS)",
    "section": "Description",
    "text": "Description\nBreadth-first search (BFS)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#usage",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#usage",
    "title": "Breadth-first search (BFS)",
    "section": "Usage",
    "text": "Usage\ngf_bfs(x, from_expr, to_expr, max_path_length = 10, edge_filter = NULL, \n  ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#arguments",
    "title": "Breadth-first search (BFS)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\nfrom_expr\nSpark SQL expression specifying valid starting vertices for the BFS.\n\n\nto_expr\nSpark SQL expression specifying valid target vertices for the BFS.\n\n\nmax_path_length\nLimit on the length of paths.\n\n\nedge_filter\nSpark SQL expression specifying edges which may be used in the search.\n\n\n…\nOptional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#examples",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#examples",
    "title": "Breadth-first search (BFS)",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ng <- gf_friends(sc) \ngf_bfs(g, from_expr = \"name = 'Esther'\", to_expr = \"age < 32\")"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html",
    "title": "Generate two blobs",
    "section": "",
    "text": "R/gf_examples.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#gf_two_blobs",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#gf_two_blobs",
    "title": "Generate two blobs",
    "section": "gf_two_blobs",
    "text": "gf_two_blobs"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#description",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#description",
    "title": "Generate two blobs",
    "section": "Description",
    "text": "Description\nTwo densely connected blobs (vertices 0->n-1 and n->2n-1) connected by a single edge (0->n)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#usage",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#usage",
    "title": "Generate two blobs",
    "section": "Usage",
    "text": "Usage\ngf_two_blobs(sc, blob_size)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#arguments",
    "title": "Generate two blobs",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nblob_size\nThe size of each blob."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#examples",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#examples",
    "title": "Generate two blobs",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ngf_two_blobs(sc, 3)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html",
    "title": "Computes the number of triangles passing through each vertex.",
    "section": "",
    "text": "R/gf_triangle_count.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#gf_triangle_count",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#gf_triangle_count",
    "title": "Computes the number of triangles passing through each vertex.",
    "section": "gf_triangle_count",
    "text": "gf_triangle_count"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#description",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#description",
    "title": "Computes the number of triangles passing through each vertex.",
    "section": "Description",
    "text": "Description\nThis algorithm ignores edge direction; i.e., all edges are treated as undirected. In a multigraph, duplicate edges will be counted only once."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#usage",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#usage",
    "title": "Computes the number of triangles passing through each vertex.",
    "section": "Usage",
    "text": "Usage\ngf_triangle_count(x, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#arguments",
    "title": "Computes the number of triangles passing through each vertex.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\n…\nOptional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#examples",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#examples",
    "title": "Computes the number of triangles passing through each vertex.",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ng <- gf_friends(sc) \ngf_triangle_count(g)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html",
    "href": "packages/graphframes/latest/reference/gf_chain.html",
    "title": "Chain graph",
    "section": "",
    "text": "R/gf_examples.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#gf_chain",
    "href": "packages/graphframes/latest/reference/gf_chain.html#gf_chain",
    "title": "Chain graph",
    "section": "gf_chain",
    "text": "gf_chain"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#description",
    "href": "packages/graphframes/latest/reference/gf_chain.html#description",
    "title": "Chain graph",
    "section": "Description",
    "text": "Description\nReturns a chain graph of the given size with Long ID type. The vertex IDs are 0, 1, …, n-1, and the edges are (0, 1), (1, 2), …., (n-2, n-1)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#usage",
    "href": "packages/graphframes/latest/reference/gf_chain.html#usage",
    "title": "Chain graph",
    "section": "Usage",
    "text": "Usage\ngf_chain(sc, n)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_chain.html#arguments",
    "title": "Chain graph",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSize of the graph to return."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#examples",
    "href": "packages/graphframes/latest/reference/gf_chain.html#examples",
    "title": "Chain graph",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ngf_chain(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_cache.html",
    "href": "packages/graphframes/latest/reference/gf_cache.html",
    "title": "Cache the GraphFrame",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_cache.html#gf_cache",
    "href": "packages/graphframes/latest/reference/gf_cache.html#gf_cache",
    "title": "Cache the GraphFrame",
    "section": "gf_cache",
    "text": "gf_cache"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_cache.html#description",
    "href": "packages/graphframes/latest/reference/gf_cache.html#description",
    "title": "Cache the GraphFrame",
    "section": "Description",
    "text": "Description\nCache the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_cache.html#usage",
    "href": "packages/graphframes/latest/reference/gf_cache.html#usage",
    "title": "Cache the GraphFrame",
    "section": "Usage",
    "text": "Usage\ngf_cache(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_cache.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_cache.html#arguments",
    "title": "Cache the GraphFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html",
    "href": "packages/graphframes/latest/reference/gf_star.html",
    "title": "Generate a star graph",
    "section": "",
    "text": "R/gf_examples.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#gf_star",
    "href": "packages/graphframes/latest/reference/gf_star.html#gf_star",
    "title": "Generate a star graph",
    "section": "gf_star",
    "text": "gf_star"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#description",
    "href": "packages/graphframes/latest/reference/gf_star.html#description",
    "title": "Generate a star graph",
    "section": "Description",
    "text": "Description\nReturns a star graph with Long ID type, consisting of a central element indexed 0 (the root) and the n other leaf vertices 1, 2, …, n."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#usage",
    "href": "packages/graphframes/latest/reference/gf_star.html#usage",
    "title": "Generate a star graph",
    "section": "Usage",
    "text": "Usage\ngf_star(sc, n)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_star.html#arguments",
    "title": "Generate a star graph",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nThe number of leaves."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#examples",
    "href": "packages/graphframes/latest/reference/gf_star.html#examples",
    "title": "Generate a star graph",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ngf_star(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html",
    "title": "PageRank",
    "section": "",
    "text": "R/gf_pagerank.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#gf_pagerank",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#gf_pagerank",
    "title": "PageRank",
    "section": "gf_pagerank",
    "text": "gf_pagerank"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#description",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#description",
    "title": "PageRank",
    "section": "Description",
    "text": "Description\nPageRank"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#usage",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#usage",
    "title": "PageRank",
    "section": "Usage",
    "text": "Usage\ngf_pagerank(x, tol = NULL, reset_probability = 0.15, max_iter = NULL, \n  source_id = NULL, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#arguments",
    "title": "PageRank",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\ntol\nTolerance.\n\n\nreset_probability\nReset probability.\n\n\nmax_iter\nMaximum number of iterations.\n\n\nsource_id\n(Optional) Source vertex for a personalized pagerank.\n\n\n…\nOptional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#examples",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#examples",
    "title": "PageRank",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ng <- gf_friends(sc) \ngf_pagerank(g, reset_probability = 0.15, tol = 0.01)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triplets.html",
    "href": "packages/graphframes/latest/reference/gf_triplets.html",
    "title": "Triplets of graph",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triplets.html#gf_triplets",
    "href": "packages/graphframes/latest/reference/gf_triplets.html#gf_triplets",
    "title": "Triplets of graph",
    "section": "gf_triplets",
    "text": "gf_triplets"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triplets.html#description",
    "href": "packages/graphframes/latest/reference/gf_triplets.html#description",
    "title": "Triplets of graph",
    "section": "Description",
    "text": "Description\nTriplets of graph"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triplets.html#usage",
    "href": "packages/graphframes/latest/reference/gf_triplets.html#usage",
    "title": "Triplets of graph",
    "section": "Usage",
    "text": "Usage\ngf_triplets(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triplets.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_triplets.html#arguments",
    "title": "Triplets of graph",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_persist.html",
    "href": "packages/graphframes/latest/reference/gf_persist.html",
    "title": "Persist the GraphFrame",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_persist.html#gf_persist",
    "href": "packages/graphframes/latest/reference/gf_persist.html#gf_persist",
    "title": "Persist the GraphFrame",
    "section": "gf_persist",
    "text": "gf_persist"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_persist.html#description",
    "href": "packages/graphframes/latest/reference/gf_persist.html#description",
    "title": "Persist the GraphFrame",
    "section": "Description",
    "text": "Description\nPersist the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_persist.html#usage",
    "href": "packages/graphframes/latest/reference/gf_persist.html#usage",
    "title": "Persist the GraphFrame",
    "section": "Usage",
    "text": "Usage\ngf_persist(x, storage_level = \"MEMORY_AND_DISK\")"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_persist.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_persist.html#arguments",
    "title": "Persist the GraphFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\nstorage_level\nThe storage level to be used. Please view the Spark Documentationfor information on what storage levels are accepted."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html",
    "href": "packages/graphframes/latest/reference/gf_scc.html",
    "title": "Strongly connected components",
    "section": "",
    "text": "R/gf_scc.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#gf_scc",
    "href": "packages/graphframes/latest/reference/gf_scc.html#gf_scc",
    "title": "Strongly connected components",
    "section": "gf_scc",
    "text": "gf_scc"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#description",
    "href": "packages/graphframes/latest/reference/gf_scc.html#description",
    "title": "Strongly connected components",
    "section": "Description",
    "text": "Description\nCompute the strongly connected component (SCC) of each vertex and return a DataFrame with each vertex assigned to the SCC containing that vertex."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#usage",
    "href": "packages/graphframes/latest/reference/gf_scc.html#usage",
    "title": "Strongly connected components",
    "section": "Usage",
    "text": "Usage\ngf_scc(x, max_iter, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_scc.html#arguments",
    "title": "Strongly connected components",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\nmax_iter\nMaximum number of iterations.\n\n\n…\nOptional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#examples",
    "href": "packages/graphframes/latest/reference/gf_scc.html#examples",
    "title": "Strongly connected components",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ng <- gf_friends(sc) \ngf_scc(g, max_iter = 10)"
  },
  {
    "objectID": "packages/sparkxgb/latest/news.html",
    "href": "packages/sparkxgb/latest/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "sparkxgb 0.1.1\n\nsparkxgb is a sparklyr extension that provides an interface to XGBoost on Spark."
  },
  {
    "objectID": "packages/sparkxgb/latest/index.html",
    "href": "packages/sparkxgb/latest/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "sparkxgb is a sparklyr extension that provides an interface to XGBoost on Spark.\n\n\n\nYou can install the development version of sparkxgb with:\n# sparkxgb requires the development version of sparklyr\ndevtools::install_github(\"rstudio/sparklyr\")\ndevtools::install_github(\"rstudio/sparkxgb\")\n\n\n\nsparkxgb supports the familiar formula interface for specifying models:\nlibrary(sparkxgb)\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris)\n\nxgb_model <- xgboost_classifier(\n  iris_tbl,\n  Species ~ .,\n  num_class = 3,\n  num_round = 50,\n  max_depth = 4\n)\n\nxgb_model %>%\n  ml_predict(iris_tbl) %>%\n  select(Species, predicted_label, starts_with(\"probability_\")) %>%\n  glimpse()\n#> Observations: ??\n#> Variables: 5\n#> Database: spark_connection\n#> $ Species                <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"…\n#> $ predicted_label        <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"…\n#> $ probability_versicolor <dbl> 0.003566429, 0.003564076, 0.003566429, 0.…\n#> $ probability_virginica  <dbl> 0.001423170, 0.002082058, 0.001423170, 0.…\n#> $ probability_setosa     <dbl> 0.9950104, 0.9943539, 0.9950104, 0.995010…\nIt also provides a Pipelines API, which means you can use a xgboost_classifier or xgboost_regressor in a pipeline as any Estimator, and do things like hyperparameter tuning:\npipeline <- ml_pipeline(sc) %>%\n  ft_r_formula(Species ~ .) %>%\n  xgboost_classifier(num_class = 3)\n\nparam_grid <- list(\n  xgboost = list(\n    max_depth = c(1, 5),\n    num_round = c(10, 50)\n  )\n)\n\ncv <- ml_cross_validator(\n  sc,\n  estimator = pipeline,\n  evaluator = ml_multiclass_classification_evaluator(\n    sc,\n    label_col = \"label\",\n    raw_prediction_col = \"rawPrediction\"\n  ),\n  estimator_param_maps = param_grid\n)\n\ncv_model <- cv %>%\n  ml_fit(iris_tbl)\n\nsummary(cv_model)\n#> Summary for CrossValidatorModel\n#>             <cross_validator_ebc61803a06b>\n#>\n#> Tuned Pipeline\n#>   with metric f1\n#>   over 4 hyperparameter sets\n#>   via 3-fold cross validation\n#>\n#> Estimator: Pipeline\n#>            <pipeline_ebc62f635bb6>\n#> Evaluator: MulticlassClassificationEvaluator\n#>            <multiclass_classification_evaluator_ebc65fbf8a19>\n#>\n#> Results Summary:\n#>          f1 num_round_1 max_depth_1\n#> 1 0.9549670          10           1\n#> 2 0.9674460          10           5\n#> 3 0.9488665          50           1\n#> 4 0.9613854          50           5"
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/index.html",
    "href": "packages/sparkxgb/latest/reference/index.html",
    "title": "sparkxgb",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\nxgboost_classifier()\nXGBoost Classifier\n\n\nxgboost_regressor()\nXGBoost Regressor"
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html",
    "title": "XGBoost Regressor",
    "section": "",
    "text": "R/xgboost_regressor.R"
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#xgboost_regressor",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#xgboost_regressor",
    "title": "XGBoost Regressor",
    "section": "xgboost_regressor",
    "text": "xgboost_regressor"
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#description",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#description",
    "title": "XGBoost Regressor",
    "section": "Description",
    "text": "Description\nXGBoost regressor for Spark."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#usage",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#usage",
    "title": "XGBoost Regressor",
    "section": "Usage",
    "text": "Usage\nxgboost_regressor( \n  x, \n  formula = NULL, \n  eta = 0.3, \n  gamma = 0, \n  max_depth = 6, \n  min_child_weight = 1, \n  max_delta_step = 0, \n  grow_policy = \"depthwise\", \n  max_bins = 16, \n  subsample = 1, \n  colsample_bytree = 1, \n  colsample_bylevel = 1, \n  lambda = 1, \n  alpha = 0, \n  tree_method = \"auto\", \n  sketch_eps = 0.03, \n  scale_pos_weight = 1, \n  sample_type = \"uniform\", \n  normalize_type = \"tree\", \n  rate_drop = 0, \n  skip_drop = 0, \n  lambda_bias = 0, \n  tree_limit = 0, \n  num_round = 1, \n  num_workers = 1, \n  nthread = 1, \n  use_external_memory = FALSE, \n  silent = 0, \n  custom_obj = NULL, \n  custom_eval = NULL, \n  missing = NaN, \n  seed = 0, \n  timeout_request_workers = 30 * 60 * 1000, \n  checkpoint_path = \"\", \n  checkpoint_interval = -1, \n  objective = \"reg:linear\", \n  base_score = 0.5, \n  train_test_ratio = 1, \n  num_early_stopping_rounds = 0, \n  objective_type = \"regression\", \n  eval_metric = NULL, \n  maximize_evaluation_metrics = FALSE, \n  base_margin_col = NULL, \n  weight_col = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"xgboost_regressor_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#arguments",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#arguments",
    "title": "XGBoost Regressor",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\neta\nStep size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features and eta actually shrinks the feature weights to make the boosting process more conservative. [default=0.3] range: [0,1]\n\n\ngamma\nMinimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be. [default=0]\n\n\nmax_depth\nMaximum depth of a tree, increase this value will make model more complex / likely to be overfitting. [default=6]\n\n\nmin_child_weight\nMinimum sum of instance weight(hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be. [default=1]\n\n\nmax_delta_step\nMaximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update. [default=0]\n\n\ngrow_policy\nGrowth policy for fast histogram algorithm.\n\n\nmax_bins\nMaximum number of bins in histogram.\n\n\nsubsample\nSubsample ratio of the training instance. Setting it to 0.5 means that XGBoost randomly collected half of the data instances to grow trees and this will prevent overfitting. [default=1] range:(0,1]\n\n\ncolsample_bytree\nSubsample ratio of columns when constructing each tree. [default=1] range: (0,1]\n\n\ncolsample_bylevel\nSubsample ratio of columns for each split, in each level. [default=1] range: (0,1]\n\n\nlambda\nL2 regularization term on weights, increase this value will make model more conservative. [default=1]\n\n\nalpha\nL1 regularization term on weights, increase this value will make model more conservative, defaults to 0.\n\n\ntree_method\nThe tree construction algorithm used in XGBoost. options: ‘auto’, ‘exact’, ‘approx’ [default=‘auto’]\n\n\nsketch_eps\nThis is only used for approximate greedy algorithm. This roughly translated into O(1 / sketch_eps) number of bins. Compared to directly select number of bins, this comes with theoretical guarantee with sketch accuracy. [default=0.03] range: (0, 1)\n\n\nscale_pos_weight\nControl the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative cases) / sum(positive cases). [default=1]\n\n\nsample_type\nParameter for Dart booster. Type of sampling algorithm. “uniform”: dropped trees are selected uniformly. “weighted”: dropped trees are selected in proportion to weight. [default=“uniform”]\n\n\nnormalize_type\nParameter of Dart booster. type of normalization algorithm, options: ‘tree’, ‘forest’ . [default=“tree”]\n\n\nrate_drop\nParameter of Dart booster. dropout rate. [default=0.0] range: [0.0, 1.0]\n\n\nskip_drop\nParameter of Dart booster. probability of skip dropout. If a dropout is skipped, new trees are added in the same manner as gbtree. [default=0.0] range: [0.0, 1.0]\n\n\nlambda_bias\nParameter of linear booster L2 regularization term on bias, default 0 (no L1 reg on bias because it is not important.)\n\n\ntree_limit\nLimit number of trees in the prediction; defaults to 0 (use all trees.)\n\n\nnum_round\nThe number of rounds for boosting.\n\n\nnum_workers\nnumber of workers used to train xgboost model. Defaults to 1.\n\n\nnthread\nNumber of threads used by per worker. Defaults to 1.\n\n\nuse_external_memory\nThe tree construction algorithm used in XGBoost. options: ‘auto’, ‘exact’, ‘approx’ [default=‘auto’]\n\n\nsilent\n0 means printing running messages, 1 means silent mode. default: 0\n\n\ncustom_obj\nCustomized objective function provided by user. Currently unsupported.\n\n\ncustom_eval\nCustomized evaluation function provided by user. Currently unsupported.\n\n\nmissing\nThe value treated as missing. default: Float.NaN\n\n\nseed\nRandom seed for the C++ part of XGBoost and train/test splitting.\n\n\ntimeout_request_workers\nthe maximum time to wait for the job requesting new workers. default: 30 minutes\n\n\ncheckpoint_path\nThe hdfs folder to load and save checkpoint boosters.\n\n\ncheckpoint_interval\nParam for set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the trained model will get checkpointed every 10 iterations. Note: checkpoint_path must also be set if the checkpoint interval is greater than 0.\n\n\nobjective\nSpecify the learning task and the corresponding learning objective. options: reg:linear, reg:logistic, binary:logistic, binary:logitraw, count:poisson, multi:softmax, multi:softprob, rank:pairwise, reg:gamma. default: reg:linear.\n\n\nbase_score\nParam for initial prediction (aka base margin) column name. Defaults to 0.5.\n\n\ntrain_test_ratio\nFraction of training points to use for testing.\n\n\nnum_early_stopping_rounds\nIf non-zero, the training will be stopped after a specified number of consecutive increases in any evaluation metric.\n\n\nobjective_type\nThe learning objective type of the specified custom objective and eval. Corresponding type will be assigned if custom objective is defined options: regression, classification.\n\n\neval_metric\nEvaluation metrics for validation data, a default metric will be assigned according to objective(rmse for regression, and error for classification, mean average precision for ranking). options: rmse, mae, logloss, error, merror, mlogloss, auc, aucpr, ndcg, map, gamma-deviance\n\n\nmaximize_evaluation_metrics\nWhether to maximize evaluation metrics. Defaults to FALSE (for minization.)\n\n\nbase_margin_col\nParam for initial prediction (aka base margin) column name.\n\n\nweight_col\nWeight column.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#details",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#details",
    "title": "XGBoost Regressor",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#value",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#value",
    "title": "XGBoost Regressor",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula\nis specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#see-also",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#see-also",
    "title": "XGBoost Regressor",
    "section": "See Also",
    "text": "See Also\nSee http://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_classifier.html",
    "href": "packages/sparkxgb/latest/reference/xgboost_classifier.html",
    "title": "XGBoost Classifier",
    "section": "",
    "text": "R/xgboost_classifier.R"
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_classifier.html#xgboost_classifier",
    "href": "packages/sparkxgb/latest/reference/xgboost_classifier.html#xgboost_classifier",
    "title": "XGBoost Classifier",
    "section": "xgboost_classifier",
    "text": "xgboost_classifier"
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_classifier.html#description",
    "href": "packages/sparkxgb/latest/reference/xgboost_classifier.html#description",
    "title": "XGBoost Classifier",
    "section": "Description",
    "text": "Description\nXGBoost classifier for Spark."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_classifier.html#usage",
    "href": "packages/sparkxgb/latest/reference/xgboost_classifier.html#usage",
    "title": "XGBoost Classifier",
    "section": "Usage",
    "text": "Usage\nxgboost_classifier( \n  x, \n  formula = NULL, \n  eta = 0.3, \n  gamma = 0, \n  max_depth = 6, \n  min_child_weight = 1, \n  max_delta_step = 0, \n  grow_policy = \"depthwise\", \n  max_bins = 16, \n  subsample = 1, \n  colsample_bytree = 1, \n  colsample_bylevel = 1, \n  lambda = 1, \n  alpha = 0, \n  tree_method = \"auto\", \n  sketch_eps = 0.03, \n  scale_pos_weight = 1, \n  sample_type = \"uniform\", \n  normalize_type = \"tree\", \n  rate_drop = 0, \n  skip_drop = 0, \n  lambda_bias = 0, \n  tree_limit = 0, \n  num_round = 1, \n  num_workers = 1, \n  nthread = 1, \n  use_external_memory = FALSE, \n  silent = 0, \n  custom_obj = NULL, \n  custom_eval = NULL, \n  missing = NaN, \n  seed = 0, \n  timeout_request_workers = 30 * 60 * 1000, \n  checkpoint_path = \"\", \n  checkpoint_interval = -1, \n  objective = \"multi:softprob\", \n  base_score = 0.5, \n  train_test_ratio = 1, \n  num_early_stopping_rounds = 0, \n  objective_type = \"classification\", \n  eval_metric = NULL, \n  maximize_evaluation_metrics = FALSE, \n  num_class = NULL, \n  base_margin_col = NULL, \n  thresholds = NULL, \n  weight_col = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  uid = random_string(\"xgboost_classifier_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_classifier.html#arguments",
    "href": "packages/sparkxgb/latest/reference/xgboost_classifier.html#arguments",
    "title": "XGBoost Classifier",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\neta\nStep size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features and eta actually shrinks the feature weights to make the boosting process more conservative. [default=0.3] range: [0,1]\n\n\ngamma\nMinimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be. [default=0]\n\n\nmax_depth\nMaximum depth of a tree, increase this value will make model more complex / likely to be overfitting. [default=6]\n\n\nmin_child_weight\nMinimum sum of instance weight(hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be. [default=1]\n\n\nmax_delta_step\nMaximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update. [default=0]\n\n\ngrow_policy\nGrowth policy for fast histogram algorithm.\n\n\nmax_bins\nMaximum number of bins in histogram.\n\n\nsubsample\nSubsample ratio of the training instance. Setting it to 0.5 means that XGBoost randomly collected half of the data instances to grow trees and this will prevent overfitting. [default=1] range:(0,1]\n\n\ncolsample_bytree\nSubsample ratio of columns when constructing each tree. [default=1] range: (0,1]\n\n\ncolsample_bylevel\nSubsample ratio of columns for each split, in each level. [default=1] range: (0,1]\n\n\nlambda\nL2 regularization term on weights, increase this value will make model more conservative. [default=1]\n\n\nalpha\nL1 regularization term on weights, increase this value will make model more conservative, defaults to 0.\n\n\ntree_method\nThe tree construction algorithm used in XGBoost. options: ‘auto’, ‘exact’, ‘approx’ [default=‘auto’]\n\n\nsketch_eps\nThis is only used for approximate greedy algorithm. This roughly translated into O(1 / sketch_eps) number of bins. Compared to directly select number of bins, this comes with theoretical guarantee with sketch accuracy. [default=0.03] range: (0, 1)\n\n\nscale_pos_weight\nControl the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative cases) / sum(positive cases). [default=1]\n\n\nsample_type\nParameter for Dart booster. Type of sampling algorithm. “uniform”: dropped trees are selected uniformly. “weighted”: dropped trees are selected in proportion to weight. [default=“uniform”]\n\n\nnormalize_type\nParameter of Dart booster. type of normalization algorithm, options: ‘tree’, ‘forest’ . [default=“tree”]\n\n\nrate_drop\nParameter of Dart booster. dropout rate. [default=0.0] range: [0.0, 1.0]\n\n\nskip_drop\nParameter of Dart booster. probability of skip dropout. If a dropout is skipped, new trees are added in the same manner as gbtree. [default=0.0] range: [0.0, 1.0]\n\n\nlambda_bias\nParameter of linear booster L2 regularization term on bias, default 0 (no L1 reg on bias because it is not important.)\n\n\ntree_limit\nLimit number of trees in the prediction; defaults to 0 (use all trees.)\n\n\nnum_round\nThe number of rounds for boosting.\n\n\nnum_workers\nnumber of workers used to train xgboost model. Defaults to 1.\n\n\nnthread\nNumber of threads used by per worker. Defaults to 1.\n\n\nuse_external_memory\nThe tree construction algorithm used in XGBoost. options: ‘auto’, ‘exact’, ‘approx’ [default=‘auto’]\n\n\nsilent\n0 means printing running messages, 1 means silent mode. default: 0\n\n\ncustom_obj\nCustomized objective function provided by user. Currently unsupported.\n\n\ncustom_eval\nCustomized evaluation function provided by user. Currently unsupported.\n\n\nmissing\nThe value treated as missing. default: Float.NaN\n\n\nseed\nRandom seed for the C++ part of XGBoost and train/test splitting.\n\n\ntimeout_request_workers\nthe maximum time to wait for the job requesting new workers. default: 30 minutes\n\n\ncheckpoint_path\nThe hdfs folder to load and save checkpoint boosters.\n\n\ncheckpoint_interval\nParam for set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the trained model will get checkpointed every 10 iterations. Note: checkpoint_path must also be set if the checkpoint interval is greater than 0.\n\n\nobjective\nSpecify the learning task and the corresponding learning objective. options: reg:linear, reg:logistic, binary:logistic, binary:logitraw, count:poisson, multi:softmax, multi:softprob, rank:pairwise, reg:gamma. default: reg:linear.\n\n\nbase_score\nParam for initial prediction (aka base margin) column name. Defaults to 0.5.\n\n\ntrain_test_ratio\nFraction of training points to use for testing.\n\n\nnum_early_stopping_rounds\nIf non-zero, the training will be stopped after a specified number of consecutive increases in any evaluation metric.\n\n\nobjective_type\nThe learning objective type of the specified custom objective and eval. Corresponding type will be assigned if custom objective is defined options: regression, classification.\n\n\neval_metric\nEvaluation metrics for validation data, a default metric will be assigned according to objective(rmse for regression, and error for classification, mean average precision for ranking). options: rmse, mae, logloss, error, merror, mlogloss, auc, aucpr, ndcg, map, gamma-deviance\n\n\nmaximize_evaluation_metrics\nWhether to maximize evaluation metrics. Defaults to FALSE (for minization.)\n\n\nnum_class\nNumber of classes.\n\n\nbase_margin_col\nParam for initial prediction (aka base margin) column name.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\nweight_col\nWeight column.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/mleap/latest/news.html",
    "href": "packages/mleap/latest/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "mleap 0.1.3\n\nSupport MLeap 0.12.0.\n\n\n\nmleap 0.1.2\n\nSupport MLeap 0.10.1 and Spark 2.3.0.\n\n\n\nmleap 0.1.1\n\nAllow heterogenous inputs (numeric and character) in mleap_transform() (#16)."
  },
  {
    "objectID": "packages/mleap/latest/index.html",
    "href": "packages/mleap/latest/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "mleap is a sparklyr extension that provides an interface to MLeap, which allows us to take Spark pipelines to production.\n\n\nmleap can be installed from CRAN via\ninstall.packages(\"mleap\")\nor, for the latest development version from GitHub, using\ndevtools::install_github(\"rstudio/mleap\")\nOnce mleap has been installed, we can install the external dependencies using\nlibrary(mleap)\ninstall_maven()\n# Alternatively, if you already have Maven installed, you can \n#  set options(maven.home = \"path/to/maven\")\ninstall_mleap()\nWe can now export Spark ML pipelines from sparklyr.\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.4.0\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\n\n# Create a pipeline and fit it\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\n\n# Export model\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, sample_input = mtcars_tbl, path = model_path)\n\n# Disconnect from Spark\nspark_disconnect(sc)\n## NULL\nAt this point, we can share mtcars_model.zip with our deployment/implementation engineers, and they would be able to embed the model in another application. See the MLeap docs for details.\nWe also provide R functions for testing that the saved models behave as expected. Here we load the previously saved model:\nmodel <- mleap_load_bundle(model_path)\nmodel\n## MLeap Transformer\n## <7e2f61ed-154b-4c9e-9926-85fa326d69ef> \n##   Name: pipeline_c1754f374a53 \n##   Format: json \n##   MLeap Version: 0.14.0\nWe can retrieve the schema associated with the model:\nmleap_model_schema(model)\n## # A tibble: 6 x 5\n##   name       type   nullable dimension io    \n##   <chr>      <chr>  <lgl>    <chr>     <chr> \n## 1 qsec       double TRUE     <NA>      input \n## 2 hp         double FALSE    <NA>      input \n## 3 wt         double TRUE     <NA>      input \n## 4 big_hp     double FALSE    <NA>      output\n## 5 features   double TRUE     (3)       output\n## 6 prediction double FALSE    <NA>      output\nThen, we create a new data frame to be scored, and make predictions using our model:\nnewdata <- tibble::tribble(\n  ~qsec, ~hp, ~wt,\n  16.2,  101, 2.68,\n  18.1,  99,  3.08\n)\n\n# Transform the data frame\ntransformed_df <- mleap_transform(model, newdata)\ndplyr::glimpse(transformed_df)\n## Observations: 2\n## Variables: 6\n## $ qsec       <dbl> 16.2, 18.1\n## $ hp         <dbl> 101, 99\n## $ wt         <dbl> 2.68, 3.08\n## $ big_hp     <dbl> 1, 0\n## $ features   <list> [[[1, 2.68, 16.2], [3]], [[0, 3.08, 18.1], [3]]]\n## $ prediction <dbl> 21.00084, 20.56445"
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html",
    "title": "Export a Spark pipeline for serving",
    "section": "",
    "text": "R/mleap.R"
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#ml_write_bundle",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#ml_write_bundle",
    "title": "Export a Spark pipeline for serving",
    "section": "ml_write_bundle",
    "text": "ml_write_bundle"
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#description",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#description",
    "title": "Export a Spark pipeline for serving",
    "section": "Description",
    "text": "Description\nThis functions serializes a Spark pipeline model into an MLeap bundle."
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#usage",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#usage",
    "title": "Export a Spark pipeline for serving",
    "section": "Usage",
    "text": "Usage\nml_write_bundle(x, sample_input, path, overwrite = FALSE)"
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#arguments",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#arguments",
    "title": "Export a Spark pipeline for serving",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark pipeline model object.\n\n\nsample_input\nA sample input Spark DataFrame with the expected schema.\n\n\npath\nWhere to save the bundle.\n\n\noverwrite\nWhether to overwrite an existing file, defaults to FALSE."
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#examples",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#examples",
    "title": "Export a Spark pipeline for serving",
    "section": "Examples",
    "text": "Examples\n\nlibrary(mleap)\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\") \nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE) \npipeline <- ml_pipeline(sc) %>% \n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>% \n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>% \n  ml_gbt_regressor(label_col = \"mpg\") \npipeline_model <- ml_fit(pipeline, mtcars_tbl) \nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\") \nml_write_bundle(pipeline_model,  \n                mtcars_tbl, \n                model_path, \n                overwrite = TRUE)"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html",
    "href": "packages/mleap/latest/reference/install_mleap.html",
    "title": "Install MLeap runtime",
    "section": "",
    "text": "R/installers.R"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#install_mleap",
    "href": "packages/mleap/latest/reference/install_mleap.html#install_mleap",
    "title": "Install MLeap runtime",
    "section": "install_mleap",
    "text": "install_mleap"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#description",
    "href": "packages/mleap/latest/reference/install_mleap.html#description",
    "title": "Install MLeap runtime",
    "section": "Description",
    "text": "Description\nInstall MLeap runtime"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#usage",
    "href": "packages/mleap/latest/reference/install_mleap.html#usage",
    "title": "Install MLeap runtime",
    "section": "Usage",
    "text": "Usage\ninstall_mleap(dir = NULL, version = NULL, use_temp_cache = TRUE)"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#arguments",
    "href": "packages/mleap/latest/reference/install_mleap.html#arguments",
    "title": "Install MLeap runtime",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\ndir\n(Optional) Directory to save the jars\n\n\nversion\nVersion of MLeap to install, defaults to the latest version tested with this package.\n\n\nuse_temp_cache\nWhether to use a temporary Maven cache directory for downloading. Setting this to TRUE prevents Maven from creating a persistent .m2/ directory. Defaults to TRUE."
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#examples",
    "href": "packages/mleap/latest/reference/install_mleap.html#examples",
    "title": "Install MLeap runtime",
    "section": "Examples",
    "text": "Examples\n\nlibrary(mleap)\ninstall_mleap()"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html",
    "href": "packages/mleap/latest/reference/install_maven.html",
    "title": "Install Maven",
    "section": "",
    "text": "R/installers.R"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#install_maven",
    "href": "packages/mleap/latest/reference/install_maven.html#install_maven",
    "title": "Install Maven",
    "section": "install_maven",
    "text": "install_maven"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#description",
    "href": "packages/mleap/latest/reference/install_maven.html#description",
    "title": "Install Maven",
    "section": "Description",
    "text": "Description\nThis function installs Apache Maven."
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#usage",
    "href": "packages/mleap/latest/reference/install_maven.html#usage",
    "title": "Install Maven",
    "section": "Usage",
    "text": "Usage\ninstall_maven(dir = NULL, version = NULL)"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#arguments",
    "href": "packages/mleap/latest/reference/install_maven.html#arguments",
    "title": "Install Maven",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\ndir\n(Optional) Directory to install maven in. Defaults to maven/ under user’s home directory.\n\n\nversion\nVersion of Maven to install, defaults to the latest version tested with this package."
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#examples",
    "href": "packages/mleap/latest/reference/install_maven.html#examples",
    "title": "Install Maven",
    "section": "Examples",
    "text": "Examples\n\nlibrary(mleap)\ninstall_maven()"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_installed_versions.html",
    "href": "packages/mleap/latest/reference/mleap_installed_versions.html",
    "title": "Find existing MLeap installations",
    "section": "",
    "text": "R/installers.R"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_installed_versions.html#mleap_installed_versions",
    "href": "packages/mleap/latest/reference/mleap_installed_versions.html#mleap_installed_versions",
    "title": "Find existing MLeap installations",
    "section": "mleap_installed_versions",
    "text": "mleap_installed_versions"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_installed_versions.html#description",
    "href": "packages/mleap/latest/reference/mleap_installed_versions.html#description",
    "title": "Find existing MLeap installations",
    "section": "Description",
    "text": "Description\nFind existing MLeap installations"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_installed_versions.html#usage",
    "href": "packages/mleap/latest/reference/mleap_installed_versions.html#usage",
    "title": "Find existing MLeap installations",
    "section": "Usage",
    "text": "Usage\nmleap_installed_versions()"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_installed_versions.html#value",
    "href": "packages/mleap/latest/reference/mleap_installed_versions.html#value",
    "title": "Find existing MLeap installations",
    "section": "Value",
    "text": "Value\nA data frame of MLeap Runtime installation versions and their locations."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html",
    "title": "MLeap model schema",
    "section": "",
    "text": "R/mleap.R"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#mleap_model_schema",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#mleap_model_schema",
    "title": "MLeap model schema",
    "section": "mleap_model_schema",
    "text": "mleap_model_schema"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#description",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#description",
    "title": "MLeap model schema",
    "section": "Description",
    "text": "Description\nReturns the schema of an MLeap transformer."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#usage",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#usage",
    "title": "MLeap model schema",
    "section": "Usage",
    "text": "Usage\nmleap_model_schema(x)"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#arguments",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#arguments",
    "title": "MLeap model schema",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#value",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#value",
    "title": "MLeap model schema",
    "section": "Value",
    "text": "Value\nA data frame of the model schema."
  },
  {
    "objectID": "packages/mleap/latest/reference/index.html",
    "href": "packages/mleap/latest/reference/index.html",
    "title": "mleap",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ninstall_maven()\nInstall Maven\n\n\ninstall_mleap()\nInstall MLeap runtime\n\n\nml_write_bundle()\nExport a Spark pipeline for serving\n\n\nmleap_installed_versions()\nFind existing MLeap installations\n\n\nmleap_load_bundle()\nLoads an MLeap bundle\n\n\nmleap_model_schema()\nMLeap model schema\n\n\nmleap_transform()\nTransform data using an MLeap model"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html",
    "title": "Loads an MLeap bundle",
    "section": "",
    "text": "R/mleap.R"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#mleap_load_bundle",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#mleap_load_bundle",
    "title": "Loads an MLeap bundle",
    "section": "mleap_load_bundle",
    "text": "mleap_load_bundle"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#description",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#description",
    "title": "Loads an MLeap bundle",
    "section": "Description",
    "text": "Description\nLoads an MLeap bundle"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#usage",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#usage",
    "title": "Loads an MLeap bundle",
    "section": "Usage",
    "text": "Usage\nmleap_load_bundle(path)"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#arguments",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#arguments",
    "title": "Loads an MLeap bundle",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\npath\nPath to the exported bundle zip file."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#value",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#value",
    "title": "Loads an MLeap bundle",
    "section": "Value",
    "text": "Value\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html",
    "href": "packages/mleap/latest/reference/mleap_transform.html",
    "title": "Transform data using an MLeap model",
    "section": "",
    "text": "R/prediction.R"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#mleap_transform",
    "href": "packages/mleap/latest/reference/mleap_transform.html#mleap_transform",
    "title": "Transform data using an MLeap model",
    "section": "mleap_transform",
    "text": "mleap_transform"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#description",
    "href": "packages/mleap/latest/reference/mleap_transform.html#description",
    "title": "Transform data using an MLeap model",
    "section": "Description",
    "text": "Description\nThis functions transforms an R data frame using an MLeap model."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#usage",
    "href": "packages/mleap/latest/reference/mleap_transform.html#usage",
    "title": "Transform data using an MLeap model",
    "section": "Usage",
    "text": "Usage\nmleap_transform(model, data)"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#arguments",
    "href": "packages/mleap/latest/reference/mleap_transform.html#arguments",
    "title": "Transform data using an MLeap model",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nmodel\nAn MLeap model object, obtained by mleap_load_bundle().\n\n\ndata\nAn R data frame."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#value",
    "href": "packages/mleap/latest/reference/mleap_transform.html#value",
    "title": "Transform data using an MLeap model",
    "section": "Value",
    "text": "Value\nA transformed data frame."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#see-also",
    "href": "packages/mleap/latest/reference/mleap_transform.html#see-also",
    "title": "Transform data using an MLeap model",
    "section": "See Also",
    "text": "See Also\n[mleap_load_bundle()]"
  },
  {
    "objectID": "packages/mleap/dev/news.html",
    "href": "packages/mleap/dev/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "mleap 0.1.3\n\nSupport MLeap 0.12.0.\n\n\n\nmleap 0.1.2\n\nSupport MLeap 0.10.1 and Spark 2.3.0.\n\n\n\nmleap 0.1.1\n\nAllow heterogenous inputs (numeric and character) in mleap_transform() (#16)."
  },
  {
    "objectID": "packages/mleap/dev/index.html",
    "href": "packages/mleap/dev/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "mleap is a sparklyr extension that provides an interface to MLeap, which allows us to take Spark pipelines to production.\n\n\nmleap can be installed from CRAN via\ninstall.packages(\"mleap\")\nor, for the latest development version from GitHub, using\ndevtools::install_github(\"rstudio/mleap\")\nOnce mleap has been installed, we can install the external dependencies using\nlibrary(mleap)\ninstall_maven()\n# Alternatively, if you already have Maven installed, you can \n#  set options(maven.home = \"path/to/maven\")\ninstall_mleap()\nWe can now export Spark ML pipelines from sparklyr.\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.4.0\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\n\n# Create a pipeline and fit it\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\n\n# Export model\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, sample_input = mtcars_tbl, path = model_path)\n\n# Disconnect from Spark\nspark_disconnect(sc)\n## NULL\nAt this point, we can share mtcars_model.zip with our deployment/implementation engineers, and they would be able to embed the model in another application. See the MLeap docs for details.\nWe also provide R functions for testing that the saved models behave as expected. Here we load the previously saved model:\nmodel <- mleap_load_bundle(model_path)\nmodel\n## MLeap Transformer\n## <7e2f61ed-154b-4c9e-9926-85fa326d69ef> \n##   Name: pipeline_c1754f374a53 \n##   Format: json \n##   MLeap Version: 0.14.0\nWe can retrieve the schema associated with the model:\nmleap_model_schema(model)\n## # A tibble: 6 x 5\n##   name       type   nullable dimension io    \n##   <chr>      <chr>  <lgl>    <chr>     <chr> \n## 1 qsec       double TRUE     <NA>      input \n## 2 hp         double FALSE    <NA>      input \n## 3 wt         double TRUE     <NA>      input \n## 4 big_hp     double FALSE    <NA>      output\n## 5 features   double TRUE     (3)       output\n## 6 prediction double FALSE    <NA>      output\nThen, we create a new data frame to be scored, and make predictions using our model:\nnewdata <- tibble::tribble(\n  ~qsec, ~hp, ~wt,\n  16.2,  101, 2.68,\n  18.1,  99,  3.08\n)\n\n# Transform the data frame\ntransformed_df <- mleap_transform(model, newdata)\ndplyr::glimpse(transformed_df)\n## Observations: 2\n## Variables: 6\n## $ qsec       <dbl> 16.2, 18.1\n## $ hp         <dbl> 101, 99\n## $ wt         <dbl> 2.68, 3.08\n## $ big_hp     <dbl> 1, 0\n## $ features   <list> [[[1, 2.68, 16.2], [3]], [[0, 3.08, 18.1], [3]]]\n## $ prediction <dbl> 21.00084, 20.56445"
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html",
    "title": "Export a Spark pipeline for serving",
    "section": "",
    "text": "R/mleap.R"
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#ml_write_bundle",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#ml_write_bundle",
    "title": "Export a Spark pipeline for serving",
    "section": "ml_write_bundle",
    "text": "ml_write_bundle"
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#description",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#description",
    "title": "Export a Spark pipeline for serving",
    "section": "Description",
    "text": "Description\nThis functions serializes a Spark pipeline model into an MLeap bundle."
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#usage",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#usage",
    "title": "Export a Spark pipeline for serving",
    "section": "Usage",
    "text": "Usage\nml_write_bundle(x, sample_input, path, overwrite = FALSE)"
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#arguments",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#arguments",
    "title": "Export a Spark pipeline for serving",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark pipeline model object.\n\n\nsample_input\nA sample input Spark DataFrame with the expected schema.\n\n\npath\nWhere to save the bundle.\n\n\noverwrite\nWhether to overwrite an existing file, defaults to FALSE."
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#examples",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#examples",
    "title": "Export a Spark pipeline for serving",
    "section": "Examples",
    "text": "Examples\n\nlibrary(mleap)\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\") \nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE) \npipeline <- ml_pipeline(sc) %>% \n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>% \n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>% \n  ml_gbt_regressor(label_col = \"mpg\") \npipeline_model <- ml_fit(pipeline, mtcars_tbl) \nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\") \nml_write_bundle(pipeline_model,  \n                mtcars_tbl, \n                model_path, \n                overwrite = TRUE)"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html",
    "href": "packages/mleap/dev/reference/install_mleap.html",
    "title": "Install MLeap runtime",
    "section": "",
    "text": "R/installers.R"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#install_mleap",
    "href": "packages/mleap/dev/reference/install_mleap.html#install_mleap",
    "title": "Install MLeap runtime",
    "section": "install_mleap",
    "text": "install_mleap"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#description",
    "href": "packages/mleap/dev/reference/install_mleap.html#description",
    "title": "Install MLeap runtime",
    "section": "Description",
    "text": "Description\nInstall MLeap runtime"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#usage",
    "href": "packages/mleap/dev/reference/install_mleap.html#usage",
    "title": "Install MLeap runtime",
    "section": "Usage",
    "text": "Usage\ninstall_mleap(dir = NULL, version = NULL, use_temp_cache = TRUE)"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#arguments",
    "href": "packages/mleap/dev/reference/install_mleap.html#arguments",
    "title": "Install MLeap runtime",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\ndir\n(Optional) Directory to save the jars\n\n\nversion\nVersion of MLeap to install, defaults to the latest version tested with this package.\n\n\nuse_temp_cache\nWhether to use a temporary Maven cache directory for downloading. Setting this to TRUE prevents Maven from creating a persistent .m2/ directory. Defaults to TRUE."
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#examples",
    "href": "packages/mleap/dev/reference/install_mleap.html#examples",
    "title": "Install MLeap runtime",
    "section": "Examples",
    "text": "Examples\n\nlibrary(mleap)\ninstall_mleap()"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html",
    "href": "packages/mleap/dev/reference/install_maven.html",
    "title": "Install Maven",
    "section": "",
    "text": "R/installers.R"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#install_maven",
    "href": "packages/mleap/dev/reference/install_maven.html#install_maven",
    "title": "Install Maven",
    "section": "install_maven",
    "text": "install_maven"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#description",
    "href": "packages/mleap/dev/reference/install_maven.html#description",
    "title": "Install Maven",
    "section": "Description",
    "text": "Description\nThis function installs Apache Maven."
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#usage",
    "href": "packages/mleap/dev/reference/install_maven.html#usage",
    "title": "Install Maven",
    "section": "Usage",
    "text": "Usage\ninstall_maven(dir = NULL, version = NULL)"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#arguments",
    "href": "packages/mleap/dev/reference/install_maven.html#arguments",
    "title": "Install Maven",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\ndir\n(Optional) Directory to install maven in. Defaults to maven/ under user’s home directory.\n\n\nversion\nVersion of Maven to install, defaults to the latest version tested with this package."
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#examples",
    "href": "packages/mleap/dev/reference/install_maven.html#examples",
    "title": "Install Maven",
    "section": "Examples",
    "text": "Examples\n\nlibrary(mleap)\ninstall_maven()"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_installed_versions.html",
    "href": "packages/mleap/dev/reference/mleap_installed_versions.html",
    "title": "Find existing MLeap installations",
    "section": "",
    "text": "R/installers.R"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_installed_versions.html#mleap_installed_versions",
    "href": "packages/mleap/dev/reference/mleap_installed_versions.html#mleap_installed_versions",
    "title": "Find existing MLeap installations",
    "section": "mleap_installed_versions",
    "text": "mleap_installed_versions"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_installed_versions.html#description",
    "href": "packages/mleap/dev/reference/mleap_installed_versions.html#description",
    "title": "Find existing MLeap installations",
    "section": "Description",
    "text": "Description\nFind existing MLeap installations"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_installed_versions.html#usage",
    "href": "packages/mleap/dev/reference/mleap_installed_versions.html#usage",
    "title": "Find existing MLeap installations",
    "section": "Usage",
    "text": "Usage\nmleap_installed_versions()"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_installed_versions.html#value",
    "href": "packages/mleap/dev/reference/mleap_installed_versions.html#value",
    "title": "Find existing MLeap installations",
    "section": "Value",
    "text": "Value\nA data frame of MLeap Runtime installation versions and their locations."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html",
    "title": "MLeap model schema",
    "section": "",
    "text": "R/mleap.R"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#mleap_model_schema",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#mleap_model_schema",
    "title": "MLeap model schema",
    "section": "mleap_model_schema",
    "text": "mleap_model_schema"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#description",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#description",
    "title": "MLeap model schema",
    "section": "Description",
    "text": "Description\nReturns the schema of an MLeap transformer."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#usage",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#usage",
    "title": "MLeap model schema",
    "section": "Usage",
    "text": "Usage\nmleap_model_schema(x)"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#arguments",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#arguments",
    "title": "MLeap model schema",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#value",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#value",
    "title": "MLeap model schema",
    "section": "Value",
    "text": "Value\nA data frame of the model schema."
  },
  {
    "objectID": "packages/mleap/dev/reference/index.html",
    "href": "packages/mleap/dev/reference/index.html",
    "title": "mleap",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ninstall_maven()\nInstall Maven\n\n\ninstall_mleap()\nInstall MLeap runtime\n\n\nml_write_bundle()\nExport a Spark pipeline for serving\n\n\nmleap_installed_versions()\nFind existing MLeap installations\n\n\nmleap_load_bundle()\nLoads an MLeap bundle\n\n\nmleap_model_schema()\nMLeap model schema\n\n\nmleap_transform()\nTransform data using an MLeap model"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html",
    "title": "Loads an MLeap bundle",
    "section": "",
    "text": "R/mleap.R"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#mleap_load_bundle",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#mleap_load_bundle",
    "title": "Loads an MLeap bundle",
    "section": "mleap_load_bundle",
    "text": "mleap_load_bundle"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#description",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#description",
    "title": "Loads an MLeap bundle",
    "section": "Description",
    "text": "Description\nLoads an MLeap bundle"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#usage",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#usage",
    "title": "Loads an MLeap bundle",
    "section": "Usage",
    "text": "Usage\nmleap_load_bundle(path)"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#arguments",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#arguments",
    "title": "Loads an MLeap bundle",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\npath\nPath to the exported bundle zip file."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#value",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#value",
    "title": "Loads an MLeap bundle",
    "section": "Value",
    "text": "Value\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html",
    "href": "packages/mleap/dev/reference/mleap_transform.html",
    "title": "Transform data using an MLeap model",
    "section": "",
    "text": "R/prediction.R"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#mleap_transform",
    "href": "packages/mleap/dev/reference/mleap_transform.html#mleap_transform",
    "title": "Transform data using an MLeap model",
    "section": "mleap_transform",
    "text": "mleap_transform"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#description",
    "href": "packages/mleap/dev/reference/mleap_transform.html#description",
    "title": "Transform data using an MLeap model",
    "section": "Description",
    "text": "Description\nThis functions transforms an R data frame using an MLeap model."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#usage",
    "href": "packages/mleap/dev/reference/mleap_transform.html#usage",
    "title": "Transform data using an MLeap model",
    "section": "Usage",
    "text": "Usage\nmleap_transform(model, data)"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#arguments",
    "href": "packages/mleap/dev/reference/mleap_transform.html#arguments",
    "title": "Transform data using an MLeap model",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nmodel\nAn MLeap model object, obtained by mleap_load_bundle().\n\n\ndata\nAn R data frame."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#value",
    "href": "packages/mleap/dev/reference/mleap_transform.html#value",
    "title": "Transform data using an MLeap model",
    "section": "Value",
    "text": "Value\nA transformed data frame."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#see-also",
    "href": "packages/mleap/dev/reference/mleap_transform.html#see-also",
    "title": "Transform data using an MLeap model",
    "section": "See Also",
    "text": "See Also\n[mleap_load_bundle()]"
  },
  {
    "objectID": "learn-more.html",
    "href": "learn-more.html",
    "title": "Learn More",
    "section": "",
    "text": "A 2 page visual guide to sparklyr functions. It provides a concise description of each function. It also has diagrams of how several Feature Transformers work.\n\n\n\n\n\n\n\n\nThis free, online book, intends to take someone unfamiliar with Spark or R and help you become proficient by teaching you a set of tools, skills and practices applicable to large-scale data science."
  },
  {
    "objectID": "get-started/prepare-data.html",
    "href": "get-started/prepare-data.html",
    "title": "Prepare Data",
    "section": "",
    "text": "sparklyr provide multiple methods to prepare data inside Spark:\nThis article will introduce each method and provide a simple example."
  },
  {
    "objectID": "get-started/prepare-data.html#exercise",
    "href": "get-started/prepare-data.html#exercise",
    "title": "Prepare Data",
    "section": "Exercise",
    "text": "Exercise\nFor the exercise start a local session of Spark. We’ll start by copying a data set from R into the Spark cluster (note that you may need to install the nycflights13)\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nflights_tbl <- copy_to(sc, nycflights13::flights, \"spark_flights\")"
  },
  {
    "objectID": "get-started/prepare-data.html#using-dplyr",
    "href": "get-started/prepare-data.html#using-dplyr",
    "title": "Prepare Data",
    "section": "Using dplyr",
    "text": "Using dplyr\nWe can use familiar dplyr commands to prepare data inside Spark. The commands run inside Spark, so there are no unnecessary data transfers between R and Spark.\nIn this example, we can see how easy it is to summarize the flights data without having to know how to write Spark SQL:\n\ndelay <- flights_tbl %>%\n  group_by(tailnum) %>%\n  summarise(\n    count = n(), \n    dist = mean(distance, na.rm = TRUE), \n    delay = mean(arr_delay, na.rm = TRUE)\n    ) %>%\n  filter(count > 20, dist < 2000, !is.na(delay)) \n\ndelay\n#> # Source: spark<?> [?? x 4]\n#>    tailnum count  dist  delay\n#>    <chr>   <dbl> <dbl>  <dbl>\n#>  1 N24211    130 1330.  7.7  \n#>  2 N793JB    283 1529.  4.72 \n#>  3 N657JB    285 1286.  5.03 \n#>  4 N633AA     24 1587. -0.625\n#>  5 N9EAMQ    248  675.  9.24 \n#>  6 N3GKAA     77 1247.  4.97 \n#>  7 N997DL     63  868.  4.90 \n#>  8 N318NB    202  814. -1.12 \n#>  9 N651JB    261 1408.  7.58 \n#> 10 N841UA     96 1208.  2.10 \n#> # … with more rows\n\nsparklyr and dplyr translate the R commands into Spark SQL for us. To see the resulting query use show_query():\n\ndplyr::show_query(delay)\n#> <SQL>\n#> SELECT *\n#> FROM (\n#>   SELECT\n#>     `tailnum`,\n#>     COUNT(*) AS `count`,\n#>     AVG(`distance`) AS `dist`,\n#>     AVG(`arr_delay`) AS `delay`\n#>   FROM `spark_flights`\n#>   GROUP BY `tailnum`\n#> ) `q01`\n#> WHERE (`count` > 20.0) AND (`dist` < 2000.0) AND (NOT((`delay` IS NULL)))\n\nNotice that the delay variable does not contain data. It only contains the dplyr commands that are to run against the Spark connection.\nFor additional documentation on using dplyr with Spark see the Manipulating Data with dplyr article in this site"
  },
  {
    "objectID": "get-started/prepare-data.html#using-sql",
    "href": "get-started/prepare-data.html#using-sql",
    "title": "Prepare Data",
    "section": "Using SQL",
    "text": "Using SQL\nIt’s also possible to execute SQL queries directly against tables within a Spark cluster. The spark_connection() object implements a DBI interface for Spark, so you can use dbGetQuery() to execute SQL and return the result as an R data frame:\n\nlibrary(DBI)\n\ndbGetQuery(sc, \"SELECT carrier, sched_dep_time, dep_time, dep_delay FROM spark_flights LIMIT 5\")\n#>   carrier sched_dep_time dep_time dep_delay\n#> 1      UA            515      517         2\n#> 2      UA            529      533         4\n#> 3      AA            540      542         2\n#> 4      B6            545      544        -1\n#> 5      DL            600      554        -6"
  },
  {
    "objectID": "get-started/prepare-data.html#using-feature-transformers",
    "href": "get-started/prepare-data.html#using-feature-transformers",
    "title": "Prepare Data",
    "section": "Using Feature Transformers",
    "text": "Using Feature Transformers\nBoth of the previous methods rely on SQL statements. Spark provides commands that make some data transformation more convenient, and without the use of SQL.\nFor example, the ft_binarizer() command simplifies the creation of a new column that indicates if the value of another column is above a certain threshold.\n\nflights_tbl %>% \n  ft_binarizer(\"dep_delay\", \"over_one\", threshold = 1) %>% \n  select(dep_delay, over_one) %>% \n  head(5)\n#> # Source: spark<?> [?? x 2]\n#>   dep_delay over_one\n#>       <dbl>    <dbl>\n#> 1         2        1\n#> 2         4        1\n#> 3         2        1\n#> 4        -1        0\n#> 5        -6        0\n\nFind a full list of the Spark Feature Transformers available through sparklyr here: Reference - FT."
  },
  {
    "objectID": "get-started/prepare-data.html#disconnect-from-spark",
    "href": "get-started/prepare-data.html#disconnect-from-spark",
    "title": "Prepare Data",
    "section": "Disconnect from Spark",
    "text": "Disconnect from Spark\nLastly, cleanup your session by disconnecting Spark:\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "get-started/read-data.html",
    "href": "get-started/read-data.html",
    "title": "Read Data",
    "section": "",
    "text": "A new Spark session will contain no data. The first step is to either load data into your Spark session’s memory, or point Spark to the location of the data so it can access the data on-demand."
  },
  {
    "objectID": "get-started/read-data.html#exercise",
    "href": "get-started/read-data.html#exercise",
    "title": "Read Data",
    "section": "Exercise",
    "text": "Exercise\nFor this exercise, we will start a “local” Spark session, and then transfer data from our R environment to the Spark session’s memory. To do that, we will use the copy_to() command:\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\ntbl_mtcars <- copy_to(sc, mtcars, \"spark_mtcars\")\n\nIf you are using the RStudio IDE, you will notice a new table in the Connections pane. The name of that table is spark_mtcars. That is the name of the data set inside the Spark memory. The tbl_mtcars variable does not contain any mtcars data, this variable contains the info that points to the location where the Spark session loaded the data to.\nCalling the tbl_mtcars variable in R will download the first 1,000 records and display them :\n\ntbl_mtcars\n#> # Source: spark<spark_mtcars> [?? x 11]\n#>      mpg   cyl  disp    hp  drat    wt  qsec    vs    am\n#>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#>  1  21       6  160    110  3.9   2.62  16.5     0     1\n#>  2  21       6  160    110  3.9   2.88  17.0     0     1\n#>  3  22.8     4  108     93  3.85  2.32  18.6     1     1\n#>  4  21.4     6  258    110  3.08  3.22  19.4     1     0\n#>  5  18.7     8  360    175  3.15  3.44  17.0     0     0\n#>  6  18.1     6  225    105  2.76  3.46  20.2     1     0\n#>  7  14.3     8  360    245  3.21  3.57  15.8     0     0\n#>  8  24.4     4  147.    62  3.69  3.19  20       1     0\n#>  9  22.8     4  141.    95  3.92  3.15  22.9     1     0\n#> 10  19.2     6  168.   123  3.92  3.44  18.3     1     0\n#> # … with more rows, and 2 more variables: gear <dbl>,\n#> #   carb <dbl>\n\nNotice that at the top of the data print out, it is noted that records were downloaded from Spark: Source: spark….\nTo clean up the session, we will now stop the Spark session:\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "get-started/read-data.html#working-with-files",
    "href": "get-started/read-data.html#working-with-files",
    "title": "Read Data",
    "section": "Working with files",
    "text": "Working with files\nIn a formal Spark environment, it will be rare when we would have to upload data from R into Spark.\nUsing sparklyr, you can tell Spark to read and write data. Spark is able to interact with multiple types of file systems, such as HDFS, S3 and local. Additionally, Spark is able to read several file types such as CSV, Parquet, Delta and JSON. sparklyr provides functions that makes it easy to access these features. See the Spark Data section for a full list of available functions.\nThe following command will tell Spark to read a CSV file, and to also load it into Spark memory.\n\n# Do not run the next following command. It is for example purposes only.\nspark_read_csv(sc, name = \"test_table\",  path = \"/test/path/test_file.csv\")"
  },
  {
    "objectID": "get-started/index.html",
    "href": "get-started/index.html",
    "title": "Install",
    "section": "",
    "text": "You can install the sparklyr package from CRAN as follows:\n\ninstall.packages(\"sparklyr\")"
  },
  {
    "objectID": "get-started/index.html#install-spark-locally",
    "href": "get-started/index.html#install-spark-locally",
    "title": "Install",
    "section": "Install Spark locally",
    "text": "Install Spark locally\n\n\n\n\n\n\nDanger\n\n\n\nThe steps in this section are only needed if you need to run Spark in your computer. If you already have a running Spark cluster that you will use to learn sparklyr, then skip this section.\n\n\nThis section is meant for developers new to sparklyr. You will need a running Spark environment to connect to. sparklyr can install Spark in your computer. The installed Spark environment is meant for learning and prototyping purposes. The installation will work on all the major Operating Systems that R works on, including Linux, MacOS, and Windows.\n\nlibrary(sparklyr)\n\nspark_install()\n\nPlease be aware that after installation, Spark is not running. The next section will explain how to start a single node Spark cluster in your machine."
  },
  {
    "objectID": "get-started/index.html#connect-to-spark",
    "href": "get-started/index.html#connect-to-spark",
    "title": "Install",
    "section": "Connect to Spark",
    "text": "Connect to Spark\nYou can use spark_connect() to connect to Spark clusters. The arguments passed to this functions depend on the type of Spark cluster you are connecting to. There are several different types of Spark clusters, such as YARN, Stand Alone and Kubernetes.\nspark_connect() is able to both start, and connect to, the single node Spark cluster in your machine. In order to do that, pass “local” as the argument for master:\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nThe sc variable now contains all of the connection information needed to interact with the cluster.\nTo learn how to connect to other types of Spark clusters, see the Deployment section of this site."
  },
  {
    "objectID": "get-started/index.html#disconnect-from-spark",
    "href": "get-started/index.html#disconnect-from-spark",
    "title": "Install",
    "section": "Disconnect from Spark",
    "text": "Disconnect from Spark\nFor “local” connection, spark_disconnect() will shut down the single node Spark environment in your machine, and tell R that the connection is no longer valid. For other types of Spark clusters, spark_disconnect() will only end the Spark session, it will not shut down the Spark cluster itself.\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "get-started/index.html#clusters",
    "href": "get-started/index.html#clusters",
    "title": "Install",
    "section": "Clusters",
    "text": "Clusters\nHere are some examples of how to use spark_connect() to connect to different types of Spark clusters:\nHadoop YARN:\nsc <- spark_connect(master = \"yarn\")\nMesos:\nsc <- spark_connect(master = \"mesos://host:port\")\nKubernetes:\nsc <- spark_connect(master = \"k8s://https://server\")\nApache Livy:\nsc <- spark_connect(master = \"http://server/livy\", method = \"livy\")\nStand Alone:\nsc <- spark_connect(master = \"spark://master-url:7077\")\nQubole: (for more info visit the Qubole page on this site)\nsc <- spark_connect(method = \"qubole\")\nDatabricks - Visit the Databricks page on this site to review the connection options"
  },
  {
    "objectID": "get-started/model-data.html",
    "href": "get-started/model-data.html",
    "title": "Model Data",
    "section": "",
    "text": "You can orchestrate machine learning algorithms in a Spark cluster via the machine learning functions within sparklyr. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning workflows."
  },
  {
    "objectID": "get-started/model-data.html#exercise",
    "href": "get-started/model-data.html#exercise",
    "title": "Model Data",
    "section": "Exercise",
    "text": "Exercise\nHere’s an example where we use ml_linear_regression() to fit a linear regression model. We’ll use the built-in mtcars dataset, and see if we can predict a car’s fuel consumption (mpg) based on its weight (wt), and the number of cylinders the engine contains (cyl). We’ll assume in each case that the relationship between mpg and each of our features is linear.\n\nInitialize the environment\nWe will start by creating a local Spark session and load the mtcars data frame to it.\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- copy_to(sc, mtcars, overwrite = TRUE)\n\n\n\nPrepare the data\nSpark provides data frame operations that makes it easier to prepare data for modeling. In this case, we will use the sdf_partition() command to divide the mtcars data into “training” and “test”.\n\npartitions <- mtcars_tbl %>%\n  select(mpg, wt, cyl) %>% \n  sdf_random_split(training = 0.5, test = 0.5, seed = 1099)\n\nNote that the newly created partitions variable does not contain data, it contains a pointer to where the data was split within Spark. That means that no data is downloaded to the R session.\n\n\nFit the model\nNext, we will fit a linear model to the training data set:\n\nfit <- partitions$training %>%\n  ml_linear_regression(mpg ~ .)\n\nfit\n#> Formula: mpg ~ .\n#> \n#> Coefficients:\n#> (Intercept)          wt         cyl \n#>   38.927395   -4.131014   -0.938832\n\nFor linear regression models produced by Spark, we can use summary() to learn a bit more about the quality of our fit, and the statistical significance of each of our predictors.\n\nsummary(fit)\n#> Deviance Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.4891 -1.5262 -0.1481  0.8508  6.3162 \n#> \n#> Coefficients:\n#> (Intercept)          wt         cyl \n#>   38.927395   -4.131014   -0.938832 \n#> \n#> R-Squared: 0.8469\n#> Root Mean Squared Error: 2.416\n\n\n\nUse the model\nWe can use ml_predict() to create a Spark data frame that contains the predictions against the testing data set.\n\npred <- ml_predict(fit, partitions$test)\n\nhead(pred)\n#> # Source: spark<?> [?? x 4]\n#>     mpg    wt   cyl prediction\n#>   <dbl> <dbl> <dbl>      <dbl>\n#> 1  14.3  3.57     8      16.7 \n#> 2  14.7  5.34     8       9.34\n#> 3  15    3.57     8      16.7 \n#> 4  15.2  3.44     8      17.2 \n#> 5  15.2  3.78     8      15.8 \n#> 6  15.5  3.52     8      16.9\n\n\n\nFurther reading\nSpark machine learning supports a wide array of algorithms and feature transformations and as illustrated above it’s easy to chain these functions together with dplyr pipelines. To learn more see the Machine Learning article on this site. For a list of Spark ML models available through sparklyr visit Reference - ML"
  }
]