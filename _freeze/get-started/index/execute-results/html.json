{
  "hash": "88d918bf81848a5fc4742bfb3e554360",
  "result": {
    "markdown": "---\ntitle: \"Install\"\nformat:\n  html:\n    theme: default\n    toc: true\nexecute:\n  eval: true\n  freeze: true\n---\n\n## Install the package\n\nYou can install the `sparklyr` package from [CRAN](https://CRAN.r-project.org) as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"sparklyr\")\n```\n:::\n\n## Install Spark locally\n\nThis section is meant for developers new to `sparklyr`.  You will need a running\nSpark environment to connect to.  `sparklyr` can install Spark in your computer. \nThe installed Spark environment is meant for learning and prototyping purposes.\nThe installation will work on all the major Operating Systems that R works on,\nincluding Linux, MacOS, and Windows.  \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n\nspark_install()\n```\n:::\n\nPlease be aware that after installation, Spark is **not** running. The next section\nwill explain how to start a single node Spark cluster in your machine.\n\n## Connect to Spark\n\nYou can use `spark_connect()` to connect to Spark clusters.  The arguments passed to\nthis functions depend on the type of Spark cluster you are connecting to. There are\nseveral different types of Spark clusters, such as YARN, Stand Alone and Kubernetes. \n\n`spark_connect()` is able to both start, and connect to, the single node Spark\ncluster in your machine.  In order to do that, pass \"local\" as the argument\nfor `master`:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n```\n:::\n\nThe `sc` variable now contains all of the connection information needed to\ninteract with the cluster.\n\nTo learn how to connect to other types of Spark clusters, see the \n[Deployment](/deployment/index.qmd) section of this site. \n\n## Disconnect from Spark\n\nFor \"local\" connection, `spark_disconnect()` will shut down the single node\nSpark environment in your machine, and tell R that the connection is no longer \nvalid.  For other types of Spark clusters, `spark_disconnect()` will only \nend the Spark session, it will not shut down the Spark cluster itself.\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_disconnect(sc)\n```\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}