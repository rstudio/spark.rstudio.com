---
title: "Grid Search Tuning"
execute:
  eval: false
  freeze: true
editor_options: 
  markdown: 
    wrap: 72
format:
  html: 
    code-tools:
      source: https://github.com/rstudio/spark.rstudio.com/blob/tune-text/source/guides/model_tuning_text.md
      toggle: false
      caption: none
---

```{r setup}
#| include: false
library(sparklyr)
library(dplyr)
library(modeldata)
```

```{mermaid}
%%| fig-width: 6.5
flowchart LR
  subgraph id1 [ ]
    subgraph id2 [ ]
      subgraph id3 [ML Pipeline]
        d[Prepare] --> m[Model]
      end
      subgraph id4 [Grid Search Tuning]
        m --> gv[Grid]
        gv-- Combo 1 -->ft1[Fit]
        ft1 --> ev1[Evaluate]
        gv-- Combo 2 -->ft2[Fit]
        ft2 --> ev2[Evaluate]     
        gv-- Combo n -->ft3[Fit]
        ft3 --> ev3[Evaluate]    
      end
    end
  end
  style id1 fill:#eee,stroke:#eee
  style id2 fill:#eee,stroke:#eee
  style d fill:#99ccff,stroke#666
  style m fill:#ccff33,stroke:#666
  style gv fill:#ffcc99,stroke:#666
  style ft1 fill:#99ffcc,stroke:#666
  style ft2 fill:#99ffcc,stroke:#666
  style ft3 fill:#99ffcc,stroke:#666
```


## Reproducing "Tuning Text Analysis" in Spark

In this article, we will reproduce the *Grid Search* tuning example
found in the `tune` package's website: [Tuning Text
Analysis](https://tune.tidymodels.org/articles/extras/text_analysis.html){target="_blank"}.
That example analyzes *Amazon's Fine Food Reviews* text data to perform
hyper parameter tuning.

In this site's [Text Modeling](textmodeling.qmd){target="_blank"}
article, the data preparation and modeling are based on the same example
from `tune`. Thanks to how modular ML Pipelines are, we are able to
split the modeling walk-through, from the tuning walk-through found in
this article. We also recommend to be familiar with the concepts from
[Intro to Model Tuning](model_tuning.qmd){target="_blank"}.

## Spark and Data Setup

For this example, we will start a local Spark session, and then copy the
*Fine Food Reviews* data to it. For more information about the data,
please see the [Data](textmodeling.qmd#data) section of the *Text
Modeling* article.

```{r}
library(sparklyr)
library(modeldata)

data("small_fine_foods")

sc <- spark_connect(master = "local", version = "3.3")

sff_training_data <- copy_to(sc, training_data)
sff_testing_data <- copy_to(sc, testing_data)
```

## ML Pipeline

As mentioned before, the data preparation and modeling in [Text
Modeling](textmodeling.qmd#recipe-and-model-specifications-1){target="_blank"}
are based on the same example from the `tune`'s website. The `recipe`
steps, and `parsnip` model are recreated with Feature Transformers, and
an ML model respectively.

Unlike `tidymodels`, there is no need to "pre-define" the arguments that
will need tuning. During tuning, Spark will automatically override the
parameters specified in the [grid](#grid). This means that it doesn't
matter that we use the exact same code for developing, and tuning the
pipeline. We can literally copy-paste, and run the resulting pipeline
code from [Text
Modeling](textmodeling.qmd#prepare-the-model-with-an-ml-pipeline).

```{r}
sff_pipeline <- ml_pipeline(sc) %>% 
  ft_tokenizer(
    input_col = "review",
    output_col = "word_list"
  ) %>% 
  ft_stop_words_remover(
    input_col = "word_list", 
    output_col = "wo_stop_words"
    ) %>% 
  ft_hashing_tf(
    input_col = "wo_stop_words", 
    output_col = "hashed_features", 
    binary = TRUE, 
    num_features = 1024
    ) %>%
  ft_normalizer(
    input_col = "hashed_features", 
    output_col = "normal_features"
    ) %>% 
  ft_r_formula(score ~ normal_features) %>% 
  ml_logistic_regression()

sff_pipeline
```

It is also worth pointing out that in a"real life" exercise,
`sff_pipeline` would probably already be loaded into our environment.
That is because we just finished modeling and, decided to test to see if
we could tune the model. Spark can re-use the exact same ML Pipeline
object for the cross validation step.

## Grid {#grid}

There is a big advantage to transforming, and modeling the data in a
single ML Pipeline. It opens the door for Spark to also alter parameters
used for data transformation, in addition to the model's parameters.
This means that we can include the parameters of the tokenization,
cleaning, hashing, and normalization steps as possible candidates for
the model tuning.

The *Tuning Text Analysis* article uses three tuning parameters. Two are
in the model, and one is in the hashing step. Here are the parameters,
and how they map between `tidymodels` and `sparklyr`:

| Parameter                             | `tidymodels` | `sparklyr`          |
|---------------------------------------|--------------|---------------------|
| Number of Terms to Hash               | `num_terms`  | `num_features`      |
| Amount of regularization in the model | `penalty`    | `elastic_net_param` |
| Proportion of pure vs ridge Lasso     | `mixture`    | `reg_param`         |

The values to tune with are taken from the [Grid
Search](https://tune.tidymodels.org/articles/extras/text_analysis.html#grid-search-1){target="_blank"}
section in the *Tuning Text Analysis* article. All that is left to do is
to create the grid itself. Just like we did in the first [Model
Tuning](model_tuning.html#grid){target="_blank"} article, we use partial
name matching to the steps we want to tune:

-   `hashing_ft` will be the name of the list object containing the
    `num_features` values

-   `logistic_regression` will be the of the list object containing the
    values of the other two parameters

Notice that the R code of the values themselves are a direct copy of the
ones used in the *Tuning Text Analysis* article.

```{r}
sff_grid <-  list(
    hashing_tf = list(
      num_features = 2^c(8, 10, 12)  
    ),
    logistic_regression = list(
      elastic_net_param = 10^seq(-3, 0, length = 20), 
      reg_param = seq(0, 1, length = 5)    
    )
  )

sff_grid
```

## Evaluate

In the *Tuning Text Analysis* article, ROC AUC is used to measure
performance. The is the default metric of
`ml_binary_classification_evaluator()` , so we only need to pass the
connection variable to the evaluator function.

```{r}
sff_evaluator <- ml_binary_classification_evaluator(sc)
```

## Model Tuning (Fit)

We will use `ml_cross_validator()` to prepare a tuning specification
inside Spark. We recommend to set the `seed` argument in order to
increase reproducibility.

Spark will automatically create the grid combinations when tuning the
model. In this case, `sff_grid` contains three parameters:

-   `num_features` has 3 values

-   `elastic_net_param` has 20 values

-   `reg_parm` has 5 values

This means that there will be 300 combinations for the tuning parameters
(3 x 20 x 5). Because we set the number of folds to 3 (`num_folds`),
Spark will run a total of 900 models (3 x 300).

```{r}
sff_cv <- ml_cross_validator(
  x = sc,
  estimator = sff_pipeline, 
  estimator_param_maps = sff_grid,
  evaluator = sff_evaluator,
  num_folds = 3,
  parallelism = 4,
  seed = 100
)

sff_cv
```

This is the step that will take the longest time. The `ml_fit()`
function will run the 900 models using the training data. There is no
need to pre-prepare the re-sampling folds, Spark will take care of that.

```{r}
sff_model <- ml_fit(
  x = sff_cv, 
  dataset = sff_training_data
  )
```

## Validation metrics

We can now extract the metrics from `sff_model` using
`ml_validation_metrics()`. The ROC AUC values will be in a column called
`areaUnderROC`. We can then take a look at the best performing models
using `dplyr`.

```{r}
sff_metrics <- ml_validation_metrics(sff_model)

library(dplyr)

sff_metrics %>% 
  arrange(desc(areaUnderROC)) %>% 
  head()
```

We will now plot the results. We will match the approach used in the
[Grid
Search](https://tune.tidymodels.org/articles/extras/text_analysis.html#grid-search-1){target="_blank"}
section of the *Tuning Text Analysis* article.

```{r}
library(ggplot2)

sff_metrics %>% 
  mutate(reg_param_1 = as.factor(reg_param_1)) %>% 
  ggplot(aes(
    x = elastic_net_param_1, 
    y = areaUnderROC, 
    color = reg_param_1
    )) +
  geom_line() +
  geom_point(size = 0.5) +
  scale_x_continuous(trans = "log10") +
  facet_wrap(~ num_features_2) +
  theme_light(base_size = 9)
```

In the plot, we can see the effects of the three parameters, and the
values that look to be the best. These effects are very similar to the
original *Tuning Text Analysis* article.

## Model selection

We can create a new ML Pipeline using the same code as the original
pipeline. We only need to change the 3 parameters values, with values
that performed best.

```{r}
new_sff_pipeline <- ml_pipeline(sc) %>% 
  ft_tokenizer(
    input_col = "review",
    output_col = "word_list"
  ) %>% 
  ft_stop_words_remover(
    input_col = "word_list", 
    output_col = "wo_stop_words"
    ) %>% 
  ft_hashing_tf(
    input_col = "wo_stop_words", 
    output_col = "hashed_features", 
    binary = TRUE, 
    num_features = 4096      
    ) %>%
  ft_normalizer(
    input_col = "hashed_features", 
    output_col = "normal_features"
    ) %>% 
  ft_r_formula(score ~ normal_features) %>% 
  ml_logistic_regression(
    elastic_net_param = 0.05,
    reg_param = 0.25  
    )
```

Now, we create a final model using the new ML Pipeline.

```{r}
new_sff_fitted <- new_sff_pipeline %>% 
  ml_fit(sff_training_data)
```

## Test data metrics

The test data set is now used to confirm that the performance gains
hold. We use it to run predictions with thew new ML Pipeline Model.

```{r}
new_sff_fitted %>% 
  ml_transform(sff_testing_data) %>% 
  ml_metrics_binary()
```

The results show an increase performance in contrast with running the ML
Pipeline Model, those results are in [Fit and
Predict](textmodeling.qmd#fit-and-predict) section of the *Text
Modeling* article.

```{r}
#| include: false
spark_disconnect(sc)
```

## Advantages of using Spark for Model Tuning

Spark's capabilities, combined with `sparklyr` 's API, provide a very
streamlined way to go from exploration, to modeling, to tuning. Even
without a "formal" Spark cluster, it is possible to take advantage of
these capabilities right from our personal computer.

If we add an actual cluster to the mix, the advantage of using Spark
raises dramatically. Usually we talk about Spark for "big data"
analysis, but in this case, we can leverage it to parallelize the 900
models across multiple machines. The ability to distribute the models
across the cluster will cut down the tune processing time. The amount of
time saved will depend on the resources and configuration of the
cluster, as well as of those of the session.
