{
  "hash": "6b5ee968b2d069888852a033151d38c2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model Data\"\nformat:\n  html:\n    theme: default\n    toc: true\nexecute:\n  eval: true\n  freeze: true\n---\n\n\n\n\nYou can orchestrate machine learning algorithms in a Spark cluster via the [machine learning](https://spark.apache.org/docs/latest/mllib-guide.html) functions \nwithin `sparklyr`. These functions connect to a set of high-level APIs built on\ntop of DataFrames that help you create and tune machine learning workflows.\n\n## Exercise\n\nHere's an example where we use `ml_linear_regression()` to fit a linear \nregression model. We'll use the built-in `mtcars` dataset, and see if we can \npredict a car's fuel consumption (`mpg`) based on its weight (`wt`), and the\nnumber of cylinders the engine contains (`cyl`). We'll assume in each case that \nthe relationship between `mpg` and each of our features is linear.\n\n### Initialize the environment\n\nWe will start by creating a local Spark session and load \nthe `mtcars` data frame to it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- copy_to(sc, mtcars, overwrite = TRUE)\n```\n:::\n\n\n### Prepare the data\n\nSpark provides data frame operations that makes it easier to prepare data for\nmodeling.  In this case, we will use the `sdf_partition()` command to \ndivide the `mtcars` data into \"training\" and \"test\". \n\n\n::: {.cell}\n\n```{.r .cell-code}\npartitions <- mtcars_tbl %>%\n  select(mpg, wt, cyl) %>% \n  sdf_random_split(training = 0.5, test = 0.5, seed = 1099)\n```\n:::\n\n\nNote that the newly created `partitions` variable does not contain data, it \ncontains a pointer to where the data was split within Spark. That means that no\ndata is downloaded to the R session.\n\n### Fit the model\n\nNext, we will fit a linear model to the training data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- partitions$training %>%\n  ml_linear_regression(mpg ~ .)\n\nfit\n#> Formula: mpg ~ .\n#> \n#> Coefficients:\n#> (Intercept)          wt         cyl \n#>   38.927395   -4.131014   -0.938832\n```\n:::\n\n\nFor linear regression models produced by Spark, we can use `summary()` to \nlearn a bit more about the quality of our fit, and the statistical significance \nof each of our predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n#> Deviance Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.4891 -1.5262 -0.1481  0.8508  6.3162 \n#> \n#> Coefficients:\n#> (Intercept)          wt         cyl \n#>   38.927395   -4.131014   -0.938832 \n#> \n#> R-Squared: 0.8469\n#> Root Mean Squared Error: 2.416\n```\n:::\n\n### Use the model\n\nWe can use `ml_predict()` to create a Spark data frame that contains the \npredictions against the testing data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred <- ml_predict(fit, partitions$test)\n\nhead(pred)\n#> # Source:   SQL [?? x 4]\n#> # Database: spark_connection\n#>     mpg    wt   cyl prediction\n#>   <dbl> <dbl> <dbl>      <dbl>\n#> 1  14.3  3.57     8      16.7 \n#> 2  14.7  5.34     8       9.34\n#> 3  15    3.57     8      16.7 \n#> 4  15.2  3.44     8      17.2 \n#> 5  15.2  3.78     8      15.8 \n#> 6  15.5  3.52     8      16.9\n```\n:::\n\n\n### Further reading\n\nSpark machine learning supports a wide array of algorithms and feature \ntransformations and as illustrated above it's easy to chain these functions\ntogether with `dplyr` pipelines. To learn more see the  [Machine Learning](/guides/mlib.qmd) \narticle on this site. For a list of Spark ML models available through `sparklyr` visit\n[Reference - ML](/packages/sparklyr/latest/reference/index.html#spark-machine-learning)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}