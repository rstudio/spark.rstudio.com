{
  "hash": "e5efcf566e67c4392528d65a43f6b94e",
  "result": {
    "markdown": "---\ntitle: \"Bind multiple Spark DataFrames by row and column\"\nexecute:\n  freeze: true\n---\n\n\n\n\n*R/mutation.R*\n\n## sdf_bind\n\n## Description\n`sdf_bind_rows()` and `sdf_bind_cols()` are implementation of the common pattern of `do.call(rbind, sdfs)` or `do.call(cbind, sdfs)` for binding many Spark DataFrames into one. \n\n\n## Usage\n```r\nsdf_bind_rows(..., id = NULL) \n\nsdf_bind_cols(...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ... | Spark tbls to combine. <br>  Each argument can either be a Spark DataFrame or a list of   Spark DataFrames <br>  When row-binding, columns are matched by name, and any missing   columns with be filled with NA. <br>  When column-binding, rows are matched by position, so all data   frames must have the same number of rows. |\n| id | Data frame identifier. <br>  When `id` is supplied, a new column of identifiers is   created to link each row to its original Spark DataFrame. The labels   are taken from the named arguments to `sdf_bind_rows()`. When a   list of Spark DataFrames is supplied, the labels are taken from the   names of the list. If no names are found a numeric sequence is   used instead. |\n\n## Details\nThe output of `sdf_bind_rows()` will contain a column if that column appears in any of the inputs. \n\n\n## Value\n`sdf_bind_rows()` and `sdf_bind_cols()` return `tbl_spark`\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}