{
  "hash": "ea085f76ee446a4606607da512e84723",
  "result": {
    "markdown": "---\ntitle: \"Text modeling\"\nexecute:\n  eval: false\n  freeze: true\n---\n\n\n\n\nFor this example we will use a local Spark connection, version 3.3\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\", version = \"3.3\")\n```\n:::\n\n\n## Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(modeldata)\n\ndata(\"small_fine_foods\")\n\ntraining_data %>% \n  head(1) %>% \n  as.list()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_training_data <- copy_to(sc, training_data)\n\nsff_testing_data <- copy_to(sc, testing_data)\n```\n:::\n\n\n## Text transformers\n\n1.  We will split each review into individual words, or tokens. The `ft_tokenizer()` function returns a in-line list containing the individual words.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    sff_training_data %>% \n      ft_tokenizer(\n        input_col = \"review\",\n        output_col = \"word_list\"\n      ) %>% \n      select(3:4)\n    ```\n    :::\n\n\n2.  There are words very common in text, words such as: \"the\", \"and\", \"or\", etc. These are called \"stop words\". Most often, stop words are not useful in analysis and modeling so it is necessary to remove them. That is exactly what `ft_stop_words_remover()` does. Spark contains a list of stop words for several languages, not only English.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    sff_training_data %>% \n      ft_tokenizer(\n        input_col = \"review\",\n        output_col = \"word_list\"\n      ) %>% \n      ft_stop_words_remover(\n        input_col = \"word_list\", \n        output_col = \"wo_stop_words\"\n        ) %>% \n      select(3:5) \n    ```\n    :::\n\n\n3.  Text hashing maps a sequence of words, or \"terms\", to their frequencies. The number of terms that are mapped can be controlled using the `num_features` argument in `ft_hashing_ft()`. Because we are eventually going to use a logistic regression model, we will need to override the frequencies from their original value to 1. This is accomplished by setting the `binary` argument to `TRUE`.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    sff_training_data %>%\n      ft_tokenizer(\n        input_col = \"review\",\n        output_col = \"word_list\"\n      ) %>% \n      ft_stop_words_remover(\n        input_col = \"word_list\", \n        output_col = \"wo_stop_words\"\n        ) %>% \n      ft_hashing_tf(\n        input_col = \"wo_stop_words\", \n        output_col = \"hashed_features\", \n        binary = TRUE, \n        num_features = 1024\n        ) %>%\n      select(3:6) \n    \n    ```\n    :::\n\n\n4.  Finally, we normalize the hashed column using `ft_normalizer()` .\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    sff_training_data %>% \n      ft_tokenizer(\n        input_col = \"review\",\n        output_col = \"word_list\"\n      ) %>% \n      ft_stop_words_remover(\n        input_col = \"word_list\", \n        output_col = \"wo_stop_words\"\n        ) %>% \n      ft_hashing_tf(\n        input_col = \"wo_stop_words\", \n        output_col = \"hashed_features\", \n        binary = TRUE, \n        num_features = 1024\n        ) %>%\n      ft_normalizer(\n        input_col = \"hashed_features\", \n        output_col = \"normal_features\"\n        ) %>% \n      select(3:7) \n    ```\n    :::\n\n\n:::{.callout-tip}\n## Important concept\n\nThe `ft_hashing_tf()` outputs the index and frequency of each term. This can be thought of as how \"dummy variables\" are created for each discrete value of a\ncategorical variable.  This means that for modeling, we will only need to use\nonly one \"column\", `hashed_features`. But, we will use `normal_features` for the\nmodel because it is derived from `hashed_features`.\n\n:::\n\n## Pipeline\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_pipeline <- ml_pipeline(sc) %>% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %>% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %>% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 1024\n    ) %>%\n  ft_normalizer(\n    input_col = \"hashed_features\", \n    output_col = \"normal_features\"\n    ) %>% \n  ft_r_formula(score ~ normal_features) %>% \n  ml_logistic_regression()  \n\nsff_pipeline\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_pipeline_model <- ml_fit(sff_pipeline, sff_training_data)\n\nsff_pipeline_model\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_test_predictions <- sff_pipeline_model %>% \n  ml_transform(sff_testing_data) \n\nglimpse(sff_test_predictions)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nml_metrics_binary(sff_test_predictions)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_disconnect(sc)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}