---
title: Tune Tidymodel in Databricks
execute:
  eval: true
  freeze: true
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| include: false
run_r <- TRUE

library(tidymodels)
library(sparklyr)

sc <- spark_connect(method = "databricks_connect", cluster_id = "1218-000327-q970zsow")
```

## Intro

`sparklyr` makes it possible to off-load a Tidymodels grid search model
tuning to Databricks. [Tidymodels](https://www.tidymodels.org/) is a
collection of packages that are designed to work together to provide
everything from resampling, to preprocessing, to model tuning, to
performance measurement.

Model tuning is a time consuming process because of the number of tuning
parameter combinations that need to be processed. Running the
combinations in parallel saves a significant amount of time. Tidymodels
provides ways to run parallel tuning, but not in Spark. `sparklyr` not
only enables this possibility, but it also makes it very simple to
access this functionality.

In Tidymodels, the
[`tune_grid()`](https://tune.tidymodels.org/reference/tune_grid.html)
function is called to execute the grid search locally. To run in
Databricks, simply call `sparklyr`'s `tune_grid_spark()` instead. It
accepts the exact same arguments as `tune_grid()` does, see @fig-code.

::::::: {#fig-code}
:::::: columns
::: {.column width="38%"}
```{r}
#| eval: false
tune::tune_grid(
  object = my_model, 
  preprocessor = my_recipe, 
  resamples = my_resamples
  
  )
```

Run locally
:::

::: {.column width="4%"}
:::

::: {.column width="58%"}
```{r}
#| eval: false
sparklyr::tune_grid_spark(
  object = my_model, 
  preprocessor = my_recipe, 
  resamples = my_resamples,
  sc = my_conn # Only additional requirement
  )
```

Run remotely in Databricks
:::
::::::

Comparing tune and sparklyr function calls
:::::::

`sparklyr` will automatically upload the needed R object to Databricks,
such as the resample object, model specification, and the preprocessing
steps. It will then run the tuning in parallel taking advantage of the R
integration in Databricks. Lastly, it will collect all of the results
back to your local R session, and return a `tune_results` object that is
indistinguishable from one made directly by Tidymodels, , see
@fig-diagram.

::: {#fig-diagram}
![](/images/guides/tidymodels-db/diagram-1.png)

How *tune_grid_spark()* works with Databricks
:::

## Example

In Tidymodels, there are some specific elements needed to perform a grid
search tuning:

-   Model specification (`parsnip`)

-   Resampled data (`rsample`)

-   Data preprocessor (`recipe`)

-   Post processor (`tailor`) *optional*

In the following example, we will create the elements just as if they
were about to be used by `tune_grid()` locally.

```{r}
#| eval: !expr 'run_r'

library(tidymodels)
library(readmission)
set.seed(1)
readmission_splits <- initial_split(readmission, strata = readmitted)

# Resampling -------------------------------------------------------
readmission_folds <- vfold_cv(
  data = training(readmission_splits),
  strata = readmitted
  )

# Preprocessing ----------------------------------------------------
recipe_basic <- recipe(readmitted ~ ., data = readmission) |>
  step_mutate(
    race = factor(case_when(
      !(race %in% c("Caucasian", "African American")) ~ "Other",
      .default = race
    ))
  ) |>
  step_unknown(all_nominal_predictors()) |>
  step_YeoJohnson(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors())

# Model specification -----------------------------------------------
spec_bt <- boost_tree(
  mode = "classification",
  mtry = tune(),       # Parameter to be tuned
  learn_rate = tune(), # Parameter to be tuned
  trees = 10
  )
```

The only required step before tuning the model in Databricks, is to
connect to a Databricks' Spark cluster. This step could have been done
in the outset of this example, but we want to showcase how it is
possible to start intending to tune locally, and then decide to off-load
the job to Databricks.

```{r}
#| eval: FALSE
library(sparklyr)

sc <- spark_connect(
  method = "databricks_connect", 
  cluster_id = "1218-000327-q970zsow" # Replace with your own cluster's ID
  )
```

The next step is to call `tune_grid_spark()`. We will pass the three
elements created in the first step, and the Databricks connection
variable. Adding the `control_grid()` was an optional step, this was
done to showcase the output from `sparklyr` when running this process.

```{r}
#| eval: !expr 'run_r'
spark_results <- tune_grid_spark(
  sc = sc, 
  object = spec_bt, 
  preprocessor = recipe_basic, 
  resamples = readmission_folds,
  control = control_grid(verbose = TRUE)
  )
```

After the process completes, the `spark_results` object can be now used
as if it was created by `tune`.

```{r}
#| eval: !expr 'run_r'
autoplot(spark_results)
```

## R and Python libraries

There are Python and R packages that need to be pre-installed in the
Databricks cluster:

**Python**

-   `rpy2`

**R**

-   `tidymodels`

-   `reticulate`

-   The modeling package used by `parsnip` in the tuning

Installation is a simple operation that is done via your Databricks web
portal. Here are the instructions that shows you how to do that:
[Databricks - Cluster
Libraries](https://docs.databricks.com/en/libraries/cluster-libraries.html).

### Install programmatically (optional)

The [`brickster`](https://databrickslabs.github.io/brickster/) package
is a complete toolkit for interacting with Databricks. It can be used to
programmatically install the Python and R libraries. It requires that
the user has rights to modify the libraries in target cluster.

```{r}
#| eval: false
library(brickster)

# DBRs have a fixed snapshot date to source the R packages. Redirecting  
# to the 'latest' snapshot to get the most recent package versions.
repo <- "https://databricks.packagemanager.posit.co/cran/__linux__/noble/latest"

# Builds the list of R packages, pointing them to the 'latest' snapshot
r_libs <- libraries(
  lib_cran("reticulate", repo = repo),
  lib_cran("tidymodels", repo = repo),
  lib_cran("xgboost", repo = repo) # (optional) Used in the example
)

db_libs_install("1218-000327-q970zsow", r_libs)

# Builds the Python package object. Specifying the version of `rpy2` to install.
py_lib <- lib_pypi("rpy2==3.6.4") |>
  libraries()

db_libs_install("1218-000327-q970zsow", py_lib)
```

As time goes by, there may be other modeling R packages that need to be
installed in Spark. Here is a template that can be used to install a
single package:

```{r}
#| eval: false
library(brickster)

lib_cran(
  package = "[Missing package]", 
  repo = "https://databricks.packagemanager.posit.co/cran/__linux__/noble/latest"
  ) |> 
  libraries() |> 
  db_libs_install("[Your cluster ID]", libraries =  _)

```
