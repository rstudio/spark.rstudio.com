---
title: "Define a Spark Compilation Specification"
aliases:
  - reference/sparklyr/latest/spark_compilation_spec.html
---

    <div>

    <div>
    <ul data-gumshoe>
    <li><a href="#arguments">Arguments</a></li>
    <li><a href="#details">Details</a></li>
    </ul>
    </div>

    <div>

    <p>For use with <code><a href='compile_package_jars.html'>compile_package_jars</a></code>. The Spark compilation
specification is used when compiling Spark extension Java Archives, and
defines which versions of Spark, as well as which versions of Scala, should
be used for compilation.</p>

    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class='fu'>spark_compilation_spec</span>(
  <span class='kw'>spark_version</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>spark_home</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>scalac_path</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>scala_filter</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>jar_name</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>jar_path</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>jar_dep</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>embedded_srcs</span> <span class='kw'>=</span> <span class='st'>"embedded_sources.R"</span>
)</code></pre></div>

    <h2 id="arguments">Arguments</h2>
    <table class="ref-arguments">

    <colgroup>
      <col class="name" />
      <col class="desc" />
    </colgroup>

    <tr>
      <td>spark_version</td>
      <td><p>The Spark version to build against. This can
be left unset if the path to a suitable Spark home is supplied.</p></td>
    </tr>
    <tr>
      <td>spark_home</td>
      <td><p>The path to a Spark home installation. This can
be left unset if <code>spark_version</code> is supplied; in such a case,
<code>sparklyr</code> will attempt to discover the associated Spark
installation using <code><a href='spark_home_dir.html'>spark_home_dir</a></code>.</p></td>
    </tr>
    <tr>
      <td>scalac_path</td>
      <td><p>The path to the <code>scalac</code> compiler to be used
during compilation of your Spark extension. Note that you should
ensure the version of <code>scalac</code> selected matches the version of
<code>scalac</code> used with the version of Spark you are compiling against.</p></td>
    </tr>
    <tr>
      <td>scala_filter</td>
      <td><p>An optional <span style="R">R</span> function that can be used to filter
which <code>scala</code> files are used during compilation. This can be
useful if you have auxiliary files that should only be included with
certain versions of Spark.</p></td>
    </tr>
    <tr>
      <td>jar_name</td>
      <td><p>The name to be assigned to the generated <code>jar</code>.</p></td>
    </tr>
    <tr>
      <td>jar_path</td>
      <td><p>The path to the <code>jar</code> tool to be used
during compilation of your Spark extension.</p></td>
    </tr>
    <tr>
      <td>jar_dep</td>
      <td><p>An optional list of additional <code>jar</code> dependencies.</p></td>
    </tr>
    <tr>
      <td>embedded_srcs</td>
      <td><p>Embedded source file(s) under <code>&lt;R package root&gt;/java</code> to
be included in the root of the resulting jar file as resources</p></td>
    </tr>
    </table>

    <h2 id="details">Details</h2>

    <p>Most Spark extensions won't need to define their own compilation specification,
and can instead rely on the default behavior of <code>compile_package_jars</code>.</p>


    </div>

    </div>




