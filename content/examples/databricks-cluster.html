---
title: "Using sparklyr with Databricks"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
params:
  width: 600
---



<div id="overview" class="section level2">
<h2>Overview</h2>
<p>This documentation demonstrates how to use <code>sparklyr</code> with RStudio Server Pro
and a Databricks cluster with Apache Spark.</p>
</div>
<div id="best-practices" class="section level2">
<h2>Best practices</h2>
<ul>
<li><strong>Remote connectivity</strong> - Install RStudio Server Pro outside of the
Databricks cluster and work with Spark remotely</li>
<li><strong>RStudio Team</strong> - Install RStudio Connect and RStudio Package Manager outside
of the Databricks cluster so that they are not limited to the resources or
ephemeral nature of a Databricks cluster</li>
<li><strong>Limit to interactive workloads</strong> - Perform interactive analyses with Spark,
then write the results to a database, file storage, or cloud storage for more
efficient retrival</li>
<li><strong>Efficiently loading results</strong> - Shiny apps that use Spark tend to have
performance issues, instead, consider reading the results from a database,
file storage, or cloud storage instead</li>
<li><strong>Persisent installation</strong> - If you cannot work with Spark remotely, you
should install RStudio Server Pro on the Driver node of a persistent
Databricks cluster as opposed to a Worker node or an ephemeral cluster</li>
</ul>
</div>
<div id="options-for-using-rstudio-with-databricks" class="section level2">
<h2>Options for using RStudio with Databricks</h2>
<p>There are two different options for using <code>sparklyr</code> with Databricks:</p>
<ul>
<li>Option 1 (Preferred option): <a href="#option-1---connecting-to-databricks-remotely">Connecting to Databricks remotely</a></li>
<li>Option 2 (Alternative option): <a href="#option-2---working-with-databricks-locally">Working with Databricks locally</a></li>
</ul>
<div id="option-1---connecting-to-databricks-remotely" class="section level3">
<h3>Option 1 - Connecting to Databricks remotely</h3>
<p>With this configuration, RStudio Server Pro is installed outside of the Spark cluster
and connects to Spark remotely using Databricks Connect.</p>
<p>This is the recommended configuration because it targets separate environments,
involves simpler configuration, avoids resource contention, and allows RStudio
Server Pro to connect to Databricks as well as other remote storage and compute
resources.</p>
<p><a href="/examples/databricks-cluster-remote">
<img src="/images/deployment/databricks/rstudio-databricks-remote.png" width='800px' align='center'/>
</a></p>
<a href="/examples/databricks-cluster-remote">
<h2>
View steps for connecting to Databricks remotely
</h2>
<p></a></p>
</div>
<div id="option-2---working-with-databricks-locally" class="section level3">
<h3>Option 2 - Working with Databricks locally</h3>
<p>With this configuration, RStudio Server Pro is installed on the Spark driver
node and allows users to work locally with Spark.</p>
<p>This configuration can result in more complex configuration, limited
connectivity to other storage and compute sources, resource contention between
RStudio Server Pro and Databricks, and maintenance concerns due to the ephemeral
nature of Databricks clusters.</p>
<p><a href="/examples/databricks-cluster-local">
<img src="/images/deployment/databricks/rstudio-databricks-local.png" width='800px' align='center'/>
</a></p>
<a href="/examples/databricks-cluster-local">
<h2>
View steps for working with Databricks locally
</h2>
<p></a></p>
<!-- The plan is to launch 4 identical EC2 server instances. One server will be the
Master node and the other 3 the worker nodes. In one of the worker nodes, we
will install RStudio server.

What makes a server the Master node is only the fact that it is running the
**master** service, while the other machines are running the **slave** service
and are pointed to that first master. This simple setup, allows us to install
the same Spark components on all 4 servers and then just add RStudio to one of
them.

The topology will look something like this:

<p><img src="images/deployment/amazon-ec2/spark-sa-setup.png" width='600px' align='center'/></p>



## AWS EC Instances

Here are the details of the EC2 instance, just deploy one at this point:

- **Type:** t2.medium
- **OS:** Ubuntu 16.04 LTS
- **Disk space:** At least 20GB
- **Security group:** Open the following ports: 8080 (Spark UI), 4040 (Spark
  Worker UI), 8088 (sparklyr UI) and 8787 (RStudio. Also open *All TCP* ports
  for the machines inside the security group.

## Spark

Perform the steps in this section on all of the servers that will be part of the
cluster.

### Install Java 8

- We will add the Java 8 repository, install it and set it as default

```{}
sudo apt-add-repository ppa:webupd8team/java
sudo apt-get update
sudo apt-get install oracle-java8-installer
sudo apt-get install oracle-java8-set-default
sudo apt-get update
```

### Download Spark

- Download and unpack a pre-compiled version of Spark. Here's is the link to the
  [official Spark download page](http://spark.apache.org/downloads.html)

```{}
wget http://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-hadoop2.7.tgz
tar -xvzf spark-2.1.0-bin-hadoop2.7.tgz
cd spark-2.1.0-bin-hadoop2.7
```

### Create and launch AMI

- We will create an image of the server. In Amazon, these are called AMIs, for
  information please see the
  [User Guide](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html).

- Launch 3 instances of the AMI

## RStudio Server

Select one of the nodes to execute this section. Please check the
[RStudio download page](https://www.rstudio.com/products/rstudio/download-server/)
for the latest version

### Install R

- In order to get the latest R core, we will need to update the source list in
  Ubuntu.

```{}
sudo sh -c 'echo "deb http://cran.rstudio.com/bin/linux/ubuntu xenial/" >> /etc/apt/sources.list'
gpg --keyserver keyserver.ubuntu.com --recv-key 0x517166190x51716619e084dab9
gpg -a --export 0x517166190x51716619e084dab9 | sudo apt-key add -
sudo apt-get update
```

- Now we can install R
```{}
sudo apt-get install r-base
sudo apt-get install gdebi-core
```

### Install RStudio

- We will download and install 1.044 of RStudio Server. To find the latest
  version, please visit the
  [RStudio website](https://www.rstudio.com/products/rstudio/download3/). In
  order to get the enhanced integration with Spark, RStudio version 1.044 or
  later will be needed.

```{}
wget https://download2.rstudio.org/rstudio-server-1.0.153-amd64.deb
sudo gdebi rstudio-server-1.0.153-amd64.deb
```

### Install dependencies

- Run the following commands

```{}
sudo apt-get -y install libcurl4-gnutls-dev
sudo apt-get -y install libssl-dev
sudo apt-get -y install libxml2-dev
```

### Add default user

- Run the following command to add a default user

```{}
sudo adduser rstudio-user
```
### Start the Master node

- Select one of the servers to become your Master node

- Run the command that starts the master service
```{}
sudo spark-2.1.0-bin-hadoop2.7/sbin/start-master.sh
```
- Close the terminal connection (optional)

### Start Worker nodes

- Start the slave service. **Important**: Use dots not dashes as separators for
  the Spark Master node's address

```{}
sudo spark-2.1.0-bin-hadoop2.7/sbin/start-slave.sh spark://[Master node's IP address]:7077
sudo spark-2.1.0-bin-hadoop2.7/sbin/start-slave.sh spark://ip-172-30-1-94.us-west-2.compute.internal:7077
```

- Close the terminal connection (optional)

### Pre-load pacakges

- Log into RStudio (port 8787)

- Use 'rstudio-user'
```r
install.packages("sparklyr")
```

### Connect to the Spark Master

- Navigate to the Spark Master's UI, typically on port 8080 <img src="images/deployment/amazon-ec2/spark-master.png" class="screenshot" width=639 />

- Note the **Spark Master URL**

- Logon to RStudio

- Run the following code


```r
library(sparklyr)

conf <- spark_config()
conf$spark.executor.memory <- "2GB"
conf$spark.memory.fraction <- 0.9

sc <- spark_connect(master="[Spark Master URL]",
              version = "2.1.0",
              config = conf,
              spark_home = "/home/ubuntu/spark-2.1.0-bin-hadoop2.7/")

``` -->
</div>
</div>
