{
  "hash": "e71c916072383dc42705332e27af7a37",
  "result": {
    "markdown": "---\ntitle: \"Get Started\"\nformat:\n  html:\n    theme: default\n    toc: true\nexecute:\n  eval: true\n  freeze: true\n---\n\n\n\n\n::: {.panel-tabset}\n\n## Install\n\n### Install the package\n\nYou can install the `sparklyr` package from [CRAN](https://CRAN.r-project.org) as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"sparklyr\")\n```\n:::\n\n### Install Spark locally\n\nThis section is meant for developers new to `sparklyr`.  You will need a running\nSpark environment to connect to.  `sparklyr` can install Spark in your computer. \nThe installed Spark environment is meant for learning and prototyping purposes.\nThe installation will work on all the major Operating Systems that R works on,\nincluding Linux, MacOS, and Windows.  \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n\nspark_install()\n```\n:::\n\nPlease be aware that after installation, Spark is **not** running. The next section\nwill explain how to start a single node Spark cluster in your machine.\n\n### Connect to Spark\n\nYou can use `spark_connect()` to connect to Spark clusters.  The arguments passed to\nthis functions depend on the type of Spark cluster you are connecting to. There are\nseveral different types of Spark clusters, such as YARN, Stand Alone and Kubernetes. \n\n`spark_connect()` is able to both start, and connect to, the single node Spark\ncluster in your machine.  In order to do that, pass \"local\" as the argument\nfor `master`:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n```\n:::\n\nThe `sc` variable now contains all of the connection information needed to\ninteract with the cluster.\n\nTo learn how to connect to other types of Spark clusters, see the \n[Deployment](/deployment/index.qmd) section of this site. \n\nFor \"local\" connection, `spark_disconnect()` will shut down the single node\nSpark environment in your machine, and tell R that the connection is no longer \nvalid.  For other types of Spark clusters, `spark_disconnect()` will only \nend the Spark session, it will not shut down the Spark cluster itself.\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_disconnect(sc)\n```\n:::\n\n\n## Read Data\n\nA new Spark session will contain no data. The first step is to either load data\ninto your Spark session's memory, or point Spark to the location of the data\nso it can access the data on-demand.\n\nFor this exercise, we will start a \"local\" Spark session, and then transfer data \nfrom our R environment to the Spark session's memory.  To do that, we will \nuse the `copy_to()` command:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\ntbl_mtcars <- copy_to(sc, mtcars, \"spark_mtcars\")\n```\n:::\n\nIf you are using the RStudio IDE, you will notice a new table in the Connections\npane.  The name of that table is **spark_mtcars**.  That is the name of the data\nset inside the Spark memory. The `tbl_mtcars` variable does not contain any\n`mtcars` data, this variable contains the info that points to the location where \nthe Spark session loaded the data to. \n\nCalling the `tbl_mtcars` variable in R will download the first 1,000 records and\ndisplay them :\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars\n```\n\n::: {.cell-output-stdout}\n```\n# Source: spark<spark_mtcars> [?? x 11]\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# … with more rows\n```\n:::\n:::\nNotice that at the top of the data print out, it is noted that records were \ndownloaded from Spark: *Source: spark...*.\n\nTo clean up the session, we will now stop the Spark session:\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_disconnect(sc)\n```\n:::\n\n### Files\n\nIn a formal Spark environment, it will be rare when we would have to upload data\nfrom R into Spark.  \n\nUsing `sparklyr`, you can tell Spark to read and write data. Spark is able to interact\nwith multiple types of file systems, such as HDFS, S3 and local. Additionally, \nSpark is able to read several file types such as CSV, Parquet, Delta and JSON.\n`sparklyr` provides functions that makes it easy to access these features.  See\nthe [Spark Data](/packages/sparklyr/latest/reference/#spark-data) section for a \nfull list of available functions.  \n\nThe following command will tell Spark to read a CSV file, and to also load it\ninto Spark memory. \n\n::: {.cell}\n\n```{.r .cell-code}\n# Do not run the next following command. It is for example purposes only.\nspark_read_csv(sc, name = \"test_table\",  path = \"/test/path/test_file.csv\")\n```\n:::\n\n\n## Prepare Data\n\nThere are three methods for working with data: \n\n- Using `dplyr` commands\n- Using SQL\n- Using Spark's feature transformers\n\n\nFor this example start a local session of Spark. We'll start by copying a \ndata set from R into the Spark cluster (note that you may need to install the \n`nycflights13`)\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nflights_tbl <- copy_to(sc, nycflights13::flights, \"spark_flights\")\n```\n:::\n\n### Using `dplyr`\n\nWe can now use all of the available `dplyr` verbs against the tables within the cluster.\n\n::: {.cell}\n\n```{.r .cell-code}\ndelay <- flights_tbl %>%\n  group_by(tailnum) %>%\n  summarise(\n    count = n(), \n    dist = mean(distance, na.rm = TRUE), \n    delay = mean(arr_delay, na.rm = TRUE)\n    ) %>%\n  filter(count > 20, dist < 2000, !is.na(delay)) \n\ndelay\n```\n\n::: {.cell-output-stdout}\n```\n# Source: spark<?> [?? x 4]\n   tailnum count  dist  delay\n   <chr>   <dbl> <dbl>  <dbl>\n 1 N24211    130 1330.  7.7  \n 2 N793JB    283 1529.  4.72 \n 3 N657JB    285 1286.  5.03 \n 4 N633AA     24 1587. -0.625\n 5 N9EAMQ    248  675.  9.24 \n 6 N3GKAA     77 1247.  4.97 \n 7 N997DL     63  868.  4.90 \n 8 N318NB    202  814. -1.12 \n 9 N651JB    261 1408.  7.58 \n10 N841UA     96 1208.  2.10 \n# … with more rows\n```\n:::\n:::\n\nFor additional documentation on using dplyr with Spark see the [Manipulating Data with `dplyr`](/get-started/dplyr.qmd) article in \nthis site\n\n### Using SQL\n\nIt's also possible to execute SQL queries directly against tables within a Spark cluster. The `spark_connection()` object implements a [DBI](https://dbi.r-dbi.org/) interface for Spark, so you can use `dbGetQuery()` to execute SQL and return the result as an R data frame:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DBI)\n\ndbGetQuery(sc, \"SELECT carrier, sched_dep_time, dep_time, dep_delay FROM spark_flights LIMIT 5\")\n```\n\n::: {.cell-output-stdout}\n```\n  carrier sched_dep_time dep_time dep_delay\n1      UA            515      517         2\n2      UA            529      533         4\n3      AA            540      542         2\n4      B6            545      544        -1\n5      DL            600      554        -6\n```\n:::\n:::\n::: {.cell}\n\n```{.r .cell-code}\nspark_disconnect(sc)\n```\n:::\n\n\n## Model Data\n\nYou can orchestrate machine learning algorithms in a Spark cluster via the [machine learning](https://spark.apache.org/docs/latest/mllib-guide.html) functions within **sparklyr**. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning workflows.\n\nHere's an example where we use [ml_linear_regression](https://spark.rstudio.com/reference/ml_linear_regression/) to fit a linear regression model. We'll use the built-in `mtcars` dataset, and see if we can predict a car's fuel consumption (`mpg`) based on its weight (`wt`), and the number of cylinders the engine contains (`cyl`). We'll assume in each case that the relationship between `mpg` and each of our features is linear.\n\n::: {.cell}\n\n```{.r .cell-code}\n# copy mtcars into spark\nmtcars_tbl <- copy_to(sc, mtcars, overwrite = TRUE)\n\n# transform our data set, and then partition into 'training', 'test'\npartitions <- mtcars_tbl %>%\n  filter(hp >= 100) %>%\n  mutate(cyl8 = cyl == 8) %>%\n  sdf_partition(training = 0.5, test = 0.5, seed = 1099)\n\n# fit a linear model to the training dataset\nfit <- partitions$training %>%\n  ml_linear_regression(response = \"mpg\", features = c(\"wt\", \"cyl\"))\nfit\n```\n:::\n\nFor linear regression models produced by Spark, we can use `summary()` to learn a bit more about the quality of our fit, and the statistical significance of each of our predictors.\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n:::\n\nSpark machine learning supports a wide array of algorithms and feature transformations and as illustrated above it's easy to chain these functions together with dplyr pipelines. To learn more see the [machine learning](https://spark.rstudio.com/mlib/) section.\n\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}