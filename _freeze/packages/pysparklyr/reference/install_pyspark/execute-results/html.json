{
  "hash": "cf138bf594545f6502a1f771b17c55e7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Installs PySpark and Python dependencies\"\nexecute:\n  freeze: true\n---\n\n\n\n*R/python-install.R*\n\n## install_pyspark\n\n## Description\n Installs PySpark and Python dependencies  Installs Databricks Connect and Python dependencies \n\n\n## Usage\n```r\n \ninstall_pyspark( \n  version = NULL, \n  envname = NULL, \n  python_version = NULL, \n  new_env = TRUE, \n  method = c(\"auto\", \"virtualenv\", \"conda\"), \n  as_job = TRUE, \n  install_ml = FALSE, \n  ... \n) \n \ninstall_databricks( \n  version = NULL, \n  cluster_id = NULL, \n  envname = NULL, \n  python_version = NULL, \n  new_env = TRUE, \n  method = c(\"auto\", \"virtualenv\", \"conda\"), \n  as_job = TRUE, \n  install_ml = FALSE, \n  ... \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| version | Version of 'databricks.connect' to install. Defaults to `NULL`. If `NULL`, it will check against PyPi to get the current library version. |\n| envname | The name of the Python Environment to use to install the Python libraries. Defaults to `NULL.` If `NULL`, a name will automatically be assigned based on the version that will be installed |\n| python_version | The minimum required version of Python to use to create the Python environment. Defaults to `NULL`. If `NULL`, it will check against PyPi to get the minimum required Python version. |\n| new_env | If `TRUE`, any existing Python virtual environment and/or Conda environment specified by `envname` is deleted first. |\n| method | The installation method to use. If creating a new environment, `\"auto\"` (the default) is equivalent to `\"virtualenv\"`. Otherwise `\"auto\"` infers the installation method based on the type of Python environment specified by `envname`. |\n| as_job | Runs the installation if using this function within the RStudio IDE. |\n| install_ml | Installs ML related Python libraries. Defaults to TRUE. This is mainly for machines with limited storage to avoid installing the rather large 'torch' library if the ML features are not going to be used. This will apply to any environment backed by 'Spark' version 3.5 or above. |\n| ... | Passed on to `reticulate::py_install()` |\n| cluster_id | Target of the cluster ID that will be used with. If provided, this value will be used to extract the cluster's version |\n\n\n\n## Value\n It returns no value to the R session. This function purpose is to create the 'Python' environment, and install the appropriate set of 'Python' libraries inside the new environment. During runtime, this function will send messages to the console describing the steps that the function is taking. For example, it will let the user know if it is getting the latest version of the Python library from 'PyPi.org', and the result of such query. \n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}