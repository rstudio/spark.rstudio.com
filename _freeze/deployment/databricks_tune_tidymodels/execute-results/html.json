{
  "hash": "4258edff1f9b1df11ca46d7eeac62613",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Tune Tidymodel in Databricks\nexecute:\n  eval: true\n  freeze: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n## Intro\n\n`sparklyr` makes it possible to off-load a Tidymodels grid search model\ntuning to Databricks. [Tidymodels](https://www.tidymodels.org/) is a\ncollection of packages that are designed to work together to provide\neverything from resampling, to preprocessing, to model tuning, to\nperformance measurement.\n\nModel tuning is a time consuming process because of the number of tuning\nparameter combinations that need to be processed. Running the\ncombinations in parallel saves a significant amount of time. Tidymodels\nprovides ways to run parallel tuning, but not in Spark. `sparklyr` not\nonly enables this possibility, but it also makes it very simple to\naccess this functionality.\n\nIn Tidymodels, the\n[`tune_grid()`](https://tune.tidymodels.org/reference/tune_grid.html)\nfunction is called to execute the grid search locally. To run in\nDatabricks, simply call `sparklyr`'s `tune_grid_spark()` instead. It\naccepts the exact same arguments as `tune_grid()` does, see @fig-code.\n\n::::::: {#fig-code}\n:::::: columns\n::: {.column width=\"38%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\ntune::tune_grid(\n  object = my_model, \n  preprocessor = my_recipe, \n  resamples = my_resamples\n  \n  )\n```\n:::\n\n\nRun locally\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"58%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nsparklyr::tune_grid_spark(\n  object = my_model, \n  preprocessor = my_recipe, \n  resamples = my_resamples,\n  sc = my_conn # Only additional requirement\n  )\n```\n:::\n\n\nRun remotely in Databricks\n:::\n::::::\n\nComparing tune and sparklyr function calls\n:::::::\n\n`sparklyr` will automatically upload the needed R object to Databricks,\nsuch as the resample object, model specification, and the preprocessing\nsteps. It will then run the tuning in parallel taking advantage of the R\nintegration in Databricks. Lastly, it will collect all of the results\nback to your local R session, and return a `tune_results` object that is\nindistinguishable from one made directly by Tidymodels, , see\n@fig-diagram.\n\n::: {#fig-diagram}\n![](/images/guides/tidymodels-db/diagram-1.png)\n\nHow *tune_grid_spark()* works with Databricks\n:::\n\n## Example\n\nIn Tidymodels, there are some specific elements needed to perform a grid\nsearch tuning:\n\n-   Model specification (`parsnip`)\n\n-   Resampled data (`rsample`)\n\n-   Data preprocessor (`recipe`)\n\n-   Post processor (`tailor`) *optional*\n\nIn the following example, we will create the elements just as if they\nwere about to be used by `tune_grid()` locally.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(readmission)\nset.seed(1)\nreadmission_splits <- initial_split(readmission, strata = readmitted)\n\n# Resampling -------------------------------------------------------\nreadmission_folds <- vfold_cv(\n  data = training(readmission_splits),\n  strata = readmitted\n  )\n\n# Preprocessing ----------------------------------------------------\nrecipe_basic <- recipe(readmitted ~ ., data = readmission) |>\n  step_mutate(\n    race = factor(case_when(\n      !(race %in% c(\"Caucasian\", \"African American\")) ~ \"Other\",\n      .default = race\n    ))\n  ) |>\n  step_unknown(all_nominal_predictors()) |>\n  step_YeoJohnson(all_numeric_predictors()) |>\n  step_normalize(all_numeric_predictors()) |>\n  step_dummy(all_nominal_predictors())\n\n# Model specification -----------------------------------------------\nspec_bt <- boost_tree(\n  mode = \"classification\",\n  mtry = tune(),       # Parameter to be tuned\n  learn_rate = tune(), # Parameter to be tuned\n  trees = 10\n  )\n```\n:::\n\n\nThe only required step before tuning the model in Databricks, is to\nconnect to a Databricks' Spark cluster. This step could have been done\nin the outset of this example, but we want to showcase how it is\npossible to start intending to tune locally, and then decide to off-load\nthe job to Databricks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n\nsc <- spark_connect(\n  method = \"databricks_connect\", \n  cluster_id = \"1218-000327-q970zsow\" # Replace with your own cluster's ID\n  )\n```\n:::\n\n\nThe next step is to call `tune_grid_spark()`. We will pass the three\nelements created in the first step, and the Databricks connection\nvariable. Adding the `control_grid()` was an optional step, this was\ndone to showcase the output from `sparklyr` when running this process.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_results <- tune_grid_spark(\n  sc = sc, \n  object = spec_bt, \n  preprocessor = recipe_basic, \n  resamples = readmission_folds,\n  control = control_grid(verbose = TRUE)\n  )\n#> i Creating pre-processing data to finalize 1 unknown parameter: \"mtry\"\n#> ℹ Uploading model, pre-processor, and other info to the Spark session\n#> ✔ Uploading model, pre-processor, and other info to the Spark session [1.5s]\n#> \n#> ℹ Uploading the re-samples to the Spark session\n#> ✔ Uploading the re-samples to the Spark session [6s]\n#> \n#> ℹ Copying the grid to the Spark session\n#> ✔ Copying the grid to the Spark session [153ms]\n#> \n#> ℹ Executing the model tuning in Spark\n#> ✔ Executing the model tuning in Spark [16.6s]\n#> \n```\n:::\n\n\nAfter the process completes, the `spark_results` object can be now used\nas if it was created by `tune`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(spark_results)\n```\n\n::: {.cell-output-display}\n![](databricks_tune_tidymodels_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## R and Python libraries\n\nThere are Python and R packages that need to be pre-installed in the\nDatabricks cluster:\n\n**Python**\n\n-   `rpy2`\n\n**R**\n\n-   `tidymodels`\n\n-   `reticulate`\n\n-   The modeling package used by `parsnip` in the tuning\n\nInstallation is a simple operation that is done via your Databricks web\nportal. Here are the instructions that shows you how to do that:\n[Databricks - Cluster\nLibraries](https://docs.databricks.com/en/libraries/cluster-libraries.html).\n\n### Install programmatically (optional)\n\nThe [`brickster`](https://databrickslabs.github.io/brickster/) package\nis a complete toolkit for interacting with Databricks. It can be used to\nprogrammatically install the Python and R libraries. It requires that\nthe user has rights to modify the libraries in target cluster.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brickster)\n\n# DBRs have a fixed snapshot date to source the R packages. Redirecting  \n# to the 'latest' snapshot to get the most recent package versions.\nrepo <- \"https://databricks.packagemanager.posit.co/cran/__linux__/noble/latest\"\n\n# Builds the list of R packages, pointing them to the 'latest' snapshot\nr_libs <- libraries(\n  lib_cran(\"reticulate\", repo = repo),\n  lib_cran(\"tidymodels\", repo = repo),\n  lib_cran(\"xgboost\", repo = repo) # (optional) Used in the example\n)\n\ndb_libs_install(\"1218-000327-q970zsow\", r_libs)\n\n# Builds the Python package object. Specifying the version of `rpy2` to install.\npy_lib <- lib_pypi(\"rpy2==3.6.4\") |>\n  libraries()\n\ndb_libs_install(\"1218-000327-q970zsow\", py_lib)\n```\n:::\n\n\nAs time goes by, there may be other modeling R packages that need to be\ninstalled in Spark. Here is a template that can be used to install a\nsingle package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brickster)\n\nlib_cran(\n  package = \"[Missing package]\", \n  repo = \"https://databricks.packagemanager.posit.co/cran/__linux__/noble/latest\"\n  ) |> \n  libraries() |> \n  db_libs_install(\"[Your cluster ID]\", libraries =  _)\n\n```\n:::\n\n",
    "supporting": [
      "databricks_tune_tidymodels_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}