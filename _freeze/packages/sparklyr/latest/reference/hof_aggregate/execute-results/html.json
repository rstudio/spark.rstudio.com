{
  "hash": "2ea08a54bcf703d80b58116a4c28570a",
  "result": {
    "markdown": "---\ntitle: \"Apply Aggregate Function to Array Column\"\nexecute:\n  freeze: true\n---\n\n\n\n\n*R/dplyr_hof.R*\n\n## hof_aggregate\n\n## Description\n\nApply an element-wise aggregation function to an array column (this is essentially a dplyr wrapper for the `aggregate(array<T>, A, function<A, T, A>[, function<A, R>]): R`\n\nbuilt-in Spark SQL functions) \n\n\n## Usage\n```r\nhof_aggregate( \n  x, \n  start, \n  merge, \n  finish = NULL, \n  expr = NULL, \n  dest_col = NULL, \n  ... \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | The Spark data frame to run aggregation on |\n| start | The starting value of the aggregation |\n| merge | The aggregation function |\n| finish | Optional param specifying a transformation to apply on the final value of the aggregation |\n| expr | The array being aggregated, could be any SQL expression evaluating to an array (default: the last column of the Spark data frame) |\n| dest_col | Column to store the aggregated result (default: expr) |\n| ... | Additional params to dplyr::mutate |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\") \n# concatenates all numbers of each array in `array_column` and add parentheses \n# around the resulting string \ncopy_to(sc, tibble::tibble(array_column = list(1:5, 21:25))) %>% \n  hof_aggregate( \n    start = \"\", \n    merge = ~ CONCAT(.y, .x), \n    finish = ~ CONCAT(\"(\", .x, \")\") \n  ) \n#> # Source: spark<?> [?? x 1]\n#>   array_column\n#>   <chr>       \n#> 1 (54321)     \n#> 2 (2524232221)\n```\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}