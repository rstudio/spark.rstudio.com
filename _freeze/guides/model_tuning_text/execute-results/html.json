{
  "hash": "2e8a907d6ba99a0c6b11f18c0f8e4aee",
  "result": {
    "markdown": "---\ntitle: \"Grid Search Tuning\"\nexecute:\n  eval: false\n  freeze: true\neditor_options: \n  markdown: \n    wrap: 72\nformat:\n  html: \n    code-tools:\n      source: https://github.com/rstudio/spark.rstudio.com/blob/tune-text/source/guides/model_tuning_text.md\n      toggle: false\n      caption: none\n---\n\n\n\n```{mermaid}\n%%| fig-width: 6.5\nflowchart LR\n  subgraph id1 [ ]\n    subgraph id2 [ ]\n      subgraph id3 [ML Pipeline]\n        d[Prepare] --> m[Model]\n      end\n      subgraph id4 [Grid Search Tuning]\n        m --> gv[Grid]\n        gv-- Combo 1 -->ft1[Fit Models]\n        ft1 --> ev1[Metric]\n        gv-- Combo 2 -->ft2[Fit Models]\n        ft2 --> ev2[Metric]     \n        gv-- Combo 3 -->ft3[Fit Models]\n        ft3 --> ev3[Metric]  \n        gv-- Combo n -->ft4[Fit Models]\n        ft4 --> ev4[Metric]          \n      end\n    end\n  end\n  style id1 fill:#eee,stroke:#eee\n  style id2 fill:#eee,stroke:#eee\n  style d fill:#99ccff,stroke#666\n  style m fill:#99ffcc,stroke:#666\n  style gv fill:#ffcc99,stroke:#666\n  style ft1 fill:#ccff99,stroke:#666\n  style ft2 fill:#ccff99,stroke:#666\n  style ft3 fill:#ccff99,stroke:#666\n  style ft4 fill:#ccff99,stroke:#666\n```\n\n```{mermaid}\n%%| fig-width: 6.5\nflowchart LR\n  subgraph id1 [ ]\n    subgraph id2 [ ]\n      gv[Grid] -- Combo n -->re[Resample]\n      re -- Fold 1 --> ft1[Fit]\n      subgraph id4 [Fit Models with Cross Validation]\n        re -- Fold 2 --> ft2[Fit]\n        re -- Fold 3 --> ft3[Fit]\n        ft1 --> ev1[Evaluate]\n        ft2 --> ev2[Evaluate]     \n        ft3 --> ev3[Evaluate]  \n        ev1 --> eva[Avgerage]\n        ev2 --> eva\n        ev3 --> eva\n      end\n      eva --> mt[Metric]\n    end\n  end\n  style id1 fill:#eee,stroke:#eee\n  style id4 fill:#ccff99,stroke:#666\n  style gv fill:#ffcc99,stroke:#666\n  style ft1 fill:#ccffff,stroke:#666\n  style ft2 fill:#ccffff,stroke:#666\n  style ft3 fill:#ccffff,stroke:#666\n  style re fill:#ffccff,stroke:#666\n  style ev1 fill:#ffff99,stroke:#666\n  style ev2 fill:#ffff99,stroke:#666\n  style ev3 fill:#ffff99,stroke:#666\n  style eva fill:#ffff66,stroke:#666\n```\n\n\n## Reproducing \"Tuning Text Analysis\" in Spark\n\nIn this article, we will reproduce the *Grid Search* tuning example\nfound in the `tune` package's website: [Tuning Text\nAnalysis](https://tune.tidymodels.org/articles/extras/text_analysis.html){target=\"_blank\"}.\nThat example analyzes *Amazon's Fine Food Reviews* text data to perform\nhyper parameter tuning.\n\nIn this site's [Text Modeling](textmodeling.qmd){target=\"_blank\"}\narticle, the data preparation and modeling are based on the same example\nfrom `tune`. Thanks to how modular ML Pipelines are, we are able to\nsplit the modeling walk-through, from the tuning walk-through found in\nthis article. We also recommend to be familiar with the concepts from\n[Intro to Model Tuning](model_tuning.qmd){target=\"_blank\"}.\n\n## Spark and Data Setup\n\nFor this example, we will start a local Spark session, and then copy the\n*Fine Food Reviews* data to it. For more information about the data,\nplease see the [Data](textmodeling.qmd#data) section of the *Text\nModeling* article.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nlibrary(modeldata)\n\ndata(\"small_fine_foods\")\n\nsc <- spark_connect(master = \"local\", version = \"3.3\")\n\nsff_training_data <- copy_to(sc, training_data)\nsff_testing_data <- copy_to(sc, testing_data)\n```\n:::\n\n\n## ML Pipeline\n\nAs mentioned before, the data preparation and modeling in [Text\nModeling](textmodeling.qmd#recipe-and-model-specifications-1){target=\"_blank\"}\nare based on the same example from the `tune`'s website. The `recipe`\nsteps, and `parsnip` model are recreated with Feature Transformers, and\nan ML model respectively.\n\nUnlike `tidymodels`, there is no need to \"pre-define\" the arguments that\nwill need tuning. During tuning, Spark will automatically override the\nparameters specified in the [grid](#grid). This means that it doesn't\nmatter that we use the exact same code for developing, and tuning the\npipeline. We can literally copy-paste, and run the resulting pipeline\ncode from [Text\nModeling](textmodeling.qmd#prepare-the-model-with-an-ml-pipeline).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_pipeline <- ml_pipeline(sc) %>% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %>% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %>% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 1024\n    ) %>%\n  ft_normalizer(\n    input_col = \"hashed_features\", \n    output_col = \"normal_features\"\n    ) %>% \n  ft_r_formula(score ~ normal_features) %>% \n  ml_logistic_regression()\n\nsff_pipeline\n```\n:::\n\n\nIt is also worth pointing out that in a\"real life\" exercise,\n`sff_pipeline` would probably already be loaded into our environment.\nThat is because we just finished modeling and, decided to test to see if\nwe could tune the model. Spark can re-use the exact same ML Pipeline\nobject for the cross validation step.\n\n## Grid {#grid}\n\nThere is a big advantage to transforming, and modeling the data in a\nsingle ML Pipeline. It opens the door for Spark to also alter parameters\nused for data transformation, in addition to the model's parameters.\nThis means that we can include the parameters of the tokenization,\ncleaning, hashing, and normalization steps as possible candidates for\nthe model tuning.\n\nThe *Tuning Text Analysis* article uses three tuning parameters. Two are\nin the model, and one is in the hashing step. Here are the parameters,\nand how they map between `tidymodels` and `sparklyr`:\n\n| Parameter                             | `tidymodels` | `sparklyr`          |\n|---------------------------------------|--------------|---------------------|\n| Number of Terms to Hash               | `num_terms`  | `num_features`      |\n| Amount of regularization in the model | `penalty`    | `elastic_net_param` |\n| Proportion of pure vs ridge Lasso     | `mixture`    | `reg_param`         |\n\nThe values to tune with are taken from the [Grid\nSearch](https://tune.tidymodels.org/articles/extras/text_analysis.html#grid-search-1){target=\"_blank\"}\nsection in the *Tuning Text Analysis* article. All that is left to do is\nto create the grid itself. Just like we did in the first [Model\nTuning](model_tuning.html#grid){target=\"_blank\"} article, we use partial\nname matching to the steps we want to tune:\n\n-   `hashing_ft` will be the name of the list object containing the\n    `num_features` values\n\n-   `logistic_regression` will be the of the list object containing the\n    values of the other two parameters\n\nNotice that the R code of the values themselves are a direct copy of the\nones used in the *Tuning Text Analysis* article.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_grid <-  list(\n    hashing_tf = list(\n      num_features = 2^c(8, 10, 12)  \n    ),\n    logistic_regression = list(\n      elastic_net_param = 10^seq(-3, 0, length = 20), \n      reg_param = seq(0, 1, length = 5)    \n    )\n  )\n\nsff_grid\n```\n:::\n\n\n## Evaluate\n\nIn the *Tuning Text Analysis* article, ROC AUC is used to measure\nperformance. The is the default metric of\n`ml_binary_classification_evaluator()` , so we only need to pass the\nconnection variable to the evaluator function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_evaluator <- ml_binary_classification_evaluator(sc)\n```\n:::\n\n\n## Model Tuning (Fit)\n\nWe will use `ml_cross_validator()` to prepare a tuning specification\ninside Spark. We recommend to set the `seed` argument in order to\nincrease reproducibility.\n\nSpark will automatically create the grid combinations when tuning the\nmodel. In this case, `sff_grid` contains three parameters:\n\n-   `num_features` has 3 values\n\n-   `elastic_net_param` has 20 values\n\n-   `reg_parm` has 5 values\n\nThis means that there will be 300 combinations for the tuning parameters\n(3 x 20 x 5). Because we set the number of folds to 3 (`num_folds`),\nSpark will run a total of 900 models (3 x 300).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_cv <- ml_cross_validator(\n  x = sc,\n  estimator = sff_pipeline, \n  estimator_param_maps = sff_grid,\n  evaluator = sff_evaluator,\n  num_folds = 3,\n  parallelism = 4,\n  seed = 100\n)\n\nsff_cv\n```\n:::\n\n\nThis is the step that will take the longest time. The `ml_fit()`\nfunction will run the 900 models using the training data. There is no\nneed to pre-prepare the re-sampling folds, Spark will take care of that.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_model <- ml_fit(\n  x = sff_cv, \n  dataset = sff_training_data\n  )\n```\n:::\n\n\n## Validation metrics\n\nWe can now extract the metrics from `sff_model` using\n`ml_validation_metrics()`. The ROC AUC values will be in a column called\n`areaUnderROC`. We can then take a look at the best performing models\nusing `dplyr`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_metrics <- ml_validation_metrics(sff_model)\n\nlibrary(dplyr)\n\nsff_metrics %>% \n  arrange(desc(areaUnderROC)) %>% \n  head()\n```\n:::\n\n\nWe will now plot the results. We will match the approach used in the\n[Grid\nSearch](https://tune.tidymodels.org/articles/extras/text_analysis.html#grid-search-1){target=\"_blank\"}\nsection of the *Tuning Text Analysis* article.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nsff_metrics %>% \n  mutate(reg_param_1 = as.factor(reg_param_1)) %>% \n  ggplot(aes(\n    x = elastic_net_param_1, \n    y = areaUnderROC, \n    color = reg_param_1\n    )) +\n  geom_line() +\n  geom_point(size = 0.5) +\n  scale_x_continuous(trans = \"log10\") +\n  facet_wrap(~ num_features_2) +\n  theme_light(base_size = 9)\n```\n:::\n\n\nIn the plot, we can see the effects of the three parameters, and the\nvalues that look to be the best. These effects are very similar to the\noriginal *Tuning Text Analysis* article.\n\n## Model selection\n\nWe can create a new ML Pipeline using the same code as the original\npipeline. We only need to change the 3 parameters values, with values\nthat performed best.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_pipeline <- ml_pipeline(sc) %>% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %>% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %>% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 4096      \n    ) %>%\n  ft_normalizer(\n    input_col = \"hashed_features\", \n    output_col = \"normal_features\"\n    ) %>% \n  ft_r_formula(score ~ normal_features) %>% \n  ml_logistic_regression(\n    elastic_net_param = 0.05,\n    reg_param = 0.25  \n    )\n```\n:::\n\n\nNow, we create a final model using the new ML Pipeline.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_fitted <- new_sff_pipeline %>% \n  ml_fit(sff_training_data)\n```\n:::\n\n\n## Test data metrics\n\nThe test data set is now used to confirm that the performance gains\nhold. We use it to run predictions with thew new ML Pipeline Model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_fitted %>% \n  ml_transform(sff_testing_data) %>% \n  ml_metrics_binary()\n```\n:::\n\n\nThe results show an increase performance in contrast with running the ML\nPipeline Model, those results are in [Fit and\nPredict](textmodeling.qmd#fit-and-predict) section of the *Text\nModeling* article.\n\n\n\n\n\n## Advantages of using Spark for Model Tuning\n\nSpark's capabilities, combined with `sparklyr` 's API, provide a very\nstreamlined way to go from exploration, to modeling, to tuning. Even\nwithout a \"formal\" Spark cluster, it is possible to take advantage of\nthese capabilities right from our personal computer.\n\nIf we add an actual cluster to the mix, the advantage of using Spark\nraises dramatically. Usually we talk about Spark for \"big data\"\nanalysis, but in this case, we can leverage it to parallelize the 900\nmodels across multiple machines. The ability to distribute the models\nacross the cluster will cut down the tune processing time. The amount of\ntime saved will depend on the resources and configuration of the\ncluster, as well as of those of the session.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}