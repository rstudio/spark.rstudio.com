{
  "hash": "5ed2398b1248e7eac264cf600a35367b",
  "result": {
    "markdown": "---\ntitle: \"Invoke distinct on a Spark DataFrame\"\nexecute:\n  freeze: true\n---\n\n\n\n\n*R/sdf_distinct.R*\n\n## sdf_distinct\n\n## Description\nInvoke distinct on a Spark DataFrame \n\n\n## Usage\n```r\nsdf_distinct(x, ..., name) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | A Spark DataFrame. |\n| ... | Optional variables to use when determining uniqueness. If there are multiple rows for a given combination of inputs, only the first row will be preserved. If omitted, will use all variables. |\n| name | A name to assign this table. Passed to [sdf_register()]. |\n\n\n## Section\n\n## Transforming Spark DataFrames\n\nThe family of functions prefixed with `sdf_` generally access the Scala Spark DataFrame API directly, as opposed to the `dplyr` interface which uses Spark SQL. These functions will 'force' any pending SQL in a `dplyr` pipeline, such that the resulting `tbl_spark` object returned will no longer have the attached 'lazy' SQL operations. Note that the underlying Spark DataFrame **does** execute its operations lazily, so that even though the pending set of operations (currently) are not exposed at the `R` level, these operations will only be executed when you explicitly `collect()` the table. \n\n\n\n\n## See Also\nOther Spark data frames:  `sdf_copy_to()`, `sdf_random_split()`, `sdf_register()`, `sdf_sample()`, `sdf_sort()`, `sdf_weighted_sample()`\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}