---
title: "Spark Connect, and Databricks Connect v2"
format:
  html:
    theme: default
    toc: true
execute:
    eval: true
    freeze: true
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| include: false

library(remotes)

#install.packages("sparklyr")
#install_github("mlverse/pysparklyr")

library(dplyr)
library(dbplyr)
library(sparklyr)
library(pysparklyr)

# install_pyspark()
```

*Last updated: `r lubridate::date()`*

## Intro

[Spark
Connect](https://spark.apache.org/docs/latest/spark-connect-overview.html)
introduced a decoupled client-server architecture that allows remote
connectivity to Spark clusters using the DataFrame API. The separation
between client and server allows Spark to be leveraged from everywhere,
and this would allow R users to interact with a cluster from the comfort
of their preferred environment, laptop or otherwise. [Databricks
Connect](https://docs.databricks.com/dev-tools/databricks-connect.html),
available in Databricks Runtime version 13 and above, is based on this
new architecture.

We are working on enhancing `sparklyr` so that it can bring the benefits
of Spark Connect to the R user community.

## The Solution

Spark Connect requires the use of a remote procedure call framework
called *gRPC.* At this time, the Spark team provides two higher level APIs
that interact with *gRPC.* One is Scala based, and the other Python based.

We are using `reticulate` to interact with the Python API. `sparklyr` extends 
the functionality, and user experience, by providing the `dplyr`back-end, `DBI` 
back-end, RStudio's Connection pane integration.

::: {#fig-connect}
```{mermaid}
flowchart LR
  subgraph lp[User's machine]
    sr[sparklyr]
    rt[reticulate]
    ps[PySpark]
    g1[gRPC]
  end
  sp[Spark]
  
  sr --> rt
  rt --> ps
  g1 <-. Network .-> sp
  ps --> g1
  
  style sr  fill:#d0efb1,stroke:#666
  style rt  fill:#d0efb1,stroke:#666
  style ps  fill:#eff,stroke:#666
  style lp  fill:#fff,stroke:#666
  style sp  fill:#f4c430,stroke:#666
  style g1  fill:#447099,stroke:#666,color:#fff
```

How `sparklyr` communicates with Spark Connect, and Databricks Connect
:::

In order to quickly iterate on enhancements and bug fixes, we have
decided to isolate the Python integration into its own package. The new
package, called `pysparklyr`, is an extension of `sparklyr`.

## Package Installation

As mentioned above, we are still working integrating `sparklyr` with
Connect. To access the new capabilities, you will need `sparklyr` version
**1.8.3** or higher, and `pysparklyr`. To install use:

``` r
install.packages("sparklyr")
install.packages("pysparklyr")
```

## Databricks Connect (ML 13+)

### Initial setup

`sparklyr` will need specific Python libraries in order to connect, and interact
with Databricks Connect. We provide a convenience function that will automatically
do the following:

- Create, or re-create, a Python environment. Based on your OS, it will choose
to create a Virtual Environment, or Conda. 

- Install the needed Python libraries

To install the latest versions of all the libraries, use:

```r
pysparklyr::install_databricks()
```

`sparklyr` will query PyPi.org to get the latest version of `databricks.connect`
and installs that version. It is recommended that the version of the 
`databricks.connect` library, matches  the DBR version of your cluster.  
To do this, pass the DBR version in the `version` argument, for example:

```r
pysparklyr::install_databricks("14.0")
```

This will create a Python environment and install `databricks.connect` version
14.0, and it will automatically name it `r-sparklyr-databricks-14.0`.  By using
this name, `sparklyr` is able to know what version of `databricks.connect` is
available inside this particular Python environment.

If you are not sure about the version of the cluster you want to interact with,
then use the `cluster_id` argument. We have added a way to pull the cluster's
information, without starting a Spark connect. This allows us to query the
cluster and get the DBR version:

```r
pysparklyr::install_databricks(cluster_id = "[Your cluster's ID]")
```

:::{.callout-important}

This step needs only to be done one time. If you need to connect to a different
cluster, but that has the same DBR version, `sparklyr` will use the same
Python environment.  If the new cluster has a different DBR version, then it is
recommended that you run the installation function using the new DBR version, or
cluster ID.

:::

### Connecting

To use with Databricks Connect, in run-time 13 or above, you will need
three configuration items:

-   Your [Workspace Instance
    URL](https://docs.databricks.com/workspace/workspace-details.html#workspace-url)
-   The ID of a currently running
    [cluster](https://docs.databricks.com/workspace/workspace-details.html#cluster-url-and-id)
    within your workspace
-   Authentication setup. At this time, `sparklyr` supports
    [PAT](https://docs.databricks.com/dev-tools/auth.html#pat)

In `spark_connect()` those items can be passed as function arguments.
Also note that to let `sparklyr` know that you are attempting to use
Databricks Connect, use `method = databricks_connect`:

``` r
library(sparklyr)

sc <- spark_connect(
  master = "[Your Workspace Instance URL]", 
  cluster_id = "[Cluster ID]"
  token = "[Your PAT - Please do not include 
           it as plain text here,  please refer 
           to the next section]",
  method = "databricks_connect"
  )
```

`sparklyr` will get retrieve the cluster's DBR version, and automatically choose
the Python environment that has the matching version of `databricks.connect`. If
it does not find a matching version, it will use the most recent version installed
in your machine. If you wish, you can specify the version by setting the
`dbr_version` argument. 

```r
sc <- spark_connect(
  cluster_id = "[Cluster ID]",
  method = "databricks_connect",
  dbr_version = "14.0"
  )
```

### Safely connect

For your safety and convenience, you can save your authentication token
(PAT), and your Workspace Instance URL in a environment variable. The
two variables are:

-   `DATABRICKS_TOKEN`
-   `DATABRICKS_HOST`

This will prevent your token to be shown in plain text in your code. You
can set the environment variables at the beginning of your R session
with `Sys.setenv()`.

Preferably, the two variables can be set for all R sessions by saving
them to the **.Renviron** file. The `usethis` package has a handy
function that opens the file so you can edit it:
`usethis::edit_r_environ()`. Then simply append the two entries to your
**.Renviron** file. Once you are done, save the **.Renviron** file, and
restart R.

After that, use `spark_connect()` to open the connection to Databricks.
You will need to only pass the `cluster_id` of your cluster, and tell
`sparklyr` that you are connecting to a Spark cluster with run time 13
or above:

``` {r}
library(sparklyr)

sc <- spark_connect(
  cluster_id = "0914-200113-rc8v8keb",
  method = "databricks_connect"
)
```

`sparklyr` will provide console message while initiating the connection. As
seen above, we are informed that `sparklyr` is getting the DBR version from the
cluster, the version returned by the cluster, and which Python environment 
it used to connect.

### RStudio's Connection pane

Thanks to the new way we are integrating with Spark, it is now possible
to display the same structure displayed in the Databricks Data Explorer
page. In Databricks, the current data structure levels are:

-   Catalog
    -   Database
        -   Table

In RStudio, you can navigate the data structure by expanding from the
top level, all the way down to the table you wish to explore. Once
expanded, the table's fields, and their types are displayed.

![](/images/deployment/connect/rstudio-connection.png)

In the Connection Pane, you can click on the **table** icon, situated to
the right of the table name, to preview the first 1,000 rows:

![](/images/deployment/connect/preview.png)

### Using the Connection to Access Data

After connecting, you can use `dbplyr`'s `in_catalog()` function to
access any table in your data catalog. You will only need to pass the
respective names of the three levels as comma separated character
entries to `in_catalog()` in this order: Catalog, Database, and Table.

Here is an example of using `tbl()` and `in_catalog()` to point to the
**trips** table, which is inside **nyctaxi** database, which is a
database inside the **samples** catalog:

```{r}
library(dplyr)
library(dbplyr)

trips <- tbl(sc, in_catalog("samples", "nyctaxi", "trips"))

trips
```

After pointing `tbl()` to that specific table, you can then use `dplyr`.

```{r}
trips %>%
  group_by(pickup_zip) %>%
  summarise(
    count = n(),
    avg_distance = mean(trip_distance, na.rm = TRUE)
  )
```

When you are done with you queries and computations, you should
disconnect from the cluster.

```{r}
spark_disconnect(sc)
```

## Spark Connect

### Initial setup

`sparklyr` will need specific Python libraries in order to connect, and interact
with Spark Connect. We provide a convenience function that will automatically
do the following:

- Create, or re-create, a Python environment. Based on your OS, it will choose
to create a Virtual Environment, or Conda. 

- Install the needed Python libraries

To install the latest versions of all the libraries, use:

```r
pysparklyr::install_pyspark()
```

`sparklyr` will query PyPi.org to get the latest version of PySpark
and installs that version. It is recommended that the version of the
PySpark library matches the Spark version of your cluster. 
To do this, pass the Spark version in the  `version` argument, for example:

```r
pysparklyr::install_pyspark("3.4")
```

We have seen Spark sessions crash, when the version of PySpark and the version
of Spark do not match. Specially, when using a newer version of PySpark is used
against an older version of Spark.  If you are having issues with your connection, 
definitely consider running the `install_pyspark()` to match that cluster's 
specific Spark version.

### Connecting

To start a session with a open source Spark cluster, via Spark Connect,
you will need to set the `master`, and `method`. The `master` will be an IP,
and maybe a port that you will need to pass. The protocol to use to put
together the proper connection URL is "sc://". For `method`, use
"spark_connect". Here is an example:

``` r
library(sparklyr)

sc <- spark_connect(
  master = "sc://[Host IP(:Host Port - optional)]", 
  method = "spark_connect"
  version = "[Version that matches your cluster]"
  )
```

If `version` is not passed, then `sparklyr` will automatically choose the 
installed Python environment with the highest PySpark version. In a console 
message, `sparklyr` will let you know which environment it will use.

### Run locally

It is possible to run Spark Connect in your machine We provide helper
functions that let you setup, and start/stop the services in locally.

If you wish to try this out, first install Spark 3.4 or above:

``` r
spark_install("3.4")
```

After installing, start the Spark Connect using:

```r
pysparklyr::spark_connect_service_start()
```

To connect to your local Spark Connect, use **localhost** as the address for 
`master`:


```r
sc <- spark_connect(
  master = "sc://localhost", 
  method = "spark_connect"
  )
```

Now, you are able to interact with your local Spark session:

```r
library(dplyr)

tbl_mtcars <- copy_to(sc, mtcars)

tbl_mtcars %>% 
  group_by(am) %>% 
  summarise(mpg = mean(mpg, na.rm = TRUE))
```

When done, you can disconnect from Spark Connect:

```r
spark_disconnect(sc)
```

The regular version of local Spark would terminate the local cluster
when the you pass `spark_disconnect()`. For Spark Connect, the local
cluster needs to be stopped independently.

```r
pysparklyr::spark_connect_service_stop()
```

## What is supported

Here is a list of what we currently support, and do not support via
`sparklyr` and Connect:

**Supported**:

-   Integration with most of the `dplyr`, and `DBI`, APIs
-   Integration with the `invoke()` command
-   RStudio Connections Pane navigation
-   Support for Personal Access Token security authentication for
    Databricks Connect
-   Support for most read and write commands. These have only been
    tested in Spark Connect.

**Not supported**:

-   **ML functions** - All functions, in `sparklyr`, that have the `ml_`
    and `ft_` are currently not supported. The reason is that Spark 3.4
    does not currently support MLlib. We expect that some ML support
    will be available in Spark 3.5. At that time we will work on
    integrating the new ML routines from Connect into `sparklyr`.

-   **SDF functions** - Most of these functions require SparkSession,
    which is currently not supported in Spark 3.4.

-   **`tidyr`** - This is ongoing work that we are focusing on in
    `sparklyr`. We are implementing these functions using PySpark
    DataFrame commands, instead of depending on the Scala
    implementation.

A more detailed list of our progress, which includes individual
functions we have confirmed that work, do not work, or will work
differently, is available here: [`pysparklyr`
progress](https://github.com/mlverse/pysparklyr/blob/main/progress.md)

## Additional setup details

If you wish to use your own Python environment, then just make sure to
load it before calling `spark_connect()`. If there is a Python
environment already loaded when you connect to your Spark cluster, then
`sparklyr` will use that environment instead. If you use your own Python
environment you will need the following libraries installed:

-   `pyspark`
-   `pandas`
-   `PyArrow`
-   `grpcio`
-   `google-api-python-client`
-   `grpcio_status`
-   `torch` *(Spark 3.5+)*
-   `torcheval` *(Spark 3.5+)*

If connecting to Databrics Connect avoid installing `pyspark`, and install 
these instead: 

-   `databricks-connect`
-   `delta-spark`