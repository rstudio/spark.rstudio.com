---
title: "Read file(s) into a Spark DataFrame using a custom reader"
aliases:
  - reference/sparklyr/latest/spark_read.html
---

    <div>

    <div>
    <ul data-gumshoe>
    <li><a href="#arguments">Arguments</a></li>
    <li><a href="#see-also">See also</a></li>
    <li><a href="#examples">Examples</a></li>
    </ul>
    </div>

    <div>

    <p>Run a custom R function on Spark workers to ingest data from one or more files
into a Spark DataFrame, assuming all files follow the same schema.</p>

    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class='fu'>spark_read</span>(<span class='no'>sc</span>, <span class='no'>paths</span>, <span class='no'>reader</span>, <span class='no'>columns</span>, <span class='kw'>packages</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='no'>...</span>)</code></pre></div>

    <h2 id="arguments">Arguments</h2>
    <table class="ref-arguments">

    <colgroup>
      <col class="name" />
      <col class="desc" />
    </colgroup>

    <tr>
      <td>sc</td>
      <td><p>A <code>spark_connection</code>.</p></td>
    </tr>
    <tr>
      <td>paths</td>
      <td><p>A character vector of one or more file URIs (e.g.,
c("hdfs://localhost:9000/file.txt", "hdfs://localhost:9000/file2.txt"))</p></td>
    </tr>
    <tr>
      <td>reader</td>
      <td><p>A self-contained R function that takes a single file URI as
argument and returns the data read from that file as a data frame.</p></td>
    </tr>
    <tr>
      <td>columns</td>
      <td><p>a named list of column names and column types of the resulting
data frame (e.g., list(column_1 = "integer", column_2 = "character")), or a
list of column names only if column types should be inferred from the data
(e.g., list("column_1", "column_2"), or NULL if column types should be
inferred and resulting data frame can have arbitrary column names</p></td>
    </tr>
    <tr>
      <td>packages</td>
      <td><p>A list of R packages to distribute to Spark workers</p></td>
    </tr>
    <tr>
      <td>...</td>
      <td><p>Optional arguments; currently unused.</p></td>
    </tr>
    </table>

    <h2 id="see-also">See also</h2>

    <div class='dont-index'><p>Other Spark serialization routines: 
<code><a href='spark_load_table.html'>spark_load_table</a>()</code>,
<code><a href='spark_read_avro.html'>spark_read_avro</a>()</code>,
<code><a href='spark_read_csv.html'>spark_read_csv</a>()</code>,
<code><a href='spark_read_delta.html'>spark_read_delta</a>()</code>,
<code><a href='spark_read_jdbc.html'>spark_read_jdbc</a>()</code>,
<code><a href='spark_read_json.html'>spark_read_json</a>()</code>,
<code><a href='spark_read_libsvm.html'>spark_read_libsvm</a>()</code>,
<code><a href='spark_read_orc.html'>spark_read_orc</a>()</code>,
<code><a href='spark_read_parquet.html'>spark_read_parquet</a>()</code>,
<code><a href='spark_read_source.html'>spark_read_source</a>()</code>,
<code><a href='spark_read_table.html'>spark_read_table</a>()</code>,
<code><a href='spark_read_text.html'>spark_read_text</a>()</code>,
<code><a href='spark_save_table.html'>spark_save_table</a>()</code>,
<code><a href='spark_write_avro.html'>spark_write_avro</a>()</code>,
<code><a href='spark_write_csv.html'>spark_write_csv</a>()</code>,
<code><a href='spark_write_delta.html'>spark_write_delta</a>()</code>,
<code><a href='spark_write_jdbc.html'>spark_write_jdbc</a>()</code>,
<code><a href='spark_write_json.html'>spark_write_json</a>()</code>,
<code><a href='spark_write_orc.html'>spark_write_orc</a>()</code>,
<code><a href='spark_write_parquet.html'>spark_write_parquet</a>()</code>,
<code><a href='spark_write_source.html'>spark_write_source</a>()</code>,
<code><a href='spark_write_table.html'>spark_write_table</a>()</code>,
<code><a href='spark_write_text.html'>spark_write_text</a>()</code></p></div>

    <h2 id="examples">Examples</h2>
    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><div class='input'><span class='kw'>if</span> (<span class='fl'>FALSE</span>) {

<span class='fu'><a href='https://rdrr.io/r/base/library.html'>library</a></span>(<span class='no'>sparklyr</span>)
<span class='no'>sc</span> <span class='kw'>&lt;-</span> <span class='fu'><a href='spark-connections.html'>spark_connect</a></span>(
  <span class='kw'>master</span> <span class='kw'>=</span> <span class='st'>"yarn"</span>,
  <span class='kw'>spark_home</span> <span class='kw'>=</span> <span class='st'>"~/spark/spark-2.4.5-bin-hadoop2.7"</span>
)

<span class='co'># This is a contrived example to show reader tasks will be distributed across</span>
<span class='co'># all Spark worker nodes</span>
<span class='fu'>spark_read</span>(
  <span class='no'>sc</span>,
  <span class='fu'><a href='https://rdrr.io/r/base/rep.html'>rep</a></span>(<span class='st'>"/dev/null"</span>, <span class='fl'>10</span>),
  <span class='kw'>reader</span> <span class='kw'>=</span> <span class='kw'>function</span>(<span class='no'>path</span>) <span class='fu'><a href='https://rdrr.io/r/base/system.html'>system</a></span>(<span class='st'>"hostname"</span>, <span class='kw'>intern</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>),
  <span class='kw'>columns</span> <span class='kw'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='kw'>hostname</span> <span class='kw'>=</span> <span class='st'>"string"</span>)
) <span class='kw'>%&gt;%</span> <span class='fu'><a href='sdf_collect.html'>sdf_collect</a></span>()
}</div></code></pre></div>

    </div>

    </div>




