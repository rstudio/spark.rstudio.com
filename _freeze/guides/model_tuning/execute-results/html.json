{
  "hash": "b976d7931a5fc3a6828dd18491fa8b5a",
  "result": {
    "markdown": "---\ntitle: \"Model tuning\"\nexecute:\n  eval: true\n  freeze: true\n---\n\n\n\n\n**Hyper parameter tuning**  is possible within Spark. `sparklyr` provides an interface\nthat makes it possible to setup, train, and measure running multiple model \nexperiments in order to tune our models. \n\nThis article walks through the basics of setting up and running a cross validation \ntuning run using the `cells` data from the `modeldata` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(cells, package = \"modeldata\")\n```\n:::\n\n\nThe goal of the experiments in the example is to see at what point does the\nnumber of trees in the model stop improving the accuracy of the predictions.  We\nwill have Spark run multiple iterations of the same model with an increasing\nnumber of trees.\n\n\n## Data setup\n\nFor this example, we will use a local connection, using Spark 3.3.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\", version = \"3.3\")\n```\n:::\n\n\nThe `cells` data is copied to the Spark session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_cells <- copy_to(sc, cells, name = \"cells_tbl\")\n```\n:::\n\n\nWe will split the data into two sets, \"training\" and \"test\".  The **test** split will\nbe treated as the \"holdout\" data to be used at the end of the process to confirm\nthat we did not over fit the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_cells_split <- tbl_cells %>% \n  select(-case) %>% \n  sdf_random_split(\n    training = 0.8, \n    test = 0.2, \n    seed = 100\n    )\n```\n:::\n\n\n## Cross validator prep\n\nPreparing the cross validator requires three elements:\n\n1. An [ML Pipeline](pipelines.qmd)\n1. A `list` object containing the \"grid\", meaning the different parameters to test\n1. An \"evaluator\" that will calculate the metrics of each model run\n\n### Pipeline\n\nIn this example we will use a very simple pipeline. It will contain a\nformula step and a Random Forest model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncells_pipeline <- sc %>% \n  ml_pipeline() %>%\n  ft_r_formula(class ~ .) %>%\n  ml_random_forest_classifier(seed = 207336481)\n```\n:::\n\n\nEach step within a pipeline receives a unique identifier. This identifier is\nmade up of the name of the step and a UID. The UID will change every time a new\npipeline is created. Here is an example of the out put for `cells_pipeline`:\n\n```r\n#> Pipeline (Estimator) with 2 stages\n#> <pipeline__a1c04c2f_b955_4917_89b6_67cb576a779b> \n#>   Stages \n#>   |--1 RFormula (Estimator)\n#>   |    <r_formula__043dd75a_7fd6_48bb_85c5_f20b321b97cb> \n#>   |     (Parameters -- Column Names)\n#>   |      features_col: features\n#>   |      label_col: label\n#>   |     (Parameters)\n#>   |      force_index_label: FALSE\n#>   |      formula: class ~ .\n#>   |      handle_invalid: error\n#>   |      stringIndexerOrderType: frequencyDesc\n#>   |--2 RandomForestClassifier (Estimator)\n#>   |    <random_forest_classifier__52fc33ff_c4dc_44ac_b842_b873957db90a> \n#>   |     (Parameters -- Column Names)\n#>   |      features_col: features\n#>   |      label_col: label\n#>   |      prediction_col: prediction\n#>   |      probability_col: probability\n#>   |      raw_prediction_col: rawPrediction\n#>   |     (Parameters)\n#>   |      bootstrap: TRUE\n#>   |      cache_node_ids: FALSE\n#>   |      checkpoint_interval: 10\n#>   |      feature_subset_strategy: auto\n#>   |      impurity: gini\n#>   |      leafCol: \n#>   |      max_bins: 32\n#>   |      max_depth: 5\n#>   |      max_memory_in_mb: 256\n#>   |      min_info_gain: 0\n#>   |      min_instances_per_node: 1\n#>   |      minWeightFractionPerNode: 0\n#>   |      num_trees: 20\n#>   |      seed: 207336481\n#>   |      subsampling_rate: 1\n```\n\n### Grid\n\nThe way we can pass the parameters to try is via a simple `list` object. `sparklyr`\nperforms partial name matching to assign the list's entries to the `pipeline`\nsteps and the parameters.\n\nThe idea is to modify the **number of trees**  for each model run. From the output \nof the ML Pipeline above, we see that we need to modify the following:\n\n```r\n#>   |    <random_forest_classifier__52fc33ff_c4dc_44ac_b842_b873957db90a> \n#>   |                    ...\n#>   |     (Parameters)\n#>   |                    ...\n#>   |      num_trees: 20\n```\n\nIn R, we create the grid spec using the following: \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncells_grid <- list(\n  random_forest_classifier = list(  \n    num_trees = 1:20 * 5\n  )\n)\n\ncells_grid\n#> $random_forest_classifier\n#> $random_forest_classifier$num_trees\n#>  [1]   5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95\n#> [20] 100\n```\n:::\n\n\nTwo things to highlight about the grid spec:\n\n- `random_forest_classifier` is used to partially match to `random_forest_classifier__52fc33ff_c4dc_44ac_b842_b873957db90a` in the pipeline. It\nis possible to pass the entire name, but that may prevent it from working if\na new pipeline is used. A new pipeline will have a different UID.\n- For `num_trees` we passed a vector of 20 values to test with\n\n### \"Evaluator\"\n\nA metric will have to be calculated for each validation set in the folds. The \nmodel tuning function requires for that to be explicitly defined as an argument\n(`ml_cross_validator()`).  There are multiple outcomes on the `class` field in the\n`tbl_cells` table, this means that we will use `ml_multiclass_classification_evaluator()` for our \nvalidation function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncells_evaluator <- ml_multiclass_classification_evaluator(\n  x = sc,\n  metric_name = \"accuracy\"\n  )\n```\n:::\n\n\n\nThe \"evaluator\" function to use is based on the type of model that is being used\nfor tuning. Here is the list of the available \"evaluator\" functions in `sparklyr`:\n\n- [`ml_binary_classification_evaluator()`](/packages/sparklyr/latest/reference/ml_evaluator.qmd)\n- [`ml_binary_classification_eval()`](/packages/sparklyr/latest/reference/ml_evaluator.qmd)\n- [`ml_multiclass_classification_evaluator()`](/packages/sparklyr/latest/reference/ml_evaluator.qmd)\n- [`ml_classification_eval()`](/packages/sparklyr/latest/reference/ml_evaluator.qmd)\n- [`ml_regression_evaluator()`](/packages/sparklyr/latest/reference/ml_evaluator.qmd)\n\n## Model tuning\n\nAll the preparations steps come together now as arguments of `ml_cross_validator()`.\nThere are two additional arguments to consider:\n\n- `num_folds`: The number of folder for the cross validation. The higher the\nnumber, the longer it will take to complete. \n- `parallelism`: The number of threads to use when running parallel algorithms. \nDefault is 1 for serial execution.\n\nIn this example,  `cells_pipeline`, `cells_grid`, and `cells_evaluator` are\npassed to the respective arguments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncells_cv <- ml_cross_validator(\n  x = sc,\n  estimator = cells_pipeline, \n  estimator_param_maps = cells_grid,\n  evaluator = cells_evaluator,\n  num_folds = 3,\n  parallelism = 1\n)\n\ncells_cv\n#> CrossValidator (Estimator)\n#> <cross_validator__46b71c67_08bf_4afd_b462_c65b7fc369bf> \n#>  (Parameters -- Tuning)\n#>   estimator: Pipeline\n#>              <pipeline__5a108123_a1e7_42af_a2d0_e4cc7b290a56> \n#>   evaluator: MulticlassClassificationEvaluator\n#>              <multiclass_classification_evaluator__ec7cad92_4702_42f3_a97a_53bce2dd021b> \n#>     with metric accuracy \n#>   num_folds: 3 \n#>   [Tuned over 20 hyperparameter sets]\n```\n:::\n\n\nThe `ml_fit()` function will actually run the model tuning. This is where the\n**training** split of the data is used.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_cv <- ml_fit(\n  x = cells_cv, \n  dataset = tbl_cells_split$training\n  )\n```\n:::\n\n\n## Validation metrics\n\nThe `ml_validation_metrics()` function will extract the metrics from each of the\nvalues passed for **number of trees** parameter. If more than one parameter would\nhave been used, the number of results would be the total number of combinations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_metrics <- ml_validation_metrics(model_cv)\n\ncv_metrics\n#>     accuracy num_trees_1\n#> 1  0.8012555           5\n#> 2  0.8078181          10\n#> 3  0.8181159          15\n#> 4  0.8231090          20\n#> 5  0.8181751          25\n#> 6  0.8182511          30\n#> 7  0.8175854          35\n#> 8  0.8225095          40\n#> 9  0.8237798          45\n#> 10 0.8151062          50\n#> 11 0.8170078          55\n#> 12 0.8218778          60\n#> 13 0.8158358          65\n#> 14 0.8169709          70\n#> 15 0.8176567          75\n#> 16 0.8182708          80\n#> 17 0.8175657          85\n#> 18 0.8189295          90\n#> 19 0.8200054          95\n#> 20 0.8170005         100\n```\n:::\n\n\nFor easier selection, we can use a quick plot to visualize how accuracy improves\nas more trees are used, and when does the benefit plateau.\n\n\n::: {.cell warnings='false'}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\ncv_metrics %>% \n  ggplot(aes(num_trees_1, accuracy)) +\n  geom_line() +\n  geom_smooth()\n```\n\n::: {.cell-output-display}\n![](model_tuning_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## Model selection \n\nAs seen in the previous section, 30 trees seems to be a good number to use. To\nfinalize, a new model is fit, using that number for `num_trees`.  \n\nA Model pipeline or a regular model could be used to do this. For this example \nwe will just use the single step of fitting a new model using \n`ml_random_forest_classifier()` directly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncell_model <- ml_random_forest_classifier(\n  tbl_cells_split$training, \n  class ~ ., \n  num_trees = 30\n  )\n```\n:::\n\n\n## Test data metrics\n\nThe final step is to confirm that the model is not over-fitted. We use the new\nmodel against the **test** split, and then piping it to `ml_metrics_multiclass()`\nto confirm that the accuracy is within the expected range.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncell_model %>% \n  ml_predict(tbl_cells_split$test) %>% \n  ml_metrics_multiclass()\n#> # A tibble: 1 Ã— 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy multiclass     0.823\n```\n:::\n\n\n",
    "supporting": [
      "model_tuning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}