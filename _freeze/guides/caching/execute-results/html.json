{
  "hash": "d35a3437b2d20c79c477aeb7e60cdbd5",
  "result": {
    "markdown": "---\ntitle: \"Understanding Spark Caching\"\nformat:\n  html:\n    theme: default\n    toc: true\nexecute:\n  eval: true\n  freeze: true\n---\n\n## Introduction\n\nSpark also supports pulling data sets into a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small dataset or when running an iterative algorithm like random forests. Since operations in Spark are lazy, caching can help force computation. `sparklyr` tools can be used to cache and un-cache DataFrames. The Spark UI will tell you which `DataFrames` and what percentages are in memory.\n\nBy using a reproducible example, we will review some of the main configuration settings, commands and command arguments that can be used that can help you get the best out of Spark's memory management options.\n\n## Preparation\n\n### Download Test Data\n\nBecause of their size, we will use trip data provided by the [NYC Taxi & Limousine Commission](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). Each file represents a month's worth of trips. We will download two files, the ones for January and February 2020.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif(!file.exists(\"jan_2020.csv\")) {\n  download.file(\n    \"https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-01.csv\",\n    \"jan_2020.csv\",\n    mode = \"wb\"\n  )  \n}\n\nif(!file.exists(\"feb_2020.csv\")) {\n  download.file(\n    \"https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-02.csv\",\n    \"feb_2020.csv\",\n    mode = \"wb\"\n  )  \n}\n```\n:::\n\n### Start a Spark session\n\nA local deployment will be used for this example.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n# Customize the connection configuration\nconf <- spark_config()\nconf$`sparklyr.shell.driver-memory` <- \"16G\"\n\n# Connect to Spark\nsc <- spark_connect(master = \"local\", config = conf)\n```\n:::\n\n## The Memory Argument\n\nIn the *spark_read\\_...* functions, the **memory** argument controls if the data will be loaded into memory as an RDD. Setting it to **FALSE** means that Spark will essentially map the file, but not make a copy of it in memory. This makes the `spark_read_csv()` command run faster, but the trade off is that any data transformation operations will take much longer.\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_read_csv(\n  sc, \n  \"taxi_jan_2020\", \n  \"jan_2020.csv\", \n  memory = FALSE\n  )\n```\n\n::: {.cell-output-stdout}\n```\n# Source: spark<taxi_jan_2020> [?? x 18]\n   VendorID tpep_pickup_datetime tpep_dropoff_dat… passenger_count trip_distance\n      <int> <chr>                <chr>                       <int>         <dbl>\n 1        1 2020-01-01 00:28:15  2020-01-01 00:33…               1          1.2 \n 2        1 2020-01-01 00:35:39  2020-01-01 00:43…               1          1.2 \n 3        1 2020-01-01 00:47:41  2020-01-01 00:53…               1          0.6 \n 4        1 2020-01-01 00:55:23  2020-01-01 01:00…               1          0.8 \n 5        2 2020-01-01 00:01:58  2020-01-01 00:04…               1          0   \n 6        2 2020-01-01 00:09:44  2020-01-01 00:10…               1          0.03\n 7        2 2020-01-01 00:39:25  2020-01-01 00:39…               1          0   \n 8        2 2019-12-18 15:27:49  2019-12-18 15:28…               1          0   \n 9        2 2019-12-18 15:30:35  2019-12-18 15:31…               4          0   \n10        1 2020-01-01 00:29:01  2020-01-01 00:40…               2          0.7 \n# … with more rows, and 13 more variables: RatecodeID <int>,\n#   store_and_fwd_flag <chr>, PULocationID <int>, DOLocationID <int>,\n#   payment_type <int>, fare_amount <dbl>, extra <dbl>, mta_tax <dbl>,\n#   tip_amount <dbl>, tolls_amount <dbl>, improvement_surcharge <dbl>,\n#   total_amount <dbl>, congestion_surcharge <dbl>\n```\n:::\n:::\n\nIn the RStudio IDE, the **taxi_jan_2020** table now shows up in the Spark tab.\n\n![](/images/guides/caching/caching-1.png){width=\"429\"}\n\nTo access the Spark Web UI, click the **Spark** button in the **RStudio Spark Tab**. As expected, the **Storage** page shows no tables loaded into memory.\n\n![](/images/guides/caching/caching-2.png){width=\"505\"}\n\n## Loading Less Data into Memory\n\nUsing the pre-processing capabilities of Spark, the data will be transformed before being loaded into memory. In this section, we will continue to build on the example started in the previous section\n\n### Lazy Transform\n\nThe following `dplyr` script will not be immediately run, so the code is processed quickly. There are some check-ups made, but for the most part it is building a Spark SQL statement in the background.\n\n::: {.cell}\n\n```{.r .cell-code}\ntrips_table <- tbl(sc,\"taxi_jan_2020\") %>%\n  filter(trip_distance > 20) %>% \n  select(VendorID, passenger_count, trip_distance)\n```\n:::\n\n### Register in Spark\n\n`sdf_register()` will register the resulting Spark SQL in Spark. The results will show up as a table called **trip_spark**. But a table of the same name is still not loaded into memory in Spark.\n\n::: {.cell}\n\n```{.r .cell-code}\nsdf_register(trips_table, \"trips_spark\")\n```\n\n::: {.cell-output-stdout}\n```\n# Source: spark<trips_spark> [?? x 3]\n   VendorID passenger_count trip_distance\n      <int>           <int>         <dbl>\n 1        2               1          23.5\n 2        1               2          22.8\n 3        2               2          37.6\n 4        2               4          20.3\n 5        1               2          29.4\n 6        2               1          25.9\n 7        2               3          22.1\n 8        2               1          21.0\n 9        2               3          20.1\n10        2               1          32.5\n# … with more rows\n```\n:::\n:::\n\n![](/images/guides/caching/caching-3.png){width=\"268\"}\n\n### Cache into Memory\n\nThe `tbl_cache()` command loads the results into an Spark RDD in memory, so any analysis from there on will not need to re-read and re-transform the original file. The resulting Spark RDD is smaller than the original file because the transformations created a smaller data set than the original file.\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_cache(sc, \"trips_spark\")\n```\n:::\n\n![](/images/guides/caching/caching-4.png){width=\"553\"}\n\n### Driver Memory\n\nIn the **Executors** page of the Spark Web UI, we can see that the Storage Memory is at about half of the 16 gigabytes requested. This is mainly because of a Spark setting called **spark.memory.fraction**, which reserves by default 40% of the memory requested.\n\n![](/images/guides/caching/caching-5.png){width=\"477\"}\n\n## Process on the fly\n\nThe plan for this exercise is to read the January file, combine it with the February file and summarize the data without bringing either file fully into memory.\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_read_csv(sc, \"taxi_feb_2020\" , \"feb_2020.csv\", memory = FALSE)\n```\n\n::: {.cell-output-stdout}\n```\n# Source: spark<taxi_feb_2020> [?? x 18]\n   VendorID tpep_pickup_datetime tpep_dropoff_dat… passenger_count trip_distance\n      <int> <chr>                <chr>                       <int>         <dbl>\n 1        1 2020-02-01 00:17:35  2020-02-01 00:30…               1          2.6 \n 2        1 2020-02-01 00:32:47  2020-02-01 01:05…               1          4.8 \n 3        1 2020-02-01 00:31:44  2020-02-01 00:43…               1          3.2 \n 4        2 2020-02-01 00:07:35  2020-02-01 00:31…               1          4.38\n 5        2 2020-02-01 00:51:43  2020-02-01 01:01…               1          2.28\n 6        1 2020-02-01 00:15:49  2020-02-01 00:20…               2          1   \n 7        1 2020-02-01 00:25:31  2020-02-01 00:50…               2          3.4 \n 8        1 2020-02-01 00:11:15  2020-02-01 00:24…               1          2.1 \n 9        2 2020-02-01 00:58:26  2020-02-01 01:02…               1          0.8 \n10        2 2020-02-01 00:03:57  2020-02-01 00:48…               1          7.22\n# … with more rows, and 13 more variables: RatecodeID <int>,\n#   store_and_fwd_flag <chr>, PULocationID <int>, DOLocationID <int>,\n#   payment_type <int>, fare_amount <dbl>, extra <dbl>, mta_tax <dbl>,\n#   tip_amount <dbl>, tolls_amount <dbl>, improvement_surcharge <dbl>,\n#   total_amount <dbl>, congestion_surcharge <dbl>\n```\n:::\n:::\n\n### Union and Transform\n\nThe `union()` command is akin to the `dplyr::bind_rows()` command. It will allow us to append the February file to the January file, and as with the previous transform, this script will be evaluated lazily.\n\n::: {.cell}\n\n```{.r .cell-code}\npassenger_count <- tbl(sc, \"taxi_jan_2020\") %>%\n  union(tbl(sc, \"taxi_feb_2020\")) %>%\n  mutate(pickup_date = as.Date(tpep_pickup_datetime)) %>% \n  count(pickup_date)\n```\n:::\n\n### Collect into R\n\nWhen receiving a `collect()` command, Spark will execute the SQL statement and send the results back to R in a data frame. In this case, R only loads 51 observations into a data frame called *passenger_count*.\n\n::: {.cell}\n\n```{.r .cell-code}\npassenger_count <- passenger_count %>%\n  collect()\n```\n:::\n\n### Plot in R\n\nNow the smaller data set can be plotted\n\n::: {.cell}\n\n```{.r .cell-code}\npassenger_count %>% \n  filter(pickup_date >= \"2020-01-01\", pickup_date <= \"2020-02-28\") %>% \n  ggplot() +\n  geom_line(aes(pickup_date, n)) +\n  theme_minimal() +\n  labs(title = \"Daily Trip Volume\", \n       subtitle = \"NYC Yellow Cab - January and February 2020\",\n       y = \"Number of Trips\",\n       x = \"\"\n       )\n```\n\n::: {.cell-output-display}\n![](caching_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_disconnect(sc)\n```\n:::",
    "supporting": [
      "caching_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}