{
  "hash": "924722a61c49c03b0c43cbeab73e95f3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Using Spark with AWS S3 buckets\"\nexecute:\n  freeze: true\n---\n\n## AWS Access Keys\n\nAWS Access Keys are needed to access S3 data. To learn how to setup a new keys, please review the AWS documentation: http://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html .We then pass the keys to R via Environment Variables:\n\n```r\nSys.setenv(AWS_ACCESS_KEY_ID=\"[Your access key]\")\nSys.setenv(AWS_SECRET_ACCESS_KEY=\"[Your secret access key]\")\n```\n\n## Connecting to Spark\n\nThere are four key settings needed to connect to Spark and use S3:\n\n* A Hadoop-AWS package\n* Executor memory (key but not critical)\n* The master URL\n* The Spark Home\n\n\n### Hadoop-AWS package: \n\nA Spark connection can be enhanced by using packages, please note that these are not R packages. For example, there are packages that tells Spark how to read CSV files, Hadoop or Hadoop in AWS. \n\nIn order to read S3 buckets, our Spark connection will need a package called `hadoop-aws`. If needed, multiple packages can be used. We experimented with many combinations of packages, and determined that for reading data in S3 we only need one. The version we used, 3.3.1, refers to the latest **Hadoop** version, so as this article ages, please visit this site to make sure that you are using the latest version: [Hadoop AWS Maven Repository](https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws)  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n\nconf <- spark_config()\n\nconf$sparklyr.defaultPackages <- \"org.apache.hadoop:hadoop-aws:3.3.1\"\n\nsc <- spark_connect(maste = \"local\", config = conf)\n```\n:::\n\n    \n\n## Data Import/Wrangle approach\n\nWe experimented with multiple approaches. Most of the factors for settling on a recommended approach were made based on the speed of each step. The premise is that we would rather wait longer during Data Import, if it meant that we can much faster Register and Cache our data subsets during Data Wrangling, especially since we would expect to end up with many subsets as we explore and model.The selected combination was the second slowest during the Import stage, but the fastest when caching a subset, by a lot. \n\nIn our original tests, it took 72 seconds to read and cache the 29 columns of the 41 million rows of data, the slowest was 77 seconds. But when it comes to registering and caching a considerably sizable subset of 3 columns and almost all of the 41 million records, this approach was 17X faster than the second fastest approach. It took 1/3 of a second to register and cache the subset, the second fastest was 5 seconds.\n\nTo implement this approach, we need to set three arguments in the `spark_csv_read()` step:\n\n* `memory`\n* `infer_schema`\n* `columns`\n\nAgain, this is a recommended approach. The columns argument is needed only if `infer_schema` is set to `FALSE.` When memory is set to `TRUE` it makes Spark load the entire dataset into memory, and setting `infer_schema` to FALSE prevents Spark from trying to figure out what the schema of the files are. By trying different combinations the memory and `infer_schema` arguments you may be able to find an approach that may better fits your needs.\n\n### Reading the schema\n\nSurprisingly, another critical detail that can easily be overlooked is choosing the right **s3 URI scheme**. There are two options: **s3n** and **s3a**. In most examples and tutorials I found, there was no reason give of why or when to use which one. The article the finally clarified it was this one: https://wiki.apache.org/hadoop/AmazonS3\n\nThe gist of it is that **s3a** is the recommended one going forward, especially for Hadoop versions 2.7 and above. This means that if we copy from older examples that used *Hadoop 2.6 we would more likely also used s3n* thus making data import much, much slower.\n\n## Data Import\n\nAfter the long introduction in the previous section, there is only one point to add about the following code chunk. If there are any `NA` values in numeric fields, then define the column as character and then convert it on later subsets using dplyr. The data import will fail if it finds any NA values on numeric fields. This is a small trade off in this approach because the next fastest one does not have this issue but is 17X slower at caching subsets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights <- spark_read_csv(sc, \"flights_spark\", \n                          path =  \"s3a://flights-data/full\", \n                          memory = TRUE, \n                          columns = list(\n                            Year = \"character\",\n                            Month = \"character\",\n                            DayofMonth = \"character\",\n                            DayOfWeek = \"character\",\n                            DepTime = \"character\",\n                            CRSDepTime = \"character\",\n                            ArrTime = \"character\",\n                            CRSArrTime = \"character\",\n                            UniqueCarrier = \"character\",\n                            FlightNum = \"character\",\n                            TailNum = \"character\",\n                            ActualElapsedTime = \"character\",\n                            CRSElapsedTime = \"character\",\n                            AirTime = \"character\",\n                            ArrDelay = \"character\",\n                            DepDelay = \"character\",\n                            Origin = \"character\",\n                            Dest = \"character\",\n                            Distance = \"character\",\n                            TaxiIn = \"character\",\n                            TaxiOut = \"character\",\n                            Cancelled = \"character\",\n                            CancellationCode = \"character\",\n                            Diverted = \"character\",\n                            CarrierDelay = \"character\",\n                            WeatherDelay = \"character\",\n                            NASDelay = \"character\",\n                            SecurityDelay = \"character\",\n                            LateAircraftDelay = \"character\"), \n                         infer_schema = FALSE)\n```\n:::\n\n\n## Data Wrangle\n\nThere are a few points we need to highlight about the following simple dyplr code:\n\nBecause there were NAs in the original fields, we have to mutate them to a number. Try coercing any variable as integer instead of numeric, this will save a lot of space when cached to Spark memory. The sdf_register command can be piped at the end of the code. After running the code, a new table will appear in the RStudio IDEâ€™s Spark tab\n\n```r\ntidy_flights <- tbl(sc, \"flights_spark\") %>%\n  mutate(ArrDelay = as.integer(ArrDelay),\n         DepDelay = as.integer(DepDelay),\n         Distance = as.integer(Distance)) %>%\n  filter(!is.na(ArrDelay)) %>%\n  select(DepDelay, ArrDelay, Distance) %>%\n  sdf_register(\"tidy_spark\")\n```\n\nAfter we use `tbl_cache()` to load the `tidy_spark` table into Spark memory. We can see the new table in the Storage page of our Spark session.\n\n```r\ntbl_cache(sc, \"tidy_spark\")\n```\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}