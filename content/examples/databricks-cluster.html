---
title: "Using sparklyr with Databricks"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
params:
  width: 600
---



<div id="overview" class="section level2">
<h2>Overview</h2>
<p>This documentation demonstrates how to use <code>sparklyr</code> with RStudio Server Pro
and a Databricks cluster with Apache Spark.</p>
</div>
<div id="best-practices" class="section level2">
<h2>Best practices</h2>
<ul>
<li><strong>Connect to Spark remotely</strong> - Install RStudio Server Pro outside of the
Databricks cluster and work with Spark remotely</li>
<li><strong>Work across RStudio Team</strong> - Install RStudio Connect and RStudio Package
Manager outside of the Databricks cluster so that they are not limited to the
resources or ephemeral nature of Databricks clusters</li>
<li><strong>Restrict workloads to interactive analysis</strong> - Only perform workloads
related to exploratory or interactive analysis with Spark, then write the
results to a database, file system, or cloud storage for more efficient
retrival in apps, reports, and APIs</li>
<li><strong>Load and query results efficiently</strong> - Because of the nature of Spark
computations and the associated overhead, Shiny apps that use Spark on the
backend tend to have performance and runtime issues; consider reading the
results from a database, file system, or cloud storage instead</li>
<li><strong>Maintain a persisent installation</strong> - If you cannot work with Spark
remotely, you should install RStudio Server Pro on the Driver node of a
long-running, persistent Databricks cluster as opposed to a worker node or an
ephemeral cluster</li>
</ul>
</div>
<div id="options-for-using-rstudio-with-databricks" class="section level2">
<h2>Options for using RStudio with Databricks</h2>
<p>There are two different options for using <code>sparklyr</code> with Databricks:</p>
<ul>
<li>Option 1:
<a href="#option-1---connecting-to-databricks-remotely">Connecting to Databricks remotely</a>
(Recommended option)</li>
<li>Option 2:
<a href="#option-2---working-inside-of-databricks">Working inside of Databricks</a>
(Alternative option)</li>
</ul>
</div>
<div id="option-1---connecting-to-databricks-remotely" class="section level2">
<h2>Option 1 - Connecting to Databricks remotely</h2>
<p>With this configuration, RStudio Server Pro is installed outside of the Spark
cluster and allows users to connect to Spark remotely using
<a href="https://docs.databricks.com/dev-tools/databricks-connect.html">Databricks Connect</a>.</p>
<p>This is the recommended configuration because it targets separate environments,
involves simpler configuration, avoids resource contention, and allows RStudio
Server Pro to connect to Databricks as well as other remote storage and compute
resources.</p>
<p><a href="/examples/databricks-cluster-remote">
<img src="/images/deployment/databricks/rstudio-databricks-remote.png" width='800px' align='center'/>
</a></p>
<a href="/examples/databricks-cluster-remote">
<h2>
View steps for connecting to Databricks remotely
</h2>
<p></a></p>
</div>
<div id="option-2---working-inside-of-databricks" class="section level2">
<h2>Option 2 - Working inside of Databricks</h2>
<p>With this configuration, RStudio Server Pro is installed on the Spark driver
node and allows users to work locally with Spark.</p>
<p>This configuration can result in increased complexity, limited connectivity to
other storage and compute sources, resource contention between RStudio Server
Pro and Databricks, and maintenance concerns due to the ephemeral nature of
Databricks clusters.</p>
<p><a href="/examples/databricks-cluster-local">
<img src="/images/deployment/databricks/rstudio-databricks-local.png" width='800px' align='center'/>
</a></p>
<a href="/examples/databricks-cluster-local">
<h2>
View steps for working inside of Databricks
</h2>
<p></a></p>
</div>
