{
  "hash": "b88d0d3ac1776a6ed12ffea4c932bfe5",
  "result": {
    "markdown": "---\ntitle: \"Prepare Data\"\nformat:\n  html:\n    theme: default\n    toc: true\nexecute:\n  eval: true\n  freeze: true\n---\n\n\n\n`sparklyr` provide multiple methods to prepare data inside Spark:\n\n- Using `dplyr` commands\n- Using SQL\n- Using Spark's feature transformers\n\nThis article will introduce each method and provide a simple example.\n\n## Exercise\n\nFor the exercise start a local session of Spark. We'll start by copying a \ndata set from R into the Spark cluster (note that you may need to install the \n`nycflights13`)\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nflights_tbl <- copy_to(sc, nycflights13::flights, \"spark_flights\")\n```\n:::\n\n## Using `dplyr`\n\nWe can use familiar `dplyr` commands to prepare data inside Spark.  The commands\nrun **inside Spark**, so there are no unnecessary data transfers between R and\nSpark.\n\nIn this example, we can see how easy it is to summarize the flights data without \nhaving to know how to write Spark SQL: \n\n::: {.cell}\n\n```{.r .cell-code}\ndelay <- flights_tbl %>%\n  group_by(tailnum) %>%\n  summarise(\n    count = n(), \n    dist = mean(distance, na.rm = TRUE), \n    delay = mean(arr_delay, na.rm = TRUE)\n    ) %>%\n  filter(count > 20, dist < 2000, !is.na(delay)) \n\ndelay\n```\n\n::: {.cell-output-stdout}\n```\n# Source: spark<?> [?? x 4]\n   tailnum count  dist  delay\n   <chr>   <dbl> <dbl>  <dbl>\n 1 N24211    130 1330.  7.7  \n 2 N793JB    283 1529.  4.72 \n 3 N657JB    285 1286.  5.03 \n 4 N633AA     24 1587. -0.625\n 5 N9EAMQ    248  675.  9.24 \n 6 N3GKAA     77 1247.  4.97 \n 7 N997DL     63  868.  4.90 \n 8 N318NB    202  814. -1.12 \n 9 N651JB    261 1408.  7.58 \n10 N841UA     96 1208.  2.10 \n# â€¦ with more rows\n```\n:::\n:::\n\n`sparklyr` and `dplyr` translate the R commands into Spark SQL for us. To see \nthe resulting query use `show_query()`:\n\n::: {.cell}\n\n```{.r .cell-code}\ndplyr::show_query(delay)\n```\n\n::: {.cell-output-stdout}\n```\n<SQL>\nSELECT *\nFROM (SELECT `tailnum`, COUNT(*) AS `count`, AVG(`distance`) AS `dist`, AVG(`arr_delay`) AS `delay`\nFROM `spark_flights`\nGROUP BY `tailnum`) `q01`\nWHERE ((`count` > 20.0) AND (`dist` < 2000.0) AND (NOT(((`delay`) IS NULL))))\n```\n:::\n:::\n\nNotice that the `delay` variable does not contain data. It only contains the\n`dplyr` commands that are to run against the Spark connection.  \n\nFor additional documentation on using dplyr with Spark see the \n[Manipulating Data with `dplyr`](/guides/dplyr.qmd) article in \nthis site\n\n## Using SQL\n\nIt's also possible to execute SQL queries directly against tables within a Spark \ncluster. The `spark_connection()` object implements a [DBI](https://dbi.r-dbi.org/) \ninterface for Spark, so you can use `dbGetQuery()` to execute SQL and return the \nresult as an R data frame:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DBI)\n\ndbGetQuery(sc, \"SELECT carrier, sched_dep_time, dep_time, dep_delay FROM spark_flights LIMIT 5\")\n```\n\n::: {.cell-output-stdout}\n```\n  carrier sched_dep_time dep_time dep_delay\n1      UA            515      517         2\n2      UA            529      533         4\n3      AA            540      542         2\n4      B6            545      544        -1\n5      DL            600      554        -6\n```\n:::\n:::\n\n## Using Feature Transformers\n\nBoth of the previous methods rely on SQL statements.  Spark provides commands that\nmake some data transformation more convenient, and without the use of SQL. \n\nFor example, the `ft_binarizer()` command simplifies the creation of a new \ncolumn that indicates if the value of another column is above a certain threshold.\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_tbl %>% \n  ft_binarizer(\"dep_delay\", \"over_one\", threshold = 1) %>% \n  select(dep_delay, over_one) %>% \n  head(5)\n```\n\n::: {.cell-output-stdout}\n```\n# Source: spark<?> [?? x 2]\n  dep_delay over_one\n      <dbl>    <dbl>\n1         2        1\n2         4        1\n3         2        1\n4        -1        0\n5        -6        0\n```\n:::\n:::\n\n\nFind a full list of the Spark Feature Transformers available through `sparklyr`\nhere: [Reference - FT](/packages/sparklyr/latest/reference/index.html#spark-feature-transformers).\n\n## Disconnect from Spark\n\nLastly, cleanup your session by disconnecting Spark:\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_disconnect(sc)\n```\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}