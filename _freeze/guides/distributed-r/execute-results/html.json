{
  "hash": "24b688dad6e73a6f4d79d0e0cb05cdd8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Distributing R Computations\"\nexecute:\n  eval: true\n  freeze: true\n---\n\n## Overview\n\n**sparklyr** provides support to run arbitrary R code at scale within your Spark Cluster through `spark_apply()`. This is especially useful where there is a need to use functionality available only in R or R packages that is not available in Apache Spark nor [Spark Packages](https://spark-packages.org/).\n\n`spark_apply()` applies an R function to a Spark object (typically, a Spark DataFrame). Spark objects are partitioned so they can be distributed across a cluster. You can use `spark_apply()` with the default partitions or you can define your own partitions with the `group_by()` argument. Your R function must return another Spark DataFrame. `spark_apply()` will run your R function on each partition and output a single Spark DataFrame.\n\n### Apply an R function to a Spark Object\n\nLets run a simple example. We will apply the identify function, `I()`, over a list of numbers we created with the `sdf_len()` function.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nsdf_len(sc, 5, repartition = 1) %>%\n  spark_apply(function(e) I(e))\n#> # Source:   table<`sparklyr_tmp__7ec9de3b_3797_4f7e_8fdb_6de49e34f20a`> [?? x 1]\n#> # Database: spark_connection\n#>      id\n#>   <int>\n#> 1     1\n#> 2     2\n#> 3     3\n#> 4     4\n#> 5     5\n```\n:::\n\n\nYour R function should be designed to operate on an R [data frame](https://stat.ethz.ch/R-manual/R-devel/library/base/html/data.frame.html). The R function passed to `spark_apply()` expects a DataFrame and will return an object that can be cast as a DataFrame. We can use the `class` function to verify the class of the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsdf_len(sc, 10, repartition = 1) %>%\n  spark_apply(function(e) class(e))\n#> # Source:   table<`sparklyr_tmp__d8c2aa8c_abfa_426a_8b21_cba3cf6d38f9`> [?? x 1]\n#> # Database: spark_connection\n#>   result    \n#>   <chr>     \n#> 1 tbl_df    \n#> 2 tbl       \n#> 3 data.frame\n```\n:::\n\n\nSpark will partition your data by hash or range so it can be distributed across a cluster. In the following example we create two partitions and count the number of rows in each partition. Then we print the first record in each partition.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrees_tbl <- sdf_copy_to(sc, trees, repartition = 2)\n\nspark_apply(\n  trees_tbl,\n  function(e) nrow(e), names = \"n\"\n  )\n#> # Source:   table<`sparklyr_tmp__89b73006_6367_4076_a796_3f760f15a5d9`> [?? x 1]\n#> # Database: spark_connection\n#>       n\n#>   <int>\n#> 1    15\n#> 2    16\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_apply(trees_tbl, function(e) head(e, 1))\n#> # Source:   table<`sparklyr_tmp__e71fb5de_31f9_4cb7_a32b_8d8efbce5e10`> [?? x 3]\n#> # Database: spark_connection\n#>   Girth Height Volume\n#>   <dbl>  <dbl>  <dbl>\n#> 1   8.3     70   10.3\n#> 2  12.9     74   22.2\n```\n:::\n\n\nWe can apply any arbitrary function to the partitions in the Spark DataFrame. For instance, we can scale or jitter the columns. Notice that `spark_apply()` applies the R function to all partitions and returns a single DataFrame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_apply(trees_tbl, function(e) scale(e))\n#> # Source:   table<`sparklyr_tmp__6a0141c5_bb36_4779_bcfc_6d415d830f54`> [?? x 3]\n#> # Database: spark_connection\n#>      Girth Height  Volume\n#>      <dbl>  <dbl>   <dbl>\n#>  1 -2.05   -0.607 -1.69  \n#>  2 -1.79   -1.43  -1.69  \n#>  3 -1.62   -1.76  -1.71  \n#>  4 -0.134  -0.276 -0.339 \n#>  5  0.0407  1.21   0.191 \n#>  6  0.128   1.54   0.390 \n#>  7  0.302  -1.27  -0.515 \n#>  8  0.302   0.221  0.0589\n#>  9  0.389   1.05   1.03  \n#> 10  0.477   0.221  0.434 \n#> # ℹ more rows\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_apply(trees_tbl, function(e) lapply(e, jitter))\n#> # Source:   table<`sparklyr_tmp__4c8243ee_7bb2_47fe_be13_2fa7bf12485c`> [?? x 3]\n#> # Database: spark_connection\n#>    Girth Height Volume\n#>    <dbl>  <dbl>  <dbl>\n#>  1  8.31   70.1   10.3\n#>  2  8.60   64.9   10.3\n#>  3  8.82   62.9   10.2\n#>  4 10.5    72.2   16.4\n#>  5 10.7    81.0   18.8\n#>  6 10.8    83.1   19.7\n#>  7 11.0    65.8   15.6\n#>  8 11.0    74.9   18.2\n#>  9 11.1    80.2   22.6\n#> 10 11.2    75.1   19.9\n#> # ℹ more rows\n```\n:::\n\n\nBy default `spark_apply()` derives the column names from the input Spark data frame. Use the `names` argument to rename or add new columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_apply(\n  trees_tbl,\n  function(e) data.frame(2.54 * e$Girth, e), names = c(\"Girth(cm)\", colnames(trees))\n  )\n#> # Source:   table<`sparklyr_tmp__5d094147_870d_4b30_82e2_ec862ef5aae4`> [?? x 4]\n#> # Database: spark_connection\n#>    `Girth(cm)` Girth Height Volume\n#>          <dbl> <dbl>  <dbl>  <dbl>\n#>  1        21.1   8.3     70   10.3\n#>  2        21.8   8.6     65   10.3\n#>  3        22.4   8.8     63   10.2\n#>  4        26.7  10.5     72   16.4\n#>  5        27.2  10.7     81   18.8\n#>  6        27.4  10.8     83   19.7\n#>  7        27.9  11       66   15.6\n#>  8        27.9  11       75   18.2\n#>  9        28.2  11.1     80   22.6\n#> 10        28.4  11.2     75   19.9\n#> # ℹ more rows\n```\n:::\n\n\n### Group By\n\nIn some cases you may want to apply your R function to specific groups in your data. For example, suppose you want to compute regression models against specific subgroups. To solve this, you can specify a `group_by()` argument. This example counts the number of rows in `iris` by species and then fits a simple linear model for each species.\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_tbl <- sdf_copy_to(sc, iris)\n\nspark_apply(iris_tbl, nrow, group_by = \"Species\")\n#> # Source:   table<`sparklyr_tmp__bc3c8bb0_eecb_427a_abd5_ef6d4740af80`> [?? x 2]\n#> # Database: spark_connection\n#>   Species    result\n#>   <chr>       <int>\n#> 1 versicolor     50\n#> 2 virginica      50\n#> 3 setosa         50\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_tbl %>%\n  spark_apply(\n    function(e) summary(lm(Petal_Length ~ Petal_Width, e))$r.squared,\n    names = \"r.squared\",\n    group_by = \"Species\"\n    )\n#> # Source:   table<`sparklyr_tmp__0f1bcbbc_a649_475f_90c4_b4a656b0e076`> [?? x 2]\n#> # Database: spark_connection\n#>   Species    r.squared\n#>   <chr>          <dbl>\n#> 1 versicolor     0.619\n#> 2 virginica      0.104\n#> 3 setosa         0.110\n```\n:::\n\n\n## Distributing Packages\n\nWith `spark_apply()` you can use any R package inside Spark. For instance, you can use the [broom](https://cran.r-project.org/package=broom) package to create a tidy data frame from linear regression output.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_apply(\n  iris_tbl,\n  function(e) broom::tidy(lm(Petal_Length ~ Petal_Width, e)),\n  names = c(\"term\", \"estimate\", \"std.error\", \"statistic\", \"p.value\"),\n  group_by = \"Species\"\n  )\n#> # Source:   table<`sparklyr_tmp__fb7417e2_7727_4d91_bdec_9fb46ce1577d`> [?? x 6]\n#> # Database: spark_connection\n#>   Species    term      estimate std.error statistic  p.value\n#>   <chr>      <chr>        <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 versicolor (Interce…    1.78     0.284       6.28 9.48e- 8\n#> 2 versicolor Petal_Wi…    1.87     0.212       8.83 1.27e-11\n#> 3 virginica  (Interce…    4.24     0.561       7.56 1.04e- 9\n#> 4 virginica  Petal_Wi…    0.647    0.275       2.36 2.25e- 2\n#> 5 setosa     (Interce…    1.33     0.0600     22.1  7.68e-27\n#> 6 setosa     Petal_Wi…    0.546    0.224       2.44 1.86e- 2\n```\n:::\n\n\nTo use R packages inside Spark, your packages must be installed on the worker nodes. The first time you call `spark_apply()` all of the contents in your local `.libPaths()` will be copied into each Spark worker node via the `SparkConf.addFile()` function. Packages will only be copied once and will persist as long as the connection remains open. It's not uncommon for R libraries to be several gigabytes in size, so be prepared for a one-time tax while the R packages are copied over to your Spark cluster. You can disable package distribution by setting `packages = FALSE`. Note: packages are not copied in local mode (`master=\"local\"`) because the packages already exist on the system.\n\n## Handling Errors\n\nIt can be more difficult to troubleshoot R issues in a cluster than in local mode. For instance, the following R code causes the distributed execution to fail and suggests you check the logs for details.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_apply(iris_tbl, function(e) stop(\"Make this fail\"))\n```\n:::\n\n\nIt is worth mentioning that different cluster providers and platforms expose worker logs in different ways. Specific documentation for your environment will point out how to retrieve these logs.\n\n## Requirements\n\nThe **R Runtime** is expected to be pre-installed in the cluster for `spark_apply()` to function. Failure to install the cluster will trigger a `Cannot run program, no such file or directory` error while attempting to use `spark_apply()`. Contact your cluster administrator to consider making the R runtime available throughout the entire cluster.\n\nA **Homogeneous Cluster** is required since the driver node distributes, and potentially compiles, packages to the workers. For instance, the driver and workers must have the same processor architecture, system libraries, etc.\n\n## Configuration\n\nThe following table describes relevant parameters while making use of `spark_apply`.\n\n| Value                             | Description                                                                                |\n|-----------------------------------|--------------------------------------------------------------------------------------------|\n| `spark.r.command`                 | The path to the R binary. Useful to select from multiple R versions.                       |\n| `sparklyr.worker.gateway.address` | The gateway address to use under each worker node. Defaults to `sparklyr.gateway.address`. |\n| `sparklyr.worker.gateway.port`    | The gateway port to use under each worker node. Defaults to `sparklyr.gateway.port`.       |\n\nFor example, one could make use of an specific R version by running:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfig <- spark_config()\nconfig[[\"spark.r.command\"]] <- \"<path-to-r-version>\"\nsc <- spark_connect(master = \"local\", config = config)\n\nsdf_len(sc, 10) %>% spark_apply(function(e) e)\n```\n:::\n\n\n## Limitations\n\n### Closures\n\nClosures are serialized using `serialize`, which is described as \"A simple low-level interface for serializing to connections.\". One of the current limitations of `serialize` is that it wont serialize objects being referenced outside of it's environment. For instance, the following function will error out since the closures references `external_value`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexternal_value <- 1\nspark_apply(iris_tbl, function(e) e + external_value)\n```\n:::\n\n\n### Livy\n\nCurrently, Livy connections do not support distributing packages since the client machine where the libraries are pre-compiled might not have the same processor architecture, not operating systems that the cluster machines.\n\n### Computing over Groups\n\nWhile performing computations over groups, `spark_apply()` will provide partitions over the selected column; however, this implies that each partition can fit into a worker node, if this is not the case an exception will be thrown. To perform operations over groups that exceed the resources of a single node, one can consider partitioning to smaller units or use `dplyr::do()` which is currently optimized for large partitions.\n\n### Package Installation\n\nSince packages are copied only once for the duration of the `spark_connect()` connection, installing additional packages is not supported while the connection is active. Therefore, if a new package needs to be installed, `spark_disconnect()` the connection, modify packages and reconnect.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}