{
  "hash": "1b4629065e08c67f7f532f5076819e27",
  "result": {
    "markdown": "---\ntitle: \"Access the Spark API\"\nexecute:\n  freeze: true\n---\n\n\n\n\n*R/spark_connection.R*\n\n## spark-api\n\n## Description\nAccess the commonly-used Spark objects associated with a Spark instance. These objects provide access to different facets of the Spark API. \n\n\n## Usage\n```r\nspark_context(sc) \n\njava_context(sc) \n\nhive_context(sc) \n\nspark_session(sc) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| sc | A `spark_connection`. |\n\n## Details\n\nThe [Scala API documentation](https://spark.apache.org/docs/latest/api/scala/)\n\nis useful for discovering what methods are available for each of these objects. Use `invoke` to call methods on these objects. \n\n## Section\n\n## Spark Context\n\nThe main entry point for Spark functionality. The **Spark Context**\n\nrepresents the connection to a Spark cluster, and can be used to create `RDD`s, accumulators and broadcast variables on that cluster. \n\n## Java Spark Context\n\nA Java-friendly version of the aforementioned **Spark Context**. \n\n## Hive Context\n\nAn instance of the Spark SQL execution engine that integrates with data stored in Hive. Configuration for Hive is read from `hive-site.xml` on the classpath. \n\nStarting with Spark >= 2.0.0, the **Hive Context** class has been deprecated -- it is superceded by the **Spark Session** class, and `hive_context` will return a **Spark Session** object instead. Note that both classes share a SQL interface, and therefore one can invoke SQL through these objects. \n\n## Spark Session\n\nAvailable since Spark 2.0.0, the **Spark Session** unifies the **Spark Context** and **Hive Context** classes into a single interface. Its use is recommended over the older APIs for code targeting Spark 2.0.0 and above. \n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}