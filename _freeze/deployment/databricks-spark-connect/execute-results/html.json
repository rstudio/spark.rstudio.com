{
  "hash": "2dc24ba0850734760821cb69e1193690",
  "result": {
    "markdown": "---\ntitle: \"Spark Connect, and Databricks Connect v2\"\nformat:\n  html:\n    theme: default\n    toc: true\nexecute:\n    eval: true\n    freeze: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n\n*Last updated: Mon Oct 16 08:42:45 2023*\n\n## Intro\n\n[Spark\nConnect](https://spark.apache.org/docs/latest/spark-connect-overview.html)\nintroduced a decoupled client-server architecture that allows remote\nconnectivity to Spark clusters using the DataFrame API. The separation\nbetween client and server allows Spark to be leveraged from everywhere,\nand this would allow R users to interact with a cluster from the comfort\nof their preferred environment, laptop or otherwise. [Databricks\nConnect](https://docs.databricks.com/dev-tools/databricks-connect.html),\navailable in Databricks Runtime version 13 and above, is based on this\nnew architecture.\n\nWe are working on enhancing `sparklyr` so that it can bring the benefits\nof Spark Connect to the R user community.\n\n## The Solution\n\nSpark Connect requires the use of a remote procedure call framework\ncalled *gRPC.* At this time, the Spark team provides two higher level APIs\nthat interact with *gRPC.* One is Scala based, and the other Python based.\n\nWe are using `reticulate` to interact with the Python API. `sparklyr` extends \nthe functionality, and user experience, by providing the `dplyr`back-end, `DBI` \nback-end, RStudio's Connection pane integration.\n\n::: {#fig-connect}\n\n```{mermaid}\nflowchart LR\n  subgraph lp[User's machine]\n    sr[sparklyr]\n    rt[reticulate]\n    ps[PySpark]\n    g1[gRPC]\n  end\n  sp[Spark]\n  \n  sr --> rt\n  rt --> ps\n  g1 <-. Network .-> sp\n  ps --> g1\n  \n  style sr  fill:#d0efb1,stroke:#666\n  style rt  fill:#d0efb1,stroke:#666\n  style ps  fill:#eff,stroke:#666\n  style lp  fill:#fff,stroke:#666\n  style sp  fill:#f4c430,stroke:#666\n  style g1  fill:#447099,stroke:#666,color:#fff\n```\n\n\nHow `sparklyr` communicates with Spark Connect, and Databricks Connect\n:::\n\nIn order to quickly iterate on enhancements and bug fixes, we have\ndecided to isolate the Python integration into its own package. The new\npackage, called `pysparklyr`, is an extension of `sparklyr`.\n\n## Package Installation\n\nAs mentioned above, we are still working integrating `sparklyr` with\nConnect. To access the new capabilities, you will need `sparklyr` version\n**1.8.3** or higher, and `pysparklyr`. To install use:\n\n``` r\ninstall.packages(\"sparklyr\")\ninstall.packages(\"pysparklyr\")\n```\n\n## Databricks Connect (ML 13+)\n\n### Initial setup\n\n`sparklyr` will need specific Python libraries in order to connect, and interact\nwith Databricks Connect. We provide a convenience function that will automatically\ndo the following:\n\n- Create, or re-create, a Python environment. Based on your OS, it will choose\nto create a Virtual Environment, or Conda. \n\n- Install the needed Python libraries\n\n::: {.callout-important} \n## WINDOWS USERS PLEASE READ\n\nSome issues have been reported using **older** CRAN version of `reticulate`.\nPlease make sure to have the latest version installed. At this time that\nversion number is **1.34.0**. \n\n:::\n\nTo install the latest versions of all the libraries, use:\n\n```r\npysparklyr::install_databricks()\n```\n\n`sparklyr` will query PyPi.org to get the latest version of `databricks.connect`\nand installs that version. It is recommended that the version of the \n`databricks.connect` library, matches  the DBR version of your cluster.  \nTo do this, pass the DBR version in the `version` argument, for example:\n\n```r\npysparklyr::install_databricks(\"14.0\")\n```\n\nThis will create a Python environment and install `databricks.connect` version\n14.0, and it will automatically name it `r-sparklyr-databricks-14.0`.  By using\nthis name, `sparklyr` is able to know what version of `databricks.connect` is\navailable inside this particular Python environment.\n\nIf you are not sure about the version of the cluster you want to interact with,\nthen use the `cluster_id` argument. We have added a way to pull the cluster's\ninformation, without starting a Spark connect. This allows us to query the\ncluster and get the DBR version:\n\n```r\npysparklyr::install_databricks(cluster_id = \"[Your cluster's ID]\")\n```\n\n\n**Important** -  This step needs only to be **done one time**. If you need to connect to a different\ncluster, but that has the same DBR version, `sparklyr` will use the same\nPython environment.  If the new cluster has a different DBR version, then it is\nrecommended that you run the installation function using the new DBR version, or\ncluster ID.\n\n\n### Connecting\n\nTo use with Databricks Connect, in run-time 13 or above, you will need\nthree configuration items:\n\n-   Your [Workspace Instance\n    URL](https://docs.databricks.com/workspace/workspace-details.html#workspace-url)\n-   The ID of a currently running\n    [cluster](https://docs.databricks.com/workspace/workspace-details.html#cluster-url-and-id)\n    within your workspace\n-   Authentication setup. At this time, `sparklyr` supports\n    [PAT](https://docs.databricks.com/dev-tools/auth.html#pat)\n\nIn `spark_connect()` those items can be passed as function arguments.\nAlso note that to let `sparklyr` know that you are attempting to use\nDatabricks Connect, use `method = databricks_connect`:\n\n``` r\nlibrary(sparklyr)\n\nsc <- spark_connect(\n  master = \"[Your Workspace Instance URL]\", \n  cluster_id = \"[Cluster ID]\"\n  token = \"[Your PAT - Please do not include \n           it as plain text here,  please refer \n           to the next section]\",\n  method = \"databricks_connect\"\n  )\n```\n\n`sparklyr` will get retrieve the cluster's DBR version, and automatically choose\nthe Python environment that has the matching version of `databricks.connect`. If\nit does not find a matching version, it will use the most recent version installed\nin your machine. If you wish, you can specify the version by setting the\n`dbr_version` argument. \n\n```r\nsc <- spark_connect(\n  cluster_id = \"[Cluster ID]\",\n  method = \"databricks_connect\",\n  dbr_version = \"14.0\"\n  )\n```\n\n### Safely connect\n\nFor your safety and convenience, you can save your authentication token\n(PAT), and your Workspace Instance URL in a environment variable. The\ntwo variables are:\n\n-   `DATABRICKS_TOKEN`\n-   `DATABRICKS_HOST`\n\nThis will prevent your token to be shown in plain text in your code. You\ncan set the environment variables at the beginning of your R session\nwith `Sys.setenv()`.\n\nPreferably, the two variables can be set for all R sessions by saving\nthem to the **.Renviron** file. The `usethis` package has a handy\nfunction that opens the file so you can edit it:\n`usethis::edit_r_environ()`. Then simply append the two entries to your\n**.Renviron** file. Once you are done, save the **.Renviron** file, and\nrestart R.\n\nAfter that, use `spark_connect()` to open the connection to Databricks.\nYou will need to only pass the `cluster_id` of your cluster, and tell\n`sparklyr` that you are connecting to a Spark cluster with run time 13\nor above:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n\nsc <- spark_connect(\n  cluster_id = \"0914-200113-rc8v8keb\",\n  method = \"databricks_connect\"\n)\n#> ! Retrieving version from cluster '0914-200113-rc8v8keb'\n#> ✔ Cluster version: '14.0'\n#> ✔ Using the 'r-sparklyr-databricks-14.0' Python environment (/Users/edgar/.virtualenvs/r-sparklyr-databricks-14.0/bin/python)\n```\n:::\n\n\n`sparklyr` will provide console message while initiating the connection. As\nseen above, we are informed that `sparklyr` is getting the DBR version from the\ncluster, the version returned by the cluster, and which Python environment \nit used to connect.\n\n### Reported Problems\n\nAs it is with any nascent implementations, we are seeing some early adopters report\nissues with their installation or connections. \n\nHere are some of the issues that have been reported to us, and the recommended\nsolution, or workaround. Each issue is **collapsed**, just expand it to see the\nbackground and recommendation. Each one is titled based on the full error message \nor, the distinctive part of the error message: \n\n::: {.callout-note collapse=\"true\"}\n## `...reticulate can only bind to copies of Python built with '--enable-shared'.`\n\nError message contains:\n\n```r\n... reticulate can only bind to copies of Python built with '--enable-shared'.\n```\n\nThis is happening because there is no Python installation that `reticulate` \ncan use. `pysparklyr` depends on that package to function. \n\n### Solution\n\nThe best way to resolve is to install an acceptable version of Python. You can run:\n\n```r\nreticulate::install_python()\n```\n\nIf the output mentions `--skip-existing [version number]` then you will have two options:\n\n1. If you're ok with replacing the existing Python, then use:\n   ```r\n    reticulate::install_python(version = \"[version number]\", force = TRUE)\n   ``` \n\n2. If you would like to keep that installation of Python then use a different version number:\n   ```r\n    reticulate::install_python(version = \"[slightly different version number]\")\n   ``` \n    For example, if the version number is `3.9.12`, then use `3.9.18`: \n  \n    ```r\n    reticulate::install_python(version = \"3.9.18\")\n    ``` \n:::\n\n::: {.callout-note collapse=\"true\"}\n## `Error: Unable to find conda binary. Is Anaconda installed?`\n\nWhen trying to connect, this error is returned:\n\n```r\nError: Unable to find conda binary. Is Anaconda installed?\n```\n\nHere is a code example when this error will generally show up:\n\n```r\nlibrary(sparklyr)\nsc <- spark_connect(cluster_id=\"xxxx\", method = \"databricks_connect\")\n## ! Retrieving version from cluster 'xxxx'\n## ✔ Cluster version: '14.0'\n## Error: Unable to find conda binary. Is Anaconda installed?\n```\n\n### Solution\n\nThis is an issue with `pysparklyr` in CRAN, there is a fix for it currently \nin the dev version (`0.1.9000` and above). Run the following to install:\n\n```r\ninstall.packages(\"pak\")\npak::pak(\"mlverse/pysparklyr\")\n```\n:::\n\n::: {.callout-note collapse=\"true\"}\n## `Error in envs[[1]] : subscript out of bounds`\n\nThis is because there is no \"standardly named\" virtual environment is found. \n\n```r\n> sc <- spark_connect(method = \"databricks_connect\")\nError in envs[[1]] : subscript out of bounds\n```\n\n### Solution\n\nTo resolve, there are 2 options:\n\n1. Run `pysparklyr::install_databricks(cluster_id = \"[your cluster's id]\")`\n2. If you want to use a environment you've setup, pass it as part of your connection code: `sc <- spark_connect(method = \"databricks_connect\", envname = \"my-customer-environment\"))`\n\nIn `pysaprklyr`, we'll add a message that says: \"no environment was found, and to please use `envname`\n:::\n\n::: {.callout-note collapse=\"true\"}\n## `! Version '' does not exist`\n\nWhen you try to install Databricks using the cluster ID, and the Host ID is invalid, this error appears:\n\n```r\n> Sys.setenv(\"DATABRICKS_HOST\" = \"https://rstudio-partner-posit-default.cloud.databricks.com/o=notvalid\")\n> pysparklyr::install_databricks(cluster_id = \"0914-200113-rc8v8keb\")\n! Retrieving version from cluster '0914-200113-rc8v8keb'\n✔ Checking if provided version is valid against PyPi.org\nError in `install_environment()`:\n! Version '' does not exist\nRun `rlang::last_trace()` to see where the error occurred.\n```\n\n### Solution\n\nRemove any parameters, and leading forward slashes, from the URL. \n\nIn the future, we'll check for that going forward and provide clearer warnings\nor errors.\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## `An error occurred while the 'sparklyr' package was updating the RStudio Connections pane`\n\nWhenever there is additional information in the Databricks Host value, as in, \nany parameters, a Connections pane issue is raised. This is because the \nHost needs to be the base url, with out any parameters\n\n```r\nSys.setenv(\"DATABRICKS_HOST\" = \"https://rstudio-partner-posit-default.cloud.databricks.com/o=notvalid\")\n\nsc <- spark_connect(\n        master = Sys.getenv(\"DATABRICKS_HOST\"),\n        cluster_id = \"[Your cluster ID]\",\n        method = \"databricks_connect\"\n        )\n# ! Retrieving version from cluster '0914-200113-rc8v8keb'\n# ... more info ...\n# Error in if (spark_connection_is_yarn(scon)) {: argument is of length zero\n# Error in     actions <- c(actions, list(YARN = list(icon = file.path(icons, : argument is of length zero\n# Error in         \"yarn-ui.png\"), callback = function() {: argument is of length zero\n# Error in         browse_url(spark_connection_yarn_ui(scon)): argument is of length zero\n# Error in     }))): argument is of length zero\n# Error in }: argument is of length zero\n# If necessary, these warnings can be squelched by setting `options(rstudio.connectionObserver.errorsSuppressed = TRUE)`.\n```\n\n### Solution\n\nRemove any parameters, and leading forward slashes, from the URL.  In the\npackage, we'll check for that going forward and provide clearer warnings\n\n:::\n\n### RStudio's Connection pane\n\nThanks to the new way we are integrating with Spark, it is now possible\nto display the same structure displayed in the Databricks Data Explorer\npage. In Databricks, the current data structure levels are:\n\n-   Catalog\n    -   Database\n        -   Table\n\nIn RStudio, you can navigate the data structure by expanding from the\ntop level, all the way down to the table you wish to explore. Once\nexpanded, the table's fields, and their types are displayed.\n\n![](/images/deployment/connect/rstudio-connection.png)\n\nIn the Connection Pane, you can click on the **table** icon, situated to\nthe right of the table name, to preview the first 1,000 rows:\n\n![](/images/deployment/connect/preview.png)\n\n### Using the Connection to Access Data\n\nAfter connecting, you can use `dbplyr`'s `in_catalog()` function to\naccess any table in your data catalog. You will only need to pass the\nrespective names of the three levels as comma separated character\nentries to `in_catalog()` in this order: Catalog, Database, and Table.\n\nHere is an example of using `tbl()` and `in_catalog()` to point to the\n**trips** table, which is inside **nyctaxi** database, which is a\ndatabase inside the **samples** catalog:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(dbplyr)\n\ntrips <- tbl(sc, in_catalog(\"samples\", \"nyctaxi\", \"trips\"))\n\ntrips\n#> # Source: spark<`samples`.`nyctaxi`.`trips`> [?? x 6]\n#>    tpep_pickup_datetime tpep_dropoff_datetime trip_distance fare_amount\n#>    <dttm>               <dttm>                        <dbl>       <dbl>\n#>  1 2016-02-14 10:52:13  2016-02-14 11:16:04            4.94        19  \n#>  2 2016-02-04 12:44:19  2016-02-04 12:46:00            0.28         3.5\n#>  3 2016-02-17 11:13:57  2016-02-17 11:17:55            0.7          5  \n#>  4 2016-02-18 04:36:07  2016-02-18 04:41:45            0.8          6  \n#>  5 2016-02-22 08:14:41  2016-02-22 08:31:52            4.51        17  \n#>  6 2016-02-05 00:45:02  2016-02-05 00:50:26            1.8          7  \n#>  7 2016-02-15 09:03:28  2016-02-15 09:18:45            2.58        12  \n#>  8 2016-02-25 13:09:26  2016-02-25 13:24:50            1.4         11  \n#>  9 2016-02-13 10:28:18  2016-02-13 10:36:36            1.21         7.5\n#> 10 2016-02-13 18:03:48  2016-02-13 18:10:24            0.6          6  \n#> # ℹ more rows\n#> # ℹ 2 more variables: pickup_zip <int>, dropoff_zip <int>\n```\n:::\n\n\nAfter pointing `tbl()` to that specific table, you can then use `dplyr`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrips %>%\n  group_by(pickup_zip) %>%\n  summarise(\n    count = n(),\n    avg_distance = mean(trip_distance, na.rm = TRUE)\n  )\n#> # Source: spark<?> [?? x 3]\n#>    pickup_zip count avg_distance\n#>         <int> <dbl>        <dbl>\n#>  1      10032    15         4.49\n#>  2      10013   273         2.98\n#>  3      10022   519         2.00\n#>  4      10162   414         2.19\n#>  5      10018  1012         2.60\n#>  6      11106    39         2.03\n#>  7      10011  1129         2.29\n#>  8      11103    16         2.75\n#>  9      11237    15         3.31\n#> 10      11422   429        15.5 \n#> # ℹ more rows\n```\n:::\n\n\nWhen you are done with you queries and computations, you should\ndisconnect from the cluster.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_disconnect(sc)\n```\n:::\n\n\n## Spark Connect\n\n### Initial setup\n\n`sparklyr` will need specific Python libraries in order to connect, and interact\nwith Spark Connect. We provide a convenience function that will automatically\ndo the following:\n\n- Create, or re-create, a Python environment. Based on your OS, it will choose\nto create a Virtual Environment, or Conda. \n\n- Install the needed Python libraries\n\nTo install the latest versions of all the libraries, use:\n\n```r\npysparklyr::install_pyspark()\n```\n\n`sparklyr` will query PyPi.org to get the latest version of PySpark\nand installs that version. It is recommended that the version of the\nPySpark library matches the Spark version of your cluster. \nTo do this, pass the Spark version in the  `version` argument, for example:\n\n```r\npysparklyr::install_pyspark(\"3.4\")\n```\n\nWe have seen Spark sessions crash, when the version of PySpark and the version\nof Spark do not match. Specially, when using a newer version of PySpark is used\nagainst an older version of Spark.  If you are having issues with your connection, \ndefinitely consider running the `install_pyspark()` to match that cluster's \nspecific Spark version.\n\n### Connecting\n\nTo start a session with a open source Spark cluster, via Spark Connect,\nyou will need to set the `master`, and `method`. The `master` will be an IP,\nand maybe a port that you will need to pass. The protocol to use to put\ntogether the proper connection URL is \"sc://\". For `method`, use\n\"spark_connect\". Here is an example:\n\n``` r\nlibrary(sparklyr)\n\nsc <- spark_connect(\n  master = \"sc://[Host IP(:Host Port - optional)]\", \n  method = \"spark_connect\"\n  version = \"[Version that matches your cluster]\"\n  )\n```\n\nIf `version` is not passed, then `sparklyr` will automatically choose the \ninstalled Python environment with the highest PySpark version. In a console \nmessage, `sparklyr` will let you know which environment it will use.\n\n### Run locally\n\nIt is possible to run Spark Connect in your machine We provide helper\nfunctions that let you setup, and start/stop the services in locally.\n\nIf you wish to try this out, first install Spark 3.4 or above:\n\n``` r\nspark_install(\"3.4\")\n```\n\nAfter installing, start the Spark Connect using:\n\n```r\npysparklyr::spark_connect_service_start()\n```\n\nTo connect to your local Spark Connect, use **localhost** as the address for \n`master`:\n\n\n```r\nsc <- spark_connect(\n  master = \"sc://localhost\", \n  method = \"spark_connect\"\n  )\n```\n\nNow, you are able to interact with your local Spark session:\n\n```r\nlibrary(dplyr)\n\ntbl_mtcars <- copy_to(sc, mtcars)\n\ntbl_mtcars %>% \n  group_by(am) %>% \n  summarise(mpg = mean(mpg, na.rm = TRUE))\n```\n\nWhen done, you can disconnect from Spark Connect:\n\n```r\nspark_disconnect(sc)\n```\n\nThe regular version of local Spark would terminate the local cluster\nwhen the you pass `spark_disconnect()`. For Spark Connect, the local\ncluster needs to be stopped independently.\n\n```r\npysparklyr::spark_connect_service_stop()\n```\n\n## What is supported\n\nHere is a list of what we currently support, and do not support via\n`sparklyr` and Connect:\n\n**Supported**:\n\n-   Integration with most of the `dplyr`, and `DBI`, APIs\n-   Integration with the `invoke()` command\n-   RStudio Connections Pane navigation\n-   Support for Personal Access Token security authentication for\n    Databricks Connect\n-   Support for most read and write commands. These have only been\n    tested in Spark Connect.\n\n**Not supported**:\n\n-   **ML functions** - All functions, in `sparklyr`, that have the `ml_`\n    and `ft_` are currently not supported. The reason is that Spark 3.4\n    does not currently support MLlib. We expect that some ML support\n    will be available in Spark 3.5. At that time we will work on\n    integrating the new ML routines from Connect into `sparklyr`.\n\n-   **SDF functions** - Most of these functions require SparkSession,\n    which is currently not supported in Spark 3.4.\n\n-   **`tidyr`** - This is ongoing work that we are focusing on in\n    `sparklyr`. We are implementing these functions using PySpark\n    DataFrame commands, instead of depending on the Scala\n    implementation.\n\nA more detailed list of our progress, which includes individual\nfunctions we have confirmed that work, do not work, or will work\ndifferently, is available here: [`pysparklyr`\nprogress](https://github.com/mlverse/pysparklyr/blob/main/progress.md)\n\n## Additional setup details\n\nIf you wish to use your own Python environment, then just make sure to\nload it before calling `spark_connect()`. If there is a Python\nenvironment already loaded when you connect to your Spark cluster, then\n`sparklyr` will use that environment instead. If you use your own Python\nenvironment you will need the following libraries installed:\n\n-   `pyspark`\n-   `pandas`\n-   `PyArrow`\n-   `grpcio`\n-   `google-api-python-client`\n-   `grpcio_status`\n-   `torch` *(Spark 3.5+)*\n-   `torcheval` *(Spark 3.5+)*\n\nIf connecting to Databrics Connect avoid installing `pyspark`, and install \nthese instead: \n\n-   `databricks-connect`\n-   `delta-spark`",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}