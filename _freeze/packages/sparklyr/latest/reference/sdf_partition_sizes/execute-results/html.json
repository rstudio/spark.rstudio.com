{
  "hash": "56b5274498ef9c57f75dd8a2488b6fe4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Compute the number of records within each partition of a Spark DataFrame\"\nexecute:\n  freeze: true\n---\n\n\n\n*R/sdf_interface.R*\n\n## sdf_partition_sizes\n\n## Description\n Compute the number of records within each partition of a Spark DataFrame \n\n\n## Usage\n```r\n \nsdf_partition_sizes(x) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | A `spark_connection`, `ml_pipeline`, or a `tbl_spark`. |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n \nlibrary(sparklyr) \nsc <- spark_connect(master = \"spark://HOST:PORT\") \nexample_sdf <- sdf_len(sc, 100L, repartition = 10L) \nexample_sdf %>% \n  sdf_partition_sizes() %>% \n  print() \n \n \n```\n:::\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}