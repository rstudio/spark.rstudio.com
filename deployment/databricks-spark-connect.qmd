---
title: "Spark Connect, and Databricks Connect v2"
format:
  html:
    theme: default
    toc: true
execute:
    eval: false
    freeze: false
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| include: false

library(remotes)

#install.packages("sparklyr")
#install_github("mlverse/pysparklyr")

library(dplyr)
library(dbplyr)
library(sparklyr)
library(pysparklyr)

# install_pyspark()
```

*Last updated: `r lubridate::date()`*

## Intro

[Spark
Connect](https://spark.apache.org/docs/latest/spark-connect-overview.html)
introduced a decoupled client-server architecture that allows remote
connectivity to Spark clusters using the DataFrame API. The separation
between client and server allows Spark to be leveraged from everywhere,
and this would allow R users to interact with a cluster from the comfort
of their preferred environment, laptop or otherwise. [Databricks
Connect](https://docs.databricks.com/dev-tools/databricks-connect.html),
available in Databricks Runtime version 13 and above, is based on this
new architecture.

We are working on enhancing `sparklyr` so that it can bring the benefits
of Spark Connect to the R user community.

## The Solution

Spark Connect requires the use of a remote procedure call framework
called *gRPC.* At this time, the Spark team provides two higher level APIs
that interact with *gRPC.* One is Scala based, and the other Python based.

We are using `reticulate` to interact with the Python API. `sparklyr` extends 
the functionality, and user experience, by providing the `dplyr`back-end, `DBI` 
back-end, RStudio's Connection pane integration.

::: {#fig-connect}
```{mermaid}
flowchart LR
  subgraph lp[User's machine]
    sr[sparklyr]
    rt[reticulate]
    ps[PySpark]
    g1[gRPC]
  end
  sp[Spark]
  
  sr --> rt
  rt --> ps
  g1 <-. Network .-> sp
  ps --> g1
  
  style sr  fill:#d0efb1,stroke:#666
  style rt  fill:#d0efb1,stroke:#666
  style ps  fill:#eff,stroke:#666
  style lp  fill:#fff,stroke:#666
  style sp  fill:#f4c430,stroke:#666
  style g1  fill:#447099,stroke:#666,color:#fff
```

How `sparklyr` communicates with Spark Connect, and Databricks Connect
:::

In order to quickly iterate on enhancements and bug fixes, we have
decided to isolate the Python integration into its own package. The new
package, called `pysparklyr`, is an extension of `sparklyr`.

## Package Installation

As mentioned above, we are still working integrating `sparklyr` with
Connect. To access the new capabilities, you will need `sparklyr` version
**1.8.3** or higher, and `pysparklyr`. To install use:

``` r
install.packages("sparklyr")
install.packages("pysparklyr")
```

## Databricks Connect (ML 13+)

### Initial setup

`sparklyr` will need specific Python libraries in order to connect, and interact
with Databricks Connect. We provide a convenience function that will automatically
do the following:

- Create, or re-create, a Python environment. Based on your OS, it will choose
to create a Virtual Environment, or Conda. 

- Install the needed Python libraries

::: {.callout-important} 
## WINDOWS USERS PLEASE READ

Some issues have been reported using **older** CRAN version of `reticulate`.
Please make sure to have the latest version installed. At this time that
version number is **1.34.0**. 

:::

To install the latest versions of all the libraries, use:

```r
pysparklyr::install_databricks()
```

`sparklyr` will query PyPi.org to get the latest version of `databricks.connect`
and installs that version. It is recommended that the version of the 
`databricks.connect` library, matches  the DBR version of your cluster.  
To do this, pass the DBR version in the `version` argument, for example:

```r
pysparklyr::install_databricks("14.0")
```

This will create a Python environment and install `databricks.connect` version
14.0, and it will automatically name it `r-sparklyr-databricks-14.0`.  By using
this name, `sparklyr` is able to know what version of `databricks.connect` is
available inside this particular Python environment.

If you are not sure about the version of the cluster you want to interact with,
then use the `cluster_id` argument. We have added a way to pull the cluster's
information, without starting a Spark connect. This allows us to query the
cluster and get the DBR version:

```r
pysparklyr::install_databricks(cluster_id = "[Your cluster's ID]")
```


**Important** -  This step needs only to be **done one time**. If you need to connect to a different
cluster, but that has the same DBR version, `sparklyr` will use the same
Python environment.  If the new cluster has a different DBR version, then it is
recommended that you run the installation function using the new DBR version, or
cluster ID.


### Connecting

To use with Databricks Connect, in run-time 13 or above, you will need
three configuration items:

-   Your [Workspace Instance
    URL](https://docs.databricks.com/workspace/workspace-details.html#workspace-url)
-   The ID of a currently running
    [cluster](https://docs.databricks.com/workspace/workspace-details.html#cluster-url-and-id)
    within your workspace
-   Authentication setup. At this time, `sparklyr` supports
    [PAT](https://docs.databricks.com/dev-tools/auth.html#pat)

In `spark_connect()` those items can be passed as function arguments.
Also note that to let `sparklyr` know that you are attempting to use
Databricks Connect, use `method = databricks_connect`:

``` r
library(sparklyr)

sc <- spark_connect(
  master = "[Your Workspace Instance URL]", 
  cluster_id = "[Cluster ID]"
  token = "[Your PAT - Please do not include 
           it as plain text here,  please refer 
           to the next section]",
  method = "databricks_connect"
  )
```

`sparklyr` will get retrieve the cluster's DBR version, and automatically choose
the Python environment that has the matching version of `databricks.connect`. If
it does not find a matching version, it will use the most recent version installed
in your machine. If you wish, you can specify the version by setting the
`dbr_version` argument. 

```r
sc <- spark_connect(
  cluster_id = "[Cluster ID]",
  method = "databricks_connect",
  dbr_version = "14.0"
  )
```



For cases when the Python environment created by `install_databricks()` will not
be used, then pass it to `spark_connect()` via the `envname` argument:

```r
sc <- spark_connect(
  cluster_id = "[Cluster ID]",
  method = "databricks_connect",
  envname = "[your Python environment's name]"
  )
```

### Safely connect

For your safety and convenience, you can save your authentication token
(PAT), and your Workspace Instance URL in a environment variable. The
two variables are:

-   `DATABRICKS_TOKEN`
-   `DATABRICKS_HOST`

This will prevent your token to be shown in plain text in your code. You
can set the environment variables at the beginning of your R session
with `Sys.setenv()`.

Preferably, the two variables can be set for all R sessions by saving
them to the **.Renviron** file. The `usethis` package has a handy
function that opens the file so you can edit it:
`usethis::edit_r_environ()`. Then simply append the two entries to your
**.Renviron** file. Once you are done, save the **.Renviron** file, and
restart R.

After that, use `spark_connect()` to open the connection to Databricks.
You will need to only pass the `cluster_id` of your cluster, and tell
`sparklyr` that you are connecting to a Spark cluster with run time 13
or above:

``` {r}
library(sparklyr)

sc <- spark_connect(
  cluster_id = "0914-200113-rc8v8keb",
  method = "databricks_connect"
)
```

`sparklyr` will provide console message while initiating the connection. As
seen above, we are informed that `sparklyr` is getting the DBR version from the
cluster, the version returned by the cluster, and which Python environment 
it used to connect.

### Restricted Python environments 

If your organization restricts Python environment creation, you can 
point `sparklyr` to the designated Python installation.  To do this, you will 
have to set the `RETICULATE_PYTHON` environment variable, and **before** 
attempting to connect to the cluster: 

```r
library(sparklyr)

Sys.setenv("RETICULATE_PYTHON" = "[path to the desired python path]")

sc <- spark_connect(
  method = "databricks_connect",
  cluster_id = "[your cluster id]"
  )
```

To successfully connect to a Databricks cluster, you will need to match the
proper version of the `databricks.connect` Python library, to the Databricks
Runtime (DBR) version in the cluster. For example, if you are trying to use a
Databricks cluster with a DBR version 14.0, then `databricks.connect` will also
need to be version 14.0. Failure to do so, can result in instability, or even
the inability to connect. 

Besides `datbricks.connect`, the Python environment will also need to have other
Python libraries installed. The full list is in the [Additional setup details](#additional-setup-details)
section.

:::{.callout-important}
If your server, or machine, has only one Python installation and, no ability to create
Conda or Virtual environments, then you will encounter issues when connecting to
a Databricks cluster with a mismatched version of `databricks.connect` to DBR.
:::

Additionally, avoid setting `RETICULATE_PYTHON` for the entire server (`Rprofile.site`).
The same goes for your user profile level (`Home\.Renviron`).  For example, 
If you are working on two projects, using two different Databricks clusters, 
and with two different Databricks Runtime (DBR) versions, will mean that you 
will need two different versions of `databricks.connect`. The recommendation is 
to set the `.Renviron` file at the project level, or to set it at the beginning 
of your R session via the code shown above. 

### Reported Problems

As it is with any nascent implementations, we are seeing some early adopters report
issues with their installation or connections. 

Here are some of the issues that have been reported to us, and the recommended
solution, or workaround. Each issue is **collapsed**, just expand it to see the
background and recommendation. Each one is titled based on the full error message 
or, the distinctive part of the error message: 

::: {.callout-note collapse="true"}
## `...reticulate can only bind to copies of Python built with '--enable-shared'.`

Error message contains:

```r
... reticulate can only bind to copies of Python built with '--enable-shared'.
```

This is happening because there is no Python installation that `reticulate` 
can use. `pysparklyr` depends on that package to function. 

### Solution

The best way to resolve is to install an acceptable version of Python. You can run:

```r
reticulate::install_python()
```

If the output mentions `--skip-existing [version number]` then you will have two options:

1. If you're ok with replacing the existing Python, then use:
   ```r
    reticulate::install_python(version = "[version number]", force = TRUE)
   ``` 

2. If you would like to keep that installation of Python then use a different version number:
   ```r
    reticulate::install_python(version = "[slightly different version number]")
   ``` 
    For example, if the version number is `3.9.12`, then use `3.9.18`: 
  
    ```r
    reticulate::install_python(version = "3.9.18")
    ``` 
:::

::: {.callout-note collapse="true"}
## `Error: Unable to find conda binary. Is Anaconda installed?`

When trying to connect, this error is returned:

```r
Error: Unable to find conda binary. Is Anaconda installed?
```

Here is a code example when this error will generally show up:

```r
library(sparklyr)
sc <- spark_connect(cluster_id="xxxx", method = "databricks_connect")
## ! Retrieving version from cluster 'xxxx'
## ✔ Cluster version: '14.0'
## Error: Unable to find conda binary. Is Anaconda installed?
```

### Solution

This is an issue with `pysparklyr` in CRAN, there is a fix for it currently 
in the dev version (`0.1.9000` and above). Run the following to install:

```r
install.packages("pak")
pak::pak("mlverse/pysparklyr")
```
:::

::: {.callout-note collapse="true"}
## `Error in envs[[1]] : subscript out of bounds`

This is because there is no "standardly named" virtual environment is found. 

```r
> sc <- spark_connect(method = "databricks_connect")
Error in envs[[1]] : subscript out of bounds
```

### Solution

To resolve, there are 2 options:

1. Run `pysparklyr::install_databricks(cluster_id = "[your cluster's id]")`
2. If you want to use a environment you've setup, pass it as part of your connection code: `sc <- spark_connect(method = "databricks_connect", envname = "my-customer-environment"))`

In `pysaprklyr`, we'll add a message that says: "no environment was found, and to please use `envname`
:::

::: {.callout-note collapse="true"}
## `! Version '' does not exist`

When you try to install Databricks using the cluster ID, and the Host ID is invalid, this error appears:

```r
> Sys.setenv("DATABRICKS_HOST" = "https://rstudio-partner-posit-default.cloud.databricks.com/o=notvalid")
> pysparklyr::install_databricks(cluster_id = "0914-200113-rc8v8keb")
! Retrieving version from cluster '0914-200113-rc8v8keb'
✔ Checking if provided version is valid against PyPi.org
Error in `install_environment()`:
! Version '' does not exist
Run `rlang::last_trace()` to see where the error occurred.
```

### Solution

Remove any parameters, and leading forward slashes, from the URL. 

In the future, we'll check for that going forward and provide clearer warnings
or errors.

:::

::: {.callout-note collapse="true"}
## `An error occurred while the 'sparklyr' package was updating the RStudio Connections pane`

Whenever there is additional information in the Databricks Host value, as in, 
any parameters, a Connections pane issue is raised. This is because the 
Host needs to be the base url, with out any parameters

```r
Sys.setenv("DATABRICKS_HOST" = "https://rstudio-partner-posit-default.cloud.databricks.com/o=notvalid")

sc <- spark_connect(
        master = Sys.getenv("DATABRICKS_HOST"),
        cluster_id = "[Your cluster ID]",
        method = "databricks_connect"
        )
# ! Retrieving version from cluster '0914-200113-rc8v8keb'
# ... more info ...
# Error in if (spark_connection_is_yarn(scon)) {: argument is of length zero
# Error in     actions <- c(actions, list(YARN = list(icon = file.path(icons, : argument is of length zero
# Error in         "yarn-ui.png"), callback = function() {: argument is of length zero
# Error in         browse_url(spark_connection_yarn_ui(scon)): argument is of length zero
# Error in     }))): argument is of length zero
# Error in }: argument is of length zero
# If necessary, these warnings can be squelched by setting `options(rstudio.connectionObserver.errorsSuppressed = TRUE)`.
```

### Solution

Remove any parameters, and leading forward slashes, from the URL.  In the
package, we'll check for that going forward and provide clearer warnings

:::

### RStudio's Connection pane

Thanks to the new way we are integrating with Spark, it is now possible
to display the same structure displayed in the Databricks Data Explorer
page. In Databricks, the current data structure levels are:

-   Catalog
    -   Database
        -   Table

In RStudio, you can navigate the data structure by expanding from the
top level, all the way down to the table you wish to explore. Once
expanded, the table's fields, and their types are displayed.

![](/images/deployment/connect/rstudio-connection.png)

In the Connection Pane, you can click on the **table** icon, situated to
the right of the table name, to preview the first 1,000 rows:

![](/images/deployment/connect/preview.png)

### Using the Connection to Access Data

After connecting, you can use `dbplyr`'s `in_catalog()` function to
access any table in your data catalog. You will only need to pass the
respective names of the three levels as comma separated character
entries to `in_catalog()` in this order: Catalog, Database, and Table.

Here is an example of using `tbl()` and `in_catalog()` to point to the
**trips** table, which is inside **nyctaxi** database, which is a
database inside the **samples** catalog:

```{r}
library(dplyr)
library(dbplyr)

trips <- tbl(sc, in_catalog("samples", "nyctaxi", "trips"))

trips
```

After pointing `tbl()` to that specific table, you can then use `dplyr`.

```{r}
trips %>%
  group_by(pickup_zip) %>%
  summarise(
    count = n(),
    avg_distance = mean(trip_distance, na.rm = TRUE)
  )
```

When you are done with you queries and computations, you should
disconnect from the cluster.

```{r}
spark_disconnect(sc)
```

## Spark Connect

### Initial setup

`sparklyr` will need specific Python libraries in order to connect, and interact
with Spark Connect. We provide a convenience function that will automatically
do the following:

- Create, or re-create, a Python environment. Based on your OS, it will choose
to create a Virtual Environment, or Conda. 

- Install the needed Python libraries

To install the latest versions of all the libraries, use:

```r
pysparklyr::install_pyspark()
```

`sparklyr` will query PyPi.org to get the latest version of PySpark
and installs that version. It is recommended that the version of the
PySpark library matches the Spark version of your cluster. 
To do this, pass the Spark version in the  `version` argument, for example:

```r
pysparklyr::install_pyspark("3.4")
```

We have seen Spark sessions crash, when the version of PySpark and the version
of Spark do not match. Specially, when using a newer version of PySpark is used
against an older version of Spark.  If you are having issues with your connection, 
definitely consider running the `install_pyspark()` to match that cluster's 
specific Spark version.

### Connecting

To start a session with a open source Spark cluster, via Spark Connect,
you will need to set the `master`, and `method`. The `master` will be an IP,
and maybe a port that you will need to pass. The protocol to use to put
together the proper connection URL is "sc://". For `method`, use
"spark_connect". Here is an example:

``` r
library(sparklyr)

sc <- spark_connect(
  master = "sc://[Host IP(:Host Port - optional)]", 
  method = "spark_connect"
  version = "[Version that matches your cluster]"
  )
```

If `version` is not passed, then `sparklyr` will automatically choose the 
installed Python environment with the highest PySpark version. In a console 
message, `sparklyr` will let you know which environment it will use.

### Run locally

It is possible to run Spark Connect in your machine We provide helper
functions that let you setup, and start/stop the services in locally.

If you wish to try this out, first install Spark 3.4 or above:

``` r
spark_install("3.4")
```

After installing, start the Spark Connect using:

```r
pysparklyr::spark_connect_service_start()
```

To connect to your local Spark Connect, use **localhost** as the address for 
`master`:


```r
sc <- spark_connect(
  master = "sc://localhost", 
  method = "spark_connect"
  )
```

Now, you are able to interact with your local Spark session:

```r
library(dplyr)

tbl_mtcars <- copy_to(sc, mtcars)

tbl_mtcars %>% 
  group_by(am) %>% 
  summarise(mpg = mean(mpg, na.rm = TRUE))
```

When done, you can disconnect from Spark Connect:

```r
spark_disconnect(sc)
```

The regular version of local Spark would terminate the local cluster
when the you pass `spark_disconnect()`. For Spark Connect, the local
cluster needs to be stopped independently.

```r
pysparklyr::spark_connect_service_stop()
```

## What is supported

Here is a list of what we currently support, and do not support via
`sparklyr` and Connect:

**Supported**:

-   Integration with most of the `dplyr`, and `DBI`, APIs
-   Integration with the `invoke()` command
-   RStudio Connections Pane navigation
-   Support for Personal Access Token security authentication for
    Databricks Connect
-   Support for most read and write commands. These have only been
    tested in Spark Connect.

**Not supported**:

-   **ML functions** - All functions, in `sparklyr`, that have the `ml_`
    and `ft_` are currently not supported. The reason is that Spark 3.4
    does not currently support MLlib. We expect that some ML support
    will be available in Spark 3.5. At that time we will work on
    integrating the new ML routines from Connect into `sparklyr`.

-   **SDF functions** - Most of these functions require SparkSession,
    which is currently not supported in Spark 3.4.

-   **`tidyr`** - This is ongoing work that we are focusing on in
    `sparklyr`. We are implementing these functions using PySpark
    DataFrame commands, instead of depending on the Scala
    implementation.

A more detailed list of our progress, which includes individual
functions we have confirmed that work, do not work, or will work
differently, is available here: [`pysparklyr`
progress](https://github.com/mlverse/pysparklyr/blob/main/progress.md)

## Additional setup details

If you wish to use your own Python environment, then just make sure to
load it before calling `spark_connect()`. If there is a Python
environment already loaded when you connect to your Spark cluster, then
`sparklyr` will use that environment instead. If you use your own Python
environment you will need the following libraries installed:

-   `pyspark`
-   `pandas`
-   `PyArrow`
-   `grpcio`
-   `google-api-python-client`
-   `grpcio_status`
-   `torch` *(Spark 3.5+)*
-   `torcheval` *(Spark 3.5+)*

If connecting to Databrics Connect avoid installing `pyspark`, and install 
these instead: 

-   `databricks-connect`
-   `delta-spark`