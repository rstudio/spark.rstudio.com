---
title: "Intro to Spark Streaming with sparklyr"
---



<div id="the-sparklyr-interface" class="section level2">
<h2>The <code>sparklyr</code> interface</h2>
<p>As stated in the Spark’s official site, <a href="(https://spark.apache.org/streaming/)">Spark Streaming</a> makes it easy to build scalable fault-tolerant streaming applications. Because is part of the Spark API, it is possible to re-use query code that queries the current state of the stream, as well as joining the streaming data with historical data. Please see <a href="https://spark.apache.org/docs/2.1.3/structured-streaming-programming-guide.html">Spark’s official documentation</a> for a deeper look into Spark Streaming.</p>
<p>The <code>sparklyr</code> interface provides the following:</p>
<ul>
<li>Ability to run <a href="/dplyr">dplyr</a>, SQL, <a href="/guides/distributed-r/">spark_apply()</a>, and <a href="/guides/pipelines/#introduction-to-ml-pipelines">PipelineModels</a> against a stream</li>
<li>Read in multiple formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc</li>
<li>Write stream results to Spark memory and the following file formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc</li>
<li>An out-of-the box <a href="#example-1---inputoutput">graph visualization</a> to monitor the stream</li>
<li>A new <a href="#example-4---shiny-integration">reactiveSpark()</a> function, that allows Shiny apps to poll the contents of the stream
create Shiny apps that are able to read the contents of the stream</li>
</ul>
</div>
<div id="interacting-with-a-stream" class="section level2">
<h2>Interacting with a stream</h2>
<p>A good way of looking at the way how Spark streams update is as a three stage operation:</p>
<ol style="list-style-type: decimal">
<li><strong>Input</strong> - Spark reads the data inside a given folder. The folder is expected to contain multiple data files, with new files being created containing the most current stream data.</li>
<li><strong>Processing</strong> - Spark applies the desired operations on top of the data. These operations could be data manipulations (<code>dplyr</code>, SQL), data transformations (<code>sdf</code> operations, PipelineModel predictions), or native R manipulations (<code>spark_apply()</code>).</li>
<li><strong>Output</strong> - The results of processing the input files are saved in a different folder.</li>
</ol>
<p>In the same way all of the read and write operations in <code>sparklyr</code> for Spark Standalone, or in <code>sparklyr</code>’s local mode, the input and output folders are actual OS file system folders. For Hadoop clusters, these will be folder locations inside the HDFS.</p>
</div>
<div id="example-1---inputoutput" class="section level2">
<h2>Example 1 - Input/Output</h2>
<p>The first intro example is a small script that can be used with a local master. The result should be to see the <code>stream_view()</code> app showing live the number of records processed for each iteration of test data being sent to the stream.</p>
<pre class="r"><code>library(future)
library(sparklyr)

sc &lt;- spark_connect(master = &quot;local&quot;, spark_version = &quot;2.3.0&quot;)

if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE)
if(file.exists(&quot;source-out&quot;)) unlink(&quot;source-out&quot;, TRUE)

stream_generate_test(iterations = 1)
read_folder &lt;- stream_read_csv(sc, &quot;source&quot;) 
write_output &lt;- stream_write_csv(read_folder, &quot;source-out&quot;)
invisible(future(stream_generate_test(interval = 0.5)))

stream_view(write_output)</code></pre>
<center>
<p>
<img src="/guides/streaming/stream_view.png" align='center' width = 500/>
</p>
</center>
<pre class="r"><code>stream_stop(write_output)
spark_disconnect(sc)</code></pre>
<div id="code-breakdown" class="section level3">
<h3>Code breakdown</h3>
<ol style="list-style-type: decimal">
<li><p>Open the Spark connection</p>
<pre class="r"><code>library(sparklyr)
sc &lt;- spark_connect(master = &quot;local&quot;, spark_version = &quot;2.3.0&quot;)</code></pre></li>
<li><p>Optional step. This resets the input and output folders. It makes it easier to run the code multiple times in a clean manner.</p>
<pre class="r"><code>if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE)
if(file.exists(&quot;source-out&quot;)) unlink(&quot;source-out&quot;, TRUE)</code></pre></li>
<li><p>Produces a single test file inside the “source” folder. This allows the “read” function to infer CSV file definition.</p>
<pre class="r"><code>stream_generate_test(iterations = 1)
list.files(&quot;source&quot;)</code></pre>
<pre><code>[1] &quot;stream_1.csv&quot;</code></pre></li>
<li><p>Points the stream reader to the folder where the streaming files will be placed. Since it is primed with a single CSV file, it will use as the expected layout of subsequent files. By default, <code>stream_read_csv()</code> creates a single integer variable data frame.</p>
<pre class="r"><code>read_folder &lt;- stream_read_csv(sc, &quot;source&quot;)</code></pre></li>
<li><p><strong>The output writer is what starts the streaming job</strong>. It will start monitoring the input folder, and then write the new results in the “source-out” folder. So as new records stream in, new files will be created in the “source-out” folder. Since there are no operations on the incoming data at this time, the output files will have the same exact raw data as the input files. The only difference is that the files and sub folders within “source-out” will be structured how Spark structures data folders.</p>
<pre class="r"><code>write_output &lt;- stream_write_csv(read_folder, &quot;source-out&quot;)
list.files(&quot;source-out&quot;)</code></pre>
<pre><code>[1] &quot;_spark_metadata&quot;                                     &quot;checkpoint&quot;
[3] &quot;part-00000-1f29719a-2314-40e1-b93d-a647a3d57154-c000.csv&quot;</code></pre></li>
<li><p>The test generation function will run 100 files every 0.2 seconds. To run the tests “out-of-sync” with the current R session, the <code>future</code> package is used.</p>
<pre class="r"><code>library(future)
invisible(future(stream_generate_test(interval = 0.2, iterations = 100)))</code></pre></li>
<li><p>The <code>stream_view()</code> function can be used before the 50 tests are complete because of the use of the <code>future</code> package. It will monitor the status of the job that <code>write_output</code> is pointing to and provide information on the amount of data coming into the “source” folder and going out into the “source-out” folder.</p>
<pre class="r"><code>stream_view(write_output)</code></pre></li>
<li><p>The monitor will continue to run even after the tests are complete. To end the experiment, stop the Shiny app and then use the following to stop the stream and close the Spark session.</p>
<pre class="r"><code>stream_stop(write_output)
spark_disconnect(sc)</code></pre></li>
</ol>
</div>
</div>
<div id="example-2---processing" class="section level2">
<h2>Example 2 - Processing</h2>
<p>The second example builds on the first. It adds a processing step that manipulates the input data before saving it to the output folder. In this case, a new binary field is added indicating if the value from <code>x</code> is over 400 or not. This time, while run the second code chunk in this example a few times during the stream tests to see the aggregated values change.</p>
<pre class="r"><code>library(future)
library(sparklyr)
library(dplyr, warn.conflicts = FALSE)

sc &lt;- spark_connect(master = &quot;local&quot;, spark_version = &quot;2.3.0&quot;)

if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE)
if(file.exists(&quot;source-out&quot;)) unlink(&quot;source-out&quot;, TRUE)

stream_generate_test(iterations = 1)
read_folder &lt;- stream_read_csv(sc, &quot;source&quot;) 

process_stream &lt;- read_folder %&gt;%
  mutate(x = as.double(x)) %&gt;%
  ft_binarizer(
    input_col = &quot;x&quot;,
    output_col = &quot;over&quot;,
    threshold = 400
  )

write_output &lt;- stream_write_csv(process_stream, &quot;source-out&quot;)
invisible(future(stream_generate_test(interval = 0.2, iterations = 100)))</code></pre>
<p>Run this code a few times during the experiment:</p>
<pre class="r"><code>spark_read_csv(sc, &quot;stream&quot;, &quot;source-out&quot;, memory = FALSE) %&gt;%
  group_by(over) %&gt;%
  tally()</code></pre>
<p>The results would look similar to this. The <code>n</code> totals will increase as the experiment progresses.</p>
<pre><code># Source:   lazy query [?? x 2]
# Database: spark_connection
   over     n
  &lt;dbl&gt; &lt;dbl&gt;
1     0 40215
2     1 60006</code></pre>
<p>Clean up after the experiment</p>
<pre class="r"><code>stream_stop(write_output)
spark_disconnect(sc)</code></pre>
<div id="code-breakdown-1" class="section level3">
<h3>Code breakdown</h3>
<ol style="list-style-type: decimal">
<li><p>The processing starts with the <code>read_folder</code> variable that contains the input stream. It coerces the integer field <code>x</code>, into a type double. This is because the next function, <code>ft_binarizer()</code> does not accept integers. The binarizer determines if <code>x</code> is over 400 or not. This is a good illustration of how <code>dplyr</code> can help simplify the manipulation needed during the processing stage.</p>
<pre class="r"><code>process_stream &lt;- read_folder %&gt;%
  mutate(x = as.double(x)) %&gt;%
  ft_binarizer(
    input_col = &quot;x&quot;,
    output_col = &quot;over&quot;,
    threshold = 400
  )</code></pre></li>
<li><p>The output now needs to write-out the processed data instead of the raw input data. Swap <code>read_folder</code> with <code>process_stream</code>.</p>
<pre class="r"><code>write_output &lt;- stream_write_csv(process_stream, &quot;source-out&quot;)</code></pre></li>
<li><p>The “source-out” folder can be treated as a if it was a single table within Spark. Using <code>spark_read_csv()</code>, the data can be mapped, but not brought into memory (<code>memory = FALSE</code>). This allows the current results to be further analyzed using regular <code>dplyr</code> commands.</p>
<pre class="r"><code>spark_read_csv(sc, &quot;stream&quot;, &quot;source-out&quot;, memory = FALSE) %&gt;%
  group_by(over) %&gt;%
  tally()</code></pre></li>
</ol>
</div>
</div>
<div id="example-3---aggregate-in-process-and-output-to-memory" class="section level2">
<h2>Example 3 - Aggregate in process and output to memory</h2>
<p>Another option is to save the results of the processing into a in-memory Spark table. Unless intentionally saving it to disk, the table and its data will only exist while the Spark session is active.</p>
<p>The biggest advantage of using Spark memory as the target, is that it will allow for aggregation to happen during processing. This is an advantage because <em>aggregation is not allowed for any file output, expect Kafka, on the input/process stage</em>.</p>
<p>Using example 2 as the base, this example code will perform some aggregations to the current stream input and save only those summarized results into Spark memory:</p>
<pre class="r"><code>library(future)
library(sparklyr)
library(dplyr, warn.conflicts = FALSE)

sc &lt;- spark_connect(master = &quot;local&quot;, spark_version = &quot;2.3.0&quot;)

if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE)

stream_generate_test(iterations = 1)
read_folder &lt;- stream_read_csv(sc, &quot;source&quot;) 

process_stream &lt;- read_folder %&gt;%
  stream_watermark() %&gt;%
  group_by(timestamp) %&gt;%
  summarise(
    max_x = max(x, na.rm = TRUE),
    min_x = min(x, na.rm = TRUE),
    count = n()
  )

write_output &lt;- stream_write_memory(process_stream, name = &quot;stream&quot;)

invisible(future(stream_generate_test()))</code></pre>
<p>Run this command a different times while the experiment is running:</p>
<pre class="r"><code>tbl(sc, &quot;stream&quot;) </code></pre>
<p>Clean up after the experiment</p>
<pre class="r"><code>stream_stop(write_output)
spark_disconnect(sc)</code></pre>
<div id="code-breakdown-2" class="section level3">
<h3>Code breakdown</h3>
<ol style="list-style-type: decimal">
<li><p>The <code>stream_watermark()</code> functions add a new <code>timestamp</code> variable that is then used in the <code>group_by()</code> command. This is required by Spark Stream to accept summarized results as output of the stream. The second step is to simply decide what kinds of aggregations we need to perform. In this case, a simply max, min and count are performed.</p>
<pre class="r"><code>process_stream &lt;- read_folder %&gt;%
  stream_watermark() %&gt;%
  group_by(timestamp) %&gt;%
  summarise(
    max_x = max(x, na.rm = TRUE),
    min_x = min(x, na.rm = TRUE),
    count = n()
  )</code></pre></li>
<li><p>The <code>spark_write_memory()</code> function is used to write the output to Spark memory. The results will appear as a table of the Spark session with the name assigned in the <code>name</code> argument, in this case the name selected is: “stream”.</p>
<pre class="r"><code>write_output &lt;- stream_write_memory(process_stream, name = &quot;stream&quot;)</code></pre></li>
<li><p>To query the current data in the “stream” table can be queried by using the <code>dplyr</code> <code>tbl()</code> command.</p>
<pre class="r"><code>tbl(sc, &quot;stream&quot;) </code></pre></li>
</ol>
</div>
</div>
<div id="example-4---shiny-integration" class="section level2">
<h2>Example 4 - Shiny integration</h2>
<p><code>sparklyr</code> provides a new Shiny function called <code>reactiveSpark()</code>. It can take a Spark data frame, in this case the one created as a result of the stream processing, and then creates a Spark memory stream table, the same way a table is created in example 3.</p>
<pre class="r"><code>library(future)
library(sparklyr)
library(dplyr, warn.conflicts = FALSE)
library(ggplot2)

sc &lt;- spark_connect(master = &quot;local&quot;, spark_version = &quot;2.3.0&quot;)

if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE)
if(file.exists(&quot;source-out&quot;)) unlink(&quot;source-out&quot;, TRUE)

stream_generate_test(iterations = 1)
read_folder &lt;- stream_read_csv(sc, &quot;source&quot;) 

process_stream &lt;- read_folder %&gt;%
  stream_watermark() %&gt;%
  group_by(timestamp) %&gt;%
  summarise(
    max_x = max(x, na.rm = TRUE),
    min_x = min(x, na.rm = TRUE),
    count = n()
  )

invisible(future(stream_generate_test(interval = 0.2, iterations = 100)))

library(shiny)
ui &lt;- function(){
  tableOutput(&quot;table&quot;)
}
server &lt;- function(input, output, session){
  
  ps &lt;- reactiveSpark(process_stream)
  
  output$table &lt;- renderTable({
    ps() %&gt;%
      mutate(timestamp = as.character(timestamp)) 
    })
}
runGadget(ui, server)</code></pre>
<div id="code-breakdown-3" class="section level3">
<h3>Code breakdown</h3>
<ol style="list-style-type: decimal">
<li><p>Notice that there is no <code>stream_write_...</code> command. The reason is that <code>reactiveSpark()</code> function contains the <code>stream_write_memory()</code> function.</p></li>
<li><p>This very basic Shiny app simply displays the output of a table in the <code>ui</code> section</p>
<pre class="r"><code>library(shiny)

ui &lt;- function(){
  tableOutput(&quot;table&quot;)
}</code></pre></li>
<li><p>In the <code>server</code> section, the <code>reactiveSpark()</code> function will update every time there’s a change to the stream and return a data frame. The results are saved to a variable called <code>ps()</code> in this script. Treat the <code>ps()</code> variable as a regular table that can be piped from, as shown in the example. In this case, the <code>timestamp</code> variable is converted to string for to make it easier to read.</p>
<pre class="r"><code>server &lt;- function(input, output, session){

  ps &lt;- reactiveSpark(process_stream)

  output$table &lt;- renderTable({
    ps() %&gt;%
      mutate(timestamp = as.character(timestamp)) 
  })
}</code></pre></li>
<li><p>Use <code>runGadget()</code> to display the Shiny app in the Viewer pane. This is optional, the app can be run using normal Shiny run functions.</p>
<pre class="r"><code>runGadget(ui, server)</code></pre></li>
</ol>
</div>
</div>
<div id="example-5---ml-pipeline-model" class="section level2">
<h2>Example 5 - ML Pipeline Model</h2>
<p>This example uses a fitted <a href="/guides/pipelines/">Pipeline Model</a> to process the input, and saves the predictions to the output. This approach would be used to apply Machine Learning on top of streaming data.</p>
<pre class="r"><code>library(sparklyr)
library(dplyr, warn.conflicts = FALSE)

sc &lt;- spark_connect(master = &quot;local&quot;, spark_version = &quot;2.3.0&quot;)

if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE)
if(file.exists(&quot;source-out&quot;)) unlink(&quot;source-out&quot;, TRUE)

df &lt;- data.frame(x = rep(1:1000), y = rep(2:1001))

stream_generate_test(df = df, iteration = 1)

model_sample &lt;- spark_read_csv(sc, &quot;sample&quot;, &quot;source&quot;)

pipeline &lt;- sc %&gt;%
  ml_pipeline() %&gt;%
  ft_r_formula(x ~ y) %&gt;%
  ml_linear_regression()

fitted_pipeline &lt;- ml_fit(pipeline, model_sample)

ml_stream &lt;- stream_read_csv(
    sc = sc, 
    path = &quot;source&quot;, 
    columns = c(x = &quot;integer&quot;, y = &quot;integer&quot;)
  )  %&gt;%
  ml_transform(fitted_pipeline, .)  %&gt;%
  select(- features) %&gt;%
  stream_write_csv(&quot;source-out&quot;)

stream_generate_test(df = df, interval = 0.5)</code></pre>
<pre class="r"><code>spark_read_csv(sc, &quot;stream&quot;, &quot;source-out&quot;, memory = FALSE) </code></pre>
<pre><code># Source: spark&lt;stream&gt; [?? x 4]
       x     y label prediction
 * &lt;int&gt; &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt;
 1   276   277   276       276.
 2   277   278   277       277.
 3   278   279   278       278.
 4   279   280   279       279.
 5   280   281   280       280.
 6   281   282   281       281.
 7   282   283   282       282.
 8   283   284   283       283.
 9   284   285   284       284.
10   285   286   285       285.
# ... with more rows</code></pre>
<pre class="r"><code>stream_stop(ml_stream)
spark_disconnect(sc)</code></pre>
<div id="code-breakdown-4" class="section level3">
<h3>Code Breakdown</h3>
<ol style="list-style-type: decimal">
<li><p>Creates and fits a pipeline</p>
<pre class="r"><code>df &lt;- data.frame(x = rep(1:1000), y = rep(2:1001))
stream_generate_test(df = df, iteration = 1)
model_sample &lt;- spark_read_csv(sc, &quot;sample&quot;, &quot;source&quot;)

pipeline &lt;- sc %&gt;%
  ml_pipeline() %&gt;%
  ft_r_formula(x ~ y) %&gt;%
  ml_linear_regression()

fitted_pipeline &lt;- ml_fit(pipeline, model_sample)</code></pre></li>
<li><p>This example pipelines the input, process and output in a single code segment. The <code>ml_transform()</code> function is used to create the predictions. Because the CSV format does not support <em>list</em> type fields, the <code>features</code> column is removed before the results are sent to the output.</p>
<pre class="r"><code>ml_stream &lt;- stream_read_csv(
    sc = sc, 
    path = &quot;source&quot;, 
    columns = c(x = &quot;integer&quot;, y = &quot;integer&quot;)
  )  %&gt;%
  ml_transform(fitted_pipeline, .)  %&gt;%
  select(- features) %&gt;%
  stream_write_csv(&quot;source-out&quot;)</code></pre></li>
</ol>
</div>
</div>
