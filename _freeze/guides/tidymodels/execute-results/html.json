{
  "hash": "c3e57856815cb7983934f8c8bb4d8bf2",
  "result": {
    "markdown": "---\ntitle: \"`tidymodels` and Spark\"\nexecute:\n  eval: false\n  freeze: true\n---\n\n\n\n## Intro\n\n`tidymodels` is a collection of packages for modeling and Machine Learning. Just as `sparklyr`, `tidymodels` uses `tidyverse` principles.\n\n`sparklyr` allows us to use `dplyr` verbs to manipulate data.  We use the same commands in R when manipulating local data or Spark data. Similarly, `sparklyr` and some packages in the `tidymodels` ecosystem offer integration.  As with any evolving framework, the available integration is not for every model and for every function.  This article aims at enumerating what is available today, and why should we consider using the `tidymodels` implementation in our day-to-day work with Spark.\n\n## Model preparation with `parsnip`\n\n`parsnip` provides a common interface to models.  This enables us to run the same model against multiple engines. `parsnip` contains translation for each of these packages, so we do not have to remember, or find out, how to setup each argument in the respective package.\n\n### Why use in Spark? \n\nIn some cases, it is better to try out model parameters on a smaller, local, data set in R. Once we are happy with the parameters, we can then run the model over the entire data set in Spark. For example, doing this for a Linear Regression model, we would use `lm()` locally, and then we would have to re-write the model using `ml_linear_regression()`.  Both of these functions have different sets of function arguments that we would need to set.  `parsnip` allows us to use the exact same set of functions and arguments when running against either back-end. With a couple of small changes, we can change the target engine (R vs Spark) and the target data set (local vs remote). Here is an example of what the model fitting looks like locally in R:\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() %>%\n  set_engine(\"lm\") %>%           # << Engine set to `lm`\n  fit(mpg ~ ., data = mtcars)    # << Local `mtcars`\n```\n:::\n\nTo switch to Spark, we just need to change the engine to `spark`, and the training data set to the remote Spark data set:\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() %>%\n  set_engine(\"spark\") %>%           # << Engine set to `spark`\n  fit(mpg ~ ., data = spark_mtcars) # << Remote `mtcars`\n```\n:::\n\n### List of supported models\n\nThere are six `parsnip` models that currently support `sparklyr` equivalent models. Here is the list: \n\n\n\n\n::: {.cell}\n\n:::\n\n### Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc <- spark_connect(\"local\")\n\nspark_mtcars <- copy_to(sc, mtcars)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(parsnip)\n\nmtcars_model <- linear_reg() %>%\n  set_engine(\"spark\") %>%\n  fit(mpg ~ ., data = spark_mtcars)\n\nmtcars_model\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() %>%\n  set_engine(\"spark\") %>%\n  translate()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_iris <- copy_to(sc, iris)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\niris_model <- rand_forest(trees = 100) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"spark\") %>% \n  fit(Species ~., data = spark_iris)\n\niris_model\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrand_forest(trees = 100) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"spark\") %>% \n  translate()\n```\n:::\n\n## Model results with `broom`\n\nThe `broom` package offers great ways to get summarized information about a \nfitted model. `sparklyr` offers integration for `parsnip` based, \n\n- `tidy()` - Summarizes information about the components of a model. A model component might be a single term in a regression, a single hypothesis, a cluster, or a class. \n\n- `glance()` - Returns a `tibble::tibble()` with exactly one row of model summaries. The summaries are typically goodness of fit measures, p-values for hypothesis tests on residuals, or model convergence information.\n\n\n\n### List of supported models\n\nCurrently, 20 Spark models support `broom` via `sparklyr`.  Here is the current list of models and the corresponding `sparklyr` function:\n\n::: {.cell}\n\n:::\n\n### Example\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\n```\n:::\n\n\n\n\n::: {.cell warnings='false'}\n\n```{.r .cell-code}\ntidy(mtcars_model)\n```\n:::\n\n::: {.cell warnings='false'}\n\n```{.r .cell-code}\nglance(mtcars_model)\n```\n:::\n::: {.cell warnings='false'}\n\n```{.r .cell-code}\ntidy(iris_model)\n```\n:::\n\n::: {.cell warnings='false'}\n\n```{.r .cell-code}\nglance(iris_model)\n```\n:::\n## Correlations using `corrr`\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(corrr)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_mtcars %>% \n  correlate()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_mtcars %>% \n  correlate() %>% \n  rplot()\n```\n:::\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}