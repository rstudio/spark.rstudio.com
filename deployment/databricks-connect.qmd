---
title: Databricks Connect v2
format:
  html:
    theme: default
    toc: true
execute:
    eval: true
    freeze: true
editor: 
  markdown: 
    wrap: 72
aliases:
  - /examples/databricks-cluster
  - /examples/databricks-cluster-odbc.html  
  - /examples/databricks-cluster-local.html
  - /examples/databricks-cluster-remote.html
  - /deployment/databricks-cluster.html
  - /deployment/databricks-cluster-odbc.html  
  - /deployment/databricks-cluster-local.html
  - /deployment/databricks-cluster-remote.html
  - /deployment/databricks-spark-connect.html  
---

```{r}
#| include: false

library(remotes)
library(dplyr)
library(dbplyr)
library(sparklyr)
library(pysparklyr)
```

*Last updated: `r lubridate::date()`*

## Intro

Databricks Connect enables the interaction with Spark clusters remotely.
It is based on Spark Connect, which enables remote connectivity thanks
to its new decoupled client-server architecture. This allows users to
interact with the Spark cluster without having to run the jobs from a
node. Additionally, it removes the requirement of having Java components
installed in the use's machine.

The API is very different than the "legacy" Spark and using the Spark
shell is no longer an option. We have decided to use Python as the new
interface. In turn, Python uses *gRPC* to interact with Spark.

::: {#fig-connect}
```{mermaid}
%%| fig-width: 10
%%| eval: true
flowchart LR
  subgraph lp[test]
    subgraph r[R]
      sr[sparklyr]
      rt[reticulate]
    end
    subgraph ps[Python]
      dc[Databricks Connect]
      g1[gRPC]
    end
  end   
  subgraph db[Databricks]
    sp[Spark]   
  end
  sr <--> rt
  rt <--> dc
  g1 <-- Internet<br>Connection --> sp
  dc <--> g1
  
  style r   fill:#fff,stroke:#666,color:#000
  style sr  fill:#fff,stroke:#666,color:#000
  style rt  fill:#fff,stroke:#666,color:#000
  style ps  fill:#fff,stroke:#666,color:#000
  style lp  fill:#fff,stroke:#666,color:#fff
  style db  fill:#fff,stroke:#666,color:#000
  style sp  fill:#fff,stroke:#666,color:#000
  style g1  fill:#fff,stroke:#666,color:#000
  style dc  fill:#fff,stroke:#666,color:#000
```

How `sparklyr` communicates with Databricks Connect
:::

We are using `reticulate` to interact with the Python API. `sparklyr`
extends the functionality, and user experience, by providing the
`dplyr`back-end, `DBI` back-end, and integration with RStudio's
Connection pane.

In order to quickly iterate on enhancements and bug fixes, we have
decided to isolate the Python integration into its own package. The new
package, called [`pysparklyr`](https://github.com/mlverse/pysparklyr),
is an extension of `sparklyr`.

## Getting Started

### Package Installation

To access Databricks Connect, you will need the following two packages:

-   `sparklyr` - 1.8.4
-   `pysparklyr` - 0.1.2

``` r
install.packages("sparklyr")
install.packages("pysparklyr")
```

### Setup credentials

To use with Databricks Connect, in run-time 13 or above, you will need
three configuration items:

1.  Your [Workspace Instance
    URL](https://docs.databricks.com/workspace/workspace-details.html#workspace-url)
2.  Your [Personal Authentication
    Token](https://docs.databricks.com/dev-tools/auth.html#pat) (PAT)
3.  Your [Cluster
    ID](https://docs.databricks.com/workspace/workspace-details.html#cluster-url-and-id)

We have developed this solution to align with other applications that
integrate with Databricks. All applications need, at minimum, a work
space *(1)*, and an authentication token *(2)*. For default values,
those applications initially look for these environment variables:

-   `DATABRICKS_HOST` - Your Workspace Instance URL
-   `DATABRICKS_TOKEN` - Your Personal Authentication Token

Environment variables work well, because they rarely vary between
projects. The thing that will change more often is the cluster you are
connecting to. Using environment variables also makes connection safer,
because token contents will not be in your code in plain text. We
recommend that you set these two variables at your **user level**. To do
this run:

``` r
usethis::edit_r_environ()
```

That command will open a text file that controls the environment
variables at the **user level**. If missing, insert the entries for the
two variables:

``` bash
DATABRICKS_HOST="Enter here your Workspace URL"
DATABRICKS_TOKEN="Enter here your personal token"
```

**This is a one time operation.** After saving and closing the file,
restart your R session.

### First time connecting

After setting up your Host and Token environment variables, you can now
connect to your cluster by simply providing the cluster's ID, and the
method to `spark_connect()`:

``` r
library(sparklyr)

sc <- spark_connect(
  cluster_id = "Enter here your cluster ID",
  method = "databricks_connect"
)
```

In order to connect and interact with Databricks, you will need a
specific set of [Python libraries](#python-libraries) installed and
available. To make it easier to get started, we provide functionality
that will automatically do the following:

-   Create or re-create the necessary Python environment. Based on your
    OS, it will choose to create a Virtual Environment or use Conda.

-   Install the needed Python libraries into the new environment.

`spark_connect()` will check to see if you have the expected Python
environment and prompt you to accept its installation if missing. Here
is an example of the code and output you would expect to see:

``` r
sc <- spark_connect(
    cluster_id = "1026-175310-7cpsh3g8",
    method = "databricks_connect"
)

#> ! Retrieving version from cluster '1026-175310-7cpsh3g8' 
#> Cluster version: '14.1' 
#> ! No viable Python Environment was identified for Databricks Connect version 14.1 
#> Do you wish to install Databricks Connect version 14.1? 
#>  
#> 1: Yes 
#> 2: No 
#> 3: Cancel 
#>  
#> Selection: 1 
```

After accepting, the Python environment will be created with a specific
name, and all of the needed Python libraries will be installed within.
After it is done, it will attempt to connect to your cluster. Here is an
abbreviated example of the output that occurs when selecting "Yes":

``` r
#> ✔ Automatically naming the environment:'r-sparklyr-databricks-14.1' 
#> Using Python: /Users/edgar/.pyenv/versions/3.10.13/bin/python3.10 
#> Creating virtual environment 'r-sparklyr-databricks-14.1' ... 
#> + /Users/edgar/.pyenv/versions/3.10.13/bin/python3.10 -m venv /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1 
#> Done! 
#>   Installing packages: pip, wheel, setuptools 
#> + /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python -m pip install --upgrade pip wheel setuptools 
#> Requirement already satisfied: pip in /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/lib/python3.10/site-packages (23.0.1) 
#> Collecting pip 
#> Using cached pip-23.3.1-py3-none-any.whl (2.1 MB) 
#> Collecting wheel 
#> Using cached wheel-0.42.0-py3-none-any.whl (65 kB) 
#> Requirement already satisfied: setuptools in /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/lib/python3.10/site-packages (65.5.0) 
...
...
...
#> Successfully installed PyArrow-14.0.1 cachetools-5.3.2 certifi-2023.11.17 charset-normalizer-3.3.2 databricks-connect-14.1.0 databricks-sdk-0.14.0 google-api-core-2.14.0 google-api-python-client-2.109.0 google-auth-2.25.0 google-auth-httplib2-0.1.1 googleapis-common-protos-1.61.0 grpcio-1.59.3 grpcio_status-1.59.3 httplib2-0.22.0 idna-3.6 numpy-1.26.2 pandas-2.1.3 protobuf-4.25.1 py4j-0.10.9.7 pyasn1-0.5.1 pyasn1-modules-0.3.0 pyparsing-3.1.1 python-dateutil-2.8.2 pytz-2023.3.post1 requests-2.31.0 rsa-4.9 six-1.16.0 tzdata-2023.3 uritemplate-4.1.1 urllib3-2.1.0 
#> ✔ Using the 'r-sparklyr-databricks-14.1' Python environment 
#> Path: /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python 
```

## Interacting with the cluster

### RStudio's Connection pane

Thanks to the new way we are integrating with Spark, it is now possible
to display the same structure displayed in the Databricks Data Explorer.
In Databricks, the current data structure levels are:

-   Catalog
    -   Database
        -   Table

In the RStudio Connections Pane, you can navigate the data structure by
expanding from the top level, all the way down to the table you wish to
explore. Once expanded, the table's fields and their types are
displayed.

![](/images/deployment/connect/rstudio-connection.png)

You can also click on the **table** icon, situated to the right of the
table name, to preview the first 1,000 rows:

![](/images/deployment/connect/preview.png)

### Using the Connection to Access Data

```{r}
library(dplyr)
library(dbplyr)
library(sparklyr)

sc <- spark_connect(
    cluster_id = "1026-175310-7cpsh3g8",
    method = "databricks_connect"
)
```

After connecting, you can use `dbplyr`'s `in_catalog()` function to
access any table in your data catalog. You will only need to pass the
respective names of the three levels as comma separated character
entries to `in_catalog()` in this order: Catalog, Database, and Table.

Here is an example of using `tbl()` and `in_catalog()` to point to the
**trips** table, which is inside **nyctaxi** database, which is inside
the \***samples** catalog:

```{r}
trips <- tbl(sc, in_catalog("samples", "nyctaxi", "trips"))

trips
```

After pointing `tbl()` to that specific table, you can then use `dplyr`
to execute queries against the data.

```{r}
trips %>%
  group_by(pickup_zip) %>%
  summarise(
    count = n(),
    avg_distance = mean(trip_distance, na.rm = TRUE)
  )
```

## Machine Learning

Machine Learning capabilities are currently available starting with
Databricks Runtime version 14.1. Compared to "legacy" Spark, Spark
Connect's ML capabilities are limited. At this time, there is only one
supported model, Logistic Regression, and two scaler transformers,
namely Standard Scaler and Max Abs Scaler. `sparklyr` makes that
functionality available.

### Using for the first time

By default, the Python environment that `sparklyr` creates does not
include libraries that relate to Machine Learning. These include Torch
and "scikit-learn". Some of the libraries are large in size and they may
have Python requirements that are challenging to new users.
Additionally, we have noticed there are not many users that need to
utilize ML capabilities at this time.

The first time an ML function is accessed through `sparklyr`, you will
be prompted to install the additional Python libraries which are needed
to access such ML capabilities.

``` r
ml_logistic_regression(tbl_mtcars, am ~ .)
#> ! Required Python libraries to run ML functions are missing
#>   Could not find: torch, torcheval, and scikit-learn
#>   Do you wish to install? (This will be a one time operation)
#> 
#> 1: Yes
#> 2: Cancel
#> 
#> Selection: 1
#> Using virtual environment '/Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1' ...
#> + /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python -m pip install --upgrade --no-user torch torcheval scikit-learn
#> Collecting torch
...
```

::: callout-note
It is possible to install the ML libraries along with the required
libraries. There may be several reasons to do this, including trying to
recreate the environment after upgrading Python in your machine. Just
pass `install_ml=TRUE` to the installation function:

``` r
install_databricks(cluster_id = "Enter your cluster's ID", install_ml = TRUE)
```

or

``` r
install_databricks(version = "14.1", install_ml = TRUE)
```
:::

### Easily fit and use

At this time, Logistic Regression is the only model supported. As usual,
there are specific data preparation steps in order to run. `sparklyr`
automates those steps, so all you have to do is pass the Spark data
frame and the formula to use:

```{r}
tbl_mtcars <- copy_to(sc, mtcars)

model1 <- ml_logistic_regression(tbl_mtcars, am ~ .)
```

The output for Spark Connect based models has been upgraded. It will
display the model parameters.

```{r}
model1
```

As shown in the following screenshot, the new output features a
first-of-its-kind tooltip, it will popup the description of the
parameter when hovered over. This functionality works when used in
RStudio, and any console that supports this kind of enhanced user
experience.

![](/images/deployment/databricks/model-output.png)

To use the model, you can run `ml_predict()`:

```{r}
ml_predict(model1, tbl_mtcars)
```

### Using feature transformers

These are the two feature transformers currently supported:

-   Standard Scaler - `ft_standard_scaler()`
-   Max Abs Scaler - `ft_max_abs_scaler()`

To access simply call the function by passing a vector of column names.
Please note that it will create a single column with an array field that
contains all of the newly scaled values.

```{r}
tbl_mtcars %>% 
  ft_standard_scaler(c("wt", "mpg"), "features") %>% 
  select(wt, mpg, features)
```

When you are done with you queries and computations, you should
disconnect from the cluster.

```{r}
spark_disconnect(sc)
```

## Reported Problems

As it is with any new implementation, we are seeing some early adopters
report issues with their installation or connection. Here are some of
the issues that have been reported and the recommended solution or
workaround. Each issue is **collapsed**, just expand it to see the
background and recommendation. Each one is titled based on the full
error message or the distinctive part of the error message.

Before reviewing the issues, please make sure to have the latest
versions of `sparklyr` and `pysparklyr` from CRAN. These are the current
version levels: - `sparklyr` - 1.8.4 - `pysparklyr` - 0.1.2

::: {.callout-note collapse="true"}
## `...reticulate can only bind to copies of Python built with '--enable-shared'.`

Error message contains:

``` r
... reticulate can only bind to copies of Python built with '--enable-shared'.
```

This is happening because there is no Python installation that
`reticulate` can use. `pysparklyr` depends on that package to function.

### Solution

The best way to resolve is to install an acceptable version of Python.
You can run:

``` r
reticulate::install_python()
```

If the output mentions `--skip-existing [version number]` then you will
have two options:

1.  If you're ok with replacing the existing Python, then use:

    ``` r
     reticulate::install_python(version = "[version number]", force = TRUE)
    ```

2.  If you would like to keep that installation of Python then use a
    different version number:

    ``` r
     reticulate::install_python(version = "[slightly different version number]")
    ```

    For example, if the version number is `3.9.12`, then use `3.9.18`:

    ``` r
    reticulate::install_python(version = "3.9.18")
    ```
:::

::: {.callout-note collapse="true"}
## `spark_connect()` returns `! Version '14.2' does not exist`

Error message contains:

``` r
  Checking if provided version is valid against PyPi.org
  Error in `install_environment()`
  ! Version '14.2' does not exist
```

This happens when `pysparklyr` gets the cluster DBR version
automatically. Usually when running `spark_connect()` or
`install_databricks()`. This occurs because the latest version of DBR,
as of today 14.2, is ahead of the `databricks.connect` version available
in `databricks.connect`.

### Solution

Current workaround is to say "No" when prompted to install version 14.2
and, if not already installed, install 14.1 using:

``` r
pysparklyr::install_databricks("14.1")
```
:::

## Environments

### Install different version of `databricks.connect`

Here are three different options to create a custom Python environment,
that will contain the needed Python libraries to interact with
Databricks Connect:

-   To install the latest versions of all the needed libraries, use:

    ``` r
    pysparklyr::install_databricks()
    ```

    `sparklyr` will query PyPi.org to get the latest version of
    `databricks.connect` and installs that version.

-   It is recommended that the version of the `databricks.connect`
    library matches the DBR version of your cluster. To do this, pass
    the DBR version in the `version` argument:

    ``` r
    pysparklyr::install_databricks("14.0")
    ```

    This will create a Python environment and install
    `databricks.connect` version 14.0, and it will automatically name it
    `r-sparklyr-databricks-14.0`. By using this name, `sparklyr` is able
    to know what version of `databricks.connect` is available inside
    this particular Python environment.

-   If you are not sure about the version of the cluster you want to
    interact with, then use the `cluster_id` argument. We have added a
    way to pull the cluster's information without starting Spark
    Connect. This allows us to query the cluster and get the DBR
    version:

    ``` r
    pysparklyr::install_databricks(cluster_id = "[Your cluster's ID]")
    ```

### Restricted Python environments

If your organization restricts Python environment creation, you can
point `sparklyr` to the designated Python installation. To do this, pass
the path to the environment in the `envname` argument of
`spark_connect()`:

``` r
library(sparklyr)

sc <- spark_connect(
  method = "databricks_connect",
  cluster_id = "Enter here your cluster ID",
  envname = "Enter here the path to your Python environment"
)
```

To successfully connect to a Databricks cluster, you will need to match
the proper version of the `databricks.connect` Python library to the
Databricks Runtime (DBR) version in the cluster. For example, if you are
trying to use a Databricks cluster with a DBR version 14.0 then
`databricks.connect` will also need to be version 14.0. Failure to do so
can result in instability or even the inability to connect.

Besides `datbricks.connect`, the Python environment will also need to
have other Python libraries installed. The full list is in the [Python
Libraries](#python-libraries) section.

::: callout-important
If your server, or machine, has only one Python installation and no
ability to create Conda or Virtual environments, then you will encounter
issues when connecting to a Databricks cluster with a mismatched version
of `databricks.connect` to DBR.
:::

**Important** - This step needs only to be **done one time**. If you
need to connect to a different cluster that has the same DBR version,
`sparklyr` will use the same Python environment. If the new cluster has
a different DBR version, then it is recommended that you run the
installation function using the new DBR version or cluster ID.

### Python Libraries {#python-libraries}

Here is the list of the Python libraries needed in order to work with
the cluster:

Required libraries:

-   `databricks-connect`
-   `delta-spark`
-   `pandas`
-   `PyArrow`
-   `grpcio`
-   `google-api-python-client`
-   `grpcio_status`

ML libraries (Optional):

-   `torch`
-   `torcheval`
-   `scikit-learn`

## Deploying to Posit Connect

We recommend that for simply accessing data from the Unity Catalog, such
as in a Shiny app, use an ODBC connection instead of a Spark session.
The advantage of this is that a connection to the Databricks Warehouse
does not require a running cluster. For more information about creating
dashboards with databases visit [Database with R
site](https://solutions.posit.co/connections/db/best-practices/dashboards/).

However, there are cases when it is necessary to deploy a solution that
requires a Spark session. For example, when there is a long running job
that needs to run on a schedule. Those kinds of jobs could be put inside
a Quarto document and published to Posit Connect, where they can run on
specific date/time intervals. Posit Connect supports Python
environments, so it is an ideal platform to deploy these kinds of
solutions.

### Preparing for deployment

When deploying to Posit Connect, there are specific pieces of
information that we need to make sure are sent over:

-   Your **cluster's ID**
-   Your **workspace URL**
-   Your **token**
-   Your **Python environment**
-   To use the `pysparklyr` extension

Based on the recommendations in this article, the **cluster's ID**
should be in the code of your document, the **workspace URL** and
**token** should already be inside the `DATABRICKS_HOST`, and
`DATABRICKS_TOKEN` environment variables. If you are using something
other than these environment variables to authenticate, then see the
section [Alternatives to Environment
Variables](#alternatives-to-environment-variables).

**Make sure that your document has a `library(pysparklyr)` call.** This
will let Posit Connect deployment process know that it needs to install
this package and its dependencies, such as `reticulate`.

The next section will introduce a function that will ease finding the
**Python environment**.

``` r
library(sparklyr)
library(pysparklyr)

sc <- spark_connect(
    cluster_id = "1026-175310-7cpsh3g8",
    method = "databricks_connect"
)
```

### Using `deploy_databricks()`

The `deploy_databricks()` function makes it easier to deploy your
content to Posit Connect. It does its best to gather all of the pieces
of information mentioned above and builds the correct command to
publish.

The path to your content is the last piece of information we need.
Ideally your content is either located in its own folder inside your
project or it is at the root level of your project. There are three ways
that you can let `deploy_databricks()` know the path to use:

-   If you are in RStudio and the document you wish to publish is open,
    `deploy_databricks()` will use the RStudio API to get the path of
    that document and then use its containing folder as the content's
    location. This is the preferred method for deployment.

-   Use the `appDir` argument to pass the path to be used. Something
    such as `here::here("my-cool-document")` would work.

-   If no document is opened in RStudio and `appDir` is left empty, then
    `getwd()` will be used

The aim of the new function is to be both flexible and helpful. It will
gather your document location, credentials, and URLs. The **Python**
location needs to defined when calling `deploy_databricks()`. If you are
in RStudio, and your document is opened, here are several ways to do
this:

-   If you know the cluster's DBR version:

    ``` r
    pysparklyr::deploy_databricks(version = "14.1")
    ```

-   The cluster's ID can also be used and `pysparklyr` will
    automatically determine the required DBR version:

    ``` r
    pysparklyr::deploy_databricks(cluster_id = "1026-175310-7cpsh3g8")
    ```

-   If you just ran the code of the content you plan to deploy, then the
    **Python** environment will already be loaded in the R session.
    `deploy_databricks()` will validate that the path of that Python
    environment conforms to one that `pysparklyr` creates and use that.
    This will happen if no `version`, `cluster_id`, or `python` argument
    is provided. At that point simply run:

    ``` r
    pysparklyr::deploy_databricks()
    ```

-   You can also pass the path to the Python environment to use by
    setting the `python` argument:

    ``` r
    pysparklyr::deploy_databricks(python = "/Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1")
    ```

Here is an example of the output returned when using
`deploy_databricks()`. Before submitting your content, you will be
prompted to confirm that the information gathered is correct. Also
notice that if you have more than one Posit Connect server setup in your
RStudio IDE, it will choose the top one as the default but allow you to
easily change it if necessary:

``` r
pysparklyr::deploy_databricks(version = "14.1")

── Starting deployment ────────────────────────────────────────────────────
- App and Spark -
ℹ Source: '/Users/edgar/r_projects/practice/test-deploy'
✔ Python:'/Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python' [1.5s]
- Publishing target -
ℹ Server: colorado.posit.co | Account: edgar
ℹ Environment variables:
  |- Host: rstudio-partner-posit-default.cloud.databricks.com
  |- Token: '<REDACTED>'

Does everything look correct?

1: Yes
2: No
3: Change Publishing Target (Posit Connect server)
```

### Alternatives to Environment Variables

The `deploy_databricks()` function has a `host`, and `token` arguments.
These arguments take precedent over the environment variables, if set.

There are a variety of reasons for you to set these arguments when
publishing. For example, locally you authenticate with a Databricks
configuration file, but when deploying, you will need to let
`deploy_databricks()` what values to use for the PAT and Host URL.
Another example could be that your deployed content may need to use a
[service
account](https://docs.databricks.com/en/dev-tools/service-principals.html),
that differs from the credentials you use when developing.

As usual, we recommend that you avoid using open text values with your
credentials in your code. An effective way of managing local-vs-remote
credentials is with the
[`config`](https://rstudio.github.io/config/articles/rsconnect.html)
package. Here is an example:

***config.yml***

``` yaml
default:
  host_url: "[Your Host URL]"
  token: "[Your Token]"

rsconnect:
  host_url: "[Your Host URL]"
  token: "[Service Token]"
```

***R script***

``` r
config <- config::get()

pysparklyr::deploy_databricks(
  version = "14.1", 
  host = config$host_url,
  token = config$token
  )
```

The integration with Posit Connect and `config`, allows your deployed
content to automatically use the values under the `rsconnect` section in
the YAML file, instead of the values from the `default` section.

## What is supported

Here is a list of what we currently support, and do not support via
`sparklyr` and Databricks Connect:

**Supported**:

-   Integration with most of the `dplyr`, and `DBI`, APIs
-   Integration with the `invoke()` command
-   RStudio Connections Pane navigation
-   Support for Personal Access Token security authentication for
    Databricks Connect
-   Support for most read and write commands. These have only been
    tested in Spark Connect.

**Not supported**:

-   **ML functions** - Very few functions, in `sparklyr`, that have the
    `ml_` and `ft_` are currently supported. The reason is that Spark
    3.4 does not currently support MLlib. We expect that some ML support
    will be available in Spark 3.5. At that time we will work on
    integrating the new ML routines from Connect into `sparklyr`.

-   **SDF functions** - Most of these functions require SparkSession,
    which is currently not supported in Spark 3.4.

-   **`tidyr`** - This is ongoing work that we are focusing on in
    `sparklyr`. We are implementing these functions using PySpark
    DataFrame commands instead of depending on the Scala implementation.
