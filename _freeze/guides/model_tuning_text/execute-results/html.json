{
  "hash": "197396fe646a70eb992a3c44ce5eb5ee",
  "result": {
    "markdown": "---\ntitle: \"Grid Search Tuning\"\nexecute:\n  eval: true\n  freeze: true\neditor_options: \n  markdown: \n    wrap: 72\n---\n\n\n\n\nThe aim of this article is to build on the knowledge from [Intro to Model\nTuning](model_tuning.qmd){target=\"_blank\"} by covering the following:\n\n-   Show how to re-use the code of an existing ML Pipeline as the base\n    for the hyper-parameter tuning\n-   Show how tuning parameters are not limited to only model's\n    parameters. We will cover how to tune data transformation parameters\n\n## Recreating \"Tuning Text Analysis\"\n\nThe example in this article is based on the [Tuning Text\nAnalysis](https://tune.tidymodels.org/articles/extras/text_analysis.html){target=\"_blank\"}\narticle found in the `tidymodels`' `tune` website. In that article, they\nuse *Amazon's Fine Food Reviews* text data to perform hyper parameter\ntuning.\n\nAs its name suggest, Model Tuning has two main phases: the modeling, and\nthe tuning. In `tidymodels`, the modeling is done using the `recepies`\nand `parsnip` packages, while the tuning is done with `tune`. `tune` is\nable to modify the the `recipe` and model's arguments for each\nexperiment.\n\nIn Spark, the modeling is done with an ML Pipeline. In itself, preparing\nthe data, and setting up the model can be complex enough to merit its\nown walk-through. For the walk-through of the model used in this\narticle, please see [Text Modeling](textmodeling.qmd){target=\"_blank\"}.\n\nThe basics of the tuning part are covered in [Intro to Model\nTuning](model_tuning.qmd){target=\"_blank\"}. To avoid duplication, any\nfoundational explanation will be linked back to that article.\n\n## Setup\n\nFor this example, we will start a local Spark session, and then copy the\n*Fine Food Reviews* data to it. For more information about the data, please \nsee the [Data](textmodeling.qmd#data) section of the *Text Modeling* article.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nlibrary(modeldata)\n\ndata(\"small_fine_foods\")\n\nsc <- spark_connect(master = \"local\", version = \"3.3\")\n\nsff_training_data <- copy_to(sc, training_data)\nsff_testing_data <- copy_to(sc, testing_data)\n```\n:::\n\n\n## Pipeline\n\nThe data preparation and modeling in [Text\nModeling](textmodeling.qmd){target=\"_blank\"} was based on the [Tuning\nText\nAnalysis](https://tune.tidymodels.org/articles/extras/text_analysis.html){target=\"_blank\"}\narticle. The main steps from from the [Recipe and Model\nSpecification](https://tune.tidymodels.org/articles/extras/text_analysis.html#recipe-and-model-specifications-1){target=\"_blank\"}\nsection were recreated using Spark, via the `sparklyr` API. The `recipe`\nsteps were recreated with Feature Transformer functions, and the\n`parsnip` model was recreated using the equivalent\n`ml_logistic_regression()` model.\n\nDuring tuning, any parameter value used in the ML Pipeline will be\noverwritten with values from the [grid](#grid). This means that it\ndoesn't matter that we use the exact same code for developing, and\ntuning the pipeline. We can literally copy-paste, and run the resulting\npipeline code from [Text\nModeling](textmodeling.qmd#prepare-the-model-with-an-ml-pipeline).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_pipeline <- ml_pipeline(sc) %>% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %>% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %>% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 1024\n    ) %>%\n  ft_normalizer(\n    input_col = \"hashed_features\", \n    output_col = \"normal_features\"\n    ) %>% \n  ft_r_formula(score ~ normal_features) %>% \n  ml_logistic_regression()\n\nsff_pipeline\n#> Pipeline (Estimator) with 6 stages\n#> <pipeline__a238a35e_541a_43f7_b22a_7d8a9b2754ef> \n#>   Stages \n#>   |--1 Tokenizer (Transformer)\n#>   |    <tokenizer__89f703c6_d2eb_4504_ab09_56afa00534bc> \n#>   |     (Parameters -- Column Names)\n#>   |      input_col: review\n#>   |      output_col: word_list\n#>   |--2 StopWordsRemover (Transformer)\n#>   |    <stop_words_remover__e20da5c1_fbc5_451c_a25d_7281e4b253c2> \n#>   |     (Parameters -- Column Names)\n#>   |      input_col: word_list\n#>   |      output_col: wo_stop_words\n#>   |--3 HashingTF (Transformer)\n#>   |    <hashing_tf__cce2290c_85c2_4eb0_af37_760b691d98a2> \n#>   |     (Parameters -- Column Names)\n#>   |      input_col: wo_stop_words\n#>   |      output_col: hashed_features\n#>   |--4 Normalizer (Transformer)\n#>   |    <normalizer__7ccc5def_3af6_4625_9a81_e62b79315067> \n#>   |     (Parameters -- Column Names)\n#>   |      input_col: hashed_features\n#>   |      output_col: normal_features\n#>   |--5 RFormula (Estimator)\n#>   |    <r_formula__7bf5f9e7_e76a_46ed_8ccd_fa322d9e4cb5> \n#>   |     (Parameters -- Column Names)\n#>   |      features_col: features\n#>   |      label_col: label\n#>   |     (Parameters)\n#>   |      force_index_label: FALSE\n#>   |      formula: score ~ normal_features\n#>   |      handle_invalid: error\n#>   |      stringIndexerOrderType: frequencyDesc\n#>   |--6 LogisticRegression (Estimator)\n#>   |    <logistic_regression__5e40252e_fe78_4b47_aa2c_1494567736cf> \n#>   |     (Parameters -- Column Names)\n#>   |      features_col: features\n#>   |      label_col: label\n#>   |      prediction_col: prediction\n#>   |      probability_col: probability\n#>   |      raw_prediction_col: rawPrediction\n#>   |     (Parameters)\n#>   |      aggregation_depth: 2\n#>   |      elastic_net_param: 0\n#>   |      family: auto\n#>   |      fit_intercept: TRUE\n#>   |      max_iter: 100\n#>   |      maxBlockSizeInMB: 0\n#>   |      reg_param: 0\n#>   |      standardization: TRUE\n#>   |      threshold: 0.5\n#>   |      tol: 1e-06\n```\n:::\n\n\nIt is also worth pointing out that in a\"real life\" exercise,\n`sff_pipeline` would probably already be loaded into our environment.\nThat is because we just finished modeling and, decided to test to see if\nwe could tune the model. Spark can re-use the exact same ML Pipeline\nobject for the cross validation step.\n\n## Grid {#grid}\n\nThere is a big advantage to transforming, and modeling the data in a\nsingle ML Pipeline. It opens the door for Spark to also alter parameters\nused for data transformation, in addition to the model's parameters.\nThis means that we can include the parameters of the tokenization,\ncleaning, hashing, and normalization steps as possible candidate for the\nmodel tuning.\n\nThe *Text Analysis* article uses three tuning parameters. Two are in the\nmodel, and one is in the hashing step. Here are the parameters, and how\nthey map between `tidymodels` and `sparklyr`:\n\n| Parameter                             | `tidymodels` | `sparklyr`          |\n|---------------------------------------|--------------|---------------------|\n| Number of Terms to Hash               | `num_terms`  | `num_features`      |\n| Amount of regularization in the model | `penalty`    | `elastic_net_param` |\n| Proportion of pure vs ridge Lasso     | `mixture`    | `reg_param`         |\n\nThe values to tune with are taken from the [Grid\nSearch](https://tune.tidymodels.org/articles/extras/text_analysis.html#grid-search-1){target=\"_blank\"}\nsection in the *Text Analysis* article. All that is left to do is to\ncreate the grid itself. Just like we did in the first [Model\nTuning](model_tuning.html#grid){target=\"_blank\"} article, we use partial\nname matching to the steps we want to tune:\n\n-   `hashing_ft` will be the name of the list object containing the\n    `num_features` values\n\n-   `logistic_regression` will be the of the list object containing the\n    values of the other two parameters\n\nNotice that the R code of the values themselves are a direct copy of the\nones used in the *Text Analysis* article.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_grid <-  list(\n    hashing_tf = list(\n      num_features = 2^c(8, 10, 12)  \n    ),\n    logistic_regression = list(\n      elastic_net_param = 10^seq(-3, 0, length = 20), \n      reg_param = seq(0, 1, length = 5)    \n    )\n  )\n\nsff_grid\n#> $hashing_tf\n#> $hashing_tf$num_features\n#> [1]  256 1024 4096\n#> \n#> \n#> $logistic_regression\n#> $logistic_regression$elastic_net_param\n#>  [1] 0.001000000 0.001438450 0.002069138 0.002976351 0.004281332 0.006158482\n#>  [7] 0.008858668 0.012742750 0.018329807 0.026366509 0.037926902 0.054555948\n#> [13] 0.078475997 0.112883789 0.162377674 0.233572147 0.335981829 0.483293024\n#> [19] 0.695192796 1.000000000\n#> \n#> $logistic_regression$reg_param\n#> [1] 0.00 0.25 0.50 0.75 1.00\n```\n:::\n\n\n## Evaluator\n\nIn the *Text Analysis* article, ROC AUC is used to measure performance.\nThe is the default metric of `ml_binary_classification_evaluator()` , so\nwe only need to pass the connection variable to the evaluator function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_evaluator <- ml_binary_classification_evaluator(sc)\n```\n:::\n\n\n## Model Tuning\n\nWe will use `ml_cross_validator()` to prepare a tuning specification\ninside Spark. We recommend to set the `seed` argument in order to\nincrease reproducibility.\n\nSpark will automatically create the grid combinations when tuning the\nmodel. In this case, `sff_grid` contains three parameters:\n\n-   `num_features` has 3 values\n\n-   `elastic_net_param` has 20 values\n\n-   `reg_parm` has 5 values\n\nThis means that there will be 300 combinations for the tuning parameters\n(3 x 20 x 5). Because we set the number of folds to 3 (`num_folds`),\nSpark will run a total of 900 models (3 x 300).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_cv <- ml_cross_validator(\n  x = sc,\n  estimator = sff_pipeline, \n  estimator_param_maps = sff_grid,\n  evaluator = sff_evaluator,\n  num_folds = 3,\n  parallelism = 4,\n  seed = 100\n)\n\nsff_cv\n#> CrossValidator (Estimator)\n#> <cross_validator__557f14fd_e0a5_4ed6_a76c_d4648fe44d28> \n#>  (Parameters -- Tuning)\n#>   estimator: Pipeline\n#>              <pipeline__a238a35e_541a_43f7_b22a_7d8a9b2754ef> \n#>   evaluator: BinaryClassificationEvaluator\n#>              <binary_classification_evaluator__51964c1a_ecd3_40a8_89ea_4cf4e2f81490> \n#>     with metric areaUnderROC \n#>   num_folds: 3 \n#>   [Tuned over 300 hyperparameter sets]\n```\n:::\n\n\nThis is the step that will take the longest time. The `ml_fit()`\nfunction will run the 900 models using the training data. There is no\nneed to pre-prepare the re-sampling folds, Spark will take care of that.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_model <- ml_fit(\n  x = sff_cv, \n  dataset = sff_training_data\n  )\n```\n:::\n\n\n## Validation metrics\n\nWe can now extract the metrics from `sff_model` using\n`ml_validation_metrics()`. The ROC AUC values will be in a column called\n`areaUnderROC`. We can then take a look at the best performing models\nusing `dplyr`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_metrics <- ml_validation_metrics(sff_model)\n\nlibrary(dplyr)\n\nsff_metrics %>% \n  arrange(desc(areaUnderROC)) %>% \n  head()\n#>   areaUnderROC reg_param_1 elastic_net_param_1 num_features_2\n#> 1    0.7858727        0.25          0.05455595           4096\n#> 2    0.7847232        0.50          0.02636651           4096\n#> 3    0.7835850        0.75          0.01832981           4096\n#> 4    0.7830411        0.50          0.01832981           4096\n#> 5    0.7830230        0.25          0.03792690           4096\n#> 6    0.7828651        0.75          0.01274275           4096\n```\n:::\n\n\nWe will now plot the results. We will match the approach used in the\n[Grid\nSearch](https://tune.tidymodels.org/articles/extras/text_analysis.html#grid-search-1){target=\"_blank\"}\nsection of the *Text Analysis* article.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nsff_metrics %>% \n  mutate(reg_param_1 = as.factor(reg_param_1)) %>% \n  ggplot(aes(\n    x = elastic_net_param_1, \n    y = areaUnderROC, \n    color = reg_param_1\n    )) +\n  geom_line() +\n  geom_point(size = 0.5) +\n  scale_x_continuous(trans = \"log10\") +\n  facet_wrap(~ num_features_2) +\n  theme_light(base_size = 9)\n```\n\n::: {.cell-output-display}\n![](model_tuning_text_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nIn the plot, we can see the effects of the three parameters, and the\nvalues that look to be the best. These effects are very similar to the\noriginal *Text Analysis* article.\n\n## Model selection\n\nWe can create a new ML Pipeline using the same code as the original\npipeline. We only need to change the 3 parameters values, with values\nthat performed best.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_pipeline <- ml_pipeline(sc) %>% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %>% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %>% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 4096      \n    ) %>%\n  ft_normalizer(\n    input_col = \"hashed_features\", \n    output_col = \"normal_features\"\n    ) %>% \n  ft_r_formula(score ~ normal_features) %>% \n  ml_logistic_regression(\n    elastic_net_param = 0.05,\n    reg_param = 0.25  \n    )\n```\n:::\n\n\nNow, we create a final model using the new ML Pipeline.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_fitted <- new_sff_pipeline %>% \n  ml_fit(sff_training_data)\n```\n:::\n\n\n## Test data metrics\n\nThe test data set is now used to confirm that the performance gains\nhold. We use it to run predictions with thew new ML Pipeline Model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_fitted %>% \n  ml_transform(sff_testing_data) %>% \n  ml_metrics_binary()\n#> # A tibble: 2 Ã— 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.783\n#> 2 pr_auc  binary         0.653\n```\n:::\n\n\nThe results show an increase performance in contrast with running the ML\nPipeline Model, those results are in [Fit and\nPredict](textmodeling.qmd#fit-and-predict) section of the *Text\nModeling* article.\n\n\n\n",
    "supporting": [
      "model_tuning_text_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}