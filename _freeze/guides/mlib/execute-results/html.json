{
  "hash": "baea5a158e9cde3e2872e846e734e7f7",
  "result": {
    "markdown": "---\ntitle: \"Spark Machine Learning Library (MLlib)\"\nexecute:\n  eval: true\n  freeze: true\naliases:\n  - /mlib\n  - /mllib\n  - /guides/mllib.html\n---\n\n\n\n\n## Overview\n\n**`sparklyr`** provides bindings to Spark's distributed [machine learning](https://spark.apache.org/docs/latest/mllib-guide.html) library. \nIn particular, `sparklyr` allows you to access the machine learning routines \nprovided by the [spark.ml](https://spark.apache.org/docs/latest/ml-guide) package. \nTogether with `sparklyr`'s [dplyr](/guides/dplyr.qmd) interface, you can easily \ncreate and tune machine learning workflows on Spark, orchestrated entirely within R.\n\n`sparklyr` provides three families of functions that you can use with Spark machine learning:\n\n-   Machine learning algorithms for analyzing data (`ml_*`)\n-   Feature transformers for manipulating individual features (`ft_*`)\n-   Functions for manipulating Spark DataFrames (`sdf_*`)\n\nAn analytic workflow with `sparklyr` might be composed of the following stages.\nFor an example see [Example Workflow](#example-workflow).\n\n1.  Perform SQL queries through the sparklyr [dplyr](/dplyr) interface\n\n1.  Use the `sdf_*` and `ft_*` family of functions to generate new columns, or \npartition your data set\n\n1.  Choose an appropriate machine learning algorithm from the `ml_*` family of \nfunctions to model your data\n\n1.  Inspect the quality of your model fit, and use it to make predictions \nwith new data.\n\n1.  Collect the results for visualization and further analysis in R\n\n## Algorithms\n\nSpark's machine learning library can be accessed from `sparklyr` through the \n`ml_*` set of functions. Visit the `sparklyr` reference page to see the complete\nlist of available algorithms: [Reference - Spark Machine Learning](/packages/sparklyr/latest/reference/index.html#spark-machine-learning)\n\n### Formulas\n\nThe `ml_*` functions take the arguments `response` and `features`. But `features`\ncan also be a formula with main effects (it currently does not accept interaction \nterms). The intercept term can be omitted by using `-1`.\n\nThe following two statements are equivalent:\n\n``` r\nml_linear_regression(z ~ -1 + x + y)\n```\n\n```r\nml_linear_regression(intercept = FALSE, response = \"z\", features = c(\"x\", \"y\"))\n```\n\n### Options\n\nThe Spark model output can be modified with the `ml_options` argument in \nthe `ml_*` functions. The `ml_options` is an *experts only* interface for \ntweaking the model output. For example, `model.transform` can be used to \nmutate the Spark model object before the fit is performed.\n\n## Transformers\n\nA model is often fit not on a data set as-is, but instead on some transformation \nof that data set. Spark provides [feature transformers](http://spark.apache.org/docs/latest/ml-features), \nfacilitating many common transformations of data within a Spark DataFrame, and \n`sparklyr` exposes these within the `ft_*` family of functions. These routines \ngenerally take one or more input columns, and generate a new output column\nformed as a transformation of those columns. Visit the `sparklyr` reference page\nto see the complete list of available transformers:\n[Reference - Feature Transformers](/packages/sparklyr/latest/reference/index.html#spark-feature-transformers)\n\n## Examples\n\nWe will use the `iris` data set to examine a handful of learning algorithms and transformers. The iris data set measures attributes for 150 flowers in 3 different species of iris.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\")\n\niris_tbl <- copy_to(sc, iris, \"iris\", overwrite = TRUE)\n\niris_tbl\n#> # Source: spark<iris> [?? x 5]\n#>    Sepal_Length Sepal_Width Petal_Length Petal_Width Species\n#>           <dbl>       <dbl>        <dbl>       <dbl> <chr>  \n#>  1          5.1         3.5          1.4         0.2 setosa \n#>  2          4.9         3            1.4         0.2 setosa \n#>  3          4.7         3.2          1.3         0.2 setosa \n#>  4          4.6         3.1          1.5         0.2 setosa \n#>  5          5           3.6          1.4         0.2 setosa \n#>  6          5.4         3.9          1.7         0.4 setosa \n#>  7          4.6         3.4          1.4         0.3 setosa \n#>  8          5           3.4          1.5         0.2 setosa \n#>  9          4.4         2.9          1.4         0.2 setosa \n#> 10          4.9         3.1          1.5         0.1 setosa \n#> # … with more rows\n```\n:::\n\n\n### K-Means Clustering\n\nUse Spark's [K-means clustering](http://spark.apache.org/docs/latest/ml-clustering.html#k-means)\nto partition a dataset into groups. K-means clustering partitions points into `k`\ngroups, such that the sum of squares from points to the assigned cluster centers\nis minimized.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkmeans_model <- iris_tbl %>%\n  ml_kmeans(k = 3, features = c(\"Petal_Length\", \"Petal_Width\"))\n\nkmeans_model\n#> K-means clustering with 3 clusters\n#> \n#> Cluster centers:\n#>   Petal_Length Petal_Width\n#> 1     5.626087    2.047826\n#> 2     1.462000    0.246000\n#> 3     4.292593    1.359259\n#> \n#> Within Set Sum of Squared Errors =  not computed.\n```\n:::\n\nRun and collect predictions into R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted <- ml_predict(kmeans_model, iris_tbl) %>%\n  collect()\n\ntable(predicted$Species, predicted$prediction)\n#>             \n#>               0  1  2\n#>   setosa      0 50  0\n#>   versicolor  2  0 48\n#>   virginica  44  0  6\n```\n:::\n\n\nUse the collected data to plot the results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted %>%\n  ggplot(aes(Petal_Length, Petal_Width)) +\n  geom_point(aes(Petal_Width, Petal_Length, col = factor(prediction + 1)),\n    size = 2, alpha = 0.5\n  ) +\n  geom_point(\n    data = kmeans_model$centers, aes(Petal_Width, Petal_Length),\n    col = scales::muted(c(\"red\", \"green\", \"blue\")),\n    pch = \"x\", size = 12\n  ) +\n  scale_color_discrete(\n    name = \"Predicted Cluster\",\n    labels = paste(\"Cluster\", 1:3)\n  ) +\n  labs(\n    x = \"Petal Length\",\n    y = \"Petal Width\",\n    title = \"K-Means Clustering\",\n    subtitle = \"Use Spark.ML to predict cluster membership with the iris dataset.\"\n  )\n```\n\n::: {.cell-output-display}\n![](mlib_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n### Linear Regression\n\nUse Spark's [linear regression](http://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression) to model the linear relationship between a response variable and one or more \nexplanatory variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_model <- iris_tbl %>%\n  ml_linear_regression(Petal_Length ~ Petal_Width)\n```\n:::\n\n\nExtract the slope and the intercept into discrete R variables. We will use them\nto plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_slope <- coef(lm_model)[[\"Petal_Width\"]]\nspark_intercept <- coef(lm_model)[[\"(Intercept)\"]]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\niris_tbl %>%\n  select(Petal_Width, Petal_Length) %>%\n  collect() %>%\n  ggplot(aes(Petal_Length, Petal_Width)) +\n  geom_point(aes(Petal_Width, Petal_Length), size = 2, alpha = 0.5) +\n  geom_abline(aes(\n    slope = spark_slope,\n    intercept = spark_intercept\n  ),\n  color = \"red\"\n  ) +\n  labs(\n    x = \"Petal Width\",\n    y = \"Petal Length\",\n    title = \"Linear Regression: Petal Length ~ Petal Width\",\n    subtitle = \"Use Spark.ML linear regression to predict petal length as a function of petal width.\"\n  )\n```\n\n::: {.cell-output-display}\n![](mlib_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n### Logistic Regression\n\nUse Spark's [logistic regression](http://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression) to perform logistic regression, modeling a binary outcome as a function \nof one or more explanatory variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_model <- iris_tbl %>% \n  mutate(is_setosa = ifelse(Species == \"setosa\", 1, 0)) %>% \n  select_if(is.numeric) %>% \n  ml_logistic_regression(is_setosa ~.)\n\nsummary(glm_model)\n#> Coefficients:\n#>  (Intercept) Sepal_Length  Sepal_Width Petal_Length  Petal_Width \n#>  -0.02904898  -7.23312634  28.56334798  -9.02580864 -20.62238442\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nml_predict(glm_model, iris_tbl) %>% \n  count(Species, prediction) \n#> # Source: spark<?> [?? x 3]\n#> # Groups: Species\n#>   Species    prediction     n\n#>   <chr>           <dbl> <dbl>\n#> 1 virginica           0    50\n#> 2 versicolor          0    50\n#> 3 setosa              1    50\n```\n:::\n\n\n### PCA\n\nUse Spark's [Principal Components Analysis (PCA)](https://spark.apache.org/docs/latest/mllib-dimensionality-reduction) to perform dimensionality reduction. PCA is a statistical method to find a\nrotation such that the first coordinate has the largest variance possible, and \neach succeeding coordinate in turn has the largest variance possible.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_model <- tbl(sc, \"iris\") %>%\n  select(-Species) %>%\n  ml_pca()\n\npca_model\n#> Explained variance:\n#> \n#>         PC1         PC2         PC3         PC4 \n#> 0.924618723 0.053066483 0.017102610 0.005212184 \n#> \n#> Rotation:\n#>                      PC1         PC2         PC3        PC4\n#> Sepal_Length -0.36138659 -0.65658877  0.58202985  0.3154872\n#> Sepal_Width   0.08452251 -0.73016143 -0.59791083 -0.3197231\n#> Petal_Length -0.85667061  0.17337266 -0.07623608 -0.4798390\n#> Petal_Width  -0.35828920  0.07548102 -0.54583143  0.7536574\n```\n:::\n\n\n### Random Forest\n\nUse Spark's [Random Forest](https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-regression) to perform regression or multiclass classification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_model <- iris_tbl %>%\n  ml_random_forest(\n    Species ~ Petal_Length + Petal_Width, type = \"classification\"\n    )\n```\n:::\n\n\nUse `ml_predict()` to use the apply the new model back to the data.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_predict <- ml_predict(rf_model, iris_tbl) \n\nglimpse(rf_predict)\n#> Rows: ??\n#> Columns: 14\n#> Database: spark_connection\n#> $ Sepal_Length           <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.…\n#> $ Sepal_Width            <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.…\n#> $ Petal_Length           <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.…\n#> $ Petal_Width            <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.…\n#> $ Species                <chr> \"setosa\", \"setosa\", \"setosa…\n#> $ features               <list> <1.4, 0.2>, <1.4, 0.2>, <1…\n#> $ label                  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ rawPrediction          <list> <20, 0, 0>, <20, 0, 0>, <2…\n#> $ probability            <list> <1, 0, 0>, <1, 0, 0>, <1, …\n#> $ prediction             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ predicted_label        <chr> \"setosa\", \"setosa\", \"setosa…\n#> $ probability_setosa     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n#> $ probability_versicolor <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ probability_virginica  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n```\n:::\n\nTo get an idea of the model effectiveness, use `count()` to compare species against\nthe prediction.  `ml_predict()` created a variable called `predicted_label`. That\nvariable contains the string value of the prediction:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_predict %>% \n  count(Species, predicted_label) \n#> # Source: spark<?> [?? x 3]\n#> # Groups: Species\n#>   Species    predicted_label     n\n#>   <chr>      <chr>           <dbl>\n#> 1 setosa     setosa             50\n#> 2 versicolor virginica           1\n#> 3 virginica  virginica          50\n#> 4 versicolor versicolor         49\n```\n:::\n\n\n\n### FT String Indexing\n\nUse `ft_string_indexer()` and `ft_index_to_string()` to convert a character \ncolumn into a numeric column and back again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nft_string2idx <- iris_tbl %>%\n  ft_string_indexer(\"Species\", \"Species_idx\") %>%\n  ft_index_to_string(\"Species_idx\", \"Species_remap\") %>% \n  select(Species, Species_remap, Species_idx)\n```\n:::\n\n\nTo see the value assigned to each value in `Species`, we can pull the aggregates\nof all the species, re-mapped species and index combinations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nft_string2idx %>% \n  group_by_all() %>% \n  summarise(count = n(), .groups = \"keep\")\n#> # Source: spark<?> [?? x 4]\n#> # Groups: Species, Species_remap, Species_idx\n#>   Species    Species_remap Species_idx count\n#>   <chr>      <chr>               <dbl> <dbl>\n#> 1 setosa     setosa                  0    50\n#> 2 versicolor versicolor              1    50\n#> 3 virginica  virginica               2    50\n```\n:::\n\n\n\n### SDF Partitioning\n\nSplit a Spark DataFrame into \"training\" and \"test\" datasets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npartitions <- iris_tbl %>%\n  sdf_random_split(training = 0.75, test = 0.25, seed = 1099)\n```\n:::\n\n\nThe `partitions` variable is now a `list` with two elements called `training`\nand `test`. It does not contain any data. It is just a pointer to where Spark\nhas separated the data, so nothing is downloaded into R. Use `partitions$training`\nto access the data the Spark has separated for that purpose. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- partitions$training %>%\n  ml_linear_regression(Petal_Length ~ Petal_Width)\n```\n:::\n\n\nUse `ml_predict()` to then calculate the **mse** of the \"test\" data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nml_predict(fit, partitions$test) %>%\n  mutate(resid = Petal_Length - prediction) %>%\n  summarize(mse = mean(resid ^ 2, na.rm = TRUE)) \n#> # Source: spark<?> [?? x 1]\n#>     mse\n#>   <dbl>\n#> 1 0.212\n```\n:::\n\n\n### Disconnect from Spark\n\nLastly, cleanup your session by disconnecting Spark:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_disconnect(sc)\n```\n:::\n",
    "supporting": [
      "mlib_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}