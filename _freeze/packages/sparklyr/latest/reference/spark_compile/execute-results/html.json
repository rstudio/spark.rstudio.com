{
  "hash": "d0ad852258b496843f7e2ec4fefb79ae",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Compile Scala sources into a Java Archive\"\nexecute:\n  freeze: true\n---\n\n\n\n*R/spark_compile.R*\n\n## spark_compile\n\n## Description\n Given a set of `scala` source files, compile them into a Java Archive (`jar`). \n\n\n## Usage\n```r\n \nspark_compile( \n  jar_name, \n  spark_home = NULL, \n  filter = NULL, \n  scalac = NULL, \n  jar = NULL, \n  jar_dep = NULL, \n  embedded_srcs = \"embedded_sources.R\" \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spark_home | The path to the Spark sources to be used alongside compilation. |\n| filter | An optional function, used to filter out discovered `scala` files during compilation. This can be used to ensure that e.g. certain files are only compiled with certain versions of Spark, and so on. |\n| scalac | The path to the `scalac` program to be used, for compilation of `scala` files. |\n| jar | The path to the `jar` program to be used, for generating of the resulting `jar`. |\n| jar_dep | An optional list of additional `jar` dependencies. |\n| embedded_srcs | Embedded source file(s) under `<R package root>/java` to be included in the root of the resulting jar file as resources |\n\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}