{
  "hash": "c379a3c2219a17c36ff241b48caa71de",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Intro to Model Tuning\"\nexecute:\n  eval: true\n  freeze: true\n---\n\n\n\n\n**Hyper parameter tuning**  is possible within Spark. `sparklyr` provides an interface\nthat makes it possible to setup, and run this kind of model tuning. \n\nThis article walks through the basics of setting up and running a cross validation \ntuning run using the `cells` data from the `modeldata` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(cells, package = \"modeldata\")\n```\n:::\n\n\nThe goal of the experiments in the example is to see at what point does the\nnumber of trees in the model stop improving the accuracy of the predictions.  We\nwill have Spark run multiple iterations of the same model with an increasing\nnumber of trees.\n\n## Data setup\n\nFor this example, we will use a local connection, using Spark 3.3.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\", version = \"3.3\")\n```\n:::\n\n\nThe `cells` data is copied to the Spark session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_cells <- copy_to(sc, cells, name = \"cells_tbl\")\n```\n:::\n\n\nWe will split the data into two sets, \"training\" and \"test\".  The **test** split will\nbe treated as the \"holdout\" data to be used at the end of the process to confirm\nthat we did not over fit the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_cells_split <- tbl_cells %>% \n  select(-case) %>% \n  sdf_random_split(\n    training = 0.8, \n    test = 0.2, \n    seed = 100\n    )\n```\n:::\n\n\n## Cross validator prep\n\nPreparing the cross validator requires three elements:\n\n1. An [ML Pipeline](pipelines.qmd)\n1. A `list` object containing the \"grid\", meaning the different parameters to test\n1. An \"evaluator\" that will calculate the metrics of each model run\n\n### Pipeline\n\nIn this example we will use a very simple pipeline. It will contain a\nformula step and a Random Forest model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncells_pipeline <- sc %>% \n  ml_pipeline() %>%\n  ft_r_formula(class ~ .) %>%\n  ml_random_forest_classifier(seed = 207336481)\n\n```\n:::\n\n\nEach step within a pipeline receives a unique identifier. This identifier is\nmade up of the name of the step and a UID. The UID will change every time a new\npipeline is created. Here is an example of the out put for `cells_pipeline`:\n\n```r\n#> Pipeline (Estimator) with 2 stages\n#> <pipeline__a1c04c2f_b955_4917_89b6_67cb576a779b> \n#>   Stages \n#>   |--1 RFormula (Estimator)\n#>   |    <r_formula__043dd75a_7fd6_48bb_85c5_f20b321b97cb> \n#>   |     (Parameters -- Column Names)\n#>   |      features_col: features\n#>   |      label_col: label\n#>   |     (Parameters)\n#>   |      force_index_label: FALSE\n#>   |      formula: class ~ .\n#>   |      handle_invalid: error\n#>   |      stringIndexerOrderType: frequencyDesc\n#>   |--2 RandomForestClassifier (Estimator)\n#>   |    <random_forest_classifier__52fc33ff_c4dc_44ac_b842_b873957db90a> \n#>   |     (Parameters -- Column Names)\n#>   |      features_col: features\n#>   |      label_col: label\n#>   |      prediction_col: prediction\n#>   |      probability_col: probability\n#>   |      raw_prediction_col: rawPrediction\n#>   |     (Parameters)\n#>   |      bootstrap: TRUE\n#>   |      cache_node_ids: FALSE\n#>   |      checkpoint_interval: 10\n#>   |      feature_subset_strategy: auto\n#>   |      impurity: gini\n#>   |      leafCol: \n#>   |      max_bins: 32\n#>   |      max_depth: 5\n#>   |      max_memory_in_mb: 256\n#>   |      min_info_gain: 0\n#>   |      min_instances_per_node: 1\n#>   |      minWeightFractionPerNode: 0\n#>   |      num_trees: 20\n#>   |      seed: 207336481\n#>   |      subsampling_rate: 1\n```\n\n### Grid\n\nThe way we can pass the parameters to try is via a simple `list` object. `sparklyr`\nperforms partial name matching to assign the list's entries to the `pipeline`\nsteps and the parameters.\n\nThe idea is to modify the **number of trees**  for each model run. From the output \nof the ML Pipeline above, we see that we need to modify the following:\n\n```r\n#>   |    <random_forest_classifier__52fc33ff_c4dc_44ac_b842_b873957db90a> \n#>   |                    ...\n#>   |     (Parameters)\n#>   |                    ...\n#>   |      num_trees: 20\n```\n\nIn R, we create the grid spec using the following: \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncells_grid <- list(\n  random_forest_classifier = list(  \n    num_trees = 1:20 * 5\n  )\n)\n\ncells_grid\n#> $random_forest_classifier\n#> $random_forest_classifier$num_trees\n#>  [1]   5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95\n#> [20] 100\n```\n:::\n\n\nTwo things to highlight about the grid spec:\n\n- `random_forest_classifier` is used to partially match to `random_forest_classifier__52fc33ff_c4dc_44ac_b842_b873957db90a` in the pipeline. It\nis possible to pass the entire name, but that may prevent it from working if\na new pipeline is used. A new pipeline will have a different UID.\n- For `num_trees` we passed a vector of 20 values to test with\n\n### \"Evaluator\"\n\nA metric will have to be calculated for each validation set in the folds. The \nmodel tuning function requires for that to be explicitly defined as an argument\n(`ml_cross_validator()`).  There are multiple outcomes on the `class` field in the\n`tbl_cells` table, this means that we will use `ml_multiclass_classification_evaluator()` for our \nvalidation function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncells_evaluator <- ml_multiclass_classification_evaluator(\n  x = sc,\n  metric_name = \"accuracy\"\n  )\n```\n:::\n\n\n\nThe \"evaluator\" function to use is based on the type of model that is being used\nfor tuning. Here is the list of the available \"evaluator\" functions in `sparklyr`:\n\n- [`ml_binary_classification_evaluator()`](/packages/sparklyr/latest/reference/ml_evaluator.qmd)\n- [`ml_binary_classification_eval()`](/packages/sparklyr/latest/reference/ml_evaluator.qmd)\n- [`ml_multiclass_classification_evaluator()`](/packages/sparklyr/latest/reference/ml_evaluator.qmd)\n- [`ml_classification_eval()`](/packages/sparklyr/latest/reference/ml_evaluator.qmd)\n- [`ml_regression_evaluator()`](/packages/sparklyr/latest/reference/ml_evaluator.qmd)\n\n## Model tuning\n\nAll the preparations steps come together now as arguments of `ml_cross_validator()`.\nThere are two additional arguments to consider:\n\n- `num_folds`: The number of folder for the cross validation. The higher the\nnumber, the longer it will take to complete. \n- `parallelism`: The number of threads to use when running parallel algorithms. \nDefault is 1 for serial execution.\n\nIn this example,  `cells_pipeline`, `cells_grid`, and `cells_evaluator` are\npassed to the respective arguments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncells_cv <- ml_cross_validator(\n  x = sc,\n  estimator = cells_pipeline, \n  estimator_param_maps = cells_grid,\n  evaluator = cells_evaluator,\n  num_folds = 5,\n  parallelism = 4\n)\n\ncells_cv\n#> CrossValidator (Estimator)\n#> <cross_validator__82bdd33e_d7be_4950_a432_c7184f4dfaad> \n#>  (Parameters -- Tuning)\n#>   estimator: Pipeline\n#>              <pipeline__54436c91_0624_440f_b7c3_e53f0254977d> \n#>   evaluator: MulticlassClassificationEvaluator\n#>              <multiclass_classification_evaluator__4fd9e076_f169_4e38_8768_bc9960fb97b6> \n#>     with metric accuracy \n#>   num_folds: 5 \n#>   [Tuned over 20 hyperparameter sets]\n```\n:::\n\n\nThe `ml_fit()` function will actually run the model tuning. This is where the\n**training** split of the data is used.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_cv <- ml_fit(\n  x = cells_cv, \n  dataset = tbl_cells_split$training\n  )\n```\n:::\n\n\n## Validation metrics\n\nThe `ml_validation_metrics()` function will extract the metrics from each of the\nvalues passed for **number of trees** parameter. If more than one parameter would\nhave been used, the number of results would be the total number of combinations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_metrics <- ml_validation_metrics(model_cv)\n\ncv_metrics\n#>     accuracy num_trees_1\n#> 1  0.8114324           5\n#> 2  0.8157122          10\n#> 3  0.8176925          15\n#> 4  0.8150990          20\n#> 5  0.8241812          25\n#> 6  0.8212221          30\n#> 7  0.8221358          35\n#> 8  0.8310173          40\n#> 9  0.8290158          45\n#> 10 0.8296345          50\n#> 11 0.8335203          55\n#> 12 0.8301165          60\n#> 13 0.8234423          65\n#> 14 0.8257469          70\n#> 15 0.8258069          75\n#> 16 0.8340482          80\n#> 17 0.8321753          85\n#> 18 0.8276960          90\n#> 19 0.8223973          95\n#> 20 0.8294530         100\n```\n:::\n\n\nFor easier selection, we can use a quick plot to visualize how accuracy improves\nas more trees are used, and when does the benefit plateau.\n\n\n::: {.cell warnings='false'}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\ncv_metrics %>% \n  ggplot(aes(num_trees_1, accuracy)) +\n  geom_line() +\n  geom_smooth()\n```\n\n::: {.cell-output-display}\n![](model_tuning_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n## Model selection \n\nAs seen in the previous section, 50 trees seems to be a good number to use. To\nfinalize, a new model is fit, using that number for `num_trees`.  \n\nA Model pipeline or a regular model could be used to do this. For this example \nwe will just use the single step of fitting a new model using \n`ml_random_forest_classifier()` directly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncell_model <- ml_random_forest_classifier(\n  tbl_cells_split$training, \n  class ~ ., \n  num_trees = 50\n  )\n```\n:::\n\n\n## Test data metrics\n\nThe final step is to confirm that the model is not over-fitted. We use the new\nmodel against the **test** split, and then piping it to `ml_metrics_multiclass()`\nto confirm that the accuracy is within the expected range.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncell_model %>% \n  ml_predict(tbl_cells_split$test) %>% \n  ml_metrics_multiclass()\n#> # A tibble: 1 × 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy multiclass     0.839\n```\n:::\n\n",
    "supporting": [
      "model_tuning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}