[
  {
    "objectID": "get-started/model-data.html",
    "href": "get-started/model-data.html",
    "title": "Model Data",
    "section": "",
    "text": "You can orchestrate machine learning algorithms in a Spark cluster via the machine learning functions within sparklyr. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning workflows.",
    "crumbs": [
      "Get Started",
      "Model Data"
    ]
  },
  {
    "objectID": "get-started/model-data.html#exercise",
    "href": "get-started/model-data.html#exercise",
    "title": "Model Data",
    "section": "Exercise",
    "text": "Exercise\nHere’s an example where we use ml_linear_regression() to fit a linear regression model. We’ll use the built-in mtcars dataset, and see if we can predict a car’s fuel consumption (mpg) based on its weight (wt), and the number of cylinders the engine contains (cyl). We’ll assume in each case that the relationship between mpg and each of our features is linear.\n\nInitialize the environment\nWe will start by creating a local Spark session and load the mtcars data frame to it.\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\")\nmtcars_tbl &lt;- copy_to(sc, mtcars, overwrite = TRUE)\n\n\n\nPrepare the data\nSpark provides data frame operations that makes it easier to prepare data for modeling. In this case, we will use the sdf_partition() command to divide the mtcars data into “training” and “test”.\n\npartitions &lt;- mtcars_tbl %&gt;%\n  select(mpg, wt, cyl) %&gt;% \n  sdf_random_split(training = 0.5, test = 0.5, seed = 1099)\n\nNote that the newly created partitions variable does not contain data, it contains a pointer to where the data was split within Spark. That means that no data is downloaded to the R session.\n\n\nFit the model\nNext, we will fit a linear model to the training data set:\n\nfit &lt;- partitions$training %&gt;%\n  ml_linear_regression(mpg ~ .)\n\nfit\n#&gt; Formula: mpg ~ .\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          wt         cyl \n#&gt;   38.927395   -4.131014   -0.938832\n\nFor linear regression models produced by Spark, we can use summary() to learn a bit more about the quality of our fit, and the statistical significance of each of our predictors.\n\nsummary(fit)\n#&gt; Deviance Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.4891 -1.5262 -0.1481  0.8508  6.3162 \n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)          wt         cyl \n#&gt;   38.927395   -4.131014   -0.938832 \n#&gt; \n#&gt; R-Squared: 0.8469\n#&gt; Root Mean Squared Error: 2.416\n\n\n\nUse the model\nWe can use ml_predict() to create a Spark data frame that contains the predictions against the testing data set.\n\npred &lt;- ml_predict(fit, partitions$test)\n\nhead(pred)\n#&gt; # Source: spark&lt;?&gt; [?? x 4]\n#&gt;     mpg    wt   cyl prediction\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1  14.3  3.57     8      16.7 \n#&gt; 2  14.7  5.34     8       9.34\n#&gt; 3  15    3.57     8      16.7 \n#&gt; 4  15.2  3.44     8      17.2 \n#&gt; 5  15.2  3.78     8      15.8 \n#&gt; 6  15.5  3.52     8      16.9\n\n\n\nFurther reading\nSpark machine learning supports a wide array of algorithms and feature transformations and as illustrated above it’s easy to chain these functions together with dplyr pipelines. To learn more see the Machine Learning article on this site. For a list of Spark ML models available through sparklyr visit Reference - ML",
    "crumbs": [
      "Get Started",
      "Model Data"
    ]
  },
  {
    "objectID": "get-started/read-data.html",
    "href": "get-started/read-data.html",
    "title": "Read Data",
    "section": "",
    "text": "A new Spark session will contain no data. The first step is to either load data into your Spark session’s memory, or point Spark to the location of the data so it can access the data on-demand.",
    "crumbs": [
      "Get Started",
      "Read Data"
    ]
  },
  {
    "objectID": "get-started/read-data.html#exercise",
    "href": "get-started/read-data.html#exercise",
    "title": "Read Data",
    "section": "Exercise",
    "text": "Exercise\nFor this exercise, we will start a “local” Spark session, and then transfer data from our R environment to the Spark session’s memory. To do that, we will use the copy_to() command:\n\nlibrary(sparklyr)\n\nsc &lt;- spark_connect(master = \"local\")\n\ntbl_mtcars &lt;- copy_to(sc, mtcars, \"spark_mtcars\")\n\nIf you are using the RStudio IDE, you will notice a new table in the Connections pane. The name of that table is spark_mtcars. That is the name of the data set inside the Spark memory. The tbl_mtcars variable does not contain any mtcars data, this variable contains the info that points to the location where the Spark session loaded the data to.\nCalling the tbl_mtcars variable in R will download the first 1,000 records and display them :\n\ntbl_mtcars\n#&gt; # Source: spark&lt;spark_mtcars&gt; [?? x 11]\n#&gt;      mpg   cyl  disp    hp  drat    wt  qsec    vs    am\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1  21       6  160    110  3.9   2.62  16.5     0     1\n#&gt;  2  21       6  160    110  3.9   2.88  17.0     0     1\n#&gt;  3  22.8     4  108     93  3.85  2.32  18.6     1     1\n#&gt;  4  21.4     6  258    110  3.08  3.22  19.4     1     0\n#&gt;  5  18.7     8  360    175  3.15  3.44  17.0     0     0\n#&gt;  6  18.1     6  225    105  2.76  3.46  20.2     1     0\n#&gt;  7  14.3     8  360    245  3.21  3.57  15.8     0     0\n#&gt;  8  24.4     4  147.    62  3.69  3.19  20       1     0\n#&gt;  9  22.8     4  141.    95  3.92  3.15  22.9     1     0\n#&gt; 10  19.2     6  168.   123  3.92  3.44  18.3     1     0\n#&gt; # … with more rows, and 2 more variables: gear &lt;dbl&gt;,\n#&gt; #   carb &lt;dbl&gt;\n\nNotice that at the top of the data print out, it is noted that records were downloaded from Spark: Source: spark….\nTo clean up the session, we will now stop the Spark session:\n\nspark_disconnect(sc)",
    "crumbs": [
      "Get Started",
      "Read Data"
    ]
  },
  {
    "objectID": "get-started/read-data.html#working-with-files",
    "href": "get-started/read-data.html#working-with-files",
    "title": "Read Data",
    "section": "Working with files",
    "text": "Working with files\nIn a formal Spark environment, it will be rare when we would have to upload data from R into Spark.\nUsing sparklyr, you can tell Spark to read and write data. Spark is able to interact with multiple types of file systems, such as HDFS, S3 and local. Additionally, Spark is able to read several file types such as CSV, Parquet, Delta and JSON. sparklyr provides functions that makes it easy to access these features. See the Spark Data section for a full list of available functions.\nThe following command will tell Spark to read a CSV file, and to also load it into Spark memory.\n\n# Do not run the next following command. It is for example purposes only.\nspark_read_csv(sc, name = \"test_table\",  path = \"/test/path/test_file.csv\")",
    "crumbs": [
      "Get Started",
      "Read Data"
    ]
  },
  {
    "objectID": "learn-more.html",
    "href": "learn-more.html",
    "title": "Learn More",
    "section": "",
    "text": "A 2 page visual guide to sparklyr functions. It provides a concise description of each function. It also has diagrams of how several Feature Transformers work.\n\n\n\n\nThis free, online book, intends to take someone unfamiliar with Spark or R and help you become proficient by teaching you a set of tools, skills and practices applicable to large-scale data science."
  },
  {
    "objectID": "learn-more.html#cheatsheet",
    "href": "learn-more.html#cheatsheet",
    "title": "Learn More",
    "section": "",
    "text": "A 2 page visual guide to sparklyr functions. It provides a concise description of each function. It also has diagrams of how several Feature Transformers work."
  },
  {
    "objectID": "learn-more.html#book",
    "href": "learn-more.html#book",
    "title": "Learn More",
    "section": "",
    "text": "This free, online book, intends to take someone unfamiliar with Spark or R and help you become proficient by teaching you a set of tools, skills and practices applicable to large-scale data science."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html",
    "title": "Loads an MLeap bundle",
    "section": "",
    "text": "R/mleap.R"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#mleap_load_bundle",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#mleap_load_bundle",
    "title": "Loads an MLeap bundle",
    "section": "mleap_load_bundle",
    "text": "mleap_load_bundle"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#description",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#description",
    "title": "Loads an MLeap bundle",
    "section": "Description",
    "text": "Description\nLoads an MLeap bundle"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#usage",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#usage",
    "title": "Loads an MLeap bundle",
    "section": "Usage",
    "text": "Usage\nmleap_load_bundle(path)"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#arguments",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#arguments",
    "title": "Loads an MLeap bundle",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\npath\nPath to the exported bundle zip file."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#value",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#value",
    "title": "Loads an MLeap bundle",
    "section": "Value",
    "text": "Value\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html",
    "title": "MLeap model schema",
    "section": "",
    "text": "R/mleap.R"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#mleap_model_schema",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#mleap_model_schema",
    "title": "MLeap model schema",
    "section": "mleap_model_schema",
    "text": "mleap_model_schema"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#description",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#description",
    "title": "MLeap model schema",
    "section": "Description",
    "text": "Description\nReturns the schema of an MLeap transformer."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#usage",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#usage",
    "title": "MLeap model schema",
    "section": "Usage",
    "text": "Usage\nmleap_model_schema(x)"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#arguments",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#arguments",
    "title": "MLeap model schema",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#value",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#value",
    "title": "MLeap model schema",
    "section": "Value",
    "text": "Value\nA data frame of the model schema."
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html",
    "href": "packages/mleap/dev/reference/install_maven.html",
    "title": "Install Maven",
    "section": "",
    "text": "R/installers.R"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#install_maven",
    "href": "packages/mleap/dev/reference/install_maven.html#install_maven",
    "title": "Install Maven",
    "section": "install_maven",
    "text": "install_maven"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#description",
    "href": "packages/mleap/dev/reference/install_maven.html#description",
    "title": "Install Maven",
    "section": "Description",
    "text": "Description\nThis function installs Apache Maven."
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#usage",
    "href": "packages/mleap/dev/reference/install_maven.html#usage",
    "title": "Install Maven",
    "section": "Usage",
    "text": "Usage\ninstall_maven(dir = NULL, version = NULL)"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#arguments",
    "href": "packages/mleap/dev/reference/install_maven.html#arguments",
    "title": "Install Maven",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\ndir\n(Optional) Directory to install maven in. Defaults to maven/ under user’s home directory.\n\n\nversion\nVersion of Maven to install, defaults to the latest version tested with this package."
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#examples",
    "href": "packages/mleap/dev/reference/install_maven.html#examples",
    "title": "Install Maven",
    "section": "Examples",
    "text": "Examples\n\nlibrary(mleap)\ninstall_maven()"
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html",
    "title": "Export a Spark pipeline for serving",
    "section": "",
    "text": "R/mleap.R"
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#ml_write_bundle",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#ml_write_bundle",
    "title": "Export a Spark pipeline for serving",
    "section": "ml_write_bundle",
    "text": "ml_write_bundle"
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#description",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#description",
    "title": "Export a Spark pipeline for serving",
    "section": "Description",
    "text": "Description\nThis functions serializes a Spark pipeline model into an MLeap bundle."
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#usage",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#usage",
    "title": "Export a Spark pipeline for serving",
    "section": "Usage",
    "text": "Usage\nml_write_bundle(x, sample_input, path, overwrite = FALSE)"
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#arguments",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#arguments",
    "title": "Export a Spark pipeline for serving",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark pipeline model object.\n\n\nsample_input\nA sample input Spark DataFrame with the expected schema.\n\n\npath\nWhere to save the bundle.\n\n\noverwrite\nWhether to overwrite an existing file, defaults to FALSE."
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#examples",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#examples",
    "title": "Export a Spark pipeline for serving",
    "section": "Examples",
    "text": "Examples\n\nlibrary(mleap)\nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\") \nmtcars_tbl &lt;- sdf_copy_to(sc, mtcars, overwrite = TRUE) \npipeline &lt;- ml_pipeline(sc) %&gt;% \n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %&gt;% \n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %&gt;% \n  ml_gbt_regressor(label_col = \"mpg\") \npipeline_model &lt;- ml_fit(pipeline, mtcars_tbl) \nmodel_path &lt;- file.path(tempdir(), \"mtcars_model.zip\") \nml_write_bundle(pipeline_model,  \n                mtcars_tbl, \n                model_path, \n                overwrite = TRUE)"
  },
  {
    "objectID": "packages/mleap/dev/news.html",
    "href": "packages/mleap/dev/news.html",
    "title": "mleap 1.0.0",
    "section": "",
    "text": "mleap 1.0.0\n\nBreaking change: ml_write_bundle() now takes the sample_input instead of dataset.\nSupport for Spark 2.4 and MLeap 0.14.\nInteger inputs are now supported (#36).\n\n\n\nmleap 0.1.3\n\nSupport MLeap 0.12.0.\n\n\n\nmleap 0.1.2\n\nSupport MLeap 0.10.1 and Spark 2.3.0.\n\n\n\nmleap 0.1.1\n\nAllow heterogenous inputs (numeric and character) in mleap_transform() (#16)."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html",
    "title": "Loads an MLeap bundle",
    "section": "",
    "text": "R/mleap.R"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#mleap_load_bundle",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#mleap_load_bundle",
    "title": "Loads an MLeap bundle",
    "section": "mleap_load_bundle",
    "text": "mleap_load_bundle"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#description",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#description",
    "title": "Loads an MLeap bundle",
    "section": "Description",
    "text": "Description\nLoads an MLeap bundle"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#usage",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#usage",
    "title": "Loads an MLeap bundle",
    "section": "Usage",
    "text": "Usage\nmleap_load_bundle(path)"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#arguments",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#arguments",
    "title": "Loads an MLeap bundle",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\npath\nPath to the exported bundle zip file."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#value",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#value",
    "title": "Loads an MLeap bundle",
    "section": "Value",
    "text": "Value\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html",
    "title": "MLeap model schema",
    "section": "",
    "text": "R/mleap.R"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#mleap_model_schema",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#mleap_model_schema",
    "title": "MLeap model schema",
    "section": "mleap_model_schema",
    "text": "mleap_model_schema"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#description",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#description",
    "title": "MLeap model schema",
    "section": "Description",
    "text": "Description\nReturns the schema of an MLeap transformer."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#usage",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#usage",
    "title": "MLeap model schema",
    "section": "Usage",
    "text": "Usage\nmleap_model_schema(x)"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#arguments",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#arguments",
    "title": "MLeap model schema",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#value",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#value",
    "title": "MLeap model schema",
    "section": "Value",
    "text": "Value\nA data frame of the model schema."
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html",
    "href": "packages/mleap/latest/reference/install_maven.html",
    "title": "Install Maven",
    "section": "",
    "text": "R/installers.R"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#install_maven",
    "href": "packages/mleap/latest/reference/install_maven.html#install_maven",
    "title": "Install Maven",
    "section": "install_maven",
    "text": "install_maven"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#description",
    "href": "packages/mleap/latest/reference/install_maven.html#description",
    "title": "Install Maven",
    "section": "Description",
    "text": "Description\nThis function installs Apache Maven."
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#usage",
    "href": "packages/mleap/latest/reference/install_maven.html#usage",
    "title": "Install Maven",
    "section": "Usage",
    "text": "Usage\ninstall_maven(dir = NULL, version = NULL)"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#arguments",
    "href": "packages/mleap/latest/reference/install_maven.html#arguments",
    "title": "Install Maven",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\ndir\n(Optional) Directory to install maven in. Defaults to maven/ under user’s home directory.\n\n\nversion\nVersion of Maven to install, defaults to the latest version tested with this package."
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#examples",
    "href": "packages/mleap/latest/reference/install_maven.html#examples",
    "title": "Install Maven",
    "section": "Examples",
    "text": "Examples\n\nlibrary(mleap)\ninstall_maven()"
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html",
    "title": "Export a Spark pipeline for serving",
    "section": "",
    "text": "R/mleap.R"
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#ml_write_bundle",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#ml_write_bundle",
    "title": "Export a Spark pipeline for serving",
    "section": "ml_write_bundle",
    "text": "ml_write_bundle"
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#description",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#description",
    "title": "Export a Spark pipeline for serving",
    "section": "Description",
    "text": "Description\nThis functions serializes a Spark pipeline model into an MLeap bundle."
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#usage",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#usage",
    "title": "Export a Spark pipeline for serving",
    "section": "Usage",
    "text": "Usage\nml_write_bundle(x, sample_input, path, overwrite = FALSE)"
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#arguments",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#arguments",
    "title": "Export a Spark pipeline for serving",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark pipeline model object.\n\n\nsample_input\nA sample input Spark DataFrame with the expected schema.\n\n\npath\nWhere to save the bundle.\n\n\noverwrite\nWhether to overwrite an existing file, defaults to FALSE."
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#examples",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#examples",
    "title": "Export a Spark pipeline for serving",
    "section": "Examples",
    "text": "Examples\n\nlibrary(mleap)\nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\") \nmtcars_tbl &lt;- sdf_copy_to(sc, mtcars, overwrite = TRUE) \npipeline &lt;- ml_pipeline(sc) %&gt;% \n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %&gt;% \n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %&gt;% \n  ml_gbt_regressor(label_col = \"mpg\") \npipeline_model &lt;- ml_fit(pipeline, mtcars_tbl) \nmodel_path &lt;- file.path(tempdir(), \"mtcars_model.zip\") \nml_write_bundle(pipeline_model,  \n                mtcars_tbl, \n                model_path, \n                overwrite = TRUE)"
  },
  {
    "objectID": "packages/mleap/latest/news.html",
    "href": "packages/mleap/latest/news.html",
    "title": "mleap 1.0.0",
    "section": "",
    "text": "mleap 1.0.0\n\nBreaking change: ml_write_bundle() now takes the sample_input instead of dataset.\nSupport for Spark 2.4 and MLeap 0.14.\nInteger inputs are now supported (#36).\n\n\n\nmleap 0.1.3\n\nSupport MLeap 0.12.0.\n\n\n\nmleap 0.1.2\n\nSupport MLeap 0.10.1 and Spark 2.3.0.\n\n\n\nmleap 0.1.1\n\nAllow heterogenous inputs (numeric and character) in mleap_transform() (#16).",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html",
    "title": "XGBoost Regressor",
    "section": "",
    "text": "R/xgboost_regressor.R"
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#xgboost_regressor",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#xgboost_regressor",
    "title": "XGBoost Regressor",
    "section": "xgboost_regressor",
    "text": "xgboost_regressor"
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#description",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#description",
    "title": "XGBoost Regressor",
    "section": "Description",
    "text": "Description\nXGBoost regressor for Spark."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#usage",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#usage",
    "title": "XGBoost Regressor",
    "section": "Usage",
    "text": "Usage\nxgboost_regressor( \n  x, \n  formula = NULL, \n  eta = 0.3, \n  gamma = 0, \n  max_depth = 6, \n  min_child_weight = 1, \n  max_delta_step = 0, \n  grow_policy = \"depthwise\", \n  max_bins = 16, \n  subsample = 1, \n  colsample_bytree = 1, \n  colsample_bylevel = 1, \n  lambda = 1, \n  alpha = 0, \n  tree_method = \"auto\", \n  sketch_eps = 0.03, \n  scale_pos_weight = 1, \n  sample_type = \"uniform\", \n  normalize_type = \"tree\", \n  rate_drop = 0, \n  skip_drop = 0, \n  lambda_bias = 0, \n  tree_limit = 0, \n  num_round = 1, \n  num_workers = 1, \n  nthread = 1, \n  use_external_memory = FALSE, \n  silent = 0, \n  custom_obj = NULL, \n  custom_eval = NULL, \n  missing = NaN, \n  seed = 0, \n  timeout_request_workers = 30 * 60 * 1000, \n  checkpoint_path = \"\", \n  checkpoint_interval = -1, \n  objective = \"reg:linear\", \n  base_score = 0.5, \n  train_test_ratio = 1, \n  num_early_stopping_rounds = 0, \n  objective_type = \"regression\", \n  eval_metric = NULL, \n  maximize_evaluation_metrics = FALSE, \n  base_margin_col = NULL, \n  weight_col = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"xgboost_regressor_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#arguments",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#arguments",
    "title": "XGBoost Regressor",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\neta\nStep size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features and eta actually shrinks the feature weights to make the boosting process more conservative. [default=0.3] range: [0,1]\n\n\ngamma\nMinimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be. [default=0]\n\n\nmax_depth\nMaximum depth of a tree, increase this value will make model more complex / likely to be overfitting. [default=6]\n\n\nmin_child_weight\nMinimum sum of instance weight(hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be. [default=1]\n\n\nmax_delta_step\nMaximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update. [default=0]\n\n\ngrow_policy\nGrowth policy for fast histogram algorithm.\n\n\nmax_bins\nMaximum number of bins in histogram.\n\n\nsubsample\nSubsample ratio of the training instance. Setting it to 0.5 means that XGBoost randomly collected half of the data instances to grow trees and this will prevent overfitting. [default=1] range:(0,1]\n\n\ncolsample_bytree\nSubsample ratio of columns when constructing each tree. [default=1] range: (0,1]\n\n\ncolsample_bylevel\nSubsample ratio of columns for each split, in each level. [default=1] range: (0,1]\n\n\nlambda\nL2 regularization term on weights, increase this value will make model more conservative. [default=1]\n\n\nalpha\nL1 regularization term on weights, increase this value will make model more conservative, defaults to 0.\n\n\ntree_method\nThe tree construction algorithm used in XGBoost. options: ‘auto’, ‘exact’, ‘approx’ [default=‘auto’]\n\n\nsketch_eps\nThis is only used for approximate greedy algorithm. This roughly translated into O(1 / sketch_eps) number of bins. Compared to directly select number of bins, this comes with theoretical guarantee with sketch accuracy. [default=0.03] range: (0, 1)\n\n\nscale_pos_weight\nControl the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative cases) / sum(positive cases). [default=1]\n\n\nsample_type\nParameter for Dart booster. Type of sampling algorithm. “uniform”: dropped trees are selected uniformly. “weighted”: dropped trees are selected in proportion to weight. [default=“uniform”]\n\n\nnormalize_type\nParameter of Dart booster. type of normalization algorithm, options: ‘tree’, ‘forest’ . [default=“tree”]\n\n\nrate_drop\nParameter of Dart booster. dropout rate. [default=0.0] range: [0.0, 1.0]\n\n\nskip_drop\nParameter of Dart booster. probability of skip dropout. If a dropout is skipped, new trees are added in the same manner as gbtree. [default=0.0] range: [0.0, 1.0]\n\n\nlambda_bias\nParameter of linear booster L2 regularization term on bias, default 0 (no L1 reg on bias because it is not important.)\n\n\ntree_limit\nLimit number of trees in the prediction; defaults to 0 (use all trees.)\n\n\nnum_round\nThe number of rounds for boosting.\n\n\nnum_workers\nnumber of workers used to train xgboost model. Defaults to 1.\n\n\nnthread\nNumber of threads used by per worker. Defaults to 1.\n\n\nuse_external_memory\nThe tree construction algorithm used in XGBoost. options: ‘auto’, ‘exact’, ‘approx’ [default=‘auto’]\n\n\nsilent\n0 means printing running messages, 1 means silent mode. default: 0\n\n\ncustom_obj\nCustomized objective function provided by user. Currently unsupported.\n\n\ncustom_eval\nCustomized evaluation function provided by user. Currently unsupported.\n\n\nmissing\nThe value treated as missing. default: Float.NaN\n\n\nseed\nRandom seed for the C++ part of XGBoost and train/test splitting.\n\n\ntimeout_request_workers\nthe maximum time to wait for the job requesting new workers. default: 30 minutes\n\n\ncheckpoint_path\nThe hdfs folder to load and save checkpoint boosters.\n\n\ncheckpoint_interval\nParam for set checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the trained model will get checkpointed every 10 iterations. Note: checkpoint_path must also be set if the checkpoint interval is greater than 0.\n\n\nobjective\nSpecify the learning task and the corresponding learning objective. options: reg:linear, reg:logistic, binary:logistic, binary:logitraw, count:poisson, multi:softmax, multi:softprob, rank:pairwise, reg:gamma. default: reg:linear.\n\n\nbase_score\nParam for initial prediction (aka base margin) column name. Defaults to 0.5.\n\n\ntrain_test_ratio\nFraction of training points to use for testing.\n\n\nnum_early_stopping_rounds\nIf non-zero, the training will be stopped after a specified number of consecutive increases in any evaluation metric.\n\n\nobjective_type\nThe learning objective type of the specified custom objective and eval. Corresponding type will be assigned if custom objective is defined options: regression, classification.\n\n\neval_metric\nEvaluation metrics for validation data, a default metric will be assigned according to objective(rmse for regression, and error for classification, mean average precision for ranking). options: rmse, mae, logloss, error, merror, mlogloss, auc, aucpr, ndcg, map, gamma-deviance\n\n\nmaximize_evaluation_metrics\nWhether to maximize evaluation metrics. Defaults to FALSE (for minization.)\n\n\nbase_margin_col\nParam for initial prediction (aka base margin) column name.\n\n\nweight_col\nWeight column.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#details",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#details",
    "title": "XGBoost Regressor",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#value",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#value",
    "title": "XGBoost Regressor",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula\nis specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#see-also",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#see-also",
    "title": "XGBoost Regressor",
    "section": "See Also",
    "text": "See Also\nSee http://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms."
  },
  {
    "objectID": "packages/sparkxgb/latest/index.html",
    "href": "packages/sparkxgb/latest/index.html",
    "title": "sparkxgb",
    "section": "",
    "text": "sparkxgb is a sparklyr extension that provides an interface to XGBoost on Spark.\n\n\n\nYou can install the development version of sparkxgb with:\n# sparkxgb requires the development version of sparklyr\ndevtools::install_github(\"rstudio/sparklyr\")\ndevtools::install_github(\"rstudio/sparkxgb\")\n\n\n\nsparkxgb supports the familiar formula interface for specifying models:\nlibrary(sparkxgb)\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc &lt;- spark_connect(master = \"local\")\niris_tbl &lt;- sdf_copy_to(sc, iris)\n\nxgb_model &lt;- xgboost_classifier(\n  iris_tbl,\n  Species ~ .,\n  num_class = 3,\n  num_round = 50,\n  max_depth = 4\n)\n\nxgb_model %&gt;%\n  ml_predict(iris_tbl) %&gt;%\n  select(Species, predicted_label, starts_with(\"probability_\")) %&gt;%\n  glimpse()\n#&gt; Observations: ??\n#&gt; Variables: 5\n#&gt; Database: spark_connection\n#&gt; $ Species                &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"…\n#&gt; $ predicted_label        &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"…\n#&gt; $ probability_versicolor &lt;dbl&gt; 0.003566429, 0.003564076, 0.003566429, 0.…\n#&gt; $ probability_virginica  &lt;dbl&gt; 0.001423170, 0.002082058, 0.001423170, 0.…\n#&gt; $ probability_setosa     &lt;dbl&gt; 0.9950104, 0.9943539, 0.9950104, 0.995010…\nIt also provides a Pipelines API, which means you can use a xgboost_classifier or xgboost_regressor in a pipeline as any Estimator, and do things like hyperparameter tuning:\npipeline &lt;- ml_pipeline(sc) %&gt;%\n  ft_r_formula(Species ~ .) %&gt;%\n  xgboost_classifier(num_class = 3)\n\nparam_grid &lt;- list(\n  xgboost = list(\n    max_depth = c(1, 5),\n    num_round = c(10, 50)\n  )\n)\n\ncv &lt;- ml_cross_validator(\n  sc,\n  estimator = pipeline,\n  evaluator = ml_multiclass_classification_evaluator(\n    sc,\n    label_col = \"label\",\n    raw_prediction_col = \"rawPrediction\"\n  ),\n  estimator_param_maps = param_grid\n)\n\ncv_model &lt;- cv %&gt;%\n  ml_fit(iris_tbl)\n\nsummary(cv_model)\n#&gt; Summary for CrossValidatorModel\n#&gt;             &lt;cross_validator_ebc61803a06b&gt;\n#&gt;\n#&gt; Tuned Pipeline\n#&gt;   with metric f1\n#&gt;   over 4 hyperparameter sets\n#&gt;   via 3-fold cross validation\n#&gt;\n#&gt; Estimator: Pipeline\n#&gt;            &lt;pipeline_ebc62f635bb6&gt;\n#&gt; Evaluator: MulticlassClassificationEvaluator\n#&gt;            &lt;multiclass_classification_evaluator_ebc65fbf8a19&gt;\n#&gt;\n#&gt; Results Summary:\n#&gt;          f1 num_round_1 max_depth_1\n#&gt; 1 0.9549670          10           1\n#&gt; 2 0.9674460          10           5\n#&gt; 3 0.9488665          50           1\n#&gt; 4 0.9613854          50           5",
    "crumbs": [
      "sparkxgb"
    ]
  },
  {
    "objectID": "packages/sparkxgb/latest/index.html#overview",
    "href": "packages/sparkxgb/latest/index.html#overview",
    "title": "sparkxgb",
    "section": "",
    "text": "sparkxgb is a sparklyr extension that provides an interface to XGBoost on Spark.",
    "crumbs": [
      "sparkxgb"
    ]
  },
  {
    "objectID": "packages/sparkxgb/latest/index.html#installation",
    "href": "packages/sparkxgb/latest/index.html#installation",
    "title": "sparkxgb",
    "section": "",
    "text": "You can install the development version of sparkxgb with:\n# sparkxgb requires the development version of sparklyr\ndevtools::install_github(\"rstudio/sparklyr\")\ndevtools::install_github(\"rstudio/sparkxgb\")",
    "crumbs": [
      "sparkxgb"
    ]
  },
  {
    "objectID": "packages/sparkxgb/latest/index.html#example",
    "href": "packages/sparkxgb/latest/index.html#example",
    "title": "sparkxgb",
    "section": "",
    "text": "sparkxgb supports the familiar formula interface for specifying models:\nlibrary(sparkxgb)\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc &lt;- spark_connect(master = \"local\")\niris_tbl &lt;- sdf_copy_to(sc, iris)\n\nxgb_model &lt;- xgboost_classifier(\n  iris_tbl,\n  Species ~ .,\n  num_class = 3,\n  num_round = 50,\n  max_depth = 4\n)\n\nxgb_model %&gt;%\n  ml_predict(iris_tbl) %&gt;%\n  select(Species, predicted_label, starts_with(\"probability_\")) %&gt;%\n  glimpse()\n#&gt; Observations: ??\n#&gt; Variables: 5\n#&gt; Database: spark_connection\n#&gt; $ Species                &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"…\n#&gt; $ predicted_label        &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"…\n#&gt; $ probability_versicolor &lt;dbl&gt; 0.003566429, 0.003564076, 0.003566429, 0.…\n#&gt; $ probability_virginica  &lt;dbl&gt; 0.001423170, 0.002082058, 0.001423170, 0.…\n#&gt; $ probability_setosa     &lt;dbl&gt; 0.9950104, 0.9943539, 0.9950104, 0.995010…\nIt also provides a Pipelines API, which means you can use a xgboost_classifier or xgboost_regressor in a pipeline as any Estimator, and do things like hyperparameter tuning:\npipeline &lt;- ml_pipeline(sc) %&gt;%\n  ft_r_formula(Species ~ .) %&gt;%\n  xgboost_classifier(num_class = 3)\n\nparam_grid &lt;- list(\n  xgboost = list(\n    max_depth = c(1, 5),\n    num_round = c(10, 50)\n  )\n)\n\ncv &lt;- ml_cross_validator(\n  sc,\n  estimator = pipeline,\n  evaluator = ml_multiclass_classification_evaluator(\n    sc,\n    label_col = \"label\",\n    raw_prediction_col = \"rawPrediction\"\n  ),\n  estimator_param_maps = param_grid\n)\n\ncv_model &lt;- cv %&gt;%\n  ml_fit(iris_tbl)\n\nsummary(cv_model)\n#&gt; Summary for CrossValidatorModel\n#&gt;             &lt;cross_validator_ebc61803a06b&gt;\n#&gt;\n#&gt; Tuned Pipeline\n#&gt;   with metric f1\n#&gt;   over 4 hyperparameter sets\n#&gt;   via 3-fold cross validation\n#&gt;\n#&gt; Estimator: Pipeline\n#&gt;            &lt;pipeline_ebc62f635bb6&gt;\n#&gt; Evaluator: MulticlassClassificationEvaluator\n#&gt;            &lt;multiclass_classification_evaluator_ebc65fbf8a19&gt;\n#&gt;\n#&gt; Results Summary:\n#&gt;          f1 num_round_1 max_depth_1\n#&gt; 1 0.9549670          10           1\n#&gt; 2 0.9674460          10           5\n#&gt; 3 0.9488665          50           1\n#&gt; 4 0.9613854          50           5",
    "crumbs": [
      "sparkxgb"
    ]
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html",
    "href": "packages/graphframes/latest/reference/gf_scc.html",
    "title": "Strongly connected components",
    "section": "",
    "text": "R/gf_scc.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#gf_scc",
    "href": "packages/graphframes/latest/reference/gf_scc.html#gf_scc",
    "title": "Strongly connected components",
    "section": "gf_scc",
    "text": "gf_scc"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#description",
    "href": "packages/graphframes/latest/reference/gf_scc.html#description",
    "title": "Strongly connected components",
    "section": "Description",
    "text": "Description\nCompute the strongly connected component (SCC) of each vertex and return a DataFrame with each vertex assigned to the SCC containing that vertex."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#usage",
    "href": "packages/graphframes/latest/reference/gf_scc.html#usage",
    "title": "Strongly connected components",
    "section": "Usage",
    "text": "Usage\ngf_scc(x, max_iter, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_scc.html#arguments",
    "title": "Strongly connected components",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\nmax_iter\nMaximum number of iterations.\n\n\n…\nOptional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#examples",
    "href": "packages/graphframes/latest/reference/gf_scc.html#examples",
    "title": "Strongly connected components",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ng &lt;- gf_friends(sc) \ngf_scc(g, max_iter = 10)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triplets.html",
    "href": "packages/graphframes/latest/reference/gf_triplets.html",
    "title": "Triplets of graph",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triplets.html#gf_triplets",
    "href": "packages/graphframes/latest/reference/gf_triplets.html#gf_triplets",
    "title": "Triplets of graph",
    "section": "gf_triplets",
    "text": "gf_triplets"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triplets.html#description",
    "href": "packages/graphframes/latest/reference/gf_triplets.html#description",
    "title": "Triplets of graph",
    "section": "Description",
    "text": "Description\nTriplets of graph"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triplets.html#usage",
    "href": "packages/graphframes/latest/reference/gf_triplets.html#usage",
    "title": "Triplets of graph",
    "section": "Usage",
    "text": "Usage\ngf_triplets(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triplets.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_triplets.html#arguments",
    "title": "Triplets of graph",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html",
    "href": "packages/graphframes/latest/reference/gf_star.html",
    "title": "Generate a star graph",
    "section": "",
    "text": "R/gf_examples.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#gf_star",
    "href": "packages/graphframes/latest/reference/gf_star.html#gf_star",
    "title": "Generate a star graph",
    "section": "gf_star",
    "text": "gf_star"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#description",
    "href": "packages/graphframes/latest/reference/gf_star.html#description",
    "title": "Generate a star graph",
    "section": "Description",
    "text": "Description\nReturns a star graph with Long ID type, consisting of a central element indexed 0 (the root) and the n other leaf vertices 1, 2, …, n."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#usage",
    "href": "packages/graphframes/latest/reference/gf_star.html#usage",
    "title": "Generate a star graph",
    "section": "Usage",
    "text": "Usage\ngf_star(sc, n)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_star.html#arguments",
    "title": "Generate a star graph",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nThe number of leaves."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#examples",
    "href": "packages/graphframes/latest/reference/gf_star.html#examples",
    "title": "Generate a star graph",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ngf_star(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html",
    "href": "packages/graphframes/latest/reference/gf_chain.html",
    "title": "Chain graph",
    "section": "",
    "text": "R/gf_examples.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#gf_chain",
    "href": "packages/graphframes/latest/reference/gf_chain.html#gf_chain",
    "title": "Chain graph",
    "section": "gf_chain",
    "text": "gf_chain"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#description",
    "href": "packages/graphframes/latest/reference/gf_chain.html#description",
    "title": "Chain graph",
    "section": "Description",
    "text": "Description\nReturns a chain graph of the given size with Long ID type. The vertex IDs are 0, 1, …, n-1, and the edges are (0, 1), (1, 2), …., (n-2, n-1)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#usage",
    "href": "packages/graphframes/latest/reference/gf_chain.html#usage",
    "title": "Chain graph",
    "section": "Usage",
    "text": "Usage\ngf_chain(sc, n)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_chain.html#arguments",
    "title": "Chain graph",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSize of the graph to return."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#examples",
    "href": "packages/graphframes/latest/reference/gf_chain.html#examples",
    "title": "Chain graph",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ngf_chain(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html",
    "title": "Generate two blobs",
    "section": "",
    "text": "R/gf_examples.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#gf_two_blobs",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#gf_two_blobs",
    "title": "Generate two blobs",
    "section": "gf_two_blobs",
    "text": "gf_two_blobs"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#description",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#description",
    "title": "Generate two blobs",
    "section": "Description",
    "text": "Description\nTwo densely connected blobs (vertices 0-&gt;n-1 and n-&gt;2n-1) connected by a single edge (0-&gt;n)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#usage",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#usage",
    "title": "Generate two blobs",
    "section": "Usage",
    "text": "Usage\ngf_two_blobs(sc, blob_size)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#arguments",
    "title": "Generate two blobs",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nblob_size\nThe size of each blob."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#examples",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#examples",
    "title": "Generate two blobs",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ngf_two_blobs(sc, 3)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edge_columns.html",
    "href": "packages/graphframes/latest/reference/gf_edge_columns.html",
    "title": "Edges column names",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edge_columns.html#gf_edge_columns",
    "href": "packages/graphframes/latest/reference/gf_edge_columns.html#gf_edge_columns",
    "title": "Edges column names",
    "section": "gf_edge_columns",
    "text": "gf_edge_columns"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edge_columns.html#description",
    "href": "packages/graphframes/latest/reference/gf_edge_columns.html#description",
    "title": "Edges column names",
    "section": "Description",
    "text": "Description\nEdges column names"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edge_columns.html#usage",
    "href": "packages/graphframes/latest/reference/gf_edge_columns.html#usage",
    "title": "Edges column names",
    "section": "Usage",
    "text": "Usage\ngf_edge_columns(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edge_columns.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_edge_columns.html#arguments",
    "title": "Edges column names",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/spark_graphframe.html",
    "href": "packages/graphframes/latest/reference/spark_graphframe.html",
    "title": "Retrieve a GraphFrame",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/spark_graphframe.html#spark_graphframe",
    "href": "packages/graphframes/latest/reference/spark_graphframe.html#spark_graphframe",
    "title": "Retrieve a GraphFrame",
    "section": "spark_graphframe",
    "text": "spark_graphframe"
  },
  {
    "objectID": "packages/graphframes/latest/reference/spark_graphframe.html#description",
    "href": "packages/graphframes/latest/reference/spark_graphframe.html#description",
    "title": "Retrieve a GraphFrame",
    "section": "Description",
    "text": "Description\nRetrieve a GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/spark_graphframe.html#usage",
    "href": "packages/graphframes/latest/reference/spark_graphframe.html#usage",
    "title": "Retrieve a GraphFrame",
    "section": "Usage",
    "text": "Usage\nspark_graphframe(x, ...) \n\nspark_graphframe(x, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/spark_graphframe.html#arguments",
    "href": "packages/graphframes/latest/reference/spark_graphframe.html#arguments",
    "title": "Retrieve a GraphFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\n…\nadditional arguments, not used"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html",
    "href": "packages/graphframes/latest/reference/gf_friends.html",
    "title": "Graph of friends in a social network.",
    "section": "",
    "text": "R/gf_examples.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#gf_friends",
    "href": "packages/graphframes/latest/reference/gf_friends.html#gf_friends",
    "title": "Graph of friends in a social network.",
    "section": "gf_friends",
    "text": "gf_friends"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#description",
    "href": "packages/graphframes/latest/reference/gf_friends.html#description",
    "title": "Graph of friends in a social network.",
    "section": "Description",
    "text": "Description\nGraph of friends in a social network."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#usage",
    "href": "packages/graphframes/latest/reference/gf_friends.html#usage",
    "title": "Graph of friends in a social network.",
    "section": "Usage",
    "text": "Usage\ngf_friends(sc)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_friends.html#arguments",
    "title": "Graph of friends in a social network.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#examples",
    "href": "packages/graphframes/latest/reference/gf_friends.html#examples",
    "title": "Graph of friends in a social network.",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\") \ngf_friends(sc)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html",
    "href": "packages/graphframes/latest/reference/gf_lpa.html",
    "title": "Label propagation algorithm (LPA)",
    "section": "",
    "text": "R/gf_lpa.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#gf_lpa",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#gf_lpa",
    "title": "Label propagation algorithm (LPA)",
    "section": "gf_lpa",
    "text": "gf_lpa"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#description",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#description",
    "title": "Label propagation algorithm (LPA)",
    "section": "Description",
    "text": "Description\nRun static Label Propagation for detecting communities in networks. Each node in the network is initially assigned to its own community. At every iteration, nodes send their community affiliation to all neighbors and update their state to the mode community affiliation of incoming messages. LPA is a standard community detection algorithm for graphs. It is very inexpensive computationally, although (1) convergence is not guaranteed and (2) one can end up with trivial solutions (all nodes are identified into a single community)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#usage",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#usage",
    "title": "Label propagation algorithm (LPA)",
    "section": "Usage",
    "text": "Usage\ngf_lpa(x, max_iter, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#arguments",
    "title": "Label propagation algorithm (LPA)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\nmax_iter\nMaximum number of iterations.\n\n\n…\nOptional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#examples",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#examples",
    "title": "Label propagation algorithm (LPA)",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ng &lt;- gf_friends(sc) \ngf_lpa(g, max_iter = 5)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_unpersist.html",
    "href": "packages/graphframes/latest/reference/gf_unpersist.html",
    "title": "Unpersist the GraphFrame",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_unpersist.html#gf_unpersist",
    "href": "packages/graphframes/latest/reference/gf_unpersist.html#gf_unpersist",
    "title": "Unpersist the GraphFrame",
    "section": "gf_unpersist",
    "text": "gf_unpersist"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_unpersist.html#description",
    "href": "packages/graphframes/latest/reference/gf_unpersist.html#description",
    "title": "Unpersist the GraphFrame",
    "section": "Description",
    "text": "Description\nUnpersist the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_unpersist.html#usage",
    "href": "packages/graphframes/latest/reference/gf_unpersist.html#usage",
    "title": "Unpersist the GraphFrame",
    "section": "Usage",
    "text": "Usage\ngf_unpersist(x, blocking = FALSE)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_unpersist.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_unpersist.html#arguments",
    "title": "Unpersist the GraphFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\nblocking\nwhether to block until all blocks are deleted"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html",
    "title": "Shortest paths",
    "section": "",
    "text": "R/gf_shortest_paths.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#gf_shortest_paths",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#gf_shortest_paths",
    "title": "Shortest paths",
    "section": "gf_shortest_paths",
    "text": "gf_shortest_paths"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#description",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#description",
    "title": "Shortest paths",
    "section": "Description",
    "text": "Description\nComputes shortest paths from every vertex to the given set of landmark vertices. Note that this takes edge direction into account."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#usage",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#usage",
    "title": "Shortest paths",
    "section": "Usage",
    "text": "Usage\ngf_shortest_paths(x, landmarks, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#arguments",
    "title": "Shortest paths",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\nlandmarks\nIDs of landmark vertices.\n\n\n…\nOptional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#examples",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#examples",
    "title": "Shortest paths",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ng &lt;- gf_friends(sc) \ngf_shortest_paths(g, landmarks = c(\"a\", \"d\"))"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html",
    "title": "Connected components",
    "section": "",
    "text": "R/gf_connected_components.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#gf_connected_components",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#gf_connected_components",
    "title": "Connected components",
    "section": "gf_connected_components",
    "text": "gf_connected_components"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#description",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#description",
    "title": "Connected components",
    "section": "Description",
    "text": "Description\nComputes the connected component membership of each vertex and returns a DataFrame of vertex information with each vertex assigned a component ID."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#usage",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#usage",
    "title": "Connected components",
    "section": "Usage",
    "text": "Usage\ngf_connected_components(x, broadcast_threshold = 1000000L, \n  algorithm = c(\"graphframes\", \"graphx\"), checkpoint_interval = 2L, \n  ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#arguments",
    "title": "Connected components",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\nbroadcast_threshold\nBroadcast threshold in propagating component assignments.\n\n\nalgorithm\nOne of ‘graphframes’ or ‘graphx’.\n\n\ncheckpoint_interval\nCheckpoint interval in terms of number of iterations.\n\n\n…\nOptional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#examples",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#examples",
    "title": "Connected components",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\n# checkpoint directory is required for gf_connected_components() \nspark_set_checkpoint_dir(sc, tempdir()) \ng &lt;- gf_friends(sc) \ngf_connected_components(g)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html",
    "title": "Create a new GraphFrame",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#gf_graphframe",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#gf_graphframe",
    "title": "Create a new GraphFrame",
    "section": "gf_graphframe",
    "text": "gf_graphframe"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#description",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#description",
    "title": "Create a new GraphFrame",
    "section": "Description",
    "text": "Description\nCreate a new GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#usage",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#usage",
    "title": "Create a new GraphFrame",
    "section": "Usage",
    "text": "Usage\ngf_graphframe(vertices = NULL, edges)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#arguments",
    "title": "Create a new GraphFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nvertices\nA tbl_spark representing vertices.\n\n\nedges\nA tbl_psark representing edges."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#examples",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#examples",
    "title": "Create a new GraphFrame",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\", version = \"2.3.0\") \nv_tbl &lt;- sdf_copy_to( \n  sc, data.frame(id = 1:3, name = LETTERS[1:3]) \n) \ne_tbl &lt;- sdf_copy_to( \n  sc, data.frame(src = c(1, 2, 2), dst = c(2, 1, 3), \n                 action = c(\"love\", \"hate\", \"follow\")) \n) \ngf_graphframe(v_tbl, e_tbl) \ngf_graphframe(edges = e_tbl)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_degrees.html",
    "href": "packages/graphframes/latest/reference/gf_degrees.html",
    "title": "Degrees of vertices",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_degrees.html#gf_degrees",
    "href": "packages/graphframes/latest/reference/gf_degrees.html#gf_degrees",
    "title": "Degrees of vertices",
    "section": "gf_degrees",
    "text": "gf_degrees"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_degrees.html#description",
    "href": "packages/graphframes/latest/reference/gf_degrees.html#description",
    "title": "Degrees of vertices",
    "section": "Description",
    "text": "Description\nDegrees of vertices"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_degrees.html#usage",
    "href": "packages/graphframes/latest/reference/gf_degrees.html#usage",
    "title": "Degrees of vertices",
    "section": "Usage",
    "text": "Usage\ngf_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_degrees.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_degrees.html#arguments",
    "title": "Degrees of vertices",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/index.html",
    "href": "packages/graphframes/latest/index.html",
    "title": "R interface for GraphFrames",
    "section": "",
    "text": "Support for GraphFrames which aims to provide the functionality of GraphX.\nPerform graph algorithms like: PageRank, ShortestPaths and many others.\nDesigned to work with sparklyr and the sparklyr extensions.\n\n\n\nFor those already using sparklyr simply run:\ninstall.packages(\"graphframes\")\n# or, for the development version,\n# devtools::install_github(\"rstudio/graphframes\")\nOtherwise, install first sparklyr from CRAN using:\ninstall.packages(\"sparklyr\")\nThe examples make use of the highschool dataset from the ggplot package.\n\n\n\nWe will calculate PageRank over the built-in “friends” dataset as follows.\nlibrary(graphframes)\nlibrary(sparklyr)\nlibrary(dplyr)\n\n# connect to spark using sparklyr\nsc &lt;- spark_connect(master = \"local\", version = \"2.3.0\")\n\n# obtain the example graph\ng &lt;- gf_friends(sc)\n\n# compute PageRank\nresults &lt;- gf_pagerank(g, tol = 0.01, reset_probability = 0.15)\nresults\n## GraphFrame\n## Vertices:\n##   $ id       &lt;chr&gt; \"f\", \"b\", \"g\", \"a\", \"d\", \"c\", \"e\"\n##   $ name     &lt;chr&gt; \"Fanny\", \"Bob\", \"Gabby\", \"Alice\", \"David\", \"Charlie\",...\n##   $ age      &lt;int&gt; 36, 36, 60, 34, 29, 30, 32\n##   $ pagerank &lt;dbl&gt; 0.3283607, 2.6555078, 0.1799821, 0.4491063, 0.3283607...\n## Edges:\n##   $ src          &lt;chr&gt; \"b\", \"c\", \"d\", \"e\", \"a\", \"a\", \"e\", \"f\"\n##   $ dst          &lt;chr&gt; \"c\", \"b\", \"a\", \"f\", \"e\", \"b\", \"d\", \"c\"\n##   $ relationship &lt;chr&gt; \"follow\", \"follow\", \"friend\", \"follow\", \"friend\",...\n##   $ weight       &lt;dbl&gt; 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0\nWe can then visualize the results by collecting the results to R:\nlibrary(tidygraph)\nlibrary(ggraph)\n\nvertices &lt;- results %&gt;%\n  gf_vertices() %&gt;%\n  collect()\n\nedges &lt;- results %&gt;%\n  gf_edges() %&gt;%\n  collect()\n\nedges %&gt;%\n  as_tbl_graph() %&gt;%\n  activate(nodes) %&gt;%\n  left_join(vertices, by = c(name = \"id\")) %&gt;%\n  ggraph(layout = \"nicely\") +\n  geom_node_label(aes(label = name.y, color = pagerank)) +\n  geom_edge_link(\n    aes(\n      alpha = weight,\n      start_cap = label_rect(node1.name.y),\n      end_cap = label_rect(node2.name.y)\n    ),\n    arrow = arrow(length = unit(4, \"mm\"))\n  ) +\n  theme_graph(fg_text_colour = 'white')\n\n\n\n\nAppart from calculating PageRank using gf_pagerank, many other functions are available, including:\n\ngf_bfs(): Breadth-first search (BFS).\ngf_connected_components(): Connected components.\ngf_shortest_paths(): Shortest paths algorithm.\ngf_scc(): Strongly connected components.\ngf_triangle_count(): Computes the number of triangles passing through each vertex and others.\ngf_degrees(): Degrees of vertices\n\nFor instance, one can calculate the degrees of vertices using gf_degrees as follows:\ngf_friends(sc) %&gt;% gf_degrees()\n## # Source: spark&lt;?&gt; [?? x 2]\n##   id    degree\n## * &lt;chr&gt;  &lt;int&gt;\n## 1 f          2\n## 2 b          3\n## 3 a          3\n## 4 c          3\n## 5 e          3\n## 6 d          2\nFinally, we disconnect from Spark:\nspark_disconnect(sc)",
    "crumbs": [
      "graphframes"
    ]
  },
  {
    "objectID": "packages/graphframes/latest/index.html#installation",
    "href": "packages/graphframes/latest/index.html#installation",
    "title": "R interface for GraphFrames",
    "section": "",
    "text": "For those already using sparklyr simply run:\ninstall.packages(\"graphframes\")\n# or, for the development version,\n# devtools::install_github(\"rstudio/graphframes\")\nOtherwise, install first sparklyr from CRAN using:\ninstall.packages(\"sparklyr\")\nThe examples make use of the highschool dataset from the ggplot package.",
    "crumbs": [
      "graphframes"
    ]
  },
  {
    "objectID": "packages/graphframes/latest/index.html#getting-started",
    "href": "packages/graphframes/latest/index.html#getting-started",
    "title": "R interface for GraphFrames",
    "section": "",
    "text": "We will calculate PageRank over the built-in “friends” dataset as follows.\nlibrary(graphframes)\nlibrary(sparklyr)\nlibrary(dplyr)\n\n# connect to spark using sparklyr\nsc &lt;- spark_connect(master = \"local\", version = \"2.3.0\")\n\n# obtain the example graph\ng &lt;- gf_friends(sc)\n\n# compute PageRank\nresults &lt;- gf_pagerank(g, tol = 0.01, reset_probability = 0.15)\nresults\n## GraphFrame\n## Vertices:\n##   $ id       &lt;chr&gt; \"f\", \"b\", \"g\", \"a\", \"d\", \"c\", \"e\"\n##   $ name     &lt;chr&gt; \"Fanny\", \"Bob\", \"Gabby\", \"Alice\", \"David\", \"Charlie\",...\n##   $ age      &lt;int&gt; 36, 36, 60, 34, 29, 30, 32\n##   $ pagerank &lt;dbl&gt; 0.3283607, 2.6555078, 0.1799821, 0.4491063, 0.3283607...\n## Edges:\n##   $ src          &lt;chr&gt; \"b\", \"c\", \"d\", \"e\", \"a\", \"a\", \"e\", \"f\"\n##   $ dst          &lt;chr&gt; \"c\", \"b\", \"a\", \"f\", \"e\", \"b\", \"d\", \"c\"\n##   $ relationship &lt;chr&gt; \"follow\", \"follow\", \"friend\", \"follow\", \"friend\",...\n##   $ weight       &lt;dbl&gt; 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0\nWe can then visualize the results by collecting the results to R:\nlibrary(tidygraph)\nlibrary(ggraph)\n\nvertices &lt;- results %&gt;%\n  gf_vertices() %&gt;%\n  collect()\n\nedges &lt;- results %&gt;%\n  gf_edges() %&gt;%\n  collect()\n\nedges %&gt;%\n  as_tbl_graph() %&gt;%\n  activate(nodes) %&gt;%\n  left_join(vertices, by = c(name = \"id\")) %&gt;%\n  ggraph(layout = \"nicely\") +\n  geom_node_label(aes(label = name.y, color = pagerank)) +\n  geom_edge_link(\n    aes(\n      alpha = weight,\n      start_cap = label_rect(node1.name.y),\n      end_cap = label_rect(node2.name.y)\n    ),\n    arrow = arrow(length = unit(4, \"mm\"))\n  ) +\n  theme_graph(fg_text_colour = 'white')",
    "crumbs": [
      "graphframes"
    ]
  },
  {
    "objectID": "packages/graphframes/latest/index.html#further-reading",
    "href": "packages/graphframes/latest/index.html#further-reading",
    "title": "R interface for GraphFrames",
    "section": "",
    "text": "Appart from calculating PageRank using gf_pagerank, many other functions are available, including:\n\ngf_bfs(): Breadth-first search (BFS).\ngf_connected_components(): Connected components.\ngf_shortest_paths(): Shortest paths algorithm.\ngf_scc(): Strongly connected components.\ngf_triangle_count(): Computes the number of triangles passing through each vertex and others.\ngf_degrees(): Degrees of vertices\n\nFor instance, one can calculate the degrees of vertices using gf_degrees as follows:\ngf_friends(sc) %&gt;% gf_degrees()\n## # Source: spark&lt;?&gt; [?? x 2]\n##   id    degree\n## * &lt;chr&gt;  &lt;int&gt;\n## 1 f          2\n## 2 b          3\n## 3 a          3\n## 4 c          3\n## 5 e          3\n## 6 d          2\nFinally, we disconnect from Spark:\nspark_disconnect(sc)",
    "crumbs": [
      "graphframes"
    ]
  },
  {
    "objectID": "packages/index.html",
    "href": "packages/index.html",
    "title": "Packages",
    "section": "",
    "text": "Package\nDescription\n\n\n\n\nmleap\nA sparklyr extension that provides an interface to MLeap, which allows us to take Spark pipelines to production.\n\n\ngraphframes\nEnables graph analysis in Spark via GraphFrames"
  },
  {
    "objectID": "packages/index.html#on-cran",
    "href": "packages/index.html#on-cran",
    "title": "Packages",
    "section": "",
    "text": "Package\nDescription\n\n\n\n\nmleap\nA sparklyr extension that provides an interface to MLeap, which allows us to take Spark pipelines to production.\n\n\ngraphframes\nEnables graph analysis in Spark via GraphFrames"
  },
  {
    "objectID": "packages/index.html#on-github",
    "href": "packages/index.html#on-github",
    "title": "Packages",
    "section": "On Github",
    "text": "On Github\n\n\n\nPackage\nDescription\n\n\n\n\nsparktf\nAllows writing of Spark DataFrame’s to TFRecord, the recommended format for persisting data to be used in training with TensorFlow\n\n\nsparkxgb\nA sparklyr extension that provides an interface to XGBoost on Spark."
  },
  {
    "objectID": "packages/sparktf/latest/reference/index.html",
    "href": "packages/sparktf/latest/reference/index.html",
    "title": "sparktf",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\nspark_read_tfrecord()\nRead a TFRecord File\n\n\nspark_write_tfrecord()\nWrite a Spark DataFrame to a TFRecord file",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparktf/latest/index.html",
    "href": "packages/sparktf/latest/index.html",
    "title": "sparktf",
    "section": "",
    "text": "sparktf is a sparklyr extension that allows writing of Spark DataFrames to TFRecord, the recommended format for persisting data to be used in training with TensorFlow.\n\n\n\nYou can install the development version of sparktf from GitHub with:\ndevtools::install_github(\"rstudio/sparktf\")\n\n\n\nWe first attach the required packages and establish a Spark connection.\nlibrary(sparktf)\nlibrary(sparklyr)\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution()\nlibrary(tfdatasets)\n\nsc &lt;- spark_connect(master = \"local\")\nCopied a sample dataset to Spark then write it to disk via spark_write_tfrecord().\ndata_path &lt;- file.path(tempdir(), \"iris\")\niris_tbl &lt;- sdf_copy_to(sc, iris)\n\niris_tbl %&gt;%\n  ft_string_indexer_model(\n    \"Species\", \"label\",\n    labels = c(\"setosa\", \"versicolor\", \"virginica\")\n  ) %&gt;%\n  spark_write_tfrecord(\n    path = data_path,\n    write_locality = \"local\"\n  )\nWe now read the saved TFRecord file and parse the contents to create a dataset object. For details, refer to the package website for tfdatasets.\ndataset &lt;- tfrecord_dataset(list.files(data_path, full.names = TRUE)) %&gt;%\n  dataset_map(function(example_proto) {\n    features &lt;- list(\n      label = tf$FixedLenFeature(shape(), tf$float32),\n      Sepal_Length = tf$FixedLenFeature(shape(), tf$float32),\n      Sepal_Width = tf$FixedLenFeature(shape(), tf$float32),\n      Petal_Length = tf$FixedLenFeature(shape(), tf$float32),\n      Petal_Width = tf$FixedLenFeature(shape(), tf$float32)\n    )\n\n    features &lt;- tf$parse_single_example(example_proto, features)\n    x &lt;- list(\n      features$Sepal_Length, features$Sepal_Width,\n      features$Petal_Length, features$Petal_Width\n      )\n    y &lt;- tf$one_hot(tf$cast(features$label, tf$int32), 3L)\n    list(x, y)\n  }) %&gt;%\n  dataset_shuffle(150) %&gt;%\n  dataset_batch(16)\nNow, we can define a Keras model using the keras package and fit it by feeding the dataset object defined above.\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_dense(32, activation = \"relu\", input_shape = 4) %&gt;%\n  layer_dense(3, activation = \"softmax\")\n\nmodel %&gt;%\n  compile(loss = \"categorical_crossentropy\", optimizer = tf$train$AdamOptimizer())\n\nhistory &lt;- model %&gt;%\n  fit(dataset, epochs = 100, verbose = 0)\nFinally, we can use the trained model to make some predictions.\nnew_data &lt;- tf$constant(c(4.9, 3.2, 1.4, 0.2), shape = c(1, 4))\nmodel(new_data)\n#&gt; tf.Tensor([[0.76382965 0.19407341 0.04209692]], shape=(1, 3), dtype=float32)",
    "crumbs": [
      "sparktf"
    ]
  },
  {
    "objectID": "packages/sparktf/latest/index.html#overview",
    "href": "packages/sparktf/latest/index.html#overview",
    "title": "sparktf",
    "section": "",
    "text": "sparktf is a sparklyr extension that allows writing of Spark DataFrames to TFRecord, the recommended format for persisting data to be used in training with TensorFlow.",
    "crumbs": [
      "sparktf"
    ]
  },
  {
    "objectID": "packages/sparktf/latest/index.html#installation",
    "href": "packages/sparktf/latest/index.html#installation",
    "title": "sparktf",
    "section": "",
    "text": "You can install the development version of sparktf from GitHub with:\ndevtools::install_github(\"rstudio/sparktf\")",
    "crumbs": [
      "sparktf"
    ]
  },
  {
    "objectID": "packages/sparktf/latest/index.html#example",
    "href": "packages/sparktf/latest/index.html#example",
    "title": "sparktf",
    "section": "",
    "text": "We first attach the required packages and establish a Spark connection.\nlibrary(sparktf)\nlibrary(sparklyr)\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution()\nlibrary(tfdatasets)\n\nsc &lt;- spark_connect(master = \"local\")\nCopied a sample dataset to Spark then write it to disk via spark_write_tfrecord().\ndata_path &lt;- file.path(tempdir(), \"iris\")\niris_tbl &lt;- sdf_copy_to(sc, iris)\n\niris_tbl %&gt;%\n  ft_string_indexer_model(\n    \"Species\", \"label\",\n    labels = c(\"setosa\", \"versicolor\", \"virginica\")\n  ) %&gt;%\n  spark_write_tfrecord(\n    path = data_path,\n    write_locality = \"local\"\n  )\nWe now read the saved TFRecord file and parse the contents to create a dataset object. For details, refer to the package website for tfdatasets.\ndataset &lt;- tfrecord_dataset(list.files(data_path, full.names = TRUE)) %&gt;%\n  dataset_map(function(example_proto) {\n    features &lt;- list(\n      label = tf$FixedLenFeature(shape(), tf$float32),\n      Sepal_Length = tf$FixedLenFeature(shape(), tf$float32),\n      Sepal_Width = tf$FixedLenFeature(shape(), tf$float32),\n      Petal_Length = tf$FixedLenFeature(shape(), tf$float32),\n      Petal_Width = tf$FixedLenFeature(shape(), tf$float32)\n    )\n\n    features &lt;- tf$parse_single_example(example_proto, features)\n    x &lt;- list(\n      features$Sepal_Length, features$Sepal_Width,\n      features$Petal_Length, features$Petal_Width\n      )\n    y &lt;- tf$one_hot(tf$cast(features$label, tf$int32), 3L)\n    list(x, y)\n  }) %&gt;%\n  dataset_shuffle(150) %&gt;%\n  dataset_batch(16)\nNow, we can define a Keras model using the keras package and fit it by feeding the dataset object defined above.\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_dense(32, activation = \"relu\", input_shape = 4) %&gt;%\n  layer_dense(3, activation = \"softmax\")\n\nmodel %&gt;%\n  compile(loss = \"categorical_crossentropy\", optimizer = tf$train$AdamOptimizer())\n\nhistory &lt;- model %&gt;%\n  fit(dataset, epochs = 100, verbose = 0)\nFinally, we can use the trained model to make some predictions.\nnew_data &lt;- tf$constant(c(4.9, 3.2, 1.4, 0.2), shape = c(1, 4))\nmodel(new_data)\n#&gt; tf.Tensor([[0.76382965 0.19407341 0.04209692]], shape=(1, 3), dtype=float32)",
    "crumbs": [
      "sparktf"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html",
    "title": "Read a Parquet file into a Spark DataFrame",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html#spark_read_parquet",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html#spark_read_parquet",
    "title": "Read a Parquet file into a Spark DataFrame",
    "section": "spark_read_parquet",
    "text": "spark_read_parquet"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html#description",
    "title": "Read a Parquet file into a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRead a Parquet file into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html#usage",
    "title": "Read a Parquet file into a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nspark_read_parquet( \n  sc, \n  name = NULL, \n  path = name, \n  options = list(), \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE, \n  columns = NULL, \n  schema = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html#arguments",
    "title": "Read a Parquet file into a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\noptions\nA list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\nschema\nA (java) read schema. Useful for optimizing read operation on nested data.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html#details",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html#details",
    "title": "Read a Parquet file into a Spark DataFrame",
    "section": "Details",
    "text": "Details\nYou can read data from HDFS (hdfs://), S3 (s3a://), as well as the local file system (file://).\nIf you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf spark.hadoop.fs.s3a.access.key, spark.hadoop.fs.s3a.secret.key or any of the methods outlined in the aws-sdk documentation Working with AWS credentials\nIn order to work with the newer s3a:// protocol also set the values for spark.hadoop.fs.s3a.impl and spark.hadoop.fs.s3a.endpoint. In addition, to support v4 of the S3 api be sure to pass the -Dcom.amazonaws.services.s3.enableV4 driver options for the config key spark.driver.extraJavaOptions\nFor instructions on how to configure s3n:// check the hadoop documentation: s3n authentication properties"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html#see-also",
    "title": "Read a Parquet file into a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_rds.html",
    "href": "packages/sparklyr/latest/reference/spark_write_rds.html",
    "title": "Write Spark DataFrame to RDS files",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_rds.html#spark_write_rds",
    "href": "packages/sparklyr/latest/reference/spark_write_rds.html#spark_write_rds",
    "title": "Write Spark DataFrame to RDS files",
    "section": "spark_write_rds",
    "text": "spark_write_rds"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_rds.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_rds.html#description",
    "title": "Write Spark DataFrame to RDS files",
    "section": "Description",
    "text": "Description\nWrite Spark dataframe to RDS files. Each partition of the dataframe will be exported to a separate RDS file so that all partitions can be processed in parallel."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_rds.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_rds.html#usage",
    "title": "Write Spark DataFrame to RDS files",
    "section": "Usage",
    "text": "Usage\nspark_write_rds(x, dest_uri)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_rds.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_rds.html#arguments",
    "title": "Write Spark DataFrame to RDS files",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame to be exported\n\n\ndest_uri\nCan be a URI template containing “partitionId” (e.g., \"hdfs://my_data_part_{partitionId}.rds\") where “partitionId” will be substituted with ID of each partition using glue, or a list of URIs to be assigned to RDS output from all partitions (e.g., \"hdfs://my_data_part_0.rds\", \"hdfs://my_data_part_1.rds\", and so on) If working with a Spark instance running locally, then all URIs should be in \"file://&lt;local file path&gt;\" form. Otherwise the scheme of the URI should reflect the underlying file system the Spark instance is working with (e.g., “hdfs://”). If the resulting list of URI(s) does not contain unique values, then it will be post-processed with make.unique() to ensure uniqueness."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_rds.html#value",
    "href": "packages/sparklyr/latest/reference/spark_write_rds.html#value",
    "title": "Write Spark DataFrame to RDS files",
    "section": "Value",
    "text": "Value\nA tibble containing partition ID and RDS file location for each partition of the input Spark dataframe."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_find.html",
    "href": "packages/sparklyr/latest/reference/stream_find.html",
    "title": "Find Stream",
    "section": "",
    "text": "R/stream_operations.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_find.html#stream_find",
    "href": "packages/sparklyr/latest/reference/stream_find.html#stream_find",
    "title": "Find Stream",
    "section": "stream_find",
    "text": "stream_find"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_find.html#description",
    "href": "packages/sparklyr/latest/reference/stream_find.html#description",
    "title": "Find Stream",
    "section": "Description",
    "text": "Description\nFinds and returns a stream based on the stream’s identifier."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_find.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_find.html#usage",
    "title": "Find Stream",
    "section": "Usage",
    "text": "Usage\nstream_find(sc, id)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_find.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_find.html#arguments",
    "title": "Find Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nid\nThe stream identifier to find."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_find.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_find.html#examples",
    "title": "Find Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\") \nsdf_len(sc, 10) %&gt;% \n  spark_write_parquet(path = \"parquet-in\") \nstream &lt;- stream_read_parquet(sc, \"parquet-in\") %&gt;% \n  stream_write_parquet(\"parquet-out\") \nstream_id &lt;- stream_id(stream) \nstream_find(sc, stream_id)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html",
    "title": "Feature Transformation – Bucketizer (Transformer)",
    "section": "",
    "text": "R/ml_feature_bucketizer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#ft_bucketizer",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#ft_bucketizer",
    "title": "Feature Transformation – Bucketizer (Transformer)",
    "section": "ft_bucketizer",
    "text": "ft_bucketizer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#description",
    "title": "Feature Transformation – Bucketizer (Transformer)",
    "section": "Description",
    "text": "Description\nSimilar to R’s cut function, this transforms a numeric column into a discretized column, with breaks specified through the splits parameter."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#usage",
    "title": "Feature Transformation – Bucketizer (Transformer)",
    "section": "Usage",
    "text": "Usage\n \nft_bucketizer( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  splits = NULL, \n  input_cols = NULL, \n  output_cols = NULL, \n  splits_array = NULL, \n  handle_invalid = \"error\", \n  uid = random_string(\"bucketizer_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#arguments",
    "title": "Feature Transformation – Bucketizer (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nsplits\nA numeric vector of cutpoints, indicating the bucket boundaries.\n\n\ninput_cols\nNames of input columns.\n\n\noutput_cols\nNames of output columns.\n\n\nsplits_array\nParameter for specifying multiple splits parameters. Each element in this array can be used to map continuous features into buckets.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error”\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#value",
    "title": "Feature Transformation – Bucketizer (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#examples",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#examples",
    "title": "Feature Transformation – Bucketizer (Transformer)",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nlibrary(dplyr) \n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \niris_tbl %&gt;% \n  ft_bucketizer( \n    input_col = \"Sepal_Length\", \n    output_col = \"Sepal_Length_bucket\", \n    splits = c(0, 4.5, 5, 8) \n  ) %&gt;% \n  select(Sepal_Length, Sepal_Length_bucket, Species) \n#&gt; # Source: spark&lt;?&gt; [?? x 3]\n#&gt;    Sepal_Length Sepal_Length_bucket Species\n#&gt;           &lt;dbl&gt;               &lt;dbl&gt; &lt;chr&gt;  \n#&gt;  1          5.1                   2 setosa \n#&gt;  2          4.9                   1 setosa \n#&gt;  3          4.7                   1 setosa \n#&gt;  4          4.6                   1 setosa \n#&gt;  5          5                     2 setosa \n#&gt;  6          5.4                   2 setosa \n#&gt;  7          4.6                   1 setosa \n#&gt;  8          5                     2 setosa \n#&gt;  9          4.4                   0 setosa \n#&gt; 10          4.9                   1 setosa \n#&gt; # … with more rows"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#see-also",
    "title": "Feature Transformation – Bucketizer (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark. Other feature transformers: ft_binarizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_crosstab.html",
    "href": "packages/sparklyr/latest/reference/sdf_crosstab.html",
    "title": "Cross Tabulation",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_crosstab.html#sdf_crosstab",
    "href": "packages/sparklyr/latest/reference/sdf_crosstab.html#sdf_crosstab",
    "title": "Cross Tabulation",
    "section": "sdf_crosstab",
    "text": "sdf_crosstab"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_crosstab.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_crosstab.html#description",
    "title": "Cross Tabulation",
    "section": "Description",
    "text": "Description\nBuilds a contingency table at each combination of factor levels."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_crosstab.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_crosstab.html#usage",
    "title": "Cross Tabulation",
    "section": "Usage",
    "text": "Usage\nsdf_crosstab(x, col1, col2)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_crosstab.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_crosstab.html#arguments",
    "title": "Cross Tabulation",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame\n\n\ncol1\nThe name of the first column. Distinct items will make the first item of each row.\n\n\ncol2\nThe name of the second column. Distinct items will make the column names of the DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_crosstab.html#value",
    "href": "packages/sparklyr/latest/reference/sdf_crosstab.html#value",
    "title": "Cross Tabulation",
    "section": "Value",
    "text": "Value\nA DataFrame containing the contingency table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_partition_sizes.html",
    "href": "packages/sparklyr/latest/reference/sdf_partition_sizes.html",
    "title": "Compute the number of records within each partition of a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#sdf_partition_sizes",
    "href": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#sdf_partition_sizes",
    "title": "Compute the number of records within each partition of a Spark DataFrame",
    "section": "sdf_partition_sizes",
    "text": "sdf_partition_sizes"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#description",
    "title": "Compute the number of records within each partition of a Spark DataFrame",
    "section": "Description",
    "text": "Description\nCompute the number of records within each partition of a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#usage",
    "title": "Compute the number of records within each partition of a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_partition_sizes(x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#arguments",
    "title": "Compute the number of records within each partition of a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#examples",
    "href": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#examples",
    "title": "Compute the number of records within each partition of a Spark DataFrame",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"spark://HOST:PORT\") \nexample_sdf &lt;- sdf_len(sc, 100L, repartition = 10L) \nexample_sdf %&gt;% \n  sdf_partition_sizes() %&gt;% \n  print()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/reexports.html",
    "href": "packages/sparklyr/latest/reference/reexports.html",
    "title": "Objects exported from other packages",
    "section": "",
    "text": "R/imports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/reexports.html#reexports",
    "href": "packages/sparklyr/latest/reference/reexports.html#reexports",
    "title": "Objects exported from other packages",
    "section": "reexports",
    "text": "reexports"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/reexports.html#description",
    "href": "packages/sparklyr/latest/reference/reexports.html#description",
    "title": "Objects exported from other packages",
    "section": "Description",
    "text": "Description\nThese objects are imported from other packages. Follow the links below to see their documentation.\ngenerics\naugment"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_last_index.html",
    "href": "packages/sparklyr/latest/reference/sdf_last_index.html",
    "title": "Returns the last index of a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_last_index.html#sdf_last_index",
    "href": "packages/sparklyr/latest/reference/sdf_last_index.html#sdf_last_index",
    "title": "Returns the last index of a Spark DataFrame",
    "section": "sdf_last_index",
    "text": "sdf_last_index"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_last_index.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_last_index.html#description",
    "title": "Returns the last index of a Spark DataFrame",
    "section": "Description",
    "text": "Description\nReturns the last index of a Spark DataFrame. The Spark mapPartitionsWithIndex function is used to iterate through the last nonempty partition of the RDD to find the last record."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_last_index.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_last_index.html#usage",
    "title": "Returns the last index of a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_last_index(x, id = \"id\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_last_index.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_last_index.html#arguments",
    "title": "Returns the last index of a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nid\nThe name of the index column."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/compile_package_jars.html",
    "href": "packages/sparklyr/latest/reference/compile_package_jars.html",
    "title": "Compile Scala sources into a Java Archive (jar)",
    "section": "",
    "text": "R/spark_compile.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/compile_package_jars.html#compile_package_jars",
    "href": "packages/sparklyr/latest/reference/compile_package_jars.html#compile_package_jars",
    "title": "Compile Scala sources into a Java Archive (jar)",
    "section": "compile_package_jars",
    "text": "compile_package_jars"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/compile_package_jars.html#description",
    "href": "packages/sparklyr/latest/reference/compile_package_jars.html#description",
    "title": "Compile Scala sources into a Java Archive (jar)",
    "section": "Description",
    "text": "Description\nCompile the scala source files contained within an R package into a Java Archive (jar) file that can be loaded and used within a Spark environment."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/compile_package_jars.html#usage",
    "href": "packages/sparklyr/latest/reference/compile_package_jars.html#usage",
    "title": "Compile Scala sources into a Java Archive (jar)",
    "section": "Usage",
    "text": "Usage\ncompile_package_jars(..., spec = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/compile_package_jars.html#arguments",
    "href": "packages/sparklyr/latest/reference/compile_package_jars.html#arguments",
    "title": "Compile Scala sources into a Java Archive (jar)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\n…\nOptional compilation specifications, as generated by spark_compilation_spec. When no arguments are passed, spark_default_compilation_spec is used instead.\n\n\nspec\nAn optional list of compilation specifications. When set, this option takes precedence over arguments passed to ...."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/inner_join.html",
    "href": "packages/sparklyr/latest/reference/inner_join.html",
    "title": "Inner join",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/inner_join.html#inner_join",
    "href": "packages/sparklyr/latest/reference/inner_join.html#inner_join",
    "title": "Inner join",
    "section": "inner_join",
    "text": "inner_join"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/inner_join.html#description",
    "href": "packages/sparklyr/latest/reference/inner_join.html#description",
    "title": "Inner join",
    "section": "Description",
    "text": "Description\nSee inner_join for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pipeline.html",
    "href": "packages/sparklyr/latest/reference/ml_pipeline.html",
    "title": "Spark ML – Pipelines",
    "section": "",
    "text": "R/ml_pipeline.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pipeline.html#ml_pipeline",
    "href": "packages/sparklyr/latest/reference/ml_pipeline.html#ml_pipeline",
    "title": "Spark ML – Pipelines",
    "section": "ml_pipeline",
    "text": "ml_pipeline"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pipeline.html#description",
    "href": "packages/sparklyr/latest/reference/ml_pipeline.html#description",
    "title": "Spark ML – Pipelines",
    "section": "Description",
    "text": "Description\nCreate Spark ML Pipelines"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pipeline.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_pipeline.html#usage",
    "title": "Spark ML – Pipelines",
    "section": "Usage",
    "text": "Usage\nml_pipeline(x, ..., uid = random_string(\"pipeline_\"))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pipeline.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_pipeline.html#arguments",
    "title": "Spark ML – Pipelines",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nEither a spark_connection or ml_pipeline_stage objects\n\n\n…\nml_pipeline_stage objects.\n\n\nuid\nA character string used to uniquely identify the ML estimator."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pipeline.html#value",
    "href": "packages/sparklyr/latest/reference/ml_pipeline.html#value",
    "title": "Spark ML – Pipelines",
    "section": "Value",
    "text": "Value\nWhen x is a spark_connection, ml_pipeline() returns an empty pipeline object. When x is a ml_pipeline_stage, ml_pipeline() returns an ml_pipeline with the stages set to x and any transformers or estimators given in ...."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/generic_call_interface.html",
    "href": "packages/sparklyr/latest/reference/generic_call_interface.html",
    "title": "Generic Call Interface",
    "section": "",
    "text": "R/spark_invoke.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/generic_call_interface.html#generic_call_interface",
    "href": "packages/sparklyr/latest/reference/generic_call_interface.html#generic_call_interface",
    "title": "Generic Call Interface",
    "section": "generic_call_interface",
    "text": "generic_call_interface"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/generic_call_interface.html#description",
    "href": "packages/sparklyr/latest/reference/generic_call_interface.html#description",
    "title": "Generic Call Interface",
    "section": "Description",
    "text": "Description\nGeneric Call Interface"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/generic_call_interface.html#arguments",
    "href": "packages/sparklyr/latest/reference/generic_call_interface.html#arguments",
    "title": "Generic Call Interface",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nspark_connection\n\n\nstatic\nIs this a static method call (including a constructor). If so then the object parameter should be the name of a class (otherwise it should be a spark_jobj instance).\n\n\nobject\nObject instance or name of class (for static)\n\n\nmethod\nName of method\n\n\n…\nCall parameters"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compilation_spec.html",
    "href": "packages/sparklyr/latest/reference/spark_compilation_spec.html",
    "title": "Define a Spark Compilation Specification",
    "section": "",
    "text": "R/spark_compile.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compilation_spec.html#spark_compilation_spec",
    "href": "packages/sparklyr/latest/reference/spark_compilation_spec.html#spark_compilation_spec",
    "title": "Define a Spark Compilation Specification",
    "section": "spark_compilation_spec",
    "text": "spark_compilation_spec"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compilation_spec.html#description",
    "href": "packages/sparklyr/latest/reference/spark_compilation_spec.html#description",
    "title": "Define a Spark Compilation Specification",
    "section": "Description",
    "text": "Description\nFor use with compile_package_jars. The Spark compilation specification is used when compiling Spark extension Java Archives, and defines which versions of Spark, as well as which versions of Scala, should be used for compilation."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compilation_spec.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_compilation_spec.html#usage",
    "title": "Define a Spark Compilation Specification",
    "section": "Usage",
    "text": "Usage\nspark_compilation_spec( \n  spark_version = NULL, \n  spark_home = NULL, \n  scalac_path = NULL, \n  scala_filter = NULL, \n  jar_name = NULL, \n  jar_path = NULL, \n  jar_dep = NULL, \n  embedded_srcs = \"embedded_sources.R\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compilation_spec.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_compilation_spec.html#arguments",
    "title": "Define a Spark Compilation Specification",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nspark_version\nThe Spark version to build against. This can be left unset if the path to a suitable Spark home is supplied.\n\n\nspark_home\nThe path to a Spark home installation. This can be left unset if spark_version is supplied; in such a case, sparklyr will attempt to discover the associated Spark installation using spark_home_dir.\n\n\nscalac_path\nThe path to the scalac compiler to be used during compilation of your Spark extension. Note that you should ensure the version of scalac selected matches the version of scalac used with the version of Spark you are compiling against.\n\n\nscala_filter\nAn optional R function that can be used to filter which scala files are used during compilation. This can be useful if you have auxiliary files that should only be included with certain versions of Spark.\n\n\njar_name\nThe name to be assigned to the generated jar.\n\n\njar_path\nThe path to the jar tool to be used during compilation of your Spark extension.\n\n\njar_dep\nAn optional list of additional jar dependencies.\n\n\nembedded_srcs\nEmbedded source file(s) under &lt;R package root&gt;/java to be included in the root of the resulting jar file as resources"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compilation_spec.html#details",
    "href": "packages/sparklyr/latest/reference/spark_compilation_spec.html#details",
    "title": "Define a Spark Compilation Specification",
    "section": "Details",
    "text": "Details\nMost Spark extensions won’t need to define their own compilation specification, and can instead rely on the default behavior of compile_package_jars."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection-class.html",
    "href": "packages/sparklyr/latest/reference/spark_connection-class.html",
    "title": "spark_connection class",
    "section": "",
    "text": "R/connection_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection-class.html#spark_connection-class",
    "href": "packages/sparklyr/latest/reference/spark_connection-class.html#spark_connection-class",
    "title": "spark_connection class",
    "section": "spark_connection-class",
    "text": "spark_connection-class"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection-class.html#description",
    "href": "packages/sparklyr/latest/reference/spark_connection-class.html#description",
    "title": "spark_connection class",
    "section": "Description",
    "text": "Description\nspark_connection class"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_checkpoint.html",
    "href": "packages/sparklyr/latest/reference/sdf_checkpoint.html",
    "title": "Checkpoint a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_checkpoint.html#sdf_checkpoint",
    "href": "packages/sparklyr/latest/reference/sdf_checkpoint.html#sdf_checkpoint",
    "title": "Checkpoint a Spark DataFrame",
    "section": "sdf_checkpoint",
    "text": "sdf_checkpoint"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_checkpoint.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_checkpoint.html#description",
    "title": "Checkpoint a Spark DataFrame",
    "section": "Description",
    "text": "Description\nCheckpoint a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_checkpoint.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_checkpoint.html#usage",
    "title": "Checkpoint a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_checkpoint(x, eager = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_checkpoint.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_checkpoint.html#arguments",
    "title": "Checkpoint a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nan object coercible to a Spark DataFrame\n\n\neager\nwhether to truncate the lineage of the DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_call_constructor.html",
    "href": "packages/sparklyr/latest/reference/ml_call_constructor.html",
    "title": "Wrap a Spark ML JVM object",
    "section": "",
    "text": "R/ml_constructor_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_call_constructor.html#ml_call_constructor",
    "href": "packages/sparklyr/latest/reference/ml_call_constructor.html#ml_call_constructor",
    "title": "Wrap a Spark ML JVM object",
    "section": "ml_call_constructor",
    "text": "ml_call_constructor"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_call_constructor.html#description",
    "href": "packages/sparklyr/latest/reference/ml_call_constructor.html#description",
    "title": "Wrap a Spark ML JVM object",
    "section": "Description",
    "text": "Description\nIdentifies the associated sparklyr ML constructor for the JVM object by inspecting its class and performing a lookup. The lookup table is specified by the sparkml/class_mapping.json files of sparklyr and the loaded extensions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_call_constructor.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_call_constructor.html#usage",
    "title": "Wrap a Spark ML JVM object",
    "section": "Usage",
    "text": "Usage\nml_call_constructor(jobj)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_call_constructor.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_call_constructor.html#arguments",
    "title": "Wrap a Spark ML JVM object",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\njobj\nThe jobj for the pipeline stage."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html",
    "title": "Feature Transformation – StringIndexer (Estimator)",
    "section": "",
    "text": "R/ml_feature_string_indexer.R,"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#ft_string_indexer",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#ft_string_indexer",
    "title": "Feature Transformation – StringIndexer (Estimator)",
    "section": "ft_string_indexer",
    "text": "ft_string_indexer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#description",
    "title": "Feature Transformation – StringIndexer (Estimator)",
    "section": "Description",
    "text": "Description\nA label indexer that maps a string column of labels to an ML column of label indices. If the input column is numeric, we cast it to string and index the string values. The indices are in [0, numLabels), ordered by label frequencies. So the most frequent label gets index 0. This function is the inverse of ft_index_to_string."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#usage",
    "title": "Feature Transformation – StringIndexer (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_string_indexer( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  handle_invalid = \"error\", \n  string_order_type = \"frequencyDesc\", \n  uid = random_string(\"string_indexer_\"), \n  ... \n) \n\nml_labels(model) \n\nft_string_indexer_model( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  labels, \n  handle_invalid = \"error\", \n  uid = random_string(\"string_indexer_model_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#arguments",
    "title": "Feature Transformation – StringIndexer (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error”\n\n\nstring_order_type\n(Spark 2.3+)How to order labels of string column. The first label after ordering is assigned an index of 0. Options are \"frequencyDesc\", \"frequencyAsc\", \"alphabetDesc\", and \"alphabetAsc\". Defaults to \"frequencyDesc\".\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused.\n\n\nmodel\nA fitted StringIndexer model returned by ft_string_indexer()\n\n\nlabels\nVector of labels, corresponding to indices to be assigned."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#details",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#details",
    "title": "Feature Transformation – StringIndexer (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#value",
    "title": "Feature Transformation – StringIndexer (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark\n\nml_labels() returns a vector of labels, corresponding to indices to be assigned."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#see-also",
    "title": "Feature Transformation – StringIndexer (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nft_index_to_string\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html",
    "href": "packages/sparklyr/latest/reference/ml_lda.html",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "",
    "text": "R/ml_clustering_lda.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#ml_lda",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#ml_lda",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "ml_lda",
    "text": "ml_lda"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#description",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#description",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "Description",
    "text": "Description\nLatent Dirichlet Allocation (LDA), a topic model designed for text documents."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#usage",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "Usage",
    "text": "Usage\n \nml_lda( \n  x, \n  formula = NULL, \n  k = 10, \n  max_iter = 20, \n  doc_concentration = NULL, \n  topic_concentration = NULL, \n  subsampling_rate = 0.05, \n  optimizer = \"online\", \n  checkpoint_interval = 10, \n  keep_last_checkpoint = TRUE, \n  learning_decay = 0.51, \n  learning_offset = 1024, \n  optimize_doc_concentration = TRUE, \n  seed = NULL, \n  features_col = \"features\", \n  topic_distribution_col = \"topicDistribution\", \n  uid = random_string(\"lda_\"), \n  ... \n) \n \nml_describe_topics(model, max_terms_per_topic = 10) \n \nml_log_likelihood(model, dataset) \n \nml_log_perplexity(model, dataset) \n \nml_topics_matrix(model)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#arguments",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nk\nThe number of clusters to create\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\ndoc_concentration\nConcentration parameter (commonly named “alpha”) for the prior placed on documents’ distributions over topics (“theta”). See details.\n\n\ntopic_concentration\nConcentration parameter (commonly named “beta” or “eta”) for the prior placed on topics’ distributions over terms.\n\n\nsubsampling_rate\n(For Online optimizer only) Fraction of the corpus to be sampled and used in each iteration of mini-batch gradient descent, in range (0, 1]. Note that this should be adjusted in synch with max_iter so the entire corpus is used. Specifically, set both so that maxIterations * miniBatchFraction greater than or equal to 1.\n\n\noptimizer\nOptimizer or inference algorithm used to estimate the LDA model. Supported: “online” for Online Variational Bayes (default) and “em” for Expectation-Maximization.\n\n\ncheckpoint_interval\nSet checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.\n\n\nkeep_last_checkpoint\n(Spark 2.0.0+) (For EM optimizer only) If using checkpointing, this indicates whether to keep the last checkpoint. If FALSE, then the checkpoint will be deleted. Deleting the checkpoint can cause failures if a data partition is lost, so set this bit with care. Note that checkpoints will be cleaned up via reference counting, regardless.\n\n\nlearning_decay\n(For Online optimizer only) Learning rate, set as an exponential decay rate. This should be between (0.5, 1.0] to guarantee asymptotic convergence. This is called “kappa” in the Online LDA paper (Hoffman et al., 2010). Default: 0.51, based on Hoffman et al.\n\n\nlearning_offset\n(For Online optimizer only) A (positive) learning parameter that downweights early iterations. Larger values make early iterations count less. This is called “tau0” in the Online LDA paper (Hoffman et al., 2010) Default: 1024, following Hoffman et al.\n\n\noptimize_doc_concentration\n(For Online optimizer only) Indicates whether the doc_concentration (Dirichlet parameter for document-topic distribution) will be optimized during training. Setting this to true will make the model more expressive and fit the training data better. Default: FALSE\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\ntopic_distribution_col\nOutput column with estimates of the topic mixture distribution for each document (often called “theta” in the literature). Returns a vector of zeros for an empty document.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments, see Details.\n\n\nmodel\nA fitted LDA model returned by ml_lda().\n\n\nmax_terms_per_topic\nMaximum number of terms to collect for each topic. Default value of 10.\n\n\ndataset\ntest corpus to use for calculating log likelihood or log perplexity"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#details",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#details",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "Details",
    "text": "Details\nFor ml_lda.tbl_spark with the formula interface, you can specify named arguments in ... that will be passed ft_regex_tokenizer(), ft_stop_words_remover(), and ft_count_vectorizer(). For example, to increase the default min_token_length, you can use ml_lda(dataset, ~ text, min_token_length = 4). Terminology for LDA:\n\n“term” = “word”: an element of the vocabulary\n“token”: instance of a term appearing in a document\n“topic”: multinomial distribution over terms representing some concept\n“document”: one piece of text, corresponding to one row in the input data\nOriginal LDA paper (journal version): Blei, Ng, and Jordan. “Latent Dirichlet Allocation.” JMLR, 2003. Input data (features_col): LDA is given a collection of documents as input data, via the features_col parameter. Each document is specified as a Vector of length vocab_size, where each entry is the count for the corresponding term (word) in the document. Feature transformers such as ft_tokenizer and ft_count_vectorizer can be useful for converting text to word count vectors"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#section",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#section",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#parameter-details",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#parameter-details",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "Parameter details",
    "text": "Parameter details\ndoc_concentration This is the parameter to a Dirichlet distribution, where larger values mean more smoothing (more regularization). If not set by the user, then doc_concentration is set automatically. If set to singleton vector [alpha], then alpha is replicated to a vector of length k in fitting. Otherwise, the doc_concentration vector must be length k. (default = automatic) Optimizer-specific parameter settings: EM\n\nCurrently only supports symmetric distributions, so all values in the vector should be the same.\nValues should be greater than 1.0\ndefault = uniformly (50 / k) + 1, where 50/k is common in LDA libraries and +1 follows from Asuncion et al. (2009), who recommend a +1 adjustment for EM.\nOnline\nValues should be greater than or equal to 0\ndefault = uniformly (1.0 / k), following the implementation from here\ntopic_concentration This is the parameter to a symmetric Dirichlet distribution. Note: The topics’ distributions over terms are called “beta” in the original LDA paper by Blei et al., but are called “phi” in many later papers such as Asuncion et al., 2009. If not set by the user, then topic_concentration is set automatically. (default = automatic) Optimizer-specific parameter settings: EM\nValue should be greater than 1.0\ndefault = 0.1 + 1, where 0.1 gives a small amount of smoothing and +1 follows Asuncion et al. (2009), who recommend a +1 adjustment for EM.\nOnline\nValue should be greater than or equal to 0\ndefault = (1.0 / k), following the implementation from here.\ntopic_distribution_col This uses a variational approximation following Hoffman et al. (2010), where the approximate distribution is called “gamma.” Technically, this method returns this approximation “gamma” for each document."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#value",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#value",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the clustering estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, an estimator is constructed then immediately fit with the input tbl_spark, returning a clustering model.\ntbl_spark, with formula or features specified: When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the estimator. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model. This signature does not apply to ml_lda().\nml_describe_topics returns a DataFrame with topics and their top-weighted terms. ml_log_likelihood calculates a lower bound on the log likelihood of the entire corpus"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#examples",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nlibrary(janeaustenr) \nlibrary(dplyr) \nsc &lt;- spark_connect(master = \"local\") \n \nlines_tbl &lt;- sdf_copy_to(sc, \n  austen_books()[c(1:30), ], \n  name = \"lines_tbl\", \n  overwrite = TRUE \n) \n \n# transform the data in a tidy form \nlines_tbl_tidy &lt;- lines_tbl %&gt;% \n  ft_tokenizer( \n    input_col = \"text\", \n    output_col = \"word_list\" \n  ) %&gt;% \n  ft_stop_words_remover( \n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\" \n  ) %&gt;% \n  mutate(text = explode(wo_stop_words)) %&gt;% \n  filter(text != \"\") %&gt;% \n  select(text, book) \n \nlda_model &lt;- lines_tbl_tidy %&gt;% \n  ml_lda(~text, k = 4) \n \n# vocabulary and topics \ntidy(lda_model) \n#&gt; New names:\n#&gt; • `` -&gt; `...1`\n#&gt; • `` -&gt; `...2`\n#&gt; • `` -&gt; `...3`\n#&gt; • `` -&gt; `...4`\n#&gt; # A tibble: 388 × 3\n#&gt;    topic term      beta\n#&gt;    &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n#&gt;  1     0 norland  0.607\n#&gt;  2     0 years    0.663\n#&gt;  3     0 lived    2.07 \n#&gt;  4     0 estate   0.615\n#&gt;  5     0 constant 0.703\n#&gt;  6     0 henry    1.96 \n#&gt;  7     0 many     0.600\n#&gt;  8     0 family   1.54 \n#&gt;  9     0 dashwood 0.585\n#&gt; 10     0 nephew   0.721\n#&gt; # … with 378 more rows"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#see-also",
    "title": "Spark ML – Latent Dirichlet Allocation",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-clustering.html for more information on the set of clustering algorithms. Other ml clustering algorithms: ml_bisecting_kmeans(), ml_gaussian_mixture(), ml_kmeans()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_uid.html",
    "href": "packages/sparklyr/latest/reference/ml_uid.html",
    "title": "Spark ML – UID",
    "section": "",
    "text": "R/ml_pipeline_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_uid.html#ml_uid",
    "href": "packages/sparklyr/latest/reference/ml_uid.html#ml_uid",
    "title": "Spark ML – UID",
    "section": "ml_uid",
    "text": "ml_uid"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_uid.html#description",
    "href": "packages/sparklyr/latest/reference/ml_uid.html#description",
    "title": "Spark ML – UID",
    "section": "Description",
    "text": "Description\nExtracts the UID of an ML object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_uid.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_uid.html#usage",
    "title": "Spark ML – UID",
    "section": "Usage",
    "text": "Usage\nml_uid(x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_uid.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_uid.html#arguments",
    "title": "Spark ML – UID",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark ML object"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html",
    "title": "Feature Transformation – Normalizer (Transformer)",
    "section": "",
    "text": "R/ml_feature_normalizer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html#ft_normalizer",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html#ft_normalizer",
    "title": "Feature Transformation – Normalizer (Transformer)",
    "section": "ft_normalizer",
    "text": "ft_normalizer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html#description",
    "title": "Feature Transformation – Normalizer (Transformer)",
    "section": "Description",
    "text": "Description\nNormalize a vector to have unit norm using the given p-norm."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html#usage",
    "title": "Feature Transformation – Normalizer (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_normalizer( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  p = 2, \n  uid = random_string(\"normalizer_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html#arguments",
    "title": "Feature Transformation – Normalizer (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\np\nNormalization in L^p space. Must be &gt;= 1. Defaults to 2.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html#value",
    "title": "Feature Transformation – Normalizer (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html#see-also",
    "title": "Feature Transformation – Normalizer (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html",
    "title": "Spark ML – Decision Trees",
    "section": "",
    "text": "R/ml_classification_decision_tree_classifier.R,"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#ml_decision_tree_classifier",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#ml_decision_tree_classifier",
    "title": "Spark ML – Decision Trees",
    "section": "ml_decision_tree_classifier",
    "text": "ml_decision_tree_classifier"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#description",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#description",
    "title": "Spark ML – Decision Trees",
    "section": "Description",
    "text": "Description\nPerform classification and regression using decision trees."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#usage",
    "title": "Spark ML – Decision Trees",
    "section": "Usage",
    "text": "Usage\n \nml_decision_tree_classifier( \n  x, \n  formula = NULL, \n  max_depth = 5, \n  max_bins = 32, \n  min_instances_per_node = 1, \n  min_info_gain = 0, \n  impurity = \"gini\", \n  seed = NULL, \n  thresholds = NULL, \n  cache_node_ids = FALSE, \n  checkpoint_interval = 10, \n  max_memory_in_mb = 256, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  uid = random_string(\"decision_tree_classifier_\"), \n  ... \n) \n \nml_decision_tree( \n  x, \n  formula = NULL, \n  type = c(\"auto\", \"regression\", \"classification\"), \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  variance_col = NULL, \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  checkpoint_interval = 10L, \n  impurity = \"auto\", \n  max_bins = 32L, \n  max_depth = 5L, \n  min_info_gain = 0, \n  min_instances_per_node = 1L, \n  seed = NULL, \n  thresholds = NULL, \n  cache_node_ids = FALSE, \n  max_memory_in_mb = 256L, \n  uid = random_string(\"decision_tree_\"), \n  response = NULL, \n  features = NULL, \n  ... \n) \n \nml_decision_tree_regressor( \n  x, \n  formula = NULL, \n  max_depth = 5, \n  max_bins = 32, \n  min_instances_per_node = 1, \n  min_info_gain = 0, \n  impurity = \"variance\", \n  seed = NULL, \n  cache_node_ids = FALSE, \n  checkpoint_interval = 10, \n  max_memory_in_mb = 256, \n  variance_col = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"decision_tree_regressor_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#arguments",
    "title": "Spark ML – Decision Trees",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nmax_depth\nMaximum depth of the tree (&gt;= 0); that is, the maximum number of nodes separating any leaves from the root of the tree.\n\n\nmax_bins\nThe maximum number of bins used for discretizing continuous features and for choosing how to split on features at each node. More bins give higher granularity.\n\n\nmin_instances_per_node\nMinimum number of instances each child must have after split.\n\n\nmin_info_gain\nMinimum information gain for a split to be considered at a tree node. Should be &gt;= 0, defaults to 0.\n\n\nimpurity\nCriterion used for information gain calculation. Supported: “entropy” and “gini” (default) for classification and “variance” (default) for regression. For ml_decision_tree, setting \"auto\" will default to the appropriate criterion based on model type.\n\n\nseed\nSeed for random numbers.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\ncache_node_ids\nIf FALSE, the algorithm will pass trees to executors to match instances with nodes. If TRUE, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Defaults to FALSE.\n\n\ncheckpoint_interval\nSet checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.\n\n\nmax_memory_in_mb\nMaximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. Defaults to 256.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details.\n\n\ntype\nThe type of model to fit. \"regression\" treats the response as a continuous variable, while \"classification\" treats the response as a categorical variable. When \"auto\" is used, the model type is inferred based on the response variable type – if it is a numeric type, then regression is used; classification otherwise.\n\n\nvariance_col\n(Optional) Column name for the biased sample variance of prediction.\n\n\nresponse\n(Deprecated) The name of the response column (as a length-one character vector.)\n\n\nfeatures\n(Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#details",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#details",
    "title": "Spark ML – Decision Trees",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows. ml_decision_tree is a wrapper around ml_decision_tree_regressor.tbl_spark and ml_decision_tree_classifier.tbl_spark and calls the appropriate method based on model type."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#value",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#value",
    "title": "Spark ML – Decision Trees",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#examples",
    "title": "Spark ML – Decision Trees",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \npartitions &lt;- iris_tbl %&gt;% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \n \niris_training &lt;- partitions$training \niris_test &lt;- partitions$test \n \ndt_model &lt;- iris_training %&gt;% \n  ml_decision_tree(Species ~ .) \n \npred &lt;- ml_predict(dt_model, iris_test) \n \nml_multiclass_classification_evaluator(pred) \n#&gt; [1] 0.9393939"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#see-also",
    "title": "Spark ML – Decision Trees",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms. Other ml algorithms: ml_aft_survival_regression(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_tree_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_tree_tidiers.html",
    "title": "Tidying methods for Spark ML tree models",
    "section": "",
    "text": "R/tidiers_ml_tree_models.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#ml_tree_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#ml_tree_tidiers",
    "title": "Tidying methods for Spark ML tree models",
    "section": "ml_tree_tidiers",
    "text": "ml_tree_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#description",
    "title": "Tidying methods for Spark ML tree models",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#usage",
    "title": "Tidying methods for Spark ML tree models",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_decision_tree_classification'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_decision_tree_regression'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_decision_tree_classification'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class '`_ml_model_decision_tree_classification`'\naugment(x, new_data = NULL, ...) \n\n## S3 method for class 'ml_model_decision_tree_regression'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class '`_ml_model_decision_tree_regression`'\naugment(x, new_data = NULL, ...) \n\n## S3 method for class 'ml_model_decision_tree_classification'\nglance(x, ...) \n\n## S3 method for class 'ml_model_decision_tree_regression'\nglance(x, ...) \n\n## S3 method for class 'ml_model_random_forest_classification'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_random_forest_regression'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_random_forest_classification'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class '`_ml_model_random_forest_classification`'\naugment(x, new_data = NULL, ...) \n\n## S3 method for class 'ml_model_random_forest_regression'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class '`_ml_model_random_forest_regression`'\naugment(x, new_data = NULL, ...) \n\n## S3 method for class 'ml_model_random_forest_classification'\nglance(x, ...) \n\n## S3 method for class 'ml_model_random_forest_regression'\nglance(x, ...) \n\n## S3 method for class 'ml_model_gbt_classification'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_gbt_regression'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_gbt_classification'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class '`_ml_model_gbt_classification`'\naugment(x, new_data = NULL, ...) \n\n## S3 method for class 'ml_model_gbt_regression'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class '`_ml_model_gbt_regression`'\naugment(x, new_data = NULL, ...) \n\n## S3 method for class 'ml_model_gbt_classification'\nglance(x, ...) \n\n## S3 method for class 'ml_model_gbt_regression'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML tree models",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction.\n\n\nnew_data\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgeom.html",
    "href": "packages/sparklyr/latest/reference/sdf_rgeom.html",
    "title": "Generate random samples from a geometric distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgeom.html#sdf_rgeom",
    "href": "packages/sparklyr/latest/reference/sdf_rgeom.html#sdf_rgeom",
    "title": "Generate random samples from a geometric distribution",
    "section": "sdf_rgeom",
    "text": "sdf_rgeom"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgeom.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rgeom.html#description",
    "title": "Generate random samples from a geometric distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a geometric distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgeom.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rgeom.html#usage",
    "title": "Generate random samples from a geometric distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rgeom(sc, n, prob, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgeom.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rgeom.html#arguments",
    "title": "Generate random samples from a geometric distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nprob\nProbability of success in each trial.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgeom.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rgeom.html#see-also",
    "title": "Generate random samples from a geometric distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html",
    "href": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html",
    "title": "Retrieves or sets advisory size of the shuffle partition",
    "section": "",
    "text": "R/spark_context_config.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#spark_advisory_shuffle_partition_size",
    "href": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#spark_advisory_shuffle_partition_size",
    "title": "Retrieves or sets advisory size of the shuffle partition",
    "section": "spark_advisory_shuffle_partition_size",
    "text": "spark_advisory_shuffle_partition_size"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#description",
    "href": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#description",
    "title": "Retrieves or sets advisory size of the shuffle partition",
    "section": "Description",
    "text": "Description\nRetrieves or sets advisory size in bytes of the shuffle partition during adaptive optimization"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#usage",
    "title": "Retrieves or sets advisory size of the shuffle partition",
    "section": "Usage",
    "text": "Usage\nspark_advisory_shuffle_partition_size(sc, size = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#arguments",
    "title": "Retrieves or sets advisory size of the shuffle partition",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nsize\nAdvisory size in bytes of the shuffle partition. Defaults to NULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#see-also",
    "title": "Retrieves or sets advisory size of the shuffle partition",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_adaptive_query_execution(), spark_auto_broadcast_join_threshold(), spark_coalesce_initial_num_partitions(), spark_coalesce_min_num_partitions(), spark_coalesce_shuffle_partitions(), spark_session_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html",
    "title": "Spark ML – Tuning",
    "section": "",
    "text": "R/ml_tuning.R, R/ml_tuning_cross_validator.R,"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#ml-tuning",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#ml-tuning",
    "title": "Spark ML – Tuning",
    "section": "ml-tuning",
    "text": "ml-tuning"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#description",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#description",
    "title": "Spark ML – Tuning",
    "section": "Description",
    "text": "Description\nPerform hyper-parameter tuning using either K-fold cross validation or train-validation split."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#usage",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#usage",
    "title": "Spark ML – Tuning",
    "section": "Usage",
    "text": "Usage\n \nml_sub_models(model) \n \nml_validation_metrics(model) \n \nml_cross_validator( \n  x, \n  estimator = NULL, \n  estimator_param_maps = NULL, \n  evaluator = NULL, \n  num_folds = 3, \n  collect_sub_models = FALSE, \n  parallelism = 1, \n  seed = NULL, \n  uid = random_string(\"cross_validator_\"), \n  ... \n) \n \nml_train_validation_split( \n  x, \n  estimator = NULL, \n  estimator_param_maps = NULL, \n  evaluator = NULL, \n  train_ratio = 0.75, \n  collect_sub_models = FALSE, \n  parallelism = 1, \n  seed = NULL, \n  uid = random_string(\"train_validation_split_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#arguments",
    "title": "Spark ML – Tuning",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nmodel\nA cross validation or train-validation-split model.\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nestimator\nA ml_estimator object.\n\n\nestimator_param_maps\nA named list of stages and hyper-parameter sets to tune. See details.\n\n\nevaluator\nA ml_evaluator object, see ml_evaluator.\n\n\nnum_folds\nNumber of folds for cross validation. Must be &gt;= 2. Default: 3\n\n\ncollect_sub_models\nWhether to collect a list of sub-models trained during tuning. If set to FALSE, then only the single best sub-model will be available after fitting. If set to true, then all sub-models will be available. Warning: For large models, collecting all sub-models can cause OOMs on the Spark driver.\n\n\nparallelism\nThe number of threads to use when running parallel algorithms. Default is 1 for serial execution.\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; currently unused.\n\n\ntrain_ratio\nRatio between train and validation data. Must be between 0 and 1. Default: 0.75"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#details",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#details",
    "title": "Spark ML – Tuning",
    "section": "Details",
    "text": "Details\nml_cross_validator() performs k-fold cross validation while ml_train_validation_split() performs tuning on one pair of train and validation datasets."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#value",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#value",
    "title": "Spark ML – Tuning",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_cross_validator or ml_traing_validation_split object.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the tuning estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a tuning estimator is constructed then immediately fit with the input tbl_spark, returning a ml_cross_validation_model or a ml_train_validation_split_model object.\nFor cross validation, ml_sub_models() returns a nested list of models, where the first layer represents fold indices and the second layer represents param maps. For train-validation split, ml_sub_models() returns a list of models, corresponding to the order of the estimator param maps. ml_validation_metrics() returns a data frame of performance metrics and hyperparameter combinations."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#examples",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#examples",
    "title": "Spark ML – Tuning",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \n# Create a pipeline \npipeline &lt;- ml_pipeline(sc) %&gt;% \n  ft_r_formula(Species ~ .) %&gt;% \n  ml_random_forest_classifier() \n \n# Specify hyperparameter grid \ngrid &lt;- list( \n  random_forest = list( \n    num_trees = c(5, 10), \n    max_depth = c(5, 10), \n    impurity = c(\"entropy\", \"gini\") \n  ) \n) \n \n# Create the cross validator object \ncv &lt;- ml_cross_validator( \n  sc, \n  estimator = pipeline, estimator_param_maps = grid, \n  evaluator = ml_multiclass_classification_evaluator(sc), \n  num_folds = 3, \n  parallelism = 4 \n) \n \n# Train the models \ncv_model &lt;- ml_fit(cv, iris_tbl) \n \n# Print the metrics \nml_validation_metrics(cv_model) \n#&gt;          f1 impurity_1 num_trees_1 max_depth_1\n#&gt; 1 0.9397887    entropy           5           5\n#&gt; 2 0.9056526    entropy          10           5\n#&gt; 3 0.9397887    entropy           5          10\n#&gt; 4 0.9056526    entropy          10          10\n#&gt; 5 0.9397887       gini           5           5\n#&gt; 6 0.9127092       gini          10           5\n#&gt; 7 0.9397887       gini           5          10\n#&gt; 8 0.9127092       gini          10          10"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.spark_connection.html",
    "href": "packages/sparklyr/latest/reference/copy_to.spark_connection.html",
    "title": "Copy an R Data Frame to Spark",
    "section": "",
    "text": "R/dplyr_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#copy_to.spark_connection",
    "href": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#copy_to.spark_connection",
    "title": "Copy an R Data Frame to Spark",
    "section": "copy_to.spark_connection",
    "text": "copy_to.spark_connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#description",
    "href": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#description",
    "title": "Copy an R Data Frame to Spark",
    "section": "Description",
    "text": "Description\nCopy an R data.frame to Spark, and return a reference to the generated Spark DataFrame as a tbl_spark. The returned object will act as a dplyr-compatible interface to the underlying Spark table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#usage",
    "href": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#usage",
    "title": "Copy an R Data Frame to Spark",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'spark_connection'\ncopy_to( \n  dest, \n  df, \n  name = spark_table_name(substitute(df)), \n  overwrite = FALSE, \n  memory = TRUE, \n  repartition = 0L, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#arguments",
    "href": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#arguments",
    "title": "Copy an R Data Frame to Spark",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\ndest\nA spark_connection.\n\n\ndf\nAn R data.frame.\n\n\nname\nThe name to assign to the copied table in Spark.\n\n\noverwrite\nBoolean; overwrite a pre-existing table with the name nameif one already exists?\n\n\nmemory\nBoolean; should the table be cached into memory?\n\n\nrepartition\nThe number of partitions to use when distributing the table across the Spark cluster. The default (0) can be used to avoid partitioning.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#value",
    "href": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#value",
    "title": "Copy an R Data Frame to Spark",
    "section": "Value",
    "text": "Value\nA tbl_spark, representing a dplyr-compatible interface to a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html",
    "title": "Partition a Spark Dataframe",
    "section": "",
    "text": "R/sdf_ml.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#sdf_random_split",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#sdf_random_split",
    "title": "Partition a Spark Dataframe",
    "section": "sdf_random_split",
    "text": "sdf_random_split"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#description",
    "title": "Partition a Spark Dataframe",
    "section": "Description",
    "text": "Description\nPartition a Spark DataFrame into multiple groups. This routine is useful for splitting a DataFrame into, for example, training and test datasets."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#usage",
    "title": "Partition a Spark Dataframe",
    "section": "Usage",
    "text": "Usage\nsdf_random_split( \n  x, \n  ..., \n  weights = NULL, \n  seed = sample(.Machine$integer.max, 1) \n) \n\nsdf_partition(x, ..., weights = NULL, seed = sample(.Machine$integer.max, 1))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#arguments",
    "title": "Partition a Spark Dataframe",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\n…\nNamed parameters, mapping table names to weights. The weights will be normalized such that they sum to 1.\n\n\nweights\nAn alternate mechanism for supplying weights – when specified, this takes precedence over the ... arguments.\n\n\nseed\nRandom seed to use for randomly partitioning the dataset. Set this if you want your partitioning to be reproducible on repeated runs."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#details",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#details",
    "title": "Partition a Spark Dataframe",
    "section": "Details",
    "text": "Details\nThe sampling weights define the probability that a particular observation will be assigned to a particular partition, not the resulting size of the partition. This implies that partitioning a DataFrame with, for example,\nsdf_random_split(x, training = 0.5, test = 0.5)\nis not guaranteed to produce training and test partitions of equal size."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#section",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#section",
    "title": "Partition a Spark Dataframe",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#transforming-spark-dataframes",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#transforming-spark-dataframes",
    "title": "Partition a Spark Dataframe",
    "section": "Transforming Spark DataFrames",
    "text": "Transforming Spark DataFrames\nThe family of functions prefixed with sdf_ generally access the Scala Spark DataFrame API directly, as opposed to the dplyr interface which uses Spark SQL. These functions will ‘force’ any pending SQL in a dplyr pipeline, such that the resulting tbl_spark object returned will no longer have the attached ‘lazy’ SQL operations. Note that the underlying Spark DataFrame does execute its operations lazily, so that even though the pending set of operations (currently) are not exposed at the R level, these operations will only be executed when you explicitly collect() the table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#value",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#value",
    "title": "Partition a Spark Dataframe",
    "section": "Value",
    "text": "Value\nAn R list of tbl_sparks."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#examples",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#examples",
    "title": "Partition a Spark Dataframe",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n# randomly partition data into a 'training' and 'test' \n# dataset, with 60% of the observations assigned to the \n# 'training' dataset, and 40% assigned to the 'test' dataset \ndata(diamonds, package = \"ggplot2\") \ndiamonds_tbl &lt;- copy_to(sc, diamonds, \"diamonds\") \npartitions &lt;- diamonds_tbl %&gt;% \n  sdf_random_split(training = 0.6, test = 0.4) \nprint(partitions) \n# alternate way of specifying weights \nweights &lt;- c(training = 0.6, test = 0.4) \ndiamonds_tbl %&gt;% sdf_random_split(weights = weights)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#see-also",
    "title": "Partition a Spark Dataframe",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_copy_to(), sdf_distinct(), sdf_register(), sdf_sample(), sdf_sort(), sdf_weighted_sample()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html",
    "title": "Read Socket Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html#stream_read_socket",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html#stream_read_socket",
    "title": "Read Socket Stream",
    "section": "stream_read_socket",
    "text": "stream_read_socket"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html#description",
    "title": "Read Socket Stream",
    "section": "Description",
    "text": "Description\nReads a Socket stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html#usage",
    "title": "Read Socket Stream",
    "section": "Usage",
    "text": "Usage\nstream_read_socket(sc, name = NULL, columns = NULL, options = list(), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html#arguments",
    "title": "Read Socket Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html#examples",
    "title": "Read Socket Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\") \n# Start socket server from terminal, example: nc -lk 9999 \nstream &lt;- stream_read_socket(sc, options = list(host = \"localhost\", port = 9999)) \nstream"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html#see-also",
    "title": "Read Socket Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write.html",
    "href": "packages/sparklyr/latest/reference/spark_write.html",
    "title": "Write Spark DataFrame to file using a custom writer",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write.html#spark_write",
    "href": "packages/sparklyr/latest/reference/spark_write.html#spark_write",
    "title": "Write Spark DataFrame to file using a custom writer",
    "section": "spark_write",
    "text": "spark_write"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write.html#description",
    "title": "Write Spark DataFrame to file using a custom writer",
    "section": "Description",
    "text": "Description\nRun a custom R function on Spark worker to write a Spark DataFrame into file(s). If Spark’s speculative execution feature is enabled (i.e., spark.speculation is true), then each write task may be executed more than once and the user-defined writer function will need to ensure no concurrent writes happen to the same file path (e.g., by appending UUID to each file name)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write.html#usage",
    "title": "Write Spark DataFrame to file using a custom writer",
    "section": "Usage",
    "text": "Usage\nspark_write(x, writer, paths, packages = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write.html#arguments",
    "title": "Write Spark DataFrame to file using a custom writer",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark Dataframe to be saved into file(s)\n\n\nwriter\nA writer function with the signature function(partition, path) where partition is a R dataframe containing all rows from one partition of the original Spark Dataframe x and path is a string specifying the file to write partition to\n\n\npaths\nA single destination path or a list of destination paths, each one specifying a location for a partition from x to be written to. If number of partition(s) in x is not equal to length(paths) then x will be re-partitioned to contain length(paths) partition(s)\n\n\npackages\nBoolean to distribute .libPaths() packages to each node, a list of packages to distribute, or a package bundle created with"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write.html#examples",
    "href": "packages/sparklyr/latest/reference/spark_write.html#examples",
    "title": "Write Spark DataFrame to file using a custom writer",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local[3]\") \n# copy some test data into a Spark Dataframe \nsdf &lt;- sdf_copy_to(sc, iris, overwrite = TRUE) \n# create a writer function \nwriter &lt;- function(df, path) { \n  write.csv(df, path) \n} \nspark_write( \n  sdf, \n  writer, \n  # re-partition sdf into 3 partitions and write them to 3 separate files \n  paths = list(\"file:///tmp/file1\", \"file:///tmp/file2\", \"file:///tmp/file3\"), \n) \n#&gt; list()\nspark_write( \n  sdf, \n  writer, \n  # save all rows into a single file \n  paths = list(\"file:///tmp/all_rows\") \n) \n#&gt; list()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html",
    "title": "Read a Text file into a Spark DataFrame",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html#spark_read_text",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html#spark_read_text",
    "title": "Read a Text file into a Spark DataFrame",
    "section": "spark_read_text",
    "text": "spark_read_text"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html#description",
    "title": "Read a Text file into a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRead a text file into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html#usage",
    "title": "Read a Text file into a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nspark_read_text( \n  sc, \n  name = NULL, \n  path = name, \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE, \n  options = list(), \n  whole = FALSE, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html#arguments",
    "title": "Read a Text file into a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\noptions\nA list of strings with additional options.\n\n\nwhole\nRead the entire text file as a single entry? Defaults to FALSE.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html#details",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html#details",
    "title": "Read a Text file into a Spark DataFrame",
    "section": "Details",
    "text": "Details\nYou can read data from HDFS (hdfs://), S3 (s3a://), as well as the local file system (file://).\nIf you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf spark.hadoop.fs.s3a.access.key, spark.hadoop.fs.s3a.secret.key or any of the methods outlined in the aws-sdk documentation Working with AWS credentials\nIn order to work with the newer s3a:// protocol also set the values for spark.hadoop.fs.s3a.impl and spark.hadoop.fs.s3a.endpoint. In addition, to support v4 of the S3 api be sure to pass the -Dcom.amazonaws.services.s3.enableV4 driver options for the config key spark.driver.extraJavaOptions\nFor instructions on how to configure s3n:// check the hadoop documentation: s3n authentication properties"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html#see-also",
    "title": "Read a Text file into a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_versions.html",
    "href": "packages/sparklyr/latest/reference/spark_versions.html",
    "title": "Retrieves a dataframe available Spark versions that van be installed.",
    "section": "",
    "text": "R/install_spark_versions.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_versions.html#spark_versions",
    "href": "packages/sparklyr/latest/reference/spark_versions.html#spark_versions",
    "title": "Retrieves a dataframe available Spark versions that van be installed.",
    "section": "spark_versions",
    "text": "spark_versions"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_versions.html#description",
    "href": "packages/sparklyr/latest/reference/spark_versions.html#description",
    "title": "Retrieves a dataframe available Spark versions that van be installed.",
    "section": "Description",
    "text": "Description\nRetrieves a dataframe available Spark versions that van be installed."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_versions.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_versions.html#usage",
    "title": "Retrieves a dataframe available Spark versions that van be installed.",
    "section": "Usage",
    "text": "Usage\nspark_versions(latest = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_versions.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_versions.html#arguments",
    "title": "Retrieves a dataframe available Spark versions that van be installed.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nlatest\nCheck for latest version?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html",
    "title": "Perform Weighted Random Sampling on a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#sdf_weighted_sample",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#sdf_weighted_sample",
    "title": "Perform Weighted Random Sampling on a Spark DataFrame",
    "section": "sdf_weighted_sample",
    "text": "sdf_weighted_sample"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#description",
    "title": "Perform Weighted Random Sampling on a Spark DataFrame",
    "section": "Description",
    "text": "Description\nDraw a random sample of rows (with or without replacement) from a Spark DataFrame If the sampling is done without replacement, then it will be conceptually equivalent to an iterative process such that in each step the probability of adding a row to the sample set is equal to its weight divided by summation of weights of all rows that are not in the sample set yet in that step."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#usage",
    "title": "Perform Weighted Random Sampling on a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_weighted_sample(x, weight_col, k, replacement = TRUE, seed = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#arguments",
    "title": "Perform Weighted Random Sampling on a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\nweight_col\nName of the weight column\n\n\nk\nSample set size\n\n\nreplacement\nWhether to sample with replacement\n\n\nseed\nAn (optional) integer seed"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#section",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#section",
    "title": "Perform Weighted Random Sampling on a Spark DataFrame",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#transforming-spark-dataframes",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#transforming-spark-dataframes",
    "title": "Perform Weighted Random Sampling on a Spark DataFrame",
    "section": "Transforming Spark DataFrames",
    "text": "Transforming Spark DataFrames\nThe family of functions prefixed with sdf_ generally access the Scala Spark DataFrame API directly, as opposed to the dplyr interface which uses Spark SQL. These functions will ‘force’ any pending SQL in a dplyr pipeline, such that the resulting tbl_spark object returned will no longer have the attached ‘lazy’ SQL operations. Note that the underlying Spark DataFrame does execute its operations lazily, so that even though the pending set of operations (currently) are not exposed at the R level, these operations will only be executed when you explicitly collect() the table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#see-also",
    "title": "Perform Weighted Random Sampling on a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_copy_to(), sdf_distinct(), sdf_random_split(), sdf_register(), sdf_sample(), sdf_sort()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_csv.html",
    "href": "packages/sparklyr/latest/reference/spark_write_csv.html",
    "title": "Write a Spark DataFrame to a CSV",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_csv.html#spark_write_csv",
    "href": "packages/sparklyr/latest/reference/spark_write_csv.html#spark_write_csv",
    "title": "Write a Spark DataFrame to a CSV",
    "section": "spark_write_csv",
    "text": "spark_write_csv"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_csv.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_csv.html#description",
    "title": "Write a Spark DataFrame to a CSV",
    "section": "Description",
    "text": "Description\nWrite a Spark DataFrame to a tabular (typically, comma-separated) file."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_csv.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_csv.html#usage",
    "title": "Write a Spark DataFrame to a CSV",
    "section": "Usage",
    "text": "Usage\nspark_write_csv( \n  x, \n  path, \n  header = TRUE, \n  delimiter = \",\", \n  quote = \"\\\"\", \n  escape = \"\\\\\", \n  charset = \"UTF-8\", \n  null_value = NULL, \n  options = list(), \n  mode = NULL, \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_csv.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_csv.html#arguments",
    "title": "Write a Spark DataFrame to a CSV",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nheader\nShould the first row of data be used as a header? Defaults to TRUE.\n\n\ndelimiter\nThe character used to delimit each column, defaults to ,.\n\n\nquote\nThe character used as a quote. Defaults to '\"'.\n\n\nescape\nThe character used to escape other characters, defaults to \\.\n\n\ncharset\nThe character set, defaults to \"UTF-8\".\n\n\nnull_value\nThe character to use for default values, defaults to NULL.\n\n\noptions\nA list of strings with additional options.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_csv.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_csv.html#see-also",
    "title": "Write a Spark DataFrame to a CSV",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_id.html",
    "href": "packages/sparklyr/latest/reference/stream_id.html",
    "title": "Spark Stream’s Identifier",
    "section": "",
    "text": "R/stream_operations.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_id.html#stream_id",
    "href": "packages/sparklyr/latest/reference/stream_id.html#stream_id",
    "title": "Spark Stream’s Identifier",
    "section": "stream_id",
    "text": "stream_id"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_id.html#description",
    "href": "packages/sparklyr/latest/reference/stream_id.html#description",
    "title": "Spark Stream’s Identifier",
    "section": "Description",
    "text": "Description\nRetrieves the identifier of the Spark stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_id.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_id.html#usage",
    "title": "Spark Stream’s Identifier",
    "section": "Usage",
    "text": "Usage\nstream_id(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_id.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_id.html#arguments",
    "title": "Spark Stream’s Identifier",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nstream\nThe spark stream object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pca_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_pca_tidiers.html",
    "title": "Tidying methods for Spark ML Principal Component Analysis",
    "section": "",
    "text": "R/tidiers_pca.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#ml_pca_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#ml_pca_tidiers",
    "title": "Tidying methods for Spark ML Principal Component Analysis",
    "section": "ml_pca_tidiers",
    "text": "ml_pca_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#description",
    "title": "Tidying methods for Spark ML Principal Component Analysis",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#usage",
    "title": "Tidying methods for Spark ML Principal Component Analysis",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_pca'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_pca'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_pca'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML Principal Component Analysis",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html",
    "title": "Write Text Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html#stream_write_text",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html#stream_write_text",
    "title": "Write Text Stream",
    "section": "stream_write_text",
    "text": "stream_write_text"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html#description",
    "title": "Write Text Stream",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into a text stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html#usage",
    "title": "Write Text Stream",
    "section": "Usage",
    "text": "Usage\nstream_write_text( \n  x, \n  path, \n  mode = c(\"append\", \"complete\", \"update\"), \n  trigger = stream_trigger_interval(), \n  checkpoint = file.path(path, \"checkpoints\", random_string(\"\")), \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html#arguments",
    "title": "Write Text Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe destination path. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\".\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous.\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html#examples",
    "title": "Write Text Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\") \ndir.create(\"text-in\") \nwriteLines(\"A text entry\", \"text-in/text.txt\") \ntext_path &lt;- file.path(\"file://\", getwd(), \"text-in\") \nstream &lt;- stream_read_text(sc, text_path) %&gt;% stream_write_text(\"text-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html#see-also",
    "title": "Write Text Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html",
    "title": "Spark ML - Clustering Evaluator",
    "section": "",
    "text": "R/ml_evaluation_clustering.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#ml_clustering_evaluator",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#ml_clustering_evaluator",
    "title": "Spark ML - Clustering Evaluator",
    "section": "ml_clustering_evaluator",
    "text": "ml_clustering_evaluator"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#description",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#description",
    "title": "Spark ML - Clustering Evaluator",
    "section": "Description",
    "text": "Description\nEvaluator for clustering results. The metric computes the Silhouette measure using the squared Euclidean distance. The Silhouette is a measure for the validation of the consistency within clusters. It ranges between 1 and -1, where a value close to 1 means that the points in a cluster are close to the other points in the same cluster and far from the points of the other clusters."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#usage",
    "title": "Spark ML - Clustering Evaluator",
    "section": "Usage",
    "text": "Usage\n \nml_clustering_evaluator( \n  x, \n  features_col = \"features\", \n  prediction_col = \"prediction\", \n  metric_name = \"silhouette\", \n  uid = random_string(\"clustering_evaluator_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#arguments",
    "title": "Spark ML - Clustering Evaluator",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection object or a tbl_spark containing label and prediction columns. The latter should be the output of sdf_predict.\n\n\nfeatures_col\nName of features column.\n\n\nprediction_col\nName of the prediction column.\n\n\nmetric_name\nThe performance metric. Currently supports “silhouette”.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#value",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#value",
    "title": "Spark ML - Clustering Evaluator",
    "section": "Value",
    "text": "Value\nThe calculated performance metric"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#examples",
    "title": "Spark ML - Clustering Evaluator",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \npartitions &lt;- iris_tbl %&gt;% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \n \niris_training &lt;- partitions$training \niris_test &lt;- partitions$test \n \nformula &lt;- Species ~ . \n \n# Train the models \nkmeans_model &lt;- ml_kmeans(iris_training, formula = formula) \nb_kmeans_model &lt;- ml_bisecting_kmeans(iris_training, formula = formula) \ngmm_model &lt;- ml_gaussian_mixture(iris_training, formula = formula) \n \n# Predict \npred_kmeans &lt;- ml_predict(kmeans_model, iris_test) \npred_b_kmeans &lt;- ml_predict(b_kmeans_model, iris_test) \npred_gmm &lt;- ml_predict(gmm_model, iris_test) \n \n# Evaluate \nml_clustering_evaluator(pred_kmeans) \n#&gt; [1] 0.8736088\nml_clustering_evaluator(pred_b_kmeans) \n#&gt; [1] 0.4433206\nml_clustering_evaluator(pred_gmm) \n#&gt; [1] 0.8677525"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html",
    "href": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html",
    "title": "Fast cbind for Spark DataFrames",
    "section": "",
    "text": "R/sdf_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#sdf_fast_bind_cols",
    "href": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#sdf_fast_bind_cols",
    "title": "Fast cbind for Spark DataFrames",
    "section": "sdf_fast_bind_cols",
    "text": "sdf_fast_bind_cols"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#description",
    "title": "Fast cbind for Spark DataFrames",
    "section": "Description",
    "text": "Description\nThis is a version of sdf_bind_cols that works by zipping RDDs. From the API docs: “Assumes that the two RDDs have the same number of partitions and the same number of elements in each partition (e.g. one was made through a map on the other).”"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#usage",
    "title": "Fast cbind for Spark DataFrames",
    "section": "Usage",
    "text": "Usage\nsdf_fast_bind_cols(...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#arguments",
    "title": "Fast cbind for Spark DataFrames",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\n…\nSpark DataFrames to cbind"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html",
    "title": "Tidying methods for Spark ML MLP",
    "section": "",
    "text": "R/tidiers_ml_multilayer_perceptron.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#ml_multilayer_perceptron_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#ml_multilayer_perceptron_tidiers",
    "title": "Tidying methods for Spark ML MLP",
    "section": "ml_multilayer_perceptron_tidiers",
    "text": "ml_multilayer_perceptron_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#description",
    "title": "Tidying methods for Spark ML MLP",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#usage",
    "title": "Tidying methods for Spark ML MLP",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_multilayer_perceptron_classification'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_multilayer_perceptron_classification'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_multilayer_perceptron_classification'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML MLP",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_als_tidiers.html",
    "title": "Tidying methods for Spark ML ALS",
    "section": "",
    "text": "R/tidiers_ml_als.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als_tidiers.html#ml_als_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_als_tidiers.html#ml_als_tidiers",
    "title": "Tidying methods for Spark ML ALS",
    "section": "ml_als_tidiers",
    "text": "ml_als_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_als_tidiers.html#description",
    "title": "Tidying methods for Spark ML ALS",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_als_tidiers.html#usage",
    "title": "Tidying methods for Spark ML ALS",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_als'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_als'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_als'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_als_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML ALS",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_summary.html",
    "href": "packages/sparklyr/latest/reference/ml_summary.html",
    "title": "Spark ML – Extraction of summary metrics",
    "section": "",
    "text": "R/ml_pipeline_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_summary.html#ml_summary",
    "href": "packages/sparklyr/latest/reference/ml_summary.html#ml_summary",
    "title": "Spark ML – Extraction of summary metrics",
    "section": "ml_summary",
    "text": "ml_summary"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_summary.html#description",
    "href": "packages/sparklyr/latest/reference/ml_summary.html#description",
    "title": "Spark ML – Extraction of summary metrics",
    "section": "Description",
    "text": "Description\nExtracts a metric from the summary object of a Spark ML model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_summary.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_summary.html#usage",
    "title": "Spark ML – Extraction of summary metrics",
    "section": "Usage",
    "text": "Usage\nml_summary(x, metric = NULL, allow_null = FALSE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_summary.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_summary.html#arguments",
    "title": "Spark ML – Extraction of summary metrics",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark ML model that has a summary.\n\n\nmetric\nThe name of the metric to extract. If not set, returns the summary object.\n\n\nallow_null\nWhether null results are allowed when the metric is not found in the summary."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/list_sparklyr_jars.html",
    "href": "packages/sparklyr/latest/reference/list_sparklyr_jars.html",
    "title": "list all sparklyr-*.jar files that have been built",
    "section": "",
    "text": "R/spark_compile.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/list_sparklyr_jars.html#list_sparklyr_jars",
    "href": "packages/sparklyr/latest/reference/list_sparklyr_jars.html#list_sparklyr_jars",
    "title": "list all sparklyr-*.jar files that have been built",
    "section": "list_sparklyr_jars",
    "text": "list_sparklyr_jars"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/list_sparklyr_jars.html#description",
    "href": "packages/sparklyr/latest/reference/list_sparklyr_jars.html#description",
    "title": "list all sparklyr-*.jar files that have been built",
    "section": "Description",
    "text": "Description\nlist all sparklyr-*.jar files that have been built"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/list_sparklyr_jars.html#usage",
    "href": "packages/sparklyr/latest/reference/list_sparklyr_jars.html#usage",
    "title": "list all sparklyr-*.jar files that have been built",
    "section": "Usage",
    "text": "Usage\nlist_sparklyr_jars()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-model-constructors.html",
    "href": "packages/sparklyr/latest/reference/ml-model-constructors.html",
    "title": "Constructors for ml_model Objects",
    "section": "",
    "text": "R/ml_model_helpers.R, R/ml_model_constructors.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-model-constructors.html#ml_supervised_pipeline",
    "href": "packages/sparklyr/latest/reference/ml-model-constructors.html#ml_supervised_pipeline",
    "title": "Constructors for ml_model Objects",
    "section": "ml_supervised_pipeline",
    "text": "ml_supervised_pipeline"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-model-constructors.html#description",
    "href": "packages/sparklyr/latest/reference/ml-model-constructors.html#description",
    "title": "Constructors for ml_model Objects",
    "section": "Description",
    "text": "Description\nFunctions for developers writing extensions for Spark ML. These functions are constructors for ml_model objects that are returned when using the formula interface."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-model-constructors.html#usage",
    "href": "packages/sparklyr/latest/reference/ml-model-constructors.html#usage",
    "title": "Constructors for ml_model Objects",
    "section": "Usage",
    "text": "Usage\nml_supervised_pipeline(predictor, dataset, formula, features_col, label_col) \n\nml_clustering_pipeline(predictor, dataset, formula, features_col) \n\nml_construct_model_supervised( \n  constructor, \n  predictor, \n  formula, \n  dataset, \n  features_col, \n  label_col, \n  ... \n) \n\nml_construct_model_clustering( \n  constructor, \n  predictor, \n  formula, \n  dataset, \n  features_col, \n  ... \n) \n\nnew_ml_model_prediction( \n  pipeline_model, \n  formula, \n  dataset, \n  label_col, \n  features_col, \n  ..., \n  class = character() \n) \n\nnew_ml_model(pipeline_model, formula, dataset, ..., class = character()) \n\nnew_ml_model_classification( \n  pipeline_model, \n  formula, \n  dataset, \n  label_col, \n  features_col, \n  predicted_label_col, \n  ..., \n  class = character() \n) \n\nnew_ml_model_regression( \n  pipeline_model, \n  formula, \n  dataset, \n  label_col, \n  features_col, \n  ..., \n  class = character() \n) \n\nnew_ml_model_clustering( \n  pipeline_model, \n  formula, \n  dataset, \n  features_col, \n  ..., \n  class = character() \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-model-constructors.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml-model-constructors.html#arguments",
    "title": "Constructors for ml_model Objects",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\npredictor\nThe pipeline stage corresponding to the ML algorithm.\n\n\ndataset\nThe training dataset.\n\n\nformula\nThe formula used for data preprocessing\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nconstructor\nThe constructor function for the ml_model.\n\n\npipeline_model\nThe pipeline model object returned by ml_supervised_pipeline().\n\n\nclass\nName of the subclass."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html",
    "title": "Sort a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#sdf_sort",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#sdf_sort",
    "title": "Sort a Spark DataFrame",
    "section": "sdf_sort",
    "text": "sdf_sort"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#description",
    "title": "Sort a Spark DataFrame",
    "section": "Description",
    "text": "Description\nSort a Spark DataFrame by one or more columns, with each column sorted in ascending order."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#usage",
    "title": "Sort a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_sort(x, columns)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#arguments",
    "title": "Sort a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\ncolumns\nThe column(s) to sort by."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#section",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#section",
    "title": "Sort a Spark DataFrame",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#transforming-spark-dataframes",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#transforming-spark-dataframes",
    "title": "Sort a Spark DataFrame",
    "section": "Transforming Spark DataFrames",
    "text": "Transforming Spark DataFrames\nThe family of functions prefixed with sdf_ generally access the Scala Spark DataFrame API directly, as opposed to the dplyr interface which uses Spark SQL. These functions will ‘force’ any pending SQL in a dplyr pipeline, such that the resulting tbl_spark object returned will no longer have the attached ‘lazy’ SQL operations. Note that the underlying Spark DataFrame does execute its operations lazily, so that even though the pending set of operations (currently) are not exposed at the R level, these operations will only be executed when you explicitly collect() the table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#see-also",
    "title": "Sort a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_copy_to(), sdf_distinct(), sdf_random_split(), sdf_register(), sdf_sample(), sdf_weighted_sample()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_is_open.html",
    "href": "packages/sparklyr/latest/reference/connection_is_open.html",
    "title": "Check whether the connection is open",
    "section": "",
    "text": "R/core_connection.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_is_open.html#connection_is_open",
    "href": "packages/sparklyr/latest/reference/connection_is_open.html#connection_is_open",
    "title": "Check whether the connection is open",
    "section": "connection_is_open",
    "text": "connection_is_open"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_is_open.html#description",
    "href": "packages/sparklyr/latest/reference/connection_is_open.html#description",
    "title": "Check whether the connection is open",
    "section": "Description",
    "text": "Description\nCheck whether the connection is open"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_is_open.html#usage",
    "href": "packages/sparklyr/latest/reference/connection_is_open.html#usage",
    "title": "Check whether the connection is open",
    "section": "Usage",
    "text": "Usage\nconnection_is_open(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_is_open.html#arguments",
    "href": "packages/sparklyr/latest/reference/connection_is_open.html#arguments",
    "title": "Check whether the connection is open",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nspark_connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_prefixspan.html",
    "href": "packages/sparklyr/latest/reference/ml_prefixspan.html",
    "title": "Frequent Pattern Mining – PrefixSpan",
    "section": "",
    "text": "R/ml_fpm_prefixspan.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_prefixspan.html#ml_prefixspan",
    "href": "packages/sparklyr/latest/reference/ml_prefixspan.html#ml_prefixspan",
    "title": "Frequent Pattern Mining – PrefixSpan",
    "section": "ml_prefixspan",
    "text": "ml_prefixspan"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_prefixspan.html#description",
    "href": "packages/sparklyr/latest/reference/ml_prefixspan.html#description",
    "title": "Frequent Pattern Mining – PrefixSpan",
    "section": "Description",
    "text": "Description\nPrefixSpan algorithm for mining frequent itemsets."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_prefixspan.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_prefixspan.html#usage",
    "title": "Frequent Pattern Mining – PrefixSpan",
    "section": "Usage",
    "text": "Usage\n \nml_prefixspan( \n  x, \n  seq_col = \"sequence\", \n  min_support = 0.1, \n  max_pattern_length = 10, \n  max_local_proj_db_size = 3.2e+07, \n  uid = random_string(\"prefixspan_\"), \n  ... \n) \n \nml_freq_seq_patterns(model)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_prefixspan.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_prefixspan.html#arguments",
    "title": "Frequent Pattern Mining – PrefixSpan",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nseq_col\nThe name of the sequence column in dataset (default “sequence”). Rows with nulls in this column are ignored.\n\n\nmin_support\nThe minimum support required to be considered a frequent sequential pattern.\n\n\nmax_pattern_length\nThe maximum length of a frequent sequential pattern. Any frequent pattern exceeding this length will not be included in the results.\n\n\nmax_local_proj_db_size\nThe maximum number of items allowed in a prefix-projected database before local iterative processing of the projected database begins. This parameter should be tuned with respect to the size of your executors.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; currently unused.\n\n\nmodel\nA Prefix Span model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_prefixspan.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_prefixspan.html#examples",
    "title": "Frequent Pattern Mining – PrefixSpan",
    "section": "Examples",
    "text": "Examples\n\n \nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\", version = \"2.4.0\") \n \nitems_df &lt;- tibble::tibble( \n  seq = list( \n    list(list(1, 2), list(3)), \n    list(list(1), list(3, 2), list(1, 2)), \n    list(list(1, 2), list(5)), \n    list(list(6)) \n  ) \n) \nitems_sdf &lt;- copy_to(sc, items_df, overwrite = TRUE) \n \nprefix_span_model &lt;- ml_prefixspan( \n  sc, \n  seq_col = \"seq\", \n  min_support = 0.5, \n  max_pattern_length = 5, \n  max_local_proj_db_size = 32000000 \n) \n \nfrequent_items &lt;- prefix_span_model$frequent_sequential_patterns(items_sdf) %&gt;% collect()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_lag.html",
    "href": "packages/sparklyr/latest/reference/stream_lag.html",
    "title": "Apply lag function to columns of a Spark Streaming DataFrame",
    "section": "",
    "text": "R/stream_operations.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_lag.html#stream_lag",
    "href": "packages/sparklyr/latest/reference/stream_lag.html#stream_lag",
    "title": "Apply lag function to columns of a Spark Streaming DataFrame",
    "section": "stream_lag",
    "text": "stream_lag"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_lag.html#description",
    "href": "packages/sparklyr/latest/reference/stream_lag.html#description",
    "title": "Apply lag function to columns of a Spark Streaming DataFrame",
    "section": "Description",
    "text": "Description\nGiven a streaming Spark dataframe as input, this function will return another streaming dataframe that contains all columns in the input and column(s) that are shifted behind by the offset(s) specified in ... (see example)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_lag.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_lag.html#usage",
    "title": "Apply lag function to columns of a Spark Streaming DataFrame",
    "section": "Usage",
    "text": "Usage\nstream_lag(x, cols, thresholds = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_lag.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_lag.html#arguments",
    "title": "Apply lag function to columns of a Spark Streaming DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a Spark Streaming DataFrame.\n\n\ncols\nA list of expressions for a single or multiple variables to create that will contain the value of a previous entry.\n\n\nthresholds\nOptional named list of timestamp column(s) and corresponding time duration(s) for deterimining whether a previous record is sufficiently recent relative to the current record. If the any of the time difference(s) between the current and a previous record is greater than the maximal duration allowed, then the previous record is discarded and will not be part of the query result. The durations can be specified with numeric types (which will be interpreted as max difference allowed in number of milliseconds between 2 UNIX timestamps) or time duration strings such as “5s”, “5sec”, “5min”, “5hour”, etc. Any timestamp column in x that is not of timestamp of date Spark SQL types will be interepreted as number of milliseconds since the UNIX epoch."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_lag.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_lag.html#examples",
    "title": "Apply lag function to columns of a Spark Streaming DataFrame",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\", version = \"2.2.0\") \nstreaming_path &lt;- tempfile(\"days_df_\") \ndays_df &lt;- tibble::tibble( \n  today = weekdays(as.Date(seq(7), origin = \"1970-01-01\")) \n) \nnum_iters &lt;- 7 \nstream_generate_test( \n  df = days_df, \n  path = streaming_path, \n  distribution = rep(nrow(days_df), num_iters), \n  iterations = num_iters \n) \nstream_read_csv(sc, streaming_path) %&gt;% \n  stream_lag(cols = c(yesterday = today ~ 1, two_days_ago = today ~ 2)) %&gt;% \n  collect() %&gt;% \n  print(n = 10L)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_log.html",
    "href": "packages/sparklyr/latest/reference/spark_apply_log.html",
    "title": "Log Writer for Spark Apply",
    "section": "",
    "text": "R/spark_apply.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_log.html#spark_apply_log",
    "href": "packages/sparklyr/latest/reference/spark_apply_log.html#spark_apply_log",
    "title": "Log Writer for Spark Apply",
    "section": "spark_apply_log",
    "text": "spark_apply_log"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_log.html#description",
    "href": "packages/sparklyr/latest/reference/spark_apply_log.html#description",
    "title": "Log Writer for Spark Apply",
    "section": "Description",
    "text": "Description\nWrites data to log under spark_apply()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_log.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_apply_log.html#usage",
    "title": "Log Writer for Spark Apply",
    "section": "Usage",
    "text": "Usage\nspark_apply_log(..., level = \"INFO\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_log.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_apply_log.html#arguments",
    "title": "Log Writer for Spark Apply",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\n…\nArguments to write to log.\n\n\nlevel\nSeverity level for this entry; recommended values: INFO, ERROR or WARN."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_along.html",
    "href": "packages/sparklyr/latest/reference/sdf_along.html",
    "title": "Create DataFrame for along Object",
    "section": "",
    "text": "R/sdf_sequence.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_along.html#sdf_along",
    "href": "packages/sparklyr/latest/reference/sdf_along.html#sdf_along",
    "title": "Create DataFrame for along Object",
    "section": "sdf_along",
    "text": "sdf_along"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_along.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_along.html#description",
    "title": "Create DataFrame for along Object",
    "section": "Description",
    "text": "Description\nCreates a DataFrame along the given object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_along.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_along.html#usage",
    "title": "Create DataFrame for along Object",
    "section": "Usage",
    "text": "Usage\nsdf_along(sc, along, repartition = NULL, type = c(\"integer\", \"integer64\"))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_along.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_along.html#arguments",
    "title": "Create DataFrame for along Object",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nalong\nTakes the length from the length of this argument.\n\n\nrepartition\nThe number of partitions to use when distributing the data across the Spark cluster.\n\n\ntype\nThe data type to use for the index, either \"integer\" or \"integer64\"."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html",
    "title": "Read a ORC file into a Spark DataFrame",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html#spark_read_orc",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html#spark_read_orc",
    "title": "Read a ORC file into a Spark DataFrame",
    "section": "spark_read_orc",
    "text": "spark_read_orc"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html#description",
    "title": "Read a ORC file into a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRead a ORC file into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html#usage",
    "title": "Read a ORC file into a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nspark_read_orc( \n  sc, \n  name = NULL, \n  path = name, \n  options = list(), \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE, \n  columns = NULL, \n  schema = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html#arguments",
    "title": "Read a ORC file into a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\noptions\nA list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\nschema\nA (java) read schema. Useful for optimizing read operation on nested data.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html#details",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html#details",
    "title": "Read a ORC file into a Spark DataFrame",
    "section": "Details",
    "text": "Details\nYou can read data from HDFS (hdfs://), S3 (s3a://), as well as the local file system (file://)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html#see-also",
    "title": "Read a ORC file into a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html",
    "title": "Feature Transformation – VectorAssembler (Transformer)",
    "section": "",
    "text": "R/ml_feature_vector_assembler.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html#ft_vector_assembler",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html#ft_vector_assembler",
    "title": "Feature Transformation – VectorAssembler (Transformer)",
    "section": "ft_vector_assembler",
    "text": "ft_vector_assembler"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html#description",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html#description",
    "title": "Feature Transformation – VectorAssembler (Transformer)",
    "section": "Description",
    "text": "Description\nCombine multiple vectors into a single row-vector; that is, where each row element of the newly generated column is a vector formed by concatenating each row element from the specified input columns."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html#usage",
    "title": "Feature Transformation – VectorAssembler (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_vector_assembler( \n  x, \n  input_cols = NULL, \n  output_col = NULL, \n  uid = random_string(\"vector_assembler_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html#arguments",
    "title": "Feature Transformation – VectorAssembler (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_cols\nThe names of the input columns\n\n\noutput_col\nThe name of the output column.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html#value",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html#value",
    "title": "Feature Transformation – VectorAssembler (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html#see-also",
    "title": "Feature Transformation – VectorAssembler (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_runif.html",
    "href": "packages/sparklyr/latest/reference/sdf_runif.html",
    "title": "Generate random samples from the uniform distribution U(0, 1).",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_runif.html#sdf_runif",
    "href": "packages/sparklyr/latest/reference/sdf_runif.html#sdf_runif",
    "title": "Generate random samples from the uniform distribution U(0, 1).",
    "section": "sdf_runif",
    "text": "sdf_runif"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_runif.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_runif.html#description",
    "title": "Generate random samples from the uniform distribution U(0, 1).",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from the uniform distribution U(0, 1)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_runif.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_runif.html#usage",
    "title": "Generate random samples from the uniform distribution U(0, 1).",
    "section": "Usage",
    "text": "Usage\nsdf_runif( \n  sc, \n  n, \n  min = 0, \n  max = 1, \n  num_partitions = NULL, \n  seed = NULL, \n  output_col = \"x\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_runif.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_runif.html#arguments",
    "title": "Generate random samples from the uniform distribution U(0, 1).",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nmin\nThe lower limit of the distribution.\n\n\nmax\nThe upper limit of the distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_runif.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_runif.html#see-also",
    "title": "Generate random samples from the uniform distribution U(0, 1).",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_table_name.html",
    "href": "packages/sparklyr/latest/reference/spark_table_name.html",
    "title": "Generate a Table Name from Expression",
    "section": "",
    "text": "R/spark_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_table_name.html#spark_table_name",
    "href": "packages/sparklyr/latest/reference/spark_table_name.html#spark_table_name",
    "title": "Generate a Table Name from Expression",
    "section": "spark_table_name",
    "text": "spark_table_name"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_table_name.html#description",
    "href": "packages/sparklyr/latest/reference/spark_table_name.html#description",
    "title": "Generate a Table Name from Expression",
    "section": "Description",
    "text": "Description\nAttempts to generate a table name from an expression; otherwise, assigns an auto-generated generic name with “sparklyr_” prefix."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_table_name.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_table_name.html#usage",
    "title": "Generate a Table Name from Expression",
    "section": "Usage",
    "text": "Usage\nspark_table_name(expr)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_table_name.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_table_name.html#arguments",
    "title": "Generate a Table Name from Expression",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nexpr\nThe expression to attempt to use as name"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_name.html",
    "href": "packages/sparklyr/latest/reference/stream_name.html",
    "title": "Spark Stream’s Name",
    "section": "",
    "text": "R/stream_operations.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_name.html#stream_name",
    "href": "packages/sparklyr/latest/reference/stream_name.html#stream_name",
    "title": "Spark Stream’s Name",
    "section": "stream_name",
    "text": "stream_name"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_name.html#description",
    "href": "packages/sparklyr/latest/reference/stream_name.html#description",
    "title": "Spark Stream’s Name",
    "section": "Description",
    "text": "Description\nRetrieves the name of the Spark stream if available."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_name.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_name.html#usage",
    "title": "Spark Stream’s Name",
    "section": "Usage",
    "text": "Usage\nstream_name(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_name.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_name.html#arguments",
    "title": "Spark Stream’s Name",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nstream\nThe spark stream object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_fpgrowth.html",
    "href": "packages/sparklyr/latest/reference/ml_fpgrowth.html",
    "title": "Frequent Pattern Mining – FPGrowth",
    "section": "",
    "text": "R/ml_fpm_fpgrowth.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_fpgrowth.html#ml_fpgrowth",
    "href": "packages/sparklyr/latest/reference/ml_fpgrowth.html#ml_fpgrowth",
    "title": "Frequent Pattern Mining – FPGrowth",
    "section": "ml_fpgrowth",
    "text": "ml_fpgrowth"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_fpgrowth.html#description",
    "href": "packages/sparklyr/latest/reference/ml_fpgrowth.html#description",
    "title": "Frequent Pattern Mining – FPGrowth",
    "section": "Description",
    "text": "Description\nA parallel FP-growth algorithm to mine frequent itemsets."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_fpgrowth.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_fpgrowth.html#usage",
    "title": "Frequent Pattern Mining – FPGrowth",
    "section": "Usage",
    "text": "Usage\nml_fpgrowth( \n  x, \n  items_col = \"items\", \n  min_confidence = 0.8, \n  min_support = 0.3, \n  prediction_col = \"prediction\", \n  uid = random_string(\"fpgrowth_\"), \n  ... \n) \n\nml_association_rules(model) \n\nml_freq_itemsets(model)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_fpgrowth.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_fpgrowth.html#arguments",
    "title": "Frequent Pattern Mining – FPGrowth",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nitems_col\nItems column name. Default: “items”\n\n\nmin_confidence\nMinimal confidence for generating Association Rule. min_confidence will not affect the mining for frequent itemsets, but will affect the association rules generation. Default: 0.8\n\n\nmin_support\nMinimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears more than (min_support * size-of-the-dataset) times will be output in the frequent itemsets. Default: 0.3\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; currently unused.\n\n\nmodel\nA fitted FPGrowth model returned by ml_fpgrowth()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency.html",
    "href": "packages/sparklyr/latest/reference/spark_dependency.html",
    "title": "Define a Spark dependency",
    "section": "",
    "text": "R/spark_extensions.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency.html#spark_dependency",
    "href": "packages/sparklyr/latest/reference/spark_dependency.html#spark_dependency",
    "title": "Define a Spark dependency",
    "section": "spark_dependency",
    "text": "spark_dependency"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency.html#description",
    "href": "packages/sparklyr/latest/reference/spark_dependency.html#description",
    "title": "Define a Spark dependency",
    "section": "Description",
    "text": "Description\nDefine a Spark dependency consisting of a set of custom JARs, Spark packages, and customized dbplyr SQL translation env."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_dependency.html#usage",
    "title": "Define a Spark dependency",
    "section": "Usage",
    "text": "Usage\nspark_dependency( \n  jars = NULL, \n  packages = NULL, \n  initializer = NULL, \n  catalog = NULL, \n  repositories = NULL, \n  dbplyr_sql_variant = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_dependency.html#arguments",
    "title": "Define a Spark dependency",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\njars\nCharacter vector of full paths to JAR files.\n\n\npackages\nCharacter vector of Spark packages names.\n\n\ninitializer\nOptional callback function called when initializing a connection.\n\n\ncatalog\nOptional location where extension JAR files can be downloaded for Livy.\n\n\nrepositories\nCharacter vector of Spark package repositories.\n\n\ndbplyr_sql_variant\nCustomization of dbplyr SQL translation env. Must be a named list of the following form: &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;  list(     scalar = list(scalar_fn1 = ..., scalar_fn2 = ..., &lt;etc&gt;),     aggregate = list(agg_fn1 = ..., agg_fn2 = ..., &lt;etc&gt;),     window = list(wnd_fn1 = ..., wnd_fn2 = ..., &lt;etc&gt;)   )See sql_variant for details.\n\n\n…\nAdditional optional arguments."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency.html#value",
    "href": "packages/sparklyr/latest/reference/spark_dependency.html#value",
    "title": "Define a Spark dependency",
    "section": "Value",
    "text": "Value\nAn object of type spark_dependency"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html",
    "href": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html",
    "title": "Extracts metrics from a fitted table",
    "section": "",
    "text": "R/ml_metrics.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#ml_metrics_multiclass",
    "href": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#ml_metrics_multiclass",
    "title": "Extracts metrics from a fitted table",
    "section": "ml_metrics_multiclass",
    "text": "ml_metrics_multiclass"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#description",
    "href": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#description",
    "title": "Extracts metrics from a fitted table",
    "section": "Description",
    "text": "Description\nThe function works best when passed a tbl_spark created by ml_predict(). The output tbl_spark will contain the correct variable types and format that the given Spark model “evaluator” expects."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#usage",
    "title": "Extracts metrics from a fitted table",
    "section": "Usage",
    "text": "Usage\n \nml_metrics_multiclass( \n  x, \n  truth = label, \n  estimate = prediction, \n  metrics = c(\"accuracy\"), \n  beta = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#arguments",
    "title": "Extracts metrics from a fitted table",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA tbl_spark containing the estimate (prediction) and the truth (value of what actually happened)\n\n\ntruth\nThe name of the column from x with an integer field containing an the indexed value for each outcome . The ml_predict() function will create a new field named label which contains the expected type and values. truth defaults to label.\n\n\nestimate\nThe name of the column from x that contains the prediction. Defaults to prediction, since its type and indexed values will match truth.\n\n\nmetrics\nA character vector with the metrics to calculate. For multiclass models the possible values are: acurracy, f_meas (F-score), recall and precision. This function translates the argument into an acceptable Spark parameter. If no translation is found, then the raw value of the argument is passed to Spark. This makes it possible to request a metric that is not listed here but, depending on version, it is available in Spark. Other metrics form multi-class models are: weightedTruePositiveRate, weightedFalsePositiveRate, weightedFMeasure, truePositiveRateByLabel, falsePositiveRateByLabel, precisionByLabel, recallByLabel, fMeasureByLabel, logLoss, hammingLoss\n\n\nbeta\nNumerical value used for precision and recall. Defaults to NULL, but if the Spark session’s verion is 3.0 and above, then NULL is changed to 1, unless something different is supplied in this argument.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#details",
    "href": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#details",
    "title": "Extracts metrics from a fitted table",
    "section": "Details",
    "text": "Details\nThe ml_metrics family of functions implement Spark’s evaluate closer to how the yardstick package works. The functions expect a table containing the truth and estimate, and return a tibble with the results. The tibble has the same format and variable names as the output of the yardstick functions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_metrics_multiclass.html#examples",
    "title": "Extracts metrics from a fitted table",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(\"local\") \ntbl_iris &lt;- copy_to(sc, iris) \niris_split &lt;- sdf_random_split(tbl_iris, training = 0.5, test = 0.5) \nmodel &lt;- ml_random_forest(iris_split$training, \"Species ~ .\") \ntbl_predictions &lt;- ml_predict(model, iris_split$test) \n \nml_metrics_multiclass(tbl_predictions) \n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy multiclass     0.951\n \n# Request different metrics \nml_metrics_multiclass(tbl_predictions, metrics = c(\"recall\", \"precision\")) \n#&gt; # A tibble: 2 × 3\n#&gt;   .metric   .estimator .estimate\n#&gt;   &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 recall    multiclass     0.951\n#&gt; 2 precision multiclass     0.957\n \n# Request metrics not translated by the function, but valid in Spark \nml_metrics_multiclass(tbl_predictions, metrics = c(\"logLoss\", \"hammingLoss\")) \n#&gt; # A tibble: 2 × 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 logLoss     multiclass    0.133 \n#&gt; 2 hammingLoss multiclass    0.0494"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_text.html",
    "href": "packages/sparklyr/latest/reference/spark_write_text.html",
    "title": "Write a Spark DataFrame to a Text file",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_text.html#spark_write_text",
    "href": "packages/sparklyr/latest/reference/spark_write_text.html#spark_write_text",
    "title": "Write a Spark DataFrame to a Text file",
    "section": "spark_write_text",
    "text": "spark_write_text"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_text.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_text.html#description",
    "title": "Write a Spark DataFrame to a Text file",
    "section": "Description",
    "text": "Description\nSerialize a Spark DataFrame to the plain text format."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_text.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_text.html#usage",
    "title": "Write a Spark DataFrame to a Text file",
    "section": "Usage",
    "text": "Usage\nspark_write_text( \n  x, \n  path, \n  mode = NULL, \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_text.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_text.html#arguments",
    "title": "Write a Spark DataFrame to a Text file",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_text.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_text.html#see-also",
    "title": "Write a Spark DataFrame to a Text file",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html",
    "title": "Feature Transformation – VectorSlicer (Transformer)",
    "section": "",
    "text": "R/ml_feature_vector_slicer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html#ft_vector_slicer",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html#ft_vector_slicer",
    "title": "Feature Transformation – VectorSlicer (Transformer)",
    "section": "ft_vector_slicer",
    "text": "ft_vector_slicer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html#description",
    "title": "Feature Transformation – VectorSlicer (Transformer)",
    "section": "Description",
    "text": "Description\nTakes a feature vector and outputs a new feature vector with a subarray of the original features."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html#usage",
    "title": "Feature Transformation – VectorSlicer (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_vector_slicer( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  indices = NULL, \n  uid = random_string(\"vector_slicer_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html#arguments",
    "title": "Feature Transformation – VectorSlicer (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nindices\nAn vector of indices to select features from a vector column. Note that the indices are 0-based.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html#value",
    "title": "Feature Transformation – VectorSlicer (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html#see-also",
    "title": "Feature Transformation – VectorSlicer (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_kubernetes.html",
    "href": "packages/sparklyr/latest/reference/spark_config_kubernetes.html",
    "title": "Kubernetes Configuration",
    "section": "",
    "text": "R/kubernetes_config.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#spark_config_kubernetes",
    "href": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#spark_config_kubernetes",
    "title": "Kubernetes Configuration",
    "section": "spark_config_kubernetes",
    "text": "spark_config_kubernetes"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#description",
    "href": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#description",
    "title": "Kubernetes Configuration",
    "section": "Description",
    "text": "Description\nConvenience function to initialize a Kubernetes configuration instead of spark_config(), exposes common properties to set in Kubernetes clusters."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#usage",
    "title": "Kubernetes Configuration",
    "section": "Usage",
    "text": "Usage\nspark_config_kubernetes( \n  master, \n  version = \"2.3.2\", \n  image = \"spark:sparklyr\", \n  driver = random_string(\"sparklyr-\"), \n  account = \"spark\", \n  jars = \"local:///opt/sparklyr\", \n  forward = TRUE, \n  executors = NULL, \n  conf = NULL, \n  timeout = 120, \n  ports = c(8880, 8881, 4040), \n  fix_config = identical(.Platform$OS.type, \"windows\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#arguments",
    "title": "Kubernetes Configuration",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nmaster\nKubernetes url to connect to, found by running kubectl cluster-info.\n\n\nversion\nThe version of Spark being used.\n\n\nimage\nContainer image to use to launch Spark and sparklyr. Also known as spark.kubernetes.container.image.\n\n\ndriver\nName of the driver pod. If not set, the driver pod name is set to “sparklyr” suffixed by id to avoid name conflicts. Also known as spark.kubernetes.driver.pod.name.\n\n\naccount\nService account that is used when running the driver pod. The driver pod uses this service account when requesting executor pods from the API server. Also known as spark.kubernetes.authenticate.driver.serviceAccountName.\n\n\njars\nPath to the sparklyr jars; either, a local path inside the container image with the sparklyr jars copied when the image was created or, a path accesible by the container where the sparklyr jars were copied. You can find a path to the sparklyr jars by running system.file(\"java/\", package = \"sparklyr\").\n\n\nforward\nShould ports used in sparklyr be forwarded automatically through Kubernetes? Default to TRUE which runs kubectl port-forward and pkill kubectlon disconnection.\n\n\nexecutors\nNumber of executors to request while connecting.\n\n\nconf\nA named list of additional entries to add to sparklyr.shell.conf.\n\n\ntimeout\nTotal seconds to wait before giving up on connection.\n\n\nports\nPorts to forward using kubectl.\n\n\nfix_config\nShould the spark-defaults.conf get fixed? TRUE for Windows.\n\n\n…\nAdditional parameters, currently not in use."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_cache.html",
    "href": "packages/sparklyr/latest/reference/tbl_cache.html",
    "title": "Cache a Spark Table",
    "section": "",
    "text": "R/tables_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_cache.html#tbl_cache",
    "href": "packages/sparklyr/latest/reference/tbl_cache.html#tbl_cache",
    "title": "Cache a Spark Table",
    "section": "tbl_cache",
    "text": "tbl_cache"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_cache.html#description",
    "href": "packages/sparklyr/latest/reference/tbl_cache.html#description",
    "title": "Cache a Spark Table",
    "section": "Description",
    "text": "Description\nForce a Spark table with name name to be loaded into memory. Operations on cached tables should normally (although not always) be more performant than the same operation performed on an uncached table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_cache.html#usage",
    "href": "packages/sparklyr/latest/reference/tbl_cache.html#usage",
    "title": "Cache a Spark Table",
    "section": "Usage",
    "text": "Usage\ntbl_cache(sc, name, force = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_cache.html#arguments",
    "href": "packages/sparklyr/latest/reference/tbl_cache.html#arguments",
    "title": "Cache a Spark Table",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe table name.\n\n\nforce\nForce the data to be loaded into memory? This is accomplished by calling the count API on the associated Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rhyper.html",
    "href": "packages/sparklyr/latest/reference/sdf_rhyper.html",
    "title": "Generate random samples from a hypergeometric distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rhyper.html#sdf_rhyper",
    "href": "packages/sparklyr/latest/reference/sdf_rhyper.html#sdf_rhyper",
    "title": "Generate random samples from a hypergeometric distribution",
    "section": "sdf_rhyper",
    "text": "sdf_rhyper"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rhyper.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rhyper.html#description",
    "title": "Generate random samples from a hypergeometric distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a hypergeometric distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rhyper.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rhyper.html#usage",
    "title": "Generate random samples from a hypergeometric distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rhyper( \n  sc, \n  nn, \n  m, \n  n, \n  k, \n  num_partitions = NULL, \n  seed = NULL, \n  output_col = \"x\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rhyper.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rhyper.html#arguments",
    "title": "Generate random samples from a hypergeometric distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nnn\nSample Size.\n\n\nm\nThe number of successes among the population.\n\n\nn\nThe number of failures among the population.\n\n\nk\nThe number of draws.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rhyper.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rhyper.html#see-also",
    "title": "Generate random samples from a hypergeometric distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_get_java.html",
    "href": "packages/sparklyr/latest/reference/spark_get_java.html",
    "title": "Find path to Java",
    "section": "",
    "text": "R/java.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_get_java.html#spark_get_java",
    "href": "packages/sparklyr/latest/reference/spark_get_java.html#spark_get_java",
    "title": "Find path to Java",
    "section": "spark_get_java",
    "text": "spark_get_java"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_get_java.html#description",
    "href": "packages/sparklyr/latest/reference/spark_get_java.html#description",
    "title": "Find path to Java",
    "section": "Description",
    "text": "Description\nFinds the path to JAVA_HOME."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_get_java.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_get_java.html#usage",
    "title": "Find path to Java",
    "section": "Usage",
    "text": "Usage\nspark_get_java(throws = FALSE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_get_java.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_get_java.html#arguments",
    "title": "Find path to Java",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nthrows\nThrow an error when path not found?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_pivot.html",
    "href": "packages/sparklyr/latest/reference/sdf_pivot.html",
    "title": "Pivot a Spark DataFrame",
    "section": "",
    "text": "R/sdf_wrapper.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_pivot.html#sdf_pivot",
    "href": "packages/sparklyr/latest/reference/sdf_pivot.html#sdf_pivot",
    "title": "Pivot a Spark DataFrame",
    "section": "sdf_pivot",
    "text": "sdf_pivot"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_pivot.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_pivot.html#description",
    "title": "Pivot a Spark DataFrame",
    "section": "Description",
    "text": "Description\nConstruct a pivot table over a Spark Dataframe, using a syntax similar to that from reshape2::dcast."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_pivot.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_pivot.html#usage",
    "title": "Pivot a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\n \nsdf_pivot(x, formula, fun.aggregate = \"count\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_pivot.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_pivot.html#arguments",
    "title": "Pivot a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nA two-sided R formula of the form x_1 + x_2 + ... ~ y_1. The left-hand side of the formula indicates which variables are used for grouping, and the right-hand side indicates which variable is used for pivoting. Currently, only a single pivot column is supported.\n\n\nfun.aggregate\nHow should the grouped dataset be aggregated? Can be a length-one character vector, giving the name of a Spark aggregation function to be called; a named R list mapping column names to an aggregation method, or an R function that is invoked on the grouped dataset."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_pivot.html#examples",
    "href": "packages/sparklyr/latest/reference/sdf_pivot.html#examples",
    "title": "Pivot a Spark DataFrame",
    "section": "Examples",
    "text": "Examples\n\n \nlibrary(sparklyr) \nlibrary(dplyr) \n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \n# aggregating by mean \niris_tbl %&gt;% \n  mutate(Petal_Width = ifelse(Petal_Width &gt; 1.5, \"High\", \"Low\")) %&gt;% \n  sdf_pivot(Petal_Width ~ Species, \n    fun.aggregate = list(Petal_Length = \"mean\") \n  ) \n#&gt; # Source: spark&lt;?&gt; [?? x 4]\n#&gt;   Petal_Width setosa versicolor virginica\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Low           1.46       4.20      5.23\n#&gt; 2 High         NA          4.82      5.57\n \n# aggregating all observations in a list \niris_tbl %&gt;% \n  mutate(Petal_Width = ifelse(Petal_Width &gt; 1.5, \"High\", \"Low\")) %&gt;% \n  sdf_pivot(Petal_Width ~ Species, \n    fun.aggregate = list(Petal_Length = \"collect_list\") \n  ) \n#&gt; # Source: spark&lt;?&gt; [?? x 4]\n#&gt;   Petal_Width setosa     versicolor virginica \n#&gt;   &lt;chr&gt;       &lt;list&gt;     &lt;list&gt;     &lt;list&gt;    \n#&gt; 1 Low         &lt;dbl [50]&gt; &lt;dbl [45]&gt; &lt;dbl [3]&gt; \n#&gt; 2 High        &lt;dbl [0]&gt;  &lt;dbl [5]&gt;  &lt;dbl [47]&gt;"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html",
    "title": "Feature Transformation – QuantileDiscretizer (Estimator)",
    "section": "",
    "text": "R/ml_feature_quantile_discretizer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#ft_quantile_discretizer",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#ft_quantile_discretizer",
    "title": "Feature Transformation – QuantileDiscretizer (Estimator)",
    "section": "ft_quantile_discretizer",
    "text": "ft_quantile_discretizer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#description",
    "title": "Feature Transformation – QuantileDiscretizer (Estimator)",
    "section": "Description",
    "text": "Description\nft_quantile_discretizer takes a column with continuous features and outputs a column with binned categorical features. The number of bins can be set using the num_buckets parameter. It is possible that the number of buckets used will be smaller than this value, for example, if there are too few distinct values of the input to create enough distinct quantiles."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#usage",
    "title": "Feature Transformation – QuantileDiscretizer (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_quantile_discretizer( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  num_buckets = 2, \n  input_cols = NULL, \n  output_cols = NULL, \n  num_buckets_array = NULL, \n  handle_invalid = \"error\", \n  relative_error = 0.001, \n  uid = random_string(\"quantile_discretizer_\"), \n  weight_column = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#arguments",
    "title": "Feature Transformation – QuantileDiscretizer (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nnum_buckets\nNumber of buckets (quantiles, or categories) into which data points are grouped. Must be greater than or equal to 2.\n\n\ninput_cols\nNames of input columns.\n\n\noutput_cols\nNames of output columns.\n\n\nnum_buckets_array\nArray of number of buckets (quantiles, or categories) into which data points are grouped. Each value must be greater than or equal to 2.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error”\n\n\nrelative_error\n(Spark 2.0.0+) Relative error (see documentation for org.apache.spark.sql.DataFrameStatFunctions.approxQuantile herefor description). Must be in the range [0, 1]. default: 0.001\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\nweight_column\nIf not NULL, then a generalized version of the Greenwald-Khanna algorithm will be run to compute weighted percentiles, with each input having a relative weight specified by the corresponding value in weight_column. The weights can be considered as relative frequencies of sample inputs.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#details",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#details",
    "title": "Feature Transformation – QuantileDiscretizer (Estimator)",
    "section": "Details",
    "text": "Details\nNaN handling: null and NaN values will be ignored from the column during QuantileDiscretizer fitting. This will produce a Bucketizer\nmodel for making predictions. During the transformation, Bucketizer\nwill raise an error when it finds NaN values in the dataset, but the user can also choose to either keep or remove NaN values within the dataset by setting handle_invalid If the user chooses to keep NaN values, they will be handled specially and placed into their own bucket, for example, if 4 buckets are used, then non-NaN data will be put into buckets[0-3], but NaNs will be counted in a special bucket[4].\nAlgorithm: The bin ranges are chosen using an approximate algorithm (see the documentation for org.apache.spark.sql.DataFrameStatFunctions.approxQuantile here for a detailed description). The precision of the approximation can be controlled with the relative_error parameter. The lower and upper bin bounds will be -Infinity and +Infinity, covering all real values.\nNote that the result may be different every time you run it, since the sample strategy behind it is non-deterministic.\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#value",
    "title": "Feature Transformation – QuantileDiscretizer (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#see-also",
    "title": "Feature Transformation – QuantileDiscretizer (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nft_bucketizer\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/find_scalac.html",
    "href": "packages/sparklyr/latest/reference/find_scalac.html",
    "title": "Discover the Scala Compiler",
    "section": "",
    "text": "R/spark_compile.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/find_scalac.html#find_scalac",
    "href": "packages/sparklyr/latest/reference/find_scalac.html#find_scalac",
    "title": "Discover the Scala Compiler",
    "section": "find_scalac",
    "text": "find_scalac"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/find_scalac.html#description",
    "href": "packages/sparklyr/latest/reference/find_scalac.html#description",
    "title": "Discover the Scala Compiler",
    "section": "Description",
    "text": "Description\nFind the scalac compiler for a particular version of scala, by scanning some common directories containing scala installations."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/find_scalac.html#usage",
    "href": "packages/sparklyr/latest/reference/find_scalac.html#usage",
    "title": "Discover the Scala Compiler",
    "section": "Usage",
    "text": "Usage\nfind_scalac(version, locations = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/find_scalac.html#arguments",
    "href": "packages/sparklyr/latest/reference/find_scalac.html#arguments",
    "title": "Discover the Scala Compiler",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nversion\nThe scala version to search for. Versions of the form major.minor will be matched against the scalac installation with version major.minor.patch; if multiple compilers are discovered the most recent one will be used.\n\n\nlocations\nAdditional locations to scan. By default, the directories /opt/scala and /usr/local/scala will be scanned."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html",
    "title": "Feature Transformation – PolynomialExpansion (Transformer)",
    "section": "",
    "text": "R/ml_feature_polynomial_expansion.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#ft_polynomial_expansion",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#ft_polynomial_expansion",
    "title": "Feature Transformation – PolynomialExpansion (Transformer)",
    "section": "ft_polynomial_expansion",
    "text": "ft_polynomial_expansion"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#description",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#description",
    "title": "Feature Transformation – PolynomialExpansion (Transformer)",
    "section": "Description",
    "text": "Description\nPerform feature expansion in a polynomial space. E.g. take a 2-variable feature vector as an example: (x, y), if we want to expand it with degree 2, then we get (x, x * x, y, x * y, y * y)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#usage",
    "title": "Feature Transformation – PolynomialExpansion (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_polynomial_expansion( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  degree = 2, \n  uid = random_string(\"polynomial_expansion_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#arguments",
    "title": "Feature Transformation – PolynomialExpansion (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\ndegree\nThe polynomial degree to expand, which should be greater than equal to 1. A value of 1 means no expansion. Default: 2\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#value",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#value",
    "title": "Feature Transformation – PolynomialExpansion (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#see-also",
    "title": "Feature Transformation – PolynomialExpansion (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_aggregate.html",
    "href": "packages/sparklyr/latest/reference/hof_aggregate.html",
    "title": "Apply Aggregate Function to Array Column",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_aggregate.html#hof_aggregate",
    "href": "packages/sparklyr/latest/reference/hof_aggregate.html#hof_aggregate",
    "title": "Apply Aggregate Function to Array Column",
    "section": "hof_aggregate",
    "text": "hof_aggregate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_aggregate.html#description",
    "href": "packages/sparklyr/latest/reference/hof_aggregate.html#description",
    "title": "Apply Aggregate Function to Array Column",
    "section": "Description",
    "text": "Description\nApply an element-wise aggregation function to an array column (this is essentially a dplyr wrapper for the aggregate(array&lt;T&gt;, A, function&lt;A, T, A&gt;[, function&lt;A, R&gt;]): R built-in Spark SQL functions)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_aggregate.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_aggregate.html#usage",
    "title": "Apply Aggregate Function to Array Column",
    "section": "Usage",
    "text": "Usage\n \nhof_aggregate( \n  x, \n  start, \n  merge, \n  finish = NULL, \n  expr = NULL, \n  dest_col = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_aggregate.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_aggregate.html#arguments",
    "title": "Apply Aggregate Function to Array Column",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark data frame to run aggregation on\n\n\nstart\nThe starting value of the aggregation\n\n\nmerge\nThe aggregation function\n\n\nfinish\nOptional param specifying a transformation to apply on the final value of the aggregation\n\n\nexpr\nThe array being aggregated, could be any SQL expression evaluating to an array (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the aggregated result (default: expr)\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_aggregate.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_aggregate.html#examples",
    "title": "Apply Aggregate Function to Array Column",
    "section": "Examples",
    "text": "Examples\n\n \n \nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\") \n# concatenates all numbers of each array in `array_column` and add parentheses \n# around the resulting string \ncopy_to(sc, tibble::tibble(array_column = list(1:5, 21:25))) %&gt;% \n  hof_aggregate( \n    start = \"\", \n    merge = ~ CONCAT(.y, .x), \n    finish = ~ CONCAT(\"(\", .x, \")\") \n  ) \n#&gt; # Source: spark&lt;?&gt; [?? x 1]\n#&gt;   array_column\n#&gt;   &lt;chr&gt;       \n#&gt; 1 (54321)     \n#&gt; 2 (2524232221)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html",
    "title": "Feature Transformation – RegexTokenizer (Transformer)",
    "section": "",
    "text": "R/ml_feature_regex_tokenizer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#ft_regex_tokenizer",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#ft_regex_tokenizer",
    "title": "Feature Transformation – RegexTokenizer (Transformer)",
    "section": "ft_regex_tokenizer",
    "text": "ft_regex_tokenizer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#description",
    "title": "Feature Transformation – RegexTokenizer (Transformer)",
    "section": "Description",
    "text": "Description\nA regex based tokenizer that extracts tokens either by using the provided regex pattern to split the text (default) or repeatedly matching the regex (if gaps is false). Optional parameters also allow filtering tokens using a minimal length. It returns an array of strings that can be empty."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#usage",
    "title": "Feature Transformation – RegexTokenizer (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_regex_tokenizer( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  gaps = TRUE, \n  min_token_length = 1, \n  pattern = \"\\\\s+\", \n  to_lower_case = TRUE, \n  uid = random_string(\"regex_tokenizer_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#arguments",
    "title": "Feature Transformation – RegexTokenizer (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\ngaps\nIndicates whether regex splits on gaps (TRUE) or matches tokens (FALSE).\n\n\nmin_token_length\nMinimum token length, greater than or equal to 0.\n\n\npattern\nThe regular expression pattern to be used.\n\n\nto_lower_case\nIndicates whether to convert all characters to lowercase before tokenizing.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#value",
    "title": "Feature Transformation – RegexTokenizer (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#see-also",
    "title": "Feature Transformation – RegexTokenizer (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html",
    "title": "Read Text Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html#stream_read_text",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html#stream_read_text",
    "title": "Read Text Stream",
    "section": "stream_read_text",
    "text": "stream_read_text"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html#description",
    "title": "Read Text Stream",
    "section": "Description",
    "text": "Description\nReads a text stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html#usage",
    "title": "Read Text Stream",
    "section": "Usage",
    "text": "Usage\nstream_read_text(sc, path, name = NULL, options = list(), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html#arguments",
    "title": "Read Text Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html#examples",
    "title": "Read Text Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\") \ndir.create(\"text-in\") \nwriteLines(\"A text entry\", \"text-in/text.txt\") \ntext_path &lt;- file.path(\"file://\", getwd(), \"text-in\") \nstream &lt;- stream_read_text(sc, text_path) %&gt;% stream_write_text(\"text-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html#see-also",
    "title": "Read Text Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html",
    "title": "Feature Transformation – Tokenizer (Transformer)",
    "section": "",
    "text": "R/ml_feature_tokenizer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html#ft_tokenizer",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html#ft_tokenizer",
    "title": "Feature Transformation – Tokenizer (Transformer)",
    "section": "ft_tokenizer",
    "text": "ft_tokenizer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html#description",
    "title": "Feature Transformation – Tokenizer (Transformer)",
    "section": "Description",
    "text": "Description\nA tokenizer that converts the input string to lowercase and then splits it by white spaces."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html#usage",
    "title": "Feature Transformation – Tokenizer (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_tokenizer( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  uid = random_string(\"tokenizer_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html#arguments",
    "title": "Feature Transformation – Tokenizer (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html#value",
    "title": "Feature Transformation – Tokenizer (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html#see-also",
    "title": "Feature Transformation – Tokenizer (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jarray.html",
    "href": "packages/sparklyr/latest/reference/jarray.html",
    "title": "Instantiate a Java array with a specific element type.",
    "section": "",
    "text": "R/utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jarray.html#jarray",
    "href": "packages/sparklyr/latest/reference/jarray.html#jarray",
    "title": "Instantiate a Java array with a specific element type.",
    "section": "jarray",
    "text": "jarray"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jarray.html#description",
    "href": "packages/sparklyr/latest/reference/jarray.html#description",
    "title": "Instantiate a Java array with a specific element type.",
    "section": "Description",
    "text": "Description\nGiven a list of Java object references, instantiate an Array[T]\ncontaining the same list of references, where T is a non-primitive type that is more specific than java.lang.Object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jarray.html#usage",
    "href": "packages/sparklyr/latest/reference/jarray.html#usage",
    "title": "Instantiate a Java array with a specific element type.",
    "section": "Usage",
    "text": "Usage\njarray(sc, x, element_type)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jarray.html#arguments",
    "href": "packages/sparklyr/latest/reference/jarray.html#arguments",
    "title": "Instantiate a Java array with a specific element type.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nx\nA list of Java object references.\n\n\nelement_type\nA valid Java class name representing the generic type parameter of the Java array to be instantiated. Each element of xmust refer to a Java object that is assignable to element_type."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jarray.html#examples",
    "href": "packages/sparklyr/latest/reference/jarray.html#examples",
    "title": "Instantiate a Java array with a specific element type.",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"spark://HOST:PORT\") \nstring_arr &lt;- jarray(sc, letters, element_type = \"java.lang.String\") \n# string_arr is now a reference to an array of type String[]"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_values.html",
    "href": "packages/sparklyr/latest/reference/hof_transform_values.html",
    "title": "Transforms values of a map",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_values.html#hof_transform_values",
    "href": "packages/sparklyr/latest/reference/hof_transform_values.html#hof_transform_values",
    "title": "Transforms values of a map",
    "section": "hof_transform_values",
    "text": "hof_transform_values"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_values.html#description",
    "href": "packages/sparklyr/latest/reference/hof_transform_values.html#description",
    "title": "Transforms values of a map",
    "section": "Description",
    "text": "Description\nApplies the transformation function specified to all values of a map (this is essentially a dplyr wrapper to the transform_values(expr, func) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_values.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_transform_values.html#usage",
    "title": "Transforms values of a map",
    "section": "Usage",
    "text": "Usage\n \nhof_transform_values(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_values.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_transform_values.html#arguments",
    "title": "Transforms values of a map",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\nfunc\nThe transformation function to apply (it should take (key, value) as arguments and return a transformed value)\n\n\nexpr\nThe map being transformed, could be any SQL expression evaluating to a map (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the transformed result (default: expr)\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_values.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_transform_values.html#examples",
    "title": "Transforms values of a map",
    "section": "Examples",
    "text": "Examples\n\n \n \nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\", version = \"3.0.0\") \nsdf &lt;- sdf_len(sc, 1) %&gt;% dplyr::mutate(m = map(\"a\", 0L, \"b\", 2L, \"c\", -1L)) \ntransformed_sdf &lt;- sdf %&gt;% hof_transform_values(~ CONCAT(.x, \" == \", .y))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_save_table.html",
    "href": "packages/sparklyr/latest/reference/spark_save_table.html",
    "title": "Saves a Spark DataFrame as a Spark table",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_save_table.html#spark_save_table",
    "href": "packages/sparklyr/latest/reference/spark_save_table.html#spark_save_table",
    "title": "Saves a Spark DataFrame as a Spark table",
    "section": "spark_save_table",
    "text": "spark_save_table"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_save_table.html#description",
    "href": "packages/sparklyr/latest/reference/spark_save_table.html#description",
    "title": "Saves a Spark DataFrame as a Spark table",
    "section": "Description",
    "text": "Description\nSaves a Spark DataFrame and as a Spark table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_save_table.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_save_table.html#usage",
    "title": "Saves a Spark DataFrame as a Spark table",
    "section": "Usage",
    "text": "Usage\nspark_save_table(x, path, mode = NULL, options = list())"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_save_table.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_save_table.html#arguments",
    "title": "Saves a Spark DataFrame as a Spark table",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_save_table.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_save_table.html#see-also",
    "title": "Saves a Spark DataFrame as a Spark table",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html",
    "title": "Feature Transformation – IndexToString (Transformer)",
    "section": "",
    "text": "R/ml_feature_index_to_string.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html#ft_index_to_string",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html#ft_index_to_string",
    "title": "Feature Transformation – IndexToString (Transformer)",
    "section": "ft_index_to_string",
    "text": "ft_index_to_string"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html#description",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html#description",
    "title": "Feature Transformation – IndexToString (Transformer)",
    "section": "Description",
    "text": "Description\nA Transformer that maps a column of indices back to a new column of corresponding string values. The index-string mapping is either from the ML attributes of the input column, or from user-supplied labels (which take precedence over ML attributes). This function is the inverse of ft_string_indexer."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html#usage",
    "title": "Feature Transformation – IndexToString (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_index_to_string( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  labels = NULL, \n  uid = random_string(\"index_to_string_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html#arguments",
    "title": "Feature Transformation – IndexToString (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nlabels\nOptional param for array of labels specifying index-string mapping.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html#value",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html#value",
    "title": "Feature Transformation – IndexToString (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html#see-also",
    "title": "Feature Transformation – IndexToString (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nft_string_indexer\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/unite.html",
    "href": "packages/sparklyr/latest/reference/unite.html",
    "title": "Unite",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/unite.html#unite",
    "href": "packages/sparklyr/latest/reference/unite.html#unite",
    "title": "Unite",
    "section": "unite",
    "text": "unite"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/unite.html#description",
    "href": "packages/sparklyr/latest/reference/unite.html#description",
    "title": "Unite",
    "section": "Description",
    "text": "Description\nSee unite for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "",
    "text": "R/ml_feature_standard_scaler.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#ft_standard_scaler",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#ft_standard_scaler",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "ft_standard_scaler",
    "text": "ft_standard_scaler"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#description",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#description",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "Description",
    "text": "Description\nStandardizes features by removing the mean and scaling to unit variance using column summary statistics on the samples in the training set. The “unit std” is computed using the corrected sample standard deviation, which is computed as the square root of the unbiased sample variance."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#usage",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "Usage",
    "text": "Usage\n \nft_standard_scaler( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  with_mean = FALSE, \n  with_std = TRUE, \n  uid = random_string(\"standard_scaler_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#arguments",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nwith_mean\nWhether to center the data with mean before scaling. It will build a dense output, so take care when applying to sparse input. Default: FALSE\n\n\nwith_std\nWhether to scale the data to unit standard deviation. Default: TRUE\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#details",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#details",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#value",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#value",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#examples",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#examples",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \nfeatures &lt;- c(\"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\") \n \niris_tbl %&gt;% \n  ft_vector_assembler( \n    input_col = features, \n    output_col = \"features_temp\" \n  ) %&gt;% \n  ft_standard_scaler( \n    input_col = \"features_temp\", \n    output_col = \"features\", \n    with_mean = TRUE \n  ) \n#&gt; # Source: spark&lt;?&gt; [?? x 7]\n#&gt;    Sepal_L…¹ Sepal…² Petal…³ Petal…⁴ Species featu…⁵ featu…⁶\n#&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;list&gt;  &lt;list&gt; \n#&gt;  1       5.1     3.5     1.4     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  2       4.9     3       1.4     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  3       4.7     3.2     1.3     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  4       4.6     3.1     1.5     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  5       5       3.6     1.4     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  6       5.4     3.9     1.7     0.4 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  7       4.6     3.4     1.4     0.3 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  8       5       3.4     1.5     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  9       4.4     2.9     1.4     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt; 10       4.9     3.1     1.5     0.1 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt; # … with more rows, and abbreviated variable names\n#&gt; #   ¹​Sepal_Length, ²​Sepal_Width, ³​Petal_Length,\n#&gt; #   ⁴​Petal_Width, ⁵​features_temp, ⁶​features"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#see-also",
    "title": "Feature Transformation – StandardScaler (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark. Other feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/mutate.html",
    "href": "packages/sparklyr/latest/reference/mutate.html",
    "title": "Mutate",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/mutate.html#mutate",
    "href": "packages/sparklyr/latest/reference/mutate.html#mutate",
    "title": "Mutate",
    "section": "mutate",
    "text": "mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/mutate.html#description",
    "href": "packages/sparklyr/latest/reference/mutate.html#description",
    "title": "Mutate",
    "section": "Description",
    "text": "Description\nSee mutate for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rweibull.html",
    "href": "packages/sparklyr/latest/reference/sdf_rweibull.html",
    "title": "Generate random samples from a Weibull distribution.",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rweibull.html#sdf_rweibull",
    "href": "packages/sparklyr/latest/reference/sdf_rweibull.html#sdf_rweibull",
    "title": "Generate random samples from a Weibull distribution.",
    "section": "sdf_rweibull",
    "text": "sdf_rweibull"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rweibull.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rweibull.html#description",
    "title": "Generate random samples from a Weibull distribution.",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Weibull distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rweibull.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rweibull.html#usage",
    "title": "Generate random samples from a Weibull distribution.",
    "section": "Usage",
    "text": "Usage\nsdf_rweibull( \n  sc, \n  n, \n  shape, \n  scale = 1, \n  num_partitions = NULL, \n  seed = NULL, \n  output_col = \"x\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rweibull.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rweibull.html#arguments",
    "title": "Generate random samples from a Weibull distribution.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nshape\nThe shape of the Weibull distribution.\n\n\nscale\nThe scale of the Weibull distribution (default: 1).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rweibull.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rweibull.html#see-also",
    "title": "Generate random samples from a Weibull distribution.",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html",
    "href": "packages/sparklyr/latest/reference/spark-connections.html",
    "title": "Manage Spark Connections",
    "section": "",
    "text": "R/connection_spark.R, R/spark_submit.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html#spark-connections",
    "href": "packages/sparklyr/latest/reference/spark-connections.html#spark-connections",
    "title": "Manage Spark Connections",
    "section": "spark-connections",
    "text": "spark-connections"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html#description",
    "href": "packages/sparklyr/latest/reference/spark-connections.html#description",
    "title": "Manage Spark Connections",
    "section": "Description",
    "text": "Description\nThese routines allow you to manage your connections to Spark.\nCall spark_disconnect() on each open Spark connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html#usage",
    "href": "packages/sparklyr/latest/reference/spark-connections.html#usage",
    "title": "Manage Spark Connections",
    "section": "Usage",
    "text": "Usage\nspark_connect( \n  master, \n  spark_home = Sys.getenv(\"SPARK_HOME\"), \n  method = c(\"shell\", \"livy\", \"databricks\", \"test\", \"qubole\"), \n  app_name = \"sparklyr\", \n  version = NULL, \n  config = spark_config(), \n  extensions = sparklyr::registered_extensions(), \n  packages = NULL, \n  scala_version = NULL, \n  ... \n) \n\nspark_connection_is_open(sc) \n\nspark_disconnect(sc, ...) \n\nspark_disconnect_all(...) \n\nspark_submit( \n  master, \n  file, \n  spark_home = Sys.getenv(\"SPARK_HOME\"), \n  app_name = \"sparklyr\", \n  version = NULL, \n  config = spark_config(), \n  extensions = sparklyr::registered_extensions(), \n  scala_version = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark-connections.html#arguments",
    "title": "Manage Spark Connections",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nmaster\nSpark cluster url to connect to. Use \"local\" to connect to a local instance of Spark installed via spark_install.\n\n\nspark_home\nThe path to a Spark installation. Defaults to the path provided by the SPARK_HOME environment variable. If SPARK_HOME is defined, it will always be used unless the version parameter is specified to force the use of a locally installed version.\n\n\nmethod\nThe method used to connect to Spark. Default connection method is \"shell\" to connect using spark-submit, use \"livy\" to perform remote connections using HTTP, or \"databricks\" when using a Databricks clusters.\n\n\napp_name\nThe application name to be used while running in the Spark cluster.\n\n\nversion\nThe version of Spark to use. Required for \"local\" Spark connections, optional otherwise.\n\n\nconfig\nCustom configuration for the generated Spark connection. See spark_config for details.\n\n\nextensions\nExtension R packages to enable for this connection. By default, all packages enabled through the use of sparklyr::register_extension will be passed here.\n\n\npackages\nA list of Spark packages to load. For example, \"delta\" or \"kafka\" to enable Delta Lake or Kafka. Also supports full versions like \"io.delta:delta-core_2.11:0.4.0\". This is similar to adding packages into the sparklyr.shell.packages configuration option. Notice that the versionparameter is used to choose the correct package, otherwise assumes the latest version is being used.\n\n\nscala_version\nLoad the sparklyr jar file that is built with the version of Scala specified (this currently only makes sense for Spark 2.4, where sparklyr will by default assume Spark 2.4 on current host is built with Scala 2.11, and therefore scala_version = '2.12' is needed if sparklyr is connecting to Spark 2.4 built with Scala 2.12)\n\n\n…\nAdditional params to be passed to each spark_disconnect() call (e.g., terminate = TRUE)\n\n\nsc\nA spark_connection.\n\n\nfile\nPath to R source file to submit for batch execution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html#details",
    "href": "packages/sparklyr/latest/reference/spark-connections.html#details",
    "title": "Manage Spark Connections",
    "section": "Details",
    "text": "Details\nBy default, when using method = \"livy\", jars are downloaded from GitHub. But an alternative path (local to Livy server or on HDFS or HTTP(s)) to sparklyr\nJAR can also be specified through the sparklyr.livy.jar setting."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html#examples",
    "href": "packages/sparklyr/latest/reference/spark-connections.html#examples",
    "title": "Manage Spark Connections",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nconf &lt;- spark_config() \nconf$`sparklyr.shell.conf` &lt;- c( \n  \"spark.executor.extraJavaOptions=-Duser.timezone='UTC'\", \n  \"spark.driver.extraJavaOptions=-Duser.timezone='UTC'\", \n  \"spark.sql.session.timeZone='UTC'\" \n) \nsc &lt;- spark_connect( \n  master = \"spark://HOST:PORT\", config = conf \n) \nconnection_is_open(sc) \nspark_disconnect(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html",
    "title": "Spark ML – Transform, fit, and predict methods (ml_ interface)",
    "section": "",
    "text": "R/ml_transformation_methods.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html#ml-transform-methods",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html#ml-transform-methods",
    "title": "Spark ML – Transform, fit, and predict methods (ml_ interface)",
    "section": "ml-transform-methods",
    "text": "ml-transform-methods"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html#description",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html#description",
    "title": "Spark ML – Transform, fit, and predict methods (ml_ interface)",
    "section": "Description",
    "text": "Description\nMethods for transformation, fit, and prediction. These are mirrors of the corresponding sdf-transform-methods."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html#usage",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html#usage",
    "title": "Spark ML – Transform, fit, and predict methods (ml_ interface)",
    "section": "Usage",
    "text": "Usage\nis_ml_transformer(x) \n\nis_ml_estimator(x) \n\nml_fit(x, dataset, ...) \n\nml_transform(x, dataset, ...) \n\nml_fit_and_transform(x, dataset, ...) \n\nml_predict(x, dataset, ...) \n\n## S3 method for class 'ml_model_classification'\nml_predict(x, dataset, probability_prefix = \"probability_\", ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html#arguments",
    "title": "Spark ML – Transform, fit, and predict methods (ml_ interface)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA ml_estimator, ml_transformer (or a list thereof), or ml_model object.\n\n\ndataset\nA tbl_spark.\n\n\n…\nOptional arguments; currently unused.\n\n\nprobability_prefix\nString used to prepend the class probability output columns."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html#details",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html#details",
    "title": "Spark ML – Transform, fit, and predict methods (ml_ interface)",
    "section": "Details",
    "text": "Details\nThese methods are"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html#value",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html#value",
    "title": "Spark ML – Transform, fit, and predict methods (ml_ interface)",
    "section": "Value",
    "text": "Value\nWhen x is an estimator, ml_fit() returns a transformer whereas ml_fit_and_transform() returns a transformed dataset. When x is a transformer, ml_transform() and ml_predict() return a transformed dataset. When ml_predict() is called on a ml_model object, additional columns (e.g. probabilities in case of classification models) are appended to the transformed output for the user’s convenience."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgamma.html",
    "href": "packages/sparklyr/latest/reference/sdf_rgamma.html",
    "title": "Generate random samples from a Gamma distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgamma.html#sdf_rgamma",
    "href": "packages/sparklyr/latest/reference/sdf_rgamma.html#sdf_rgamma",
    "title": "Generate random samples from a Gamma distribution",
    "section": "sdf_rgamma",
    "text": "sdf_rgamma"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgamma.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rgamma.html#description",
    "title": "Generate random samples from a Gamma distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Gamma distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgamma.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rgamma.html#usage",
    "title": "Generate random samples from a Gamma distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rgamma( \n  sc, \n  n, \n  shape, \n  rate = 1, \n  num_partitions = NULL, \n  seed = NULL, \n  output_col = \"x\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgamma.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rgamma.html#arguments",
    "title": "Generate random samples from a Gamma distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nshape\nShape parameter (greater than 0) for the Gamma distribution.\n\n\nrate\nRate parameter (greater than 0) for the Gamma distribution (scale is 1/rate).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgamma.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rgamma.html#see-also",
    "title": "Generate random samples from a Gamma distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_find.html",
    "href": "packages/sparklyr/latest/reference/spark_install_find.html",
    "title": "Find a given Spark installation by version.",
    "section": "",
    "text": "R/install_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_find.html#spark_install_find",
    "href": "packages/sparklyr/latest/reference/spark_install_find.html#spark_install_find",
    "title": "Find a given Spark installation by version.",
    "section": "spark_install_find",
    "text": "spark_install_find"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_find.html#description",
    "href": "packages/sparklyr/latest/reference/spark_install_find.html#description",
    "title": "Find a given Spark installation by version.",
    "section": "Description",
    "text": "Description\nFind a given Spark installation by version."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_find.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_install_find.html#usage",
    "title": "Find a given Spark installation by version.",
    "section": "Usage",
    "text": "Usage\nspark_install_find( \n  version = NULL, \n  hadoop_version = NULL, \n  installed_only = TRUE, \n  latest = FALSE, \n  hint = FALSE \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_find.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_install_find.html#arguments",
    "title": "Find a given Spark installation by version.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nversion\nVersion of Spark to install. See spark_available_versions for a list of supported versions\n\n\nhadoop_version\nVersion of Hadoop to install. See spark_available_versions for a list of supported versions\n\n\ninstalled_only\nSearch only the locally installed versions?\n\n\nlatest\nCheck for latest version?\n\n\nhint\nOn failure should the installation code be provided?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/src_databases.html",
    "href": "packages/sparklyr/latest/reference/src_databases.html",
    "title": "Show database list",
    "section": "",
    "text": "R/tables_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/src_databases.html#src_databases",
    "href": "packages/sparklyr/latest/reference/src_databases.html#src_databases",
    "title": "Show database list",
    "section": "src_databases",
    "text": "src_databases"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/src_databases.html#description",
    "href": "packages/sparklyr/latest/reference/src_databases.html#description",
    "title": "Show database list",
    "section": "Description",
    "text": "Description\nShow database list"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/src_databases.html#usage",
    "href": "packages/sparklyr/latest/reference/src_databases.html#usage",
    "title": "Show database list",
    "section": "Usage",
    "text": "Usage\nsrc_databases(sc, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/src_databases.html#arguments",
    "href": "packages/sparklyr/latest/reference/src_databases.html#arguments",
    "title": "Show database list",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_dim.html",
    "href": "packages/sparklyr/latest/reference/sdf_dim.html",
    "title": "Support for Dimension Operations",
    "section": "",
    "text": "R/sdf_dim.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_dim.html#sdf_dim",
    "href": "packages/sparklyr/latest/reference/sdf_dim.html#sdf_dim",
    "title": "Support for Dimension Operations",
    "section": "sdf_dim",
    "text": "sdf_dim"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_dim.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_dim.html#description",
    "title": "Support for Dimension Operations",
    "section": "Description",
    "text": "Description\nsdf_dim(), sdf_nrow() and sdf_ncol() provide similar functionality to dim(), nrow() and ncol()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_dim.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_dim.html#usage",
    "title": "Support for Dimension Operations",
    "section": "Usage",
    "text": "Usage\nsdf_dim(x) \n\nsdf_nrow(x) \n\nsdf_ncol(x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_dim.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_dim.html#arguments",
    "title": "Support for Dimension Operations",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object (usually a spark_tbl)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html",
    "href": "packages/sparklyr/latest/reference/sdf_register.html",
    "title": "Register a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#sdf_register",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#sdf_register",
    "title": "Register a Spark DataFrame",
    "section": "sdf_register",
    "text": "sdf_register"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#description",
    "title": "Register a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRegisters a Spark DataFrame (giving it a table name for the Spark SQL context), and returns a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#usage",
    "title": "Register a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_register(x, name = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#arguments",
    "title": "Register a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame.\n\n\nname\nA name to assign this table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#section",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#section",
    "title": "Register a Spark DataFrame",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#transforming-spark-dataframes",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#transforming-spark-dataframes",
    "title": "Register a Spark DataFrame",
    "section": "Transforming Spark DataFrames",
    "text": "Transforming Spark DataFrames\nThe family of functions prefixed with sdf_ generally access the Scala Spark DataFrame API directly, as opposed to the dplyr interface which uses Spark SQL. These functions will ‘force’ any pending SQL in a dplyr pipeline, such that the resulting tbl_spark object returned will no longer have the attached ‘lazy’ SQL operations. Note that the underlying Spark DataFrame does execute its operations lazily, so that even though the pending set of operations (currently) are not exposed at the R level, these operations will only be executed when you explicitly collect() the table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#see-also",
    "title": "Register a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_copy_to(), sdf_distinct(), sdf_random_split(), sdf_sample(), sdf_sort(), sdf_weighted_sample()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ensure.html",
    "href": "packages/sparklyr/latest/reference/ensure.html",
    "title": "Enforce Specific Structure for R Objects",
    "section": "",
    "text": "R/precondition.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ensure.html#ensure",
    "href": "packages/sparklyr/latest/reference/ensure.html#ensure",
    "title": "Enforce Specific Structure for R Objects",
    "section": "ensure",
    "text": "ensure"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ensure.html#description",
    "href": "packages/sparklyr/latest/reference/ensure.html#description",
    "title": "Enforce Specific Structure for R Objects",
    "section": "Description",
    "text": "Description\nThese routines are useful when preparing to pass objects to a Spark routine, as it is often necessary to ensure certain parameters are scalar integers, or scalar doubles, and so on."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ensure.html#arguments",
    "href": "packages/sparklyr/latest/reference/ensure.html#arguments",
    "title": "Enforce Specific Structure for R Objects",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nobject\nAn R object.\n\n\nallow.na\nAre NA values permitted for this object?\n\n\nallow.null\nAre NULL values permitted for this object?\n\n\ndefault\nIf object is NULL, what value should be used in its place? If default is specified, allow.nullis ignored (and assumed to be TRUE)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_lda_tidiers.html",
    "title": "Tidying methods for Spark ML LDA models",
    "section": "",
    "text": "R/tidiers_ml_lda.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#ml_lda_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#ml_lda_tidiers",
    "title": "Tidying methods for Spark ML LDA models",
    "section": "ml_lda_tidiers",
    "text": "ml_lda_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#description",
    "title": "Tidying methods for Spark ML LDA models",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#usage",
    "title": "Tidying methods for Spark ML LDA models",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_lda'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_lda'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_lda'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML LDA models",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html",
    "title": "Tidying methods for Spark ML Survival Regression",
    "section": "",
    "text": "R/tidiers_ml_aft_survival_regression.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#ml_survival_regression_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#ml_survival_regression_tidiers",
    "title": "Tidying methods for Spark ML Survival Regression",
    "section": "ml_survival_regression_tidiers",
    "text": "ml_survival_regression_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#description",
    "title": "Tidying methods for Spark ML Survival Regression",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#usage",
    "title": "Tidying methods for Spark ML Survival Regression",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_aft_survival_regression'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_aft_survival_regression'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_aft_survival_regression'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML Survival Regression",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/quote_sql_name.html",
    "href": "packages/sparklyr/latest/reference/quote_sql_name.html",
    "title": "Translate input character vector or symbol to a SQL identifier",
    "section": "",
    "text": "R/sql_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/quote_sql_name.html#quote_sql_name",
    "href": "packages/sparklyr/latest/reference/quote_sql_name.html#quote_sql_name",
    "title": "Translate input character vector or symbol to a SQL identifier",
    "section": "quote_sql_name",
    "text": "quote_sql_name"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/quote_sql_name.html#description",
    "href": "packages/sparklyr/latest/reference/quote_sql_name.html#description",
    "title": "Translate input character vector or symbol to a SQL identifier",
    "section": "Description",
    "text": "Description\nCalls dbplyr::translate_sql_ on the input character vector or symbol to obtain the corresponding SQL identifier that is escaped and quoted properly"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/quote_sql_name.html#usage",
    "href": "packages/sparklyr/latest/reference/quote_sql_name.html#usage",
    "title": "Translate input character vector or symbol to a SQL identifier",
    "section": "Usage",
    "text": "Usage\nquote_sql_name(x, con = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html",
    "title": "Tidying methods for Spark ML unsupervised models",
    "section": "",
    "text": "R/tidiers_ml_unsupervised_models.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#ml_unsupervised_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#ml_unsupervised_tidiers",
    "title": "Tidying methods for Spark ML unsupervised models",
    "section": "ml_unsupervised_tidiers",
    "text": "ml_unsupervised_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#description",
    "title": "Tidying methods for Spark ML unsupervised models",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#usage",
    "title": "Tidying methods for Spark ML unsupervised models",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_kmeans'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_kmeans'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_kmeans'\nglance(x, ...) \n\n## S3 method for class 'ml_model_bisecting_kmeans'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_bisecting_kmeans'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_bisecting_kmeans'\nglance(x, ...) \n\n## S3 method for class 'ml_model_gaussian_mixture'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_gaussian_mixture'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_gaussian_mixture'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML unsupervised models",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/distinct.html",
    "href": "packages/sparklyr/latest/reference/distinct.html",
    "title": "Distinct",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/distinct.html#distinct",
    "href": "packages/sparklyr/latest/reference/distinct.html#distinct",
    "title": "Distinct",
    "section": "distinct",
    "text": "distinct"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/distinct.html#description",
    "href": "packages/sparklyr/latest/reference/distinct.html#description",
    "title": "Distinct",
    "section": "Description",
    "text": "Description\nSee distinct for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_exists.html",
    "href": "packages/sparklyr/latest/reference/hof_exists.html",
    "title": "Determine Whether Some Element Exists in an Array Column",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_exists.html#hof_exists",
    "href": "packages/sparklyr/latest/reference/hof_exists.html#hof_exists",
    "title": "Determine Whether Some Element Exists in an Array Column",
    "section": "hof_exists",
    "text": "hof_exists"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_exists.html#description",
    "href": "packages/sparklyr/latest/reference/hof_exists.html#description",
    "title": "Determine Whether Some Element Exists in an Array Column",
    "section": "Description",
    "text": "Description\nDetermines whether an element satisfying the given predicate exists in each array from an array column (this is essentially a dplyr wrapper for the exists(array&lt;T&gt;, function&lt;T, Boolean&gt;): Boolean built-in Spark SQL function)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_exists.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_exists.html#usage",
    "title": "Determine Whether Some Element Exists in an Array Column",
    "section": "Usage",
    "text": "Usage\nhof_exists(x, pred, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_exists.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_exists.html#arguments",
    "title": "Determine Whether Some Element Exists in an Array Column",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark data frame to search\n\n\npred\nA boolean predicate\n\n\nexpr\nThe array being searched (could be any SQL expression evaluating to an array)\n\n\ndest_col\nColumn to store the search result\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html",
    "title": "Read the Schema of a Spark DataFrame",
    "section": "",
    "text": "R/sdf_wrapper.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html#sdf_schema",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html#sdf_schema",
    "title": "Read the Schema of a Spark DataFrame",
    "section": "sdf_schema",
    "text": "sdf_schema"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html#description",
    "title": "Read the Schema of a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRead the schema of a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html#usage",
    "title": "Read the Schema of a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_schema(x, expand_nested_cols = FALSE, expand_struct_cols = FALSE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html#arguments",
    "title": "Read the Schema of a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nexpand_nested_cols\nWhether to expand columns containing nested array of structs (which are usually created by tidyr::nest on a Spark data frame)\n\n\nexpand_struct_cols\nWhether to expand columns containing structs"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html#details",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html#details",
    "title": "Read the Schema of a Spark DataFrame",
    "section": "Details",
    "text": "Details\nThe type column returned gives the string representation of the underlying Spark type for that column; for example, a vector of numeric values would be returned with the type \"DoubleType\". Please see the Spark Scala API Documentation\nfor information on what types are available and exposed by Spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html#value",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html#value",
    "title": "Read the Schema of a Spark DataFrame",
    "section": "Value",
    "text": "Value\nAn R list, with each list element describing the name and type of a column."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_binary.html",
    "href": "packages/sparklyr/latest/reference/spark_read_binary.html",
    "title": "Read binary data into a Spark DataFrame.",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_binary.html#spark_read_binary",
    "href": "packages/sparklyr/latest/reference/spark_read_binary.html#spark_read_binary",
    "title": "Read binary data into a Spark DataFrame.",
    "section": "spark_read_binary",
    "text": "spark_read_binary"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_binary.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_binary.html#description",
    "title": "Read binary data into a Spark DataFrame.",
    "section": "Description",
    "text": "Description\nRead binary files within a directory and convert each file into a record within the resulting Spark dataframe. The output will be a Spark dataframe with the following columns and possibly partition columns:\n-path: StringType\n-modificationTime: TimestampType\n-length: LongType\n-content: BinaryType"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_binary.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_binary.html#usage",
    "title": "Read binary data into a Spark DataFrame.",
    "section": "Usage",
    "text": "Usage\nspark_read_binary( \n  sc, \n  name = NULL, \n  dir = name, \n  path_glob_filter = \"*\", \n  recursive_file_lookup = FALSE, \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_binary.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_binary.html#arguments",
    "title": "Read binary data into a Spark DataFrame.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\ndir\nDirectory to read binary files from.\n\n\npath_glob_filter\nGlob pattern of binary files to be loaded (e.g., “*.jpg”).\n\n\nrecursive_file_lookup\nIf FALSE (default), then partition discovery will be enabled (i.e., if a partition naming scheme is present, then partitions specified by subdirectory names such as “date=2019-07-01” will be created and files outside subdirectories following a partition naming scheme will be ignored). If TRUE, then all nested directories will be searched even if their names do not follow a partition naming scheme.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_binary.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_binary.html#see-also",
    "title": "Read binary data into a Spark DataFrame.",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install.html",
    "href": "packages/sparklyr/latest/reference/spark_install.html",
    "title": "Download and install various versions of Spark",
    "section": "",
    "text": "R/install_spark.R, R/install_spark_versions.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install.html#spark_install",
    "href": "packages/sparklyr/latest/reference/spark_install.html#spark_install",
    "title": "Download and install various versions of Spark",
    "section": "spark_install",
    "text": "spark_install"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install.html#description",
    "href": "packages/sparklyr/latest/reference/spark_install.html#description",
    "title": "Download and install various versions of Spark",
    "section": "Description",
    "text": "Description\nInstall versions of Spark for use with local Spark connections (i.e. spark_connect(master = \"local\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_install.html#usage",
    "title": "Download and install various versions of Spark",
    "section": "Usage",
    "text": "Usage\nspark_install( \n  version = NULL, \n  hadoop_version = NULL, \n  reset = TRUE, \n  logging = \"INFO\", \n  verbose = interactive() \n) \n\nspark_uninstall(version, hadoop_version) \n\nspark_install_dir() \n\nspark_install_tar(tarfile) \n\nspark_installed_versions() \n\nspark_available_versions( \n  show_hadoop = FALSE, \n  show_minor = FALSE, \n  show_future = FALSE \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_install.html#arguments",
    "title": "Download and install various versions of Spark",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nversion\nVersion of Spark to install. See spark_available_versions for a list of supported versions\n\n\nhadoop_version\nVersion of Hadoop to install. See spark_available_versions for a list of supported versions\n\n\nreset\nAttempts to reset settings to defaults.\n\n\nlogging\nLogging level to configure install. Supported options: “WARN”, “INFO”\n\n\nverbose\nReport information as Spark is downloaded / installed\n\n\ntarfile\nPath to TAR file conforming to the pattern spark-###-bin-(hadoop)?### where ### reference spark and hadoop versions respectively.\n\n\nshow_hadoop\nShow Hadoop distributions?\n\n\nshow_minor\nShow minor Spark versions?\n\n\nshow_future\nShould future versions which have not been released be shown?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install.html#value",
    "href": "packages/sparklyr/latest/reference/spark_install.html#value",
    "title": "Download and install various versions of Spark",
    "section": "Value",
    "text": "Value\nList with information about the installed version."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html",
    "title": "Feature Transformation – StopWordsRemover (Transformer)",
    "section": "",
    "text": "R/ml_feature_stop_words_remover.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#ft_stop_words_remover",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#ft_stop_words_remover",
    "title": "Feature Transformation – StopWordsRemover (Transformer)",
    "section": "ft_stop_words_remover",
    "text": "ft_stop_words_remover"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#description",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#description",
    "title": "Feature Transformation – StopWordsRemover (Transformer)",
    "section": "Description",
    "text": "Description\nA feature transformer that filters out stop words from input."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#usage",
    "title": "Feature Transformation – StopWordsRemover (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_stop_words_remover( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  case_sensitive = FALSE, \n  stop_words = ml_default_stop_words(spark_connection(x), \"english\"), \n  uid = random_string(\"stop_words_remover_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#arguments",
    "title": "Feature Transformation – StopWordsRemover (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\ncase_sensitive\nWhether to do a case sensitive comparison over the stop words.\n\n\nstop_words\nThe words to be filtered out.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#value",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#value",
    "title": "Feature Transformation – StopWordsRemover (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#see-also",
    "title": "Feature Transformation – StopWordsRemover (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nml_default_stop_words\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_forall.html",
    "href": "packages/sparklyr/latest/reference/hof_forall.html",
    "title": "Checks whether all elements in an array satisfy a predicate",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_forall.html#hof_forall",
    "href": "packages/sparklyr/latest/reference/hof_forall.html#hof_forall",
    "title": "Checks whether all elements in an array satisfy a predicate",
    "section": "hof_forall",
    "text": "hof_forall"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_forall.html#description",
    "href": "packages/sparklyr/latest/reference/hof_forall.html#description",
    "title": "Checks whether all elements in an array satisfy a predicate",
    "section": "Description",
    "text": "Description\nChecks whether the predicate specified holds for all elements in an array (this is essentially a dplyr wrapper to the forall(expr, pred) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_forall.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_forall.html#usage",
    "title": "Checks whether all elements in an array satisfy a predicate",
    "section": "Usage",
    "text": "Usage\n \nhof_forall(x, pred, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_forall.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_forall.html#arguments",
    "title": "Checks whether all elements in an array satisfy a predicate",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\npred\nThe predicate to test (it should take an array element as argument and return a boolean value)\n\n\nexpr\nThe array being tested, could be any SQL expression evaluating to an array (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the boolean result (default: expr)\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_forall.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_forall.html#examples",
    "title": "Checks whether all elements in an array satisfy a predicate",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \n \nsc &lt;- spark_connect(master = \"local\", version = \"3.0.0\") \ndf &lt;- tibble::tibble( \n  x = list(c(1, 2, 3, 4, 5), c(6, 7, 8, 9, 10)), \n  y = list(c(1, 4, 2, 8, 5), c(7, 1, 4, 2, 8)), \n) \nsdf &lt;- sdf_copy_to(sc, df, overwrite = TRUE) \n \nall_positive_tbl &lt;- sdf %&gt;% \n  hof_forall(pred = ~ .x &gt; 0, expr = y, dest_col = all_positive) %&gt;% \n  dplyr::select(all_positive)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html",
    "title": "Feature Transformation – Binarizer (Transformer)",
    "section": "",
    "text": "R/ml_feature_binarizer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#ft_binarizer",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#ft_binarizer",
    "title": "Feature Transformation – Binarizer (Transformer)",
    "section": "ft_binarizer",
    "text": "ft_binarizer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#description",
    "title": "Feature Transformation – Binarizer (Transformer)",
    "section": "Description",
    "text": "Description\nApply thresholding to a column, such that values less than or equal to the threshold are assigned the value 0.0, and values greater than the threshold are assigned the value 1.0. Column output is numeric for compatibility with other modeling functions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#usage",
    "title": "Feature Transformation – Binarizer (Transformer)",
    "section": "Usage",
    "text": "Usage\n \nft_binarizer( \n  x, \n  input_col, \n  output_col, \n  threshold = 0, \n  uid = random_string(\"binarizer_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#arguments",
    "title": "Feature Transformation – Binarizer (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nthreshold\nThreshold used to binarize continuous features.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#value",
    "title": "Feature Transformation – Binarizer (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#examples",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#examples",
    "title": "Feature Transformation – Binarizer (Transformer)",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nlibrary(dplyr) \n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \niris_tbl %&gt;% \n  ft_binarizer( \n    input_col = \"Sepal_Length\", \n    output_col = \"Sepal_Length_bin\", \n    threshold = 5 \n  ) %&gt;% \n  select(Sepal_Length, Sepal_Length_bin, Species) \n#&gt; # Source: spark&lt;?&gt; [?? x 3]\n#&gt;    Sepal_Length Sepal_Length_bin Species\n#&gt;           &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;  \n#&gt;  1          5.1                1 setosa \n#&gt;  2          4.9                0 setosa \n#&gt;  3          4.7                0 setosa \n#&gt;  4          4.6                0 setosa \n#&gt;  5          5                  0 setosa \n#&gt;  6          5.4                1 setosa \n#&gt;  7          4.6                0 setosa \n#&gt;  8          5                  0 setosa \n#&gt;  9          4.4                0 setosa \n#&gt; 10          4.9                0 setosa \n#&gt; # … with more rows"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#see-also",
    "title": "Feature Transformation – Binarizer (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark. Other feature transformers: ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/join.tbl_spark.html",
    "href": "packages/sparklyr/latest/reference/join.tbl_spark.html",
    "title": "Join Spark tbls.",
    "section": "",
    "text": "R/dplyr_join.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/join.tbl_spark.html#join.tbl_spark",
    "href": "packages/sparklyr/latest/reference/join.tbl_spark.html#join.tbl_spark",
    "title": "Join Spark tbls.",
    "section": "join.tbl_spark",
    "text": "join.tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/join.tbl_spark.html#description",
    "href": "packages/sparklyr/latest/reference/join.tbl_spark.html#description",
    "title": "Join Spark tbls.",
    "section": "Description",
    "text": "Description\nThese functions are wrappers around their dplyr equivalents that set Spark SQL-compliant values for the suffix argument by replacing dots (.) with underscores (_). See [join] for a description of the general purpose of the functions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/join.tbl_spark.html#usage",
    "href": "packages/sparklyr/latest/reference/join.tbl_spark.html#usage",
    "title": "Join Spark tbls.",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'tbl_spark'\ninner_join( \n  x, \n  y, \n  by = NULL, \n  copy = FALSE, \n  suffix = c(\"_x\", \"_y\"), \n  auto_index = FALSE, \n  ..., \n  sql_on = NULL \n) \n\n## S3 method for class 'tbl_spark'\nleft_join( \n  x, \n  y, \n  by = NULL, \n  copy = FALSE, \n  suffix = c(\"_x\", \"_y\"), \n  auto_index = FALSE, \n  ..., \n  sql_on = NULL \n) \n\n## S3 method for class 'tbl_spark'\nright_join( \n  x, \n  y, \n  by = NULL, \n  copy = FALSE, \n  suffix = c(\"_x\", \"_y\"), \n  auto_index = FALSE, \n  ..., \n  sql_on = NULL \n) \n\n## S3 method for class 'tbl_spark'\nfull_join( \n  x, \n  y, \n  by = NULL, \n  copy = FALSE, \n  suffix = c(\"_x\", \"_y\"), \n  auto_index = FALSE, \n  ..., \n  sql_on = NULL \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/join.tbl_spark.html#arguments",
    "href": "packages/sparklyr/latest/reference/join.tbl_spark.html#arguments",
    "title": "Join Spark tbls.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx, y\nA pair of lazy data frames backed by database queries.\n\n\nby\nA character vector of variables to join by. If NULL, the default, *_join() will perform a natural join, using all variables in common across x and y. A message lists the variables so that you can check they’re correct; suppress the message by supplying by explicitly. To join by different variables on x and y, use a named vector. For example, by = c(\"a\" = \"b\") will match x$a to y$b. To join by multiple variables, use a vector with length &gt; 1. For example, by = c(\"a\", \"b\") will match x$a to y$a and x$b to y$b. Use a named vector to match different variables in x and y. For example, by = c(\"a\" = \"b\", \"c\" = \"d\") will match x$a to y$b and x$c to y$d. To perform a cross-join, generating all combinations of x and y, use by = character().\n\n\ncopy\nIf x and y are not from the same data source, and copy is TRUE, then y will be copied into a temporary table in same database as x. *_join() will automatically run ANALYZE on the created table in the hope that this will make you queries as efficient as possible by giving more data to the query planner. This allows you to join tables across srcs, but it’s potentially expensive operation so you must opt into it.\n\n\nsuffix\nIf there are non-joined duplicate variables in x and y, these suffixes will be added to the output to disambiguate them. Should be a character vector of length 2.\n\n\nauto_index\nif copy is TRUE, automatically create indices for the variables in by. This may speed up the join if there are matching indexes in x.\n\n\n…\nOther parameters passed onto methods.\n\n\nsql_on\nA custom join predicate as an SQL expression. Usually joins use column equality, but you can perform more complex queries by supply sql_on which should be a SQL expression that uses LHS and RHS aliases to refer to the left-hand side or right-hand side of the join respectively."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_insert_table.html",
    "href": "packages/sparklyr/latest/reference/spark_insert_table.html",
    "title": "Inserts a Spark DataFrame into a Spark table",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_insert_table.html#spark_insert_table",
    "href": "packages/sparklyr/latest/reference/spark_insert_table.html#spark_insert_table",
    "title": "Inserts a Spark DataFrame into a Spark table",
    "section": "spark_insert_table",
    "text": "spark_insert_table"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_insert_table.html#description",
    "href": "packages/sparklyr/latest/reference/spark_insert_table.html#description",
    "title": "Inserts a Spark DataFrame into a Spark table",
    "section": "Description",
    "text": "Description\nInserts a Spark DataFrame into a Spark table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_insert_table.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_insert_table.html#usage",
    "title": "Inserts a Spark DataFrame into a Spark table",
    "section": "Usage",
    "text": "Usage\nspark_insert_table( \n  x, \n  name, \n  mode = NULL, \n  overwrite = FALSE, \n  options = list(), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_insert_table.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_insert_table.html#arguments",
    "title": "Inserts a Spark DataFrame into a Spark table",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nname\nThe name to assign to the newly generated table.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_insert_table.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_insert_table.html#see-also",
    "title": "Inserts a Spark DataFrame into a Spark table",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html",
    "href": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html",
    "title": "Retrieve the Spark connection’s SQL catalog implementation property",
    "section": "",
    "text": "R/spark_connection.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#get_spark_sql_catalog_implementation",
    "href": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#get_spark_sql_catalog_implementation",
    "title": "Retrieve the Spark connection’s SQL catalog implementation property",
    "section": "get_spark_sql_catalog_implementation",
    "text": "get_spark_sql_catalog_implementation"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#description",
    "href": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#description",
    "title": "Retrieve the Spark connection’s SQL catalog implementation property",
    "section": "Description",
    "text": "Description\nRetrieve the Spark connection’s SQL catalog implementation property"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#usage",
    "href": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#usage",
    "title": "Retrieve the Spark connection’s SQL catalog implementation property",
    "section": "Usage",
    "text": "Usage\nget_spark_sql_catalog_implementation(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#arguments",
    "href": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#arguments",
    "title": "Retrieve the Spark connection’s SQL catalog implementation property",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nspark_connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#value",
    "href": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#value",
    "title": "Retrieve the Spark connection’s SQL catalog implementation property",
    "section": "Value",
    "text": "Value\nspark.sql.catalogImplementation property from the connection’s runtime configuration"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html",
    "title": "Feature Transformation – HashingTF (Transformer)",
    "section": "",
    "text": "R/ml_feature_hashing_tf.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html#ft_hashing_tf",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html#ft_hashing_tf",
    "title": "Feature Transformation – HashingTF (Transformer)",
    "section": "ft_hashing_tf",
    "text": "ft_hashing_tf"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html#description",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html#description",
    "title": "Feature Transformation – HashingTF (Transformer)",
    "section": "Description",
    "text": "Description\nMaps a sequence of terms to their term frequencies using the hashing trick."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html#usage",
    "title": "Feature Transformation – HashingTF (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_hashing_tf( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  binary = FALSE, \n  num_features = 2^18, \n  uid = random_string(\"hashing_tf_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html#arguments",
    "title": "Feature Transformation – HashingTF (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nbinary\nBinary toggle to control term frequency counts. If true, all non-zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. (default = FALSE)\n\n\nnum_features\nNumber of features. Should be greater than 0. (default = 2^18)\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html#value",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html#value",
    "title": "Feature Transformation – HashingTF (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html#see-also",
    "title": "Feature Transformation – HashingTF (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_json.html",
    "href": "packages/sparklyr/latest/reference/spark_write_json.html",
    "title": "Write a Spark DataFrame to a JSON file",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_json.html#spark_write_json",
    "href": "packages/sparklyr/latest/reference/spark_write_json.html#spark_write_json",
    "title": "Write a Spark DataFrame to a JSON file",
    "section": "spark_write_json",
    "text": "spark_write_json"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_json.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_json.html#description",
    "title": "Write a Spark DataFrame to a JSON file",
    "section": "Description",
    "text": "Description\nSerialize a Spark DataFrame to the JavaScript Object Notation format."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_json.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_json.html#usage",
    "title": "Write a Spark DataFrame to a JSON file",
    "section": "Usage",
    "text": "Usage\nspark_write_json( \n  x, \n  path, \n  mode = NULL, \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_json.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_json.html#arguments",
    "title": "Write a Spark DataFrame to a JSON file",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_json.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_json.html#see-also",
    "title": "Write a Spark DataFrame to a JSON file",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html",
    "href": "packages/sparklyr/latest/reference/ft_idf.html",
    "title": "Feature Transformation – IDF (Estimator)",
    "section": "",
    "text": "R/ml_feature_idf.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#ft_idf",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#ft_idf",
    "title": "Feature Transformation – IDF (Estimator)",
    "section": "ft_idf",
    "text": "ft_idf"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#description",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#description",
    "title": "Feature Transformation – IDF (Estimator)",
    "section": "Description",
    "text": "Description\nCompute the Inverse Document Frequency (IDF) given a collection of documents."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#usage",
    "title": "Feature Transformation – IDF (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_idf( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  min_doc_freq = 0, \n  uid = random_string(\"idf_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#arguments",
    "title": "Feature Transformation – IDF (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nmin_doc_freq\nThe minimum number of documents in which a term should appear. Default: 0\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#details",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#details",
    "title": "Feature Transformation – IDF (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#value",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#value",
    "title": "Feature Transformation – IDF (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#see-also",
    "title": "Feature Transformation – IDF (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html",
    "title": "Feature Transformation – VectorIndexer (Estimator)",
    "section": "",
    "text": "R/ml_feature_vector_indexer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#ft_vector_indexer",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#ft_vector_indexer",
    "title": "Feature Transformation – VectorIndexer (Estimator)",
    "section": "ft_vector_indexer",
    "text": "ft_vector_indexer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#description",
    "title": "Feature Transformation – VectorIndexer (Estimator)",
    "section": "Description",
    "text": "Description\nIndexing categorical feature columns in a dataset of Vector."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#usage",
    "title": "Feature Transformation – VectorIndexer (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_vector_indexer( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  handle_invalid = \"error\", \n  max_categories = 20, \n  uid = random_string(\"vector_indexer_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#arguments",
    "title": "Feature Transformation – VectorIndexer (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error”\n\n\nmax_categories\nThreshold for the number of values a categorical feature can take. If a feature is found to have &gt; max_categories values, then it is declared continuous. Must be greater than or equal to 2. Defaults to 20.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#details",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#details",
    "title": "Feature Transformation – VectorIndexer (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#value",
    "title": "Feature Transformation – VectorIndexer (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#see-also",
    "title": "Feature Transformation – VectorIndexer (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stop.html",
    "href": "packages/sparklyr/latest/reference/stream_stop.html",
    "title": "Stops a Spark Stream",
    "section": "",
    "text": "R/stream_operations.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stop.html#stream_stop",
    "href": "packages/sparklyr/latest/reference/stream_stop.html#stream_stop",
    "title": "Stops a Spark Stream",
    "section": "stream_stop",
    "text": "stream_stop"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stop.html#description",
    "href": "packages/sparklyr/latest/reference/stream_stop.html#description",
    "title": "Stops a Spark Stream",
    "section": "Description",
    "text": "Description\nStops processing data from a Spark stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stop.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_stop.html#usage",
    "title": "Stops a Spark Stream",
    "section": "Usage",
    "text": "Usage\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stop.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_stop.html#arguments",
    "title": "Stops a Spark Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nstream\nThe spark stream object to be stopped."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-constructors.html",
    "href": "packages/sparklyr/latest/reference/ml-constructors.html",
    "title": "Constructors for Pipeline Stages",
    "section": "",
    "text": "R/ml_transformer_and_estimator.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-constructors.html#ml-constructors",
    "href": "packages/sparklyr/latest/reference/ml-constructors.html#ml-constructors",
    "title": "Constructors for Pipeline Stages",
    "section": "ml-constructors",
    "text": "ml-constructors"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-constructors.html#description",
    "href": "packages/sparklyr/latest/reference/ml-constructors.html#description",
    "title": "Constructors for Pipeline Stages",
    "section": "Description",
    "text": "Description\nFunctions for developers writing extensions for Spark ML."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-constructors.html#usage",
    "href": "packages/sparklyr/latest/reference/ml-constructors.html#usage",
    "title": "Constructors for Pipeline Stages",
    "section": "Usage",
    "text": "Usage\nnew_ml_transformer(jobj, ..., class = character()) \n\nnew_ml_prediction_model(jobj, ..., class = character()) \n\nnew_ml_classification_model(jobj, ..., class = character()) \n\nnew_ml_probabilistic_classification_model(jobj, ..., class = character()) \n\nnew_ml_clustering_model(jobj, ..., class = character()) \n\nnew_ml_estimator(jobj, ..., class = character()) \n\nnew_ml_predictor(jobj, ..., class = character()) \n\nnew_ml_classifier(jobj, ..., class = character()) \n\nnew_ml_probabilistic_classifier(jobj, ..., class = character())"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-constructors.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml-constructors.html#arguments",
    "title": "Constructors for Pipeline Stages",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\njobj\nPointer to the pipeline stage object.\n\n\n…\n(Optional) additional attributes of the object.\n\n\nclass\nName of class."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-api.html",
    "href": "packages/sparklyr/latest/reference/spark-api.html",
    "title": "Access the Spark API",
    "section": "",
    "text": "R/spark_connection.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-api.html#spark-api",
    "href": "packages/sparklyr/latest/reference/spark-api.html#spark-api",
    "title": "Access the Spark API",
    "section": "spark-api",
    "text": "spark-api"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-api.html#description",
    "href": "packages/sparklyr/latest/reference/spark-api.html#description",
    "title": "Access the Spark API",
    "section": "Description",
    "text": "Description\nAccess the commonly-used Spark objects associated with a Spark instance. These objects provide access to different facets of the Spark API."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-api.html#usage",
    "href": "packages/sparklyr/latest/reference/spark-api.html#usage",
    "title": "Access the Spark API",
    "section": "Usage",
    "text": "Usage\nspark_context(sc) \n\njava_context(sc) \n\nhive_context(sc) \n\nspark_session(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-api.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark-api.html#arguments",
    "title": "Access the Spark API",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-api.html#details",
    "href": "packages/sparklyr/latest/reference/spark-api.html#details",
    "title": "Access the Spark API",
    "section": "Details",
    "text": "Details\nThe Scala API documentation\nis useful for discovering what methods are available for each of these objects. Use invoke to call methods on these objects."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-api.html#section",
    "href": "packages/sparklyr/latest/reference/spark-api.html#section",
    "title": "Access the Spark API",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-api.html#spark-context",
    "href": "packages/sparklyr/latest/reference/spark-api.html#spark-context",
    "title": "Access the Spark API",
    "section": "Spark Context",
    "text": "Spark Context\nThe main entry point for Spark functionality. The Spark Context\nrepresents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-api.html#java-spark-context",
    "href": "packages/sparklyr/latest/reference/spark-api.html#java-spark-context",
    "title": "Access the Spark API",
    "section": "Java Spark Context",
    "text": "Java Spark Context\nA Java-friendly version of the aforementioned Spark Context."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-api.html#hive-context",
    "href": "packages/sparklyr/latest/reference/spark-api.html#hive-context",
    "title": "Access the Spark API",
    "section": "Hive Context",
    "text": "Hive Context\nAn instance of the Spark SQL execution engine that integrates with data stored in Hive. Configuration for Hive is read from hive-site.xml on the classpath.\nStarting with Spark &gt;= 2.0.0, the Hive Context class has been deprecated – it is superceded by the Spark Session class, and hive_context will return a Spark Session object instead. Note that both classes share a SQL interface, and therefore one can invoke SQL through these objects."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-api.html#spark-session",
    "href": "packages/sparklyr/latest/reference/spark-api.html#spark-session",
    "title": "Access the Spark API",
    "section": "Spark Session",
    "text": "Spark Session\nAvailable since Spark 2.0.0, the Spark Session unifies the Spark Context and Hive Context classes into a single interface. Its use is recommended over the older APIs for code targeting Spark 2.0.0 and above."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_bundle.html",
    "href": "packages/sparklyr/latest/reference/spark_apply_bundle.html",
    "title": "Create Bundle for Spark Apply",
    "section": "",
    "text": "R/spark_apply_bundle.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_bundle.html#spark_apply_bundle",
    "href": "packages/sparklyr/latest/reference/spark_apply_bundle.html#spark_apply_bundle",
    "title": "Create Bundle for Spark Apply",
    "section": "spark_apply_bundle",
    "text": "spark_apply_bundle"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_bundle.html#description",
    "href": "packages/sparklyr/latest/reference/spark_apply_bundle.html#description",
    "title": "Create Bundle for Spark Apply",
    "section": "Description",
    "text": "Description\nCreates a bundle of packages for spark_apply()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_bundle.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_apply_bundle.html#usage",
    "title": "Create Bundle for Spark Apply",
    "section": "Usage",
    "text": "Usage\nspark_apply_bundle(packages = TRUE, base_path = getwd(), session_id = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_bundle.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_apply_bundle.html#arguments",
    "title": "Create Bundle for Spark Apply",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\npackages\nList of packages to pack or TRUE to pack all.\n\n\nbase_path\nBase path used to store the resulting bundle.\n\n\nsession_id\nAn optional ID string to include in the bundle file name to allow the bundle to be session-specific"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_wider.html",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_wider.html",
    "title": "Unnest wider",
    "section": "",
    "text": "R/sdf_unnest_wider.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#sdf_unnest_wider",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#sdf_unnest_wider",
    "title": "Unnest wider",
    "section": "sdf_unnest_wider",
    "text": "sdf_unnest_wider"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#description",
    "title": "Unnest wider",
    "section": "Description",
    "text": "Description\nFlatten a struct column within a Spark dataframe into one or more columns, similar what to tidyr::unnest_wider does to an R dataframe"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#usage",
    "title": "Unnest wider",
    "section": "Usage",
    "text": "Usage\n \nsdf_unnest_wider( \n  data, \n  col, \n  names_sep = NULL, \n  names_repair = \"check_unique\", \n  ptype = list(), \n  transform = list() \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#arguments",
    "title": "Unnest wider",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\ndata\nThe Spark dataframe to be unnested\n\n\ncol\nThe struct column to extract components from\n\n\nnames_sep\nIf NULL, the default, the names will be left as is. If a string, the inner and outer names will be pasted together using names_sep as the delimiter.\n\n\nnames_repair\nStrategy for fixing duplicate column names (the semantic will be exactly identical to that of .name_repair option in tibble)\n\n\nptype\nOptionally, supply an R data frame prototype for the output. Each column of the unnested result will be casted based on the Spark equivalent of the type of the column with the same name within ptype, e.g., if ptype has a column x of type character, then column x of the unnested result will be casted from its original SQL type to StringType.\n\n\ntransform\nOptionally, a named list of transformation functions applied to each component (e.g., list(x = as.character) to cast column x to String)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#examples",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#examples",
    "title": "Unnest wider",
    "section": "Examples",
    "text": "Examples\n\n \nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\", version = \"2.4.0\") \n \nsdf &lt;- copy_to( \n  sc, \n  tibble::tibble( \n    x = 1:3, \n    y = list(list(a = 1, b = 2), list(a = 3, b = 4), list(a = 5, b = 6)) \n  ) \n) \n \n# flatten struct column 'y' into two separate columns 'y_a' and 'y_b' \nunnested &lt;- sdf %&gt;% sdf_unnest_wider(y, names_sep = \"_\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stats.html",
    "href": "packages/sparklyr/latest/reference/stream_stats.html",
    "title": "Stream Statistics",
    "section": "",
    "text": "R/stream_view.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stats.html#stream_stats",
    "href": "packages/sparklyr/latest/reference/stream_stats.html#stream_stats",
    "title": "Stream Statistics",
    "section": "stream_stats",
    "text": "stream_stats"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stats.html#description",
    "href": "packages/sparklyr/latest/reference/stream_stats.html#description",
    "title": "Stream Statistics",
    "section": "Description",
    "text": "Description\nCollects streaming statistics, usually, to be used with stream_render()\nto render streaming statistics."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stats.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_stats.html#usage",
    "title": "Stream Statistics",
    "section": "Usage",
    "text": "Usage\nstream_stats(stream, stats = list())"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stats.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_stats.html#arguments",
    "title": "Stream Statistics",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nstream\nThe stream to collect statistics from.\n\n\nstats\nAn optional stats object generated using stream_stats()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stats.html#value",
    "href": "packages/sparklyr/latest/reference/stream_stats.html#value",
    "title": "Stream Statistics",
    "section": "Value",
    "text": "Value\nA stats object containing streaming statistics that can be passed back to the stats parameter to continue aggregating streaming stats."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stats.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_stats.html#examples",
    "title": "Stream Statistics",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\") \nsdf_len(sc, 10) %&gt;% \n  spark_write_parquet(path = \"parquet-in\") \nstream &lt;- stream_read_parquet(sc, \"parquet-in\") %&gt;% \n  stream_write_parquet(\"parquet-out\") \nstream_stats(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_parquet.html",
    "href": "packages/sparklyr/latest/reference/spark_write_parquet.html",
    "title": "Write a Spark DataFrame to a Parquet file",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_parquet.html#spark_write_parquet",
    "href": "packages/sparklyr/latest/reference/spark_write_parquet.html#spark_write_parquet",
    "title": "Write a Spark DataFrame to a Parquet file",
    "section": "spark_write_parquet",
    "text": "spark_write_parquet"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_parquet.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_parquet.html#description",
    "title": "Write a Spark DataFrame to a Parquet file",
    "section": "Description",
    "text": "Description\nSerialize a Spark DataFrame to the Parquet format."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_parquet.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_parquet.html#usage",
    "title": "Write a Spark DataFrame to a Parquet file",
    "section": "Usage",
    "text": "Usage\nspark_write_parquet( \n  x, \n  path, \n  mode = NULL, \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_parquet.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_parquet.html#arguments",
    "title": "Write a Spark DataFrame to a Parquet file",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_parquet.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_parquet.html#see-also",
    "title": "Write a Spark DataFrame to a Parquet file",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_drop_duplicates.html",
    "href": "packages/sparklyr/latest/reference/sdf_drop_duplicates.html",
    "title": "Remove duplicates from a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_drop_duplicates.html#sdf_drop_duplicates",
    "href": "packages/sparklyr/latest/reference/sdf_drop_duplicates.html#sdf_drop_duplicates",
    "title": "Remove duplicates from a Spark DataFrame",
    "section": "sdf_drop_duplicates",
    "text": "sdf_drop_duplicates"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_drop_duplicates.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_drop_duplicates.html#description",
    "title": "Remove duplicates from a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRemove duplicates from a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_drop_duplicates.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_drop_duplicates.html#usage",
    "title": "Remove duplicates from a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_drop_duplicates(x, cols = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_drop_duplicates.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_drop_duplicates.html#arguments",
    "title": "Remove duplicates from a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercible to a Spark DataFrame\n\n\ncols\nSubset of Columns to consider, given as a character vector"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_sequential_id.html",
    "href": "packages/sparklyr/latest/reference/sdf_with_sequential_id.html",
    "title": "Add a Sequential ID Column to a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_sequential_id.html#sdf_with_sequential_id",
    "href": "packages/sparklyr/latest/reference/sdf_with_sequential_id.html#sdf_with_sequential_id",
    "title": "Add a Sequential ID Column to a Spark DataFrame",
    "section": "sdf_with_sequential_id",
    "text": "sdf_with_sequential_id"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_sequential_id.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_with_sequential_id.html#description",
    "title": "Add a Sequential ID Column to a Spark DataFrame",
    "section": "Description",
    "text": "Description\nAdd a sequential ID column to a Spark DataFrame. The Spark zipWithIndex function is used to produce these. This differs from sdf_with_unique_id in that the IDs generated are independent of partitioning."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_sequential_id.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_with_sequential_id.html#usage",
    "title": "Add a Sequential ID Column to a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_with_sequential_id(x, id = \"id\", from = 1L)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_sequential_id.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_with_sequential_id.html#arguments",
    "title": "Add a Sequential ID Column to a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nid\nThe name of the column to host the generated IDs.\n\n\nfrom\nThe starting value of the id column"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/random_string.html",
    "href": "packages/sparklyr/latest/reference/random_string.html",
    "title": "Random string generation",
    "section": "",
    "text": "R/utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/random_string.html#random_string",
    "href": "packages/sparklyr/latest/reference/random_string.html#random_string",
    "title": "Random string generation",
    "section": "random_string",
    "text": "random_string"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/random_string.html#description",
    "href": "packages/sparklyr/latest/reference/random_string.html#description",
    "title": "Random string generation",
    "section": "Description",
    "text": "Description\nGenerate a random string with a given prefix."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/random_string.html#usage",
    "href": "packages/sparklyr/latest/reference/random_string.html#usage",
    "title": "Random string generation",
    "section": "Usage",
    "text": "Usage\nrandom_string(prefix = \"table\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/random_string.html#arguments",
    "href": "packages/sparklyr/latest/reference/random_string.html#arguments",
    "title": "Random string generation",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nprefix\nA length-one character vector."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/reactivespark.html",
    "href": "packages/sparklyr/latest/reference/reactivespark.html",
    "title": "Reactive spark reader",
    "section": "",
    "text": "R/stream_shiny.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/reactivespark.html#reactivespark",
    "href": "packages/sparklyr/latest/reference/reactivespark.html#reactivespark",
    "title": "Reactive spark reader",
    "section": "reactiveSpark",
    "text": "reactiveSpark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/reactivespark.html#description",
    "href": "packages/sparklyr/latest/reference/reactivespark.html#description",
    "title": "Reactive spark reader",
    "section": "Description",
    "text": "Description\nGiven a spark object, returns a reactive data source for the contents of the spark object. This function is most useful to read Spark streams."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/reactivespark.html#usage",
    "href": "packages/sparklyr/latest/reference/reactivespark.html#usage",
    "title": "Reactive spark reader",
    "section": "Usage",
    "text": "Usage\nreactiveSpark(x, intervalMillis = 1000, session = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/reactivespark.html#arguments",
    "href": "packages/sparklyr/latest/reference/reactivespark.html#arguments",
    "title": "Reactive spark reader",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\nintervalMillis\nApproximate number of milliseconds to wait to retrieve updated data frame. This can be a numeric value, or a function that returns a numeric value.\n\n\nsession\nThe user session to associate this file reader with, or NULL if none. If non-null, the reader will automatically stop when the session ends."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_set_param.html",
    "href": "packages/sparklyr/latest/reference/jobj_set_param.html",
    "title": "Parameter Setting for JVM Objects",
    "section": "",
    "text": "R/ml_pipeline_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_set_param.html#jobj_set_param",
    "href": "packages/sparklyr/latest/reference/jobj_set_param.html#jobj_set_param",
    "title": "Parameter Setting for JVM Objects",
    "section": "jobj_set_param",
    "text": "jobj_set_param"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_set_param.html#description",
    "href": "packages/sparklyr/latest/reference/jobj_set_param.html#description",
    "title": "Parameter Setting for JVM Objects",
    "section": "Description",
    "text": "Description\nSets a parameter value for a pipeline stage object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_set_param.html#usage",
    "href": "packages/sparklyr/latest/reference/jobj_set_param.html#usage",
    "title": "Parameter Setting for JVM Objects",
    "section": "Usage",
    "text": "Usage\njobj_set_param(jobj, setter, value, min_version = NULL, default = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_set_param.html#arguments",
    "href": "packages/sparklyr/latest/reference/jobj_set_param.html#arguments",
    "title": "Parameter Setting for JVM Objects",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\njobj\nA pipeline stage jobj.\n\n\nsetter\nThe name of the setter method as a string.\n\n\nvalue\nThe value to be set.\n\n\nmin_version\nThe minimum required Spark version for this parameter to be valid.\n\n\ndefault\nThe default value of the parameter, to be used together with min_version. An error is thrown if the user’s Spark version is older than min_version and value differs from default."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection_find.html",
    "href": "packages/sparklyr/latest/reference/spark_connection_find.html",
    "title": "Find Spark Connection",
    "section": "",
    "text": "R/connection_instances.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection_find.html#spark_connection_find",
    "href": "packages/sparklyr/latest/reference/spark_connection_find.html#spark_connection_find",
    "title": "Find Spark Connection",
    "section": "spark_connection_find",
    "text": "spark_connection_find"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection_find.html#description",
    "href": "packages/sparklyr/latest/reference/spark_connection_find.html#description",
    "title": "Find Spark Connection",
    "section": "Description",
    "text": "Description\nFinds an active spark connection in the environment given the connection parameters."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection_find.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_connection_find.html#usage",
    "title": "Find Spark Connection",
    "section": "Usage",
    "text": "Usage\nspark_connection_find(master = NULL, app_name = NULL, method = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection_find.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_connection_find.html#arguments",
    "title": "Find Spark Connection",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nmaster\nThe Spark master parameter.\n\n\napp_name\nThe Spark application name.\n\n\nmethod\nThe method used to connect to Spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_delta.html",
    "href": "packages/sparklyr/latest/reference/stream_write_delta.html",
    "title": "Write Delta Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_delta.html#stream_write_delta",
    "href": "packages/sparklyr/latest/reference/stream_write_delta.html#stream_write_delta",
    "title": "Write Delta Stream",
    "section": "stream_write_delta",
    "text": "stream_write_delta"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_delta.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_delta.html#description",
    "title": "Write Delta Stream",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into a Delta Lake table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_delta.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_delta.html#usage",
    "title": "Write Delta Stream",
    "section": "Usage",
    "text": "Usage\nstream_write_delta( \n  x, \n  path, \n  mode = c(\"append\", \"complete\", \"update\"), \n  checkpoint = file.path(\"checkpoints\", random_string(\"\")), \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_delta.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_delta.html#arguments",
    "title": "Write Delta Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\".\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_delta.html#details",
    "href": "packages/sparklyr/latest/reference/stream_write_delta.html#details",
    "title": "Write Delta Stream",
    "section": "Details",
    "text": "Details\nPlease note that Delta Lake requires installing the appropriate package by setting the packages parameter to \"delta\" in spark_connect()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_delta.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_delta.html#examples",
    "title": "Write Delta Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\", version = \"2.4.0\", packages = \"delta\") \ndir.create(\"text-in\") \nwriteLines(\"A text entry\", \"text-in/text.txt\") \ntext_path &lt;- file.path(\"file://\", getwd(), \"text-in\") \nstream &lt;- stream_read_text(sc, text_path) %&gt;% stream_write_delta(path = \"delta-test\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_delta.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_delta.html#see-also",
    "title": "Write Delta Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_robust_scaler.html",
    "href": "packages/sparklyr/latest/reference/ft_robust_scaler.html",
    "title": "Feature Transformation – RobustScaler (Estimator)",
    "section": "",
    "text": "R/ml_feature_robust_scaler.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_robust_scaler.html#ft_robust_scaler",
    "href": "packages/sparklyr/latest/reference/ft_robust_scaler.html#ft_robust_scaler",
    "title": "Feature Transformation – RobustScaler (Estimator)",
    "section": "ft_robust_scaler",
    "text": "ft_robust_scaler"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_robust_scaler.html#description",
    "href": "packages/sparklyr/latest/reference/ft_robust_scaler.html#description",
    "title": "Feature Transformation – RobustScaler (Estimator)",
    "section": "Description",
    "text": "Description\nRobustScaler removes the median and scales the data according to the quantile range. The quantile range is by default IQR (Interquartile Range, quantile range between the 1st quartile = 25th quantile and the 3rd quartile = 75th quantile) but can be configured. Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and quantile range are then stored to be used on later data using the transform method. Note that missing values are ignored in the computation of medians and ranges."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_robust_scaler.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_robust_scaler.html#usage",
    "title": "Feature Transformation – RobustScaler (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_robust_scaler( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  lower = 0.25, \n  upper = 0.75, \n  with_centering = TRUE, \n  with_scaling = TRUE, \n  relative_error = 0.001, \n  uid = random_string(\"ft_robust_scaler_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_robust_scaler.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_robust_scaler.html#arguments",
    "title": "Feature Transformation – RobustScaler (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nlower\nLower quantile to calculate quantile range.\n\n\nupper\nUpper quantile to calculate quantile range.\n\n\nwith_centering\nWhether to center data with median.\n\n\nwith_scaling\nWhether to scale the data to quantile range.\n\n\nrelative_error\nThe target relative error for quantile computation.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_robust_scaler.html#details",
    "href": "packages/sparklyr/latest/reference/ft_robust_scaler.html#details",
    "title": "Feature Transformation – RobustScaler (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_robust_scaler.html#value",
    "href": "packages/sparklyr/latest/reference/ft_robust_scaler.html#value",
    "title": "Feature Transformation – RobustScaler (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_robust_scaler.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_robust_scaler.html#see-also",
    "title": "Feature Transformation – RobustScaler (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke.html",
    "href": "packages/sparklyr/latest/reference/invoke.html",
    "title": "Invoke a Method on a JVM Object",
    "section": "",
    "text": "R/spark_invoke.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke.html#invoke",
    "href": "packages/sparklyr/latest/reference/invoke.html#invoke",
    "title": "Invoke a Method on a JVM Object",
    "section": "invoke",
    "text": "invoke"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke.html#description",
    "href": "packages/sparklyr/latest/reference/invoke.html#description",
    "title": "Invoke a Method on a JVM Object",
    "section": "Description",
    "text": "Description\nInvoke methods on Java object references. These functions provide a mechanism for invoking various Java object methods directly from R."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke.html#usage",
    "href": "packages/sparklyr/latest/reference/invoke.html#usage",
    "title": "Invoke a Method on a JVM Object",
    "section": "Usage",
    "text": "Usage\ninvoke(jobj, method, ...) \n\ninvoke_static(sc, class, method, ...) \n\ninvoke_new(sc, class, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke.html#arguments",
    "href": "packages/sparklyr/latest/reference/invoke.html#arguments",
    "title": "Invoke a Method on a JVM Object",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\njobj\nAn R object acting as a Java object reference (typically, a spark_jobj).\n\n\nmethod\nThe name of the method to be invoked.\n\n\n…\nOptional arguments, currently unused.\n\n\nsc\nA spark_connection.\n\n\nclass\nThe name of the Java class whose methods should be invoked."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke.html#details",
    "href": "packages/sparklyr/latest/reference/invoke.html#details",
    "title": "Invoke a Method on a JVM Object",
    "section": "Details",
    "text": "Details\nUse each of these functions in the following scenarios:"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke.html#examples",
    "href": "packages/sparklyr/latest/reference/invoke.html#examples",
    "title": "Invoke a Method on a JVM Object",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"spark://HOST:PORT\") \nspark_context(sc) %&gt;% \n  invoke(\"textFile\", \"file.csv\", 1L) %&gt;% \n  invoke(\"count\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_to_avro.html",
    "href": "packages/sparklyr/latest/reference/sdf_to_avro.html",
    "title": "Convert column(s) to avro format",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_to_avro.html#sdf_to_avro",
    "href": "packages/sparklyr/latest/reference/sdf_to_avro.html#sdf_to_avro",
    "title": "Convert column(s) to avro format",
    "section": "sdf_to_avro",
    "text": "sdf_to_avro"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_to_avro.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_to_avro.html#description",
    "title": "Convert column(s) to avro format",
    "section": "Description",
    "text": "Description\nConvert column(s) to avro format"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_to_avro.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_to_avro.html#usage",
    "title": "Convert column(s) to avro format",
    "section": "Usage",
    "text": "Usage\nsdf_to_avro(x, cols = colnames(x))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_to_avro.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_to_avro.html#arguments",
    "title": "Convert column(s) to avro format",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercible to a Spark DataFrame\n\n\ncols\nSubset of Columns to convert into avro format"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_exists.html",
    "href": "packages/sparklyr/latest/reference/spark_config_exists.html",
    "title": "A helper function to check value exist under spark_config()",
    "section": "",
    "text": "R/config_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_exists.html#spark_config_exists",
    "href": "packages/sparklyr/latest/reference/spark_config_exists.html#spark_config_exists",
    "title": "A helper function to check value exist under spark_config()",
    "section": "spark_config_exists",
    "text": "spark_config_exists"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_exists.html#description",
    "href": "packages/sparklyr/latest/reference/spark_config_exists.html#description",
    "title": "A helper function to check value exist under spark_config()",
    "section": "Description",
    "text": "Description\nA helper function to check value exist under spark_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_exists.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_config_exists.html#usage",
    "title": "A helper function to check value exist under spark_config()",
    "section": "Usage",
    "text": "Usage\nspark_config_exists(config, name, default = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_exists.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_config_exists.html#arguments",
    "title": "A helper function to check value exist under spark_config()",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nconfig\nThe configuration list from spark_config()\n\n\nname\nThe name of the configuration entry\n\n\ndefault\nThe default value to use when entry is not present"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_read_column.html",
    "href": "packages/sparklyr/latest/reference/sdf_read_column.html",
    "title": "Read a Column from a Spark DataFrame",
    "section": "",
    "text": "R/sdf_wrapper.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_read_column.html#sdf_read_column",
    "href": "packages/sparklyr/latest/reference/sdf_read_column.html#sdf_read_column",
    "title": "Read a Column from a Spark DataFrame",
    "section": "sdf_read_column",
    "text": "sdf_read_column"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_read_column.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_read_column.html#description",
    "title": "Read a Column from a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRead a single column from a Spark DataFrame, and return the contents of that column back to R."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_read_column.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_read_column.html#usage",
    "title": "Read a Column from a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_read_column(x, column)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_read_column.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_read_column.html#arguments",
    "title": "Read a Column from a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ncolumn\nThe name of a column within x."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_read_column.html#details",
    "href": "packages/sparklyr/latest/reference/sdf_read_column.html#details",
    "title": "Read a Column from a Spark DataFrame",
    "section": "Details",
    "text": "Details\nIt is expected for this operation to preserve row order."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_uncache.html",
    "href": "packages/sparklyr/latest/reference/tbl_uncache.html",
    "title": "Uncache a Spark Table",
    "section": "",
    "text": "R/tables_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_uncache.html#tbl_uncache",
    "href": "packages/sparklyr/latest/reference/tbl_uncache.html#tbl_uncache",
    "title": "Uncache a Spark Table",
    "section": "tbl_uncache",
    "text": "tbl_uncache"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_uncache.html#description",
    "href": "packages/sparklyr/latest/reference/tbl_uncache.html#description",
    "title": "Uncache a Spark Table",
    "section": "Description",
    "text": "Description\nForce a Spark table with name name to be unloaded from memory."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_uncache.html#usage",
    "href": "packages/sparklyr/latest/reference/tbl_uncache.html#usage",
    "title": "Uncache a Spark Table",
    "section": "Usage",
    "text": "Usage\ntbl_uncache(sc, name)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_uncache.html#arguments",
    "href": "packages/sparklyr/latest/reference/tbl_uncache.html#arguments",
    "title": "Uncache a Spark Table",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe table name."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_orc.html",
    "href": "packages/sparklyr/latest/reference/stream_write_orc.html",
    "title": "Write a ORC Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_orc.html#stream_write_orc",
    "href": "packages/sparklyr/latest/reference/stream_write_orc.html#stream_write_orc",
    "title": "Write a ORC Stream",
    "section": "stream_write_orc",
    "text": "stream_write_orc"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_orc.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_orc.html#description",
    "title": "Write a ORC Stream",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into an ORC stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_orc.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_orc.html#usage",
    "title": "Write a ORC Stream",
    "section": "Usage",
    "text": "Usage\nstream_write_orc( \n  x, \n  path, \n  mode = c(\"append\", \"complete\", \"update\"), \n  trigger = stream_trigger_interval(), \n  checkpoint = file.path(path, \"checkpoints\", random_string(\"\")), \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_orc.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_orc.html#arguments",
    "title": "Write a ORC Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe destination path. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\".\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous.\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_orc.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_orc.html#examples",
    "title": "Write a ORC Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\") \nsdf_len(sc, 10) %&gt;% spark_write_orc(\"orc-in\") \nstream &lt;- stream_read_orc(sc, \"orc-in\") %&gt;% stream_write_orc(\"orc-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_orc.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_orc.html#see-also",
    "title": "Write a ORC Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_jdbc.html",
    "href": "packages/sparklyr/latest/reference/spark_write_jdbc.html",
    "title": "Writes a Spark DataFrame into a JDBC table",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_jdbc.html#spark_write_jdbc",
    "href": "packages/sparklyr/latest/reference/spark_write_jdbc.html#spark_write_jdbc",
    "title": "Writes a Spark DataFrame into a JDBC table",
    "section": "spark_write_jdbc",
    "text": "spark_write_jdbc"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_jdbc.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_jdbc.html#description",
    "title": "Writes a Spark DataFrame into a JDBC table",
    "section": "Description",
    "text": "Description\nWrites a Spark DataFrame into a JDBC table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_jdbc.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_jdbc.html#usage",
    "title": "Writes a Spark DataFrame into a JDBC table",
    "section": "Usage",
    "text": "Usage\nspark_write_jdbc( \n  x, \n  name, \n  mode = NULL, \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_jdbc.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_jdbc.html#arguments",
    "title": "Writes a Spark DataFrame into a JDBC table",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nname\nThe name to assign to the newly generated table.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_jdbc.html#examples",
    "href": "packages/sparklyr/latest/reference/spark_write_jdbc.html#examples",
    "title": "Writes a Spark DataFrame into a JDBC table",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect( \n  master = \"local\", \n  config = list( \n    `sparklyr.shell.driver-class-path` = \"/usr/share/java/mysql-connector-java-8.0.25.jar\" \n  ) \n) \nspark_write_jdbc( \n  sdf_len(sc, 10), \n  name = \"my_sql_table\", \n  options = list( \n    url = \"jdbc:mysql://localhost:3306/my_sql_schema\", \n    driver = \"com.mysql.jdbc.Driver\", \n    user = \"me\", \n    password = \"******\", \n    dbtable = \"my_sql_table\" \n  ) \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_jdbc.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_jdbc.html#see-also",
    "title": "Writes a Spark DataFrame into a JDBC table",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_len.html",
    "href": "packages/sparklyr/latest/reference/sdf_len.html",
    "title": "Create DataFrame for Length",
    "section": "",
    "text": "R/sdf_sequence.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_len.html#sdf_len",
    "href": "packages/sparklyr/latest/reference/sdf_len.html#sdf_len",
    "title": "Create DataFrame for Length",
    "section": "sdf_len",
    "text": "sdf_len"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_len.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_len.html#description",
    "title": "Create DataFrame for Length",
    "section": "Description",
    "text": "Description\nCreates a DataFrame for the given length."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_len.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_len.html#usage",
    "title": "Create DataFrame for Length",
    "section": "Usage",
    "text": "Usage\nsdf_len(sc, length, repartition = NULL, type = c(\"integer\", \"integer64\"))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_len.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_len.html#arguments",
    "title": "Create DataFrame for Length",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nlength\nThe desired length of the sequence.\n\n\nrepartition\nThe number of partitions to use when distributing the data across the Spark cluster.\n\n\ntype\nThe data type to use for the index, either \"integer\" or \"integer64\"."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_interval.html",
    "href": "packages/sparklyr/latest/reference/stream_trigger_interval.html",
    "title": "Spark Stream Interval Trigger",
    "section": "",
    "text": "R/stream_operations.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_interval.html#stream_trigger_interval",
    "href": "packages/sparklyr/latest/reference/stream_trigger_interval.html#stream_trigger_interval",
    "title": "Spark Stream Interval Trigger",
    "section": "stream_trigger_interval",
    "text": "stream_trigger_interval"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_interval.html#description",
    "href": "packages/sparklyr/latest/reference/stream_trigger_interval.html#description",
    "title": "Spark Stream Interval Trigger",
    "section": "Description",
    "text": "Description\nCreates a Spark structured streaming trigger to execute over the specified interval."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_interval.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_trigger_interval.html#usage",
    "title": "Spark Stream Interval Trigger",
    "section": "Usage",
    "text": "Usage\nstream_trigger_interval(interval = 1000)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_interval.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_trigger_interval.html#arguments",
    "title": "Spark Stream Interval Trigger",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\ninterval\nThe execution interval specified in milliseconds."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_interval.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_trigger_interval.html#see-also",
    "title": "Spark Stream Interval Trigger",
    "section": "See Also",
    "text": "See Also\nstream_trigger_continuous"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_watermark.html",
    "href": "packages/sparklyr/latest/reference/stream_watermark.html",
    "title": "Watermark Stream",
    "section": "",
    "text": "R/stream_operations.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_watermark.html#stream_watermark",
    "href": "packages/sparklyr/latest/reference/stream_watermark.html#stream_watermark",
    "title": "Watermark Stream",
    "section": "stream_watermark",
    "text": "stream_watermark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_watermark.html#description",
    "href": "packages/sparklyr/latest/reference/stream_watermark.html#description",
    "title": "Watermark Stream",
    "section": "Description",
    "text": "Description\nEnsures a stream has a watermark defined, which is required for some operations over streams."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_watermark.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_watermark.html#usage",
    "title": "Watermark Stream",
    "section": "Usage",
    "text": "Usage\nstream_watermark(x, column = \"timestamp\", threshold = \"10 minutes\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_watermark.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_watermark.html#arguments",
    "title": "Watermark Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a Spark Streaming DataFrame.\n\n\ncolumn\nThe name of the column that contains the event time of the row, if the column is missing, a column with the current time will be added.\n\n\nthreshold\nThe minimum delay to wait to data to arrive late, defaults to ten minutes."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_configuration.html",
    "href": "packages/sparklyr/latest/reference/spark_configuration.html",
    "title": "Runtime configuration interface for the Spark Session",
    "section": "",
    "text": "R/spark_context_config.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_configuration.html#spark_session_config",
    "href": "packages/sparklyr/latest/reference/spark_configuration.html#spark_session_config",
    "title": "Runtime configuration interface for the Spark Session",
    "section": "spark_session_config",
    "text": "spark_session_config"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_configuration.html#description",
    "href": "packages/sparklyr/latest/reference/spark_configuration.html#description",
    "title": "Runtime configuration interface for the Spark Session",
    "section": "Description",
    "text": "Description\nRetrieves or sets runtime configuration entries for the Spark Session"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_configuration.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_configuration.html#usage",
    "title": "Runtime configuration interface for the Spark Session",
    "section": "Usage",
    "text": "Usage\nspark_session_config(sc, config = TRUE, value = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_configuration.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_configuration.html#arguments",
    "title": "Runtime configuration interface for the Spark Session",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nconfig\nThe configuration entry name(s) (e.g., \"spark.sql.shuffle.partitions\"). Defaults to NULL to retrieve all configuration entries.\n\n\nvalue\nThe configuration value to be set. Defaults to NULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_configuration.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_configuration.html#see-also",
    "title": "Runtime configuration interface for the Spark Session",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_adaptive_query_execution(), spark_advisory_shuffle_partition_size(), spark_auto_broadcast_join_threshold(), spark_coalesce_initial_num_partitions(), spark_coalesce_min_num_partitions(), spark_coalesce_shuffle_partitions()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat_array.html",
    "href": "packages/sparklyr/latest/reference/jfloat_array.html",
    "title": "Instantiate an Array[Float].",
    "section": "",
    "text": "R/utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat_array.html#jfloat_array",
    "href": "packages/sparklyr/latest/reference/jfloat_array.html#jfloat_array",
    "title": "Instantiate an Array[Float].",
    "section": "jfloat_array",
    "text": "jfloat_array"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat_array.html#description",
    "href": "packages/sparklyr/latest/reference/jfloat_array.html#description",
    "title": "Instantiate an Array[Float].",
    "section": "Description",
    "text": "Description\nInstantiate an Array[Float] object with the value specified. NOTE: this method is useful when one has to invoke a Java/Scala method requiring an Array[Float] as one of its parameters."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat_array.html#usage",
    "href": "packages/sparklyr/latest/reference/jfloat_array.html#usage",
    "title": "Instantiate an Array[Float].",
    "section": "Usage",
    "text": "Usage\njfloat_array(sc, x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat_array.html#arguments",
    "href": "packages/sparklyr/latest/reference/jfloat_array.html#arguments",
    "title": "Instantiate an Array[Float].",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nx\nA numeric vector in R."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat_array.html#examples",
    "href": "packages/sparklyr/latest/reference/jfloat_array.html#examples",
    "title": "Instantiate an Array[Float].",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"spark://HOST:PORT\") \njflt_arr &lt;- jfloat_array(sc, c(-1.23e-8, 0, -1.23e-8)) \n# jflt_arr is now a reference an array of java.lang.Float"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_jdbc.html",
    "href": "packages/sparklyr/latest/reference/spark_read_jdbc.html",
    "title": "Read from JDBC connection into a Spark DataFrame.",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_jdbc.html#spark_read_jdbc",
    "href": "packages/sparklyr/latest/reference/spark_read_jdbc.html#spark_read_jdbc",
    "title": "Read from JDBC connection into a Spark DataFrame.",
    "section": "spark_read_jdbc",
    "text": "spark_read_jdbc"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_jdbc.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_jdbc.html#description",
    "title": "Read from JDBC connection into a Spark DataFrame.",
    "section": "Description",
    "text": "Description\nRead from JDBC connection into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_jdbc.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_jdbc.html#usage",
    "title": "Read from JDBC connection into a Spark DataFrame.",
    "section": "Usage",
    "text": "Usage\nspark_read_jdbc( \n  sc, \n  name, \n  options = list(), \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE, \n  columns = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_jdbc.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_jdbc.html#arguments",
    "title": "Read from JDBC connection into a Spark DataFrame.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\noptions\nA list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_jdbc.html#examples",
    "href": "packages/sparklyr/latest/reference/spark_read_jdbc.html#examples",
    "title": "Read from JDBC connection into a Spark DataFrame.",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect( \n  master = \"local\", \n  config = list( \n    `sparklyr.shell.driver-class-path` = \"/usr/share/java/mysql-connector-java-8.0.25.jar\" \n  ) \n) \nspark_read_jdbc( \n  sc, \n  name = \"my_sql_table\", \n  options = list( \n    url = \"jdbc:mysql://localhost:3306/my_sql_schema\", \n    driver = \"com.mysql.jdbc.Driver\", \n    user = \"me\", \n    password = \"******\", \n    dbtable = \"my_sql_table\" \n  ) \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_jdbc.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_jdbc.html#see-also",
    "title": "Read from JDBC connection into a Spark DataFrame.",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_r_formula.html",
    "href": "packages/sparklyr/latest/reference/ft_r_formula.html",
    "title": "Feature Transformation – RFormula (Estimator)",
    "section": "",
    "text": "R/ml_feature_r_formula.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_r_formula.html#ft_r_formula",
    "href": "packages/sparklyr/latest/reference/ft_r_formula.html#ft_r_formula",
    "title": "Feature Transformation – RFormula (Estimator)",
    "section": "ft_r_formula",
    "text": "ft_r_formula"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_r_formula.html#description",
    "href": "packages/sparklyr/latest/reference/ft_r_formula.html#description",
    "title": "Feature Transformation – RFormula (Estimator)",
    "section": "Description",
    "text": "Description\nImplements the transforms required for fitting a dataset against an R model formula. Currently we support a limited subset of the R operators, including ~, ., :, +, and -. Also see the R formula docs here: http://stat.ethz.ch/R-manual/R-patched/library/stats/html/formula.html"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_r_formula.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_r_formula.html#usage",
    "title": "Feature Transformation – RFormula (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_r_formula( \n  x, \n  formula = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  force_index_label = FALSE, \n  uid = random_string(\"r_formula_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_r_formula.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_r_formula.html#arguments",
    "title": "Feature Transformation – RFormula (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nR formula as a character string or a formula. Formula objects are converted to character strings directly and the environment is not captured.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nforce_index_label\n(Spark 2.1.0+) Force to index label whether it is numeric or string type. Usually we index label only when it is string type. If the formula was used by classification algorithms, we can force to index label even it is numeric type by setting this param with true. Default: FALSE.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_r_formula.html#details",
    "href": "packages/sparklyr/latest/reference/ft_r_formula.html#details",
    "title": "Feature Transformation – RFormula (Estimator)",
    "section": "Details",
    "text": "Details\nThe basic operators in the formula are:\n\n~ separate target and terms\n\nconcat terms, “+ 0” means removing intercept\n\n\nremove a term, “- 1” means removing intercept\n\n: interaction (multiplication for numeric values, or binarized categorical values)\n. all columns except target\nSuppose a and b are double columns, we use the following simple examples to illustrate the effect of RFormula:\ny ~ a + b means model y ~ w0 + w1 * a + w2 * b\nwhere `w0` is the intercept and `w1, w2` are coefficients.     \ny ~ a + b + a:b - 1 means model y ~ w1 * a + w2 * b + w3 * a * b\nwhere `w1, w2, w3` are coefficients.   \n\nRFormula produces a vector column of features and a double or string column of label. Like when formulas are used in R for linear regression, string input columns will be one-hot encoded, and numeric columns will be cast to doubles. If the label column is of type string, it will be first transformed to double with StringIndexer. If the label column does not exist in the DataFrame, the output label column will be created from the specified response variable in the formula.\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_r_formula.html#value",
    "href": "packages/sparklyr/latest/reference/ft_r_formula.html#value",
    "title": "Feature Transformation – RFormula (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_r_formula.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_r_formula.html#see-also",
    "title": "Feature Transformation – RFormula (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbeta.html",
    "href": "packages/sparklyr/latest/reference/sdf_rbeta.html",
    "title": "Generate random samples from a Beta distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbeta.html#sdf_rbeta",
    "href": "packages/sparklyr/latest/reference/sdf_rbeta.html#sdf_rbeta",
    "title": "Generate random samples from a Beta distribution",
    "section": "sdf_rbeta",
    "text": "sdf_rbeta"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbeta.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rbeta.html#description",
    "title": "Generate random samples from a Beta distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Betal distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbeta.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rbeta.html#usage",
    "title": "Generate random samples from a Beta distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rbeta( \n  sc, \n  n, \n  shape1, \n  shape2, \n  num_partitions = NULL, \n  seed = NULL, \n  output_col = \"x\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbeta.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rbeta.html#arguments",
    "title": "Generate random samples from a Beta distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nshape1\nNon-negative parameter (alpha) of the Beta distribution.\n\n\nshape2\nNon-negative parameter (beta) of the Beta distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbeta.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rbeta.html#see-also",
    "title": "Generate random samples from a Beta distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_spark_shinyapp.html",
    "href": "packages/sparklyr/latest/reference/connection_spark_shinyapp.html",
    "title": "A Shiny app that can be used to construct a spark_connect statement",
    "section": "",
    "text": "R/connection_shinyapp.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_spark_shinyapp.html#connection_spark_shinyapp",
    "href": "packages/sparklyr/latest/reference/connection_spark_shinyapp.html#connection_spark_shinyapp",
    "title": "A Shiny app that can be used to construct a spark_connect statement",
    "section": "connection_spark_shinyapp",
    "text": "connection_spark_shinyapp"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_spark_shinyapp.html#description",
    "href": "packages/sparklyr/latest/reference/connection_spark_shinyapp.html#description",
    "title": "A Shiny app that can be used to construct a spark_connect statement",
    "section": "Description",
    "text": "Description\nA Shiny app that can be used to construct a spark_connect statement"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_spark_shinyapp.html#usage",
    "href": "packages/sparklyr/latest/reference/connection_spark_shinyapp.html#usage",
    "title": "A Shiny app that can be used to construct a spark_connect statement",
    "section": "Usage",
    "text": "Usage\nconnection_spark_shinyapp()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_from_avro.html",
    "href": "packages/sparklyr/latest/reference/sdf_from_avro.html",
    "title": "Convert column(s) from avro format",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_from_avro.html#sdf_from_avro",
    "href": "packages/sparklyr/latest/reference/sdf_from_avro.html#sdf_from_avro",
    "title": "Convert column(s) from avro format",
    "section": "sdf_from_avro",
    "text": "sdf_from_avro"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_from_avro.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_from_avro.html#description",
    "title": "Convert column(s) from avro format",
    "section": "Description",
    "text": "Description\nConvert column(s) from avro format"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_from_avro.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_from_avro.html#usage",
    "title": "Convert column(s) from avro format",
    "section": "Usage",
    "text": "Usage\nsdf_from_avro(x, cols)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_from_avro.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_from_avro.html#arguments",
    "title": "Convert column(s) from avro format",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercible to a Spark DataFrame\n\n\ncols\nNamed list of columns to transform from Avro format plus a valid Avro schema string for each column, where column names are keys and column schema strings are values (e.g., c(example_primitive_col = \"string\", example_complex_col = \"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"person\\\",\\\"fields\\\":[ {\\\"name\\\":\\\"person_name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"person_id\\\",\\\"type\\\":\\\"long\\\"}]}\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html",
    "href": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html",
    "title": "Spark ML – Gaussian Mixture clustering.",
    "section": "",
    "text": "R/ml_clustering_gaussian_mixture.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#ml_gaussian_mixture",
    "href": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#ml_gaussian_mixture",
    "title": "Spark ML – Gaussian Mixture clustering.",
    "section": "ml_gaussian_mixture",
    "text": "ml_gaussian_mixture"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#description",
    "href": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#description",
    "title": "Spark ML – Gaussian Mixture clustering.",
    "section": "Description",
    "text": "Description\nThis class performs expectation maximization for multivariate Gaussian Mixture Models (GMMs). A GMM represents a composite distribution of independent Gaussian distributions with associated “mixing” weights specifying each’s contribution to the composite. Given a set of sample points, this class will maximize the log-likelihood for a mixture of k Gaussians, iterating until the log-likelihood changes by less than tol, or until it has reached the max number of iterations. While this process is generally guaranteed to converge, it is not guaranteed to find a global optimum."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#usage",
    "title": "Spark ML – Gaussian Mixture clustering.",
    "section": "Usage",
    "text": "Usage\n \nml_gaussian_mixture( \n  x, \n  formula = NULL, \n  k = 2, \n  max_iter = 100, \n  tol = 0.01, \n  seed = NULL, \n  features_col = \"features\", \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  uid = random_string(\"gaussian_mixture_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#arguments",
    "title": "Spark ML – Gaussian Mixture clustering.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nk\nThe number of clusters to create\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments, see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#value",
    "href": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#value",
    "title": "Spark ML – Gaussian Mixture clustering.",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the clustering estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, an estimator is constructed then immediately fit with the input tbl_spark, returning a clustering model.\ntbl_spark, with formula or features specified: When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the estimator. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model. This signature does not apply to ml_lda()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#examples",
    "title": "Spark ML – Gaussian Mixture clustering.",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \ngmm_model &lt;- ml_gaussian_mixture(iris_tbl, Species ~ .) \npred &lt;- sdf_predict(iris_tbl, gmm_model) \n#&gt; Warning: 'sdf_predict' is deprecated.\n#&gt; Use 'ml_predict' instead.\n#&gt; See help(\"Deprecated\")\nml_clustering_evaluator(pred) \n#&gt; [1] 0.4772758"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#see-also",
    "title": "Spark ML – Gaussian Mixture clustering.",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-clustering.html for more information on the set of clustering algorithms. Other ml clustering algorithms: ml_bisecting_kmeans(), ml_kmeans(), ml_lda()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform.html",
    "href": "packages/sparklyr/latest/reference/hof_transform.html",
    "title": "Transform Array Column",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform.html#hof_transform",
    "href": "packages/sparklyr/latest/reference/hof_transform.html#hof_transform",
    "title": "Transform Array Column",
    "section": "hof_transform",
    "text": "hof_transform"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform.html#description",
    "href": "packages/sparklyr/latest/reference/hof_transform.html#description",
    "title": "Transform Array Column",
    "section": "Description",
    "text": "Description\nApply an element-wise transformation function to an array column (this is essentially a dplyr wrapper for the transform(array&lt;T&gt;, function&lt;T, U&gt;): array&lt;U&gt; and the transform(array&lt;T&gt;, function&lt;T, Int, U&gt;): array&lt;U&gt; built-in Spark SQL functions)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_transform.html#usage",
    "title": "Transform Array Column",
    "section": "Usage",
    "text": "Usage\n \nhof_transform(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_transform.html#arguments",
    "title": "Transform Array Column",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark data frame to transform\n\n\nfunc\nThe transformation to apply\n\n\nexpr\nThe array being transformed, could be any SQL expression evaluating to an array (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the transformed result (default: expr)\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_transform.html#examples",
    "title": "Transform Array Column",
    "section": "Examples",
    "text": "Examples\n\n \n \nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\") \n# applies the (x -&gt; x * x) transformation to elements of all arrays \ncopy_to(sc, tibble::tibble(arr = list(1:5, 21:25))) %&gt;% \n  hof_transform(~ .x * .x) \n#&gt; # Source: spark&lt;?&gt; [?? x 1]\n#&gt;   arr      \n#&gt;   &lt;list&gt;   \n#&gt; 1 &lt;dbl [5]&gt;\n#&gt; 2 &lt;dbl [5]&gt;"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_generate_test.html",
    "href": "packages/sparklyr/latest/reference/stream_generate_test.html",
    "title": "Generate Test Stream",
    "section": "",
    "text": "R/stream_operations.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_generate_test.html#stream_generate_test",
    "href": "packages/sparklyr/latest/reference/stream_generate_test.html#stream_generate_test",
    "title": "Generate Test Stream",
    "section": "stream_generate_test",
    "text": "stream_generate_test"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_generate_test.html#description",
    "href": "packages/sparklyr/latest/reference/stream_generate_test.html#description",
    "title": "Generate Test Stream",
    "section": "Description",
    "text": "Description\nGenerates a local test stream, useful when testing streams locally."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_generate_test.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_generate_test.html#usage",
    "title": "Generate Test Stream",
    "section": "Usage",
    "text": "Usage\nstream_generate_test( \n  df = rep(1:1000), \n  path = \"source\", \n  distribution = floor(10 + 1e+05 * stats::dbinom(1:20, 20, 0.5)), \n  iterations = 50, \n  interval = 1 \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_generate_test.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_generate_test.html#arguments",
    "title": "Generate Test Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\ndf\nThe data frame used as a source of rows to the stream, will be cast to data frame if needed. Defaults to a sequence of one thousand entries.\n\n\npath\nPath to save stream of files to, defaults to \"source\".\n\n\ndistribution\nThe distribution of rows to use over each iteration, defaults to a binomial distribution. The stream will cycle through the distribution if needed.\n\n\niterations\nNumber of iterations to execute before stopping, defaults to fifty.\n\n\ninterval\nThe inverval in seconds use to write the stream, defaults to one second."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_generate_test.html#details",
    "href": "packages/sparklyr/latest/reference/stream_generate_test.html#details",
    "title": "Generate Test Stream",
    "section": "Details",
    "text": "Details\nThis function requires the callr package to be installed."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rt.html",
    "href": "packages/sparklyr/latest/reference/sdf_rt.html",
    "title": "Generate random samples from a t-distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rt.html#sdf_rt",
    "href": "packages/sparklyr/latest/reference/sdf_rt.html#sdf_rt",
    "title": "Generate random samples from a t-distribution",
    "section": "sdf_rt",
    "text": "sdf_rt"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rt.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rt.html#description",
    "title": "Generate random samples from a t-distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a t-distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rt.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rt.html#usage",
    "title": "Generate random samples from a t-distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rt(sc, n, df, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rt.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rt.html#arguments",
    "title": "Generate random samples from a t-distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\ndf\nDegrees of freedom (&gt; 0, maybe non-integer).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rt.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rt.html#see-also",
    "title": "Generate random samples from a t-distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_ngram.html",
    "href": "packages/sparklyr/latest/reference/ft_ngram.html",
    "title": "Feature Transformation – NGram (Transformer)",
    "section": "",
    "text": "R/ml_feature_ngram.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_ngram.html#ft_ngram",
    "href": "packages/sparklyr/latest/reference/ft_ngram.html#ft_ngram",
    "title": "Feature Transformation – NGram (Transformer)",
    "section": "ft_ngram",
    "text": "ft_ngram"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_ngram.html#description",
    "href": "packages/sparklyr/latest/reference/ft_ngram.html#description",
    "title": "Feature Transformation – NGram (Transformer)",
    "section": "Description",
    "text": "Description\nA feature transformer that converts the input array of strings into an array of n-grams. Null values in the input array are ignored. It returns an array of n-grams where each n-gram is represented by a space-separated string of words."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_ngram.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_ngram.html#usage",
    "title": "Feature Transformation – NGram (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_ngram( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  n = 2, \n  uid = random_string(\"ngram_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_ngram.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_ngram.html#arguments",
    "title": "Feature Transformation – NGram (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nn\nMinimum n-gram length, greater than or equal to 1. Default: 2, bigram features\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_ngram.html#details",
    "href": "packages/sparklyr/latest/reference/ft_ngram.html#details",
    "title": "Feature Transformation – NGram (Transformer)",
    "section": "Details",
    "text": "Details\nWhen the input is empty, an empty array is returned. When the input array length is less than n (number of elements per n-gram), no n-grams are returned."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_ngram.html#value",
    "href": "packages/sparklyr/latest/reference/ft_ngram.html#value",
    "title": "Feature Transformation – NGram (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_ngram.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_ngram.html#see-also",
    "title": "Feature Transformation – NGram (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_feature_hasher.html",
    "href": "packages/sparklyr/latest/reference/ft_feature_hasher.html",
    "title": "Feature Transformation – FeatureHasher (Transformer)",
    "section": "",
    "text": "R/ml_feature_feature_hasher.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_feature_hasher.html#ft_feature_hasher",
    "href": "packages/sparklyr/latest/reference/ft_feature_hasher.html#ft_feature_hasher",
    "title": "Feature Transformation – FeatureHasher (Transformer)",
    "section": "ft_feature_hasher",
    "text": "ft_feature_hasher"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_feature_hasher.html#description",
    "href": "packages/sparklyr/latest/reference/ft_feature_hasher.html#description",
    "title": "Feature Transformation – FeatureHasher (Transformer)",
    "section": "Description",
    "text": "Description\nFeature Transformation – FeatureHasher (Transformer)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_feature_hasher.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_feature_hasher.html#usage",
    "title": "Feature Transformation – FeatureHasher (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_feature_hasher( \n  x, \n  input_cols = NULL, \n  output_col = NULL, \n  num_features = 2^18, \n  categorical_cols = NULL, \n  uid = random_string(\"feature_hasher_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_feature_hasher.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_feature_hasher.html#arguments",
    "title": "Feature Transformation – FeatureHasher (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_cols\nNames of input columns.\n\n\noutput_col\nName of output column.\n\n\nnum_features\nNumber of features. Defaults to \\(list(\"2^18\")\\).\n\n\ncategorical_cols\nNumeric columns to treat as categorical features. By default only string and boolean columns are treated as categorical, so this param can be used to explicitly specify the numerical columns to treat as categorical.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_feature_hasher.html#details",
    "href": "packages/sparklyr/latest/reference/ft_feature_hasher.html#details",
    "title": "Feature Transformation – FeatureHasher (Transformer)",
    "section": "Details",
    "text": "Details\nFeature hashing projects a set of categorical or numerical features into a feature vector of specified dimension (typically substantially smaller than that of the original feature space). This is done using the hashing trick https://en.wikipedia.org/wiki/Feature_hashing to map features to indices in the feature vector.\nThe FeatureHasher transformer operates on multiple columns. Each column may contain either numeric or categorical features. Behavior and handling of column data types is as follows: -Numeric columns: For numeric features, the hash value of the column name is used to map the feature value to its index in the feature vector. By default, numeric features are not treated as categorical (even when they are integers). To treat them as categorical, specify the relevant columns in categoricalCols. -String columns: For categorical features, the hash value of the string “column_name=value” is used to map to the vector index, with an indicator value of 1.0. Thus, categorical features are “one-hot” encoded (similarly to using OneHotEncoder with drop_last=FALSE). -Boolean columns: Boolean values are treated in the same way as string columns. That is, boolean features are represented as “column_name=true” or “column_name=false”, with an indicator value of 1.0.\nNull (missing) values are ignored (implicitly zero in the resulting feature vector).\nThe hash function used here is also the MurmurHash 3 used in HashingTF. Since a simple modulo on the hashed value is used to determine the vector index, it is advisable to use a power of two as the num_features parameter; otherwise the features will not be mapped evenly to the vector indices."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_feature_hasher.html#value",
    "href": "packages/sparklyr/latest/reference/ft_feature_hasher.html#value",
    "title": "Feature Transformation – FeatureHasher (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_feature_hasher.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_feature_hasher.html#see-also",
    "title": "Feature Transformation – FeatureHasher (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_coalesce.html",
    "href": "packages/sparklyr/latest/reference/sdf_coalesce.html",
    "title": "Coalesces a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_coalesce.html#sdf_coalesce",
    "href": "packages/sparklyr/latest/reference/sdf_coalesce.html#sdf_coalesce",
    "title": "Coalesces a Spark DataFrame",
    "section": "sdf_coalesce",
    "text": "sdf_coalesce"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_coalesce.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_coalesce.html#description",
    "title": "Coalesces a Spark DataFrame",
    "section": "Description",
    "text": "Description\nCoalesces a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_coalesce.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_coalesce.html#usage",
    "title": "Coalesces a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_coalesce(x, partitions)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_coalesce.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_coalesce.html#arguments",
    "title": "Coalesces a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\npartitions\nnumber of partitions"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hive_context_config.html",
    "href": "packages/sparklyr/latest/reference/hive_context_config.html",
    "title": "Runtime configuration interface for Hive",
    "section": "",
    "text": "R/spark_context_config.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hive_context_config.html#hive_context_config",
    "href": "packages/sparklyr/latest/reference/hive_context_config.html#hive_context_config",
    "title": "Runtime configuration interface for Hive",
    "section": "hive_context_config",
    "text": "hive_context_config"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hive_context_config.html#description",
    "href": "packages/sparklyr/latest/reference/hive_context_config.html#description",
    "title": "Runtime configuration interface for Hive",
    "section": "Description",
    "text": "Description\nRetrieves the runtime configuration interface for Hive."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hive_context_config.html#usage",
    "href": "packages/sparklyr/latest/reference/hive_context_config.html#usage",
    "title": "Runtime configuration interface for Hive",
    "section": "Usage",
    "text": "Usage\nhive_context_config(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hive_context_config.html#arguments",
    "href": "packages/sparklyr/latest/reference/hive_context_config.html#arguments",
    "title": "Runtime configuration interface for Hive",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke_method.html",
    "href": "packages/sparklyr/latest/reference/j_invoke_method.html",
    "title": "Generic Call Interface",
    "section": "",
    "text": "R/spark_invoke.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke_method.html#j_invoke_method",
    "href": "packages/sparklyr/latest/reference/j_invoke_method.html#j_invoke_method",
    "title": "Generic Call Interface",
    "section": "j_invoke_method",
    "text": "j_invoke_method"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke_method.html#description",
    "href": "packages/sparklyr/latest/reference/j_invoke_method.html#description",
    "title": "Generic Call Interface",
    "section": "Description",
    "text": "Description\nCall a Java method and retrieve the return value through a JVM object reference."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke_method.html#usage",
    "href": "packages/sparklyr/latest/reference/j_invoke_method.html#usage",
    "title": "Generic Call Interface",
    "section": "Usage",
    "text": "Usage\nj_invoke_method(sc, static, object, method, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke_method.html#arguments",
    "href": "packages/sparklyr/latest/reference/j_invoke_method.html#arguments",
    "title": "Generic Call Interface",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nspark_connection\n\n\nstatic\nIs this a static method call (including a constructor). If so then the object parameter should be the name of a class (otherwise it should be a spark_jobj instance).\n\n\nobject\nObject instance or name of class (for static)\n\n\nmethod\nName of method\n\n\n…\nCall parameters"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_service.html",
    "href": "packages/sparklyr/latest/reference/livy_service.html",
    "title": "Start Livy",
    "section": "",
    "text": "R/livy_service.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_service.html#livy_service_start",
    "href": "packages/sparklyr/latest/reference/livy_service.html#livy_service_start",
    "title": "Start Livy",
    "section": "livy_service_start",
    "text": "livy_service_start"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_service.html#description",
    "href": "packages/sparklyr/latest/reference/livy_service.html#description",
    "title": "Start Livy",
    "section": "Description",
    "text": "Description\nStarts the livy service.\nStops the running instances of the livy service."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_service.html#usage",
    "href": "packages/sparklyr/latest/reference/livy_service.html#usage",
    "title": "Start Livy",
    "section": "Usage",
    "text": "Usage\nlivy_service_start( \n  version = NULL, \n  spark_version = NULL, \n  stdout = \"\", \n  stderr = \"\", \n  ... \n) \n\nlivy_service_stop()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_service.html#arguments",
    "href": "packages/sparklyr/latest/reference/livy_service.html#arguments",
    "title": "Start Livy",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nversion\nThe version of livy to use.\n\n\nspark_version\nThe version of spark to connect to.\n\n\nstdout, stderr\nwhere output to ‘stdout’ or ‘stderr’ should be sent. Same options as system2.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_regression.html",
    "href": "packages/sparklyr/latest/reference/ml_linear_regression.html",
    "title": "Spark ML – Linear Regression",
    "section": "",
    "text": "R/ml_regression_linear_regression.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_regression.html#ml_linear_regression",
    "href": "packages/sparklyr/latest/reference/ml_linear_regression.html#ml_linear_regression",
    "title": "Spark ML – Linear Regression",
    "section": "ml_linear_regression",
    "text": "ml_linear_regression"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_regression.html#description",
    "href": "packages/sparklyr/latest/reference/ml_linear_regression.html#description",
    "title": "Spark ML – Linear Regression",
    "section": "Description",
    "text": "Description\nPerform regression using linear regression."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_regression.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_linear_regression.html#usage",
    "title": "Spark ML – Linear Regression",
    "section": "Usage",
    "text": "Usage\n \nml_linear_regression( \n  x, \n  formula = NULL, \n  fit_intercept = TRUE, \n  elastic_net_param = 0, \n  reg_param = 0, \n  max_iter = 100, \n  weight_col = NULL, \n  loss = \"squaredError\", \n  solver = \"auto\", \n  standardization = TRUE, \n  tol = 1e-06, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"linear_regression_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_regression.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_linear_regression.html#arguments",
    "title": "Spark ML – Linear Regression",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nfit_intercept\nBoolean; should the model be fit with an intercept term?\n\n\nelastic_net_param\nElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n\n\nreg_param\nRegularization parameter (aka lambda)\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nweight_col\nThe name of the column to use as weights for the model fit.\n\n\nloss\nThe loss function to be optimized. Supported options: “squaredError” and “huber”. Default: “squaredError”\n\n\nsolver\nSolver algorithm for optimization.\n\n\nstandardization\nWhether to standardize the training features before fitting the model.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_regression.html#details",
    "href": "packages/sparklyr/latest/reference/ml_linear_regression.html#details",
    "title": "Spark ML – Linear Regression",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_regression.html#value",
    "href": "packages/sparklyr/latest/reference/ml_linear_regression.html#value",
    "title": "Spark ML – Linear Regression",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_regression.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_linear_regression.html#examples",
    "title": "Spark ML – Linear Regression",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \nmtcars_tbl &lt;- sdf_copy_to(sc, mtcars, name = \"mtcars_tbl\", overwrite = TRUE) \n \npartitions &lt;- mtcars_tbl %&gt;% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \n \nmtcars_training &lt;- partitions$training \nmtcars_test &lt;- partitions$test \n \nlm_model &lt;- mtcars_training %&gt;% \n  ml_linear_regression(mpg ~ .) \n \npred &lt;- ml_predict(lm_model, mtcars_test) \n \nml_regression_evaluator(pred, label_col = \"mpg\") \n#&gt; [1] 2.881163"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_regression.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_linear_regression.html#see-also",
    "title": "Spark ML – Linear Regression",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms. Other ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_residuals.html",
    "href": "packages/sparklyr/latest/reference/sdf_residuals.html",
    "title": "Model Residuals",
    "section": "",
    "text": "R/ml_model_generalized_linear_regression.R,"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_residuals.html#sdf_residuals.ml_model_generalized_linear_regression",
    "href": "packages/sparklyr/latest/reference/sdf_residuals.html#sdf_residuals.ml_model_generalized_linear_regression",
    "title": "Model Residuals",
    "section": "sdf_residuals.ml_model_generalized_linear_regression",
    "text": "sdf_residuals.ml_model_generalized_linear_regression"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_residuals.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_residuals.html#description",
    "title": "Model Residuals",
    "section": "Description",
    "text": "Description\nThis generic method returns a Spark DataFrame with model residuals added as a column to the model training data."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_residuals.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_residuals.html#usage",
    "title": "Model Residuals",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_generalized_linear_regression'\nsdf_residuals( \n  object, \n  type = c(\"deviance\", \"pearson\", \"working\", \"response\"), \n  ... \n) \n\n## S3 method for class 'ml_model_linear_regression'\nsdf_residuals(object, ...) \n\nsdf_residuals(object, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_residuals.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_residuals.html#arguments",
    "title": "Model Residuals",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nobject\nSpark ML model object.\n\n\ntype\ntype of residuals which should be returned.\n\n\n…\nadditional arguments"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_packages.html",
    "href": "packages/sparklyr/latest/reference/spark_config_packages.html",
    "title": "Creates Spark Configuration",
    "section": "",
    "text": "R/config_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_packages.html#spark_config_packages",
    "href": "packages/sparklyr/latest/reference/spark_config_packages.html#spark_config_packages",
    "title": "Creates Spark Configuration",
    "section": "spark_config_packages",
    "text": "spark_config_packages"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_packages.html#description",
    "href": "packages/sparklyr/latest/reference/spark_config_packages.html#description",
    "title": "Creates Spark Configuration",
    "section": "Description",
    "text": "Description\nCreates Spark Configuration"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_packages.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_config_packages.html#usage",
    "title": "Creates Spark Configuration",
    "section": "Usage",
    "text": "Usage\nspark_config_packages(config, packages, version, scala_version = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_packages.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_config_packages.html#arguments",
    "title": "Creates Spark Configuration",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nconfig\nThe Spark configuration object.\n\n\npackages\nA list of named packages or versioned packagese to add.\n\n\nversion\nThe version of Spark being used.\n\n\nscala_version\nAcceptable Scala version of packages to be loaded\n\n\n…\nAdditional configurations"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_continuous.html",
    "href": "packages/sparklyr/latest/reference/stream_trigger_continuous.html",
    "title": "Spark Stream Continuous Trigger",
    "section": "",
    "text": "R/stream_operations.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#stream_trigger_continuous",
    "href": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#stream_trigger_continuous",
    "title": "Spark Stream Continuous Trigger",
    "section": "stream_trigger_continuous",
    "text": "stream_trigger_continuous"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#description",
    "href": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#description",
    "title": "Spark Stream Continuous Trigger",
    "section": "Description",
    "text": "Description\nCreates a Spark structured streaming trigger to execute continuously. This mode is the most performant but not all operations are supported."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#usage",
    "title": "Spark Stream Continuous Trigger",
    "section": "Usage",
    "text": "Usage\nstream_trigger_continuous(checkpoint = 5000)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#arguments",
    "title": "Spark Stream Continuous Trigger",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\ncheckpoint\nThe checkpoint interval specified in milliseconds."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#see-also",
    "title": "Spark Stream Continuous Trigger",
    "section": "See Also",
    "text": "See Also\nstream_trigger_interval"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_json.html",
    "href": "packages/sparklyr/latest/reference/spark_read_json.html",
    "title": "Read a JSON file into a Spark DataFrame",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_json.html#spark_read_json",
    "href": "packages/sparklyr/latest/reference/spark_read_json.html#spark_read_json",
    "title": "Read a JSON file into a Spark DataFrame",
    "section": "spark_read_json",
    "text": "spark_read_json"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_json.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_json.html#description",
    "title": "Read a JSON file into a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRead a table serialized in the JavaScript Object Notation format into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_json.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_json.html#usage",
    "title": "Read a JSON file into a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nspark_read_json( \n  sc, \n  name = NULL, \n  path = name, \n  options = list(), \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE, \n  columns = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_json.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_json.html#arguments",
    "title": "Read a JSON file into a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\noptions\nA list of strings with additional options.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_json.html#details",
    "href": "packages/sparklyr/latest/reference/spark_read_json.html#details",
    "title": "Read a JSON file into a Spark DataFrame",
    "section": "Details",
    "text": "Details\nYou can read data from HDFS (hdfs://), S3 (s3a://), as well as the local file system (file://).\nIf you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf spark.hadoop.fs.s3a.access.key, spark.hadoop.fs.s3a.secret.key or any of the methods outlined in the aws-sdk documentation Working with AWS credentials\nIn order to work with the newer s3a:// protocol also set the values for spark.hadoop.fs.s3a.impl and spark.hadoop.fs.s3a.endpoint. In addition, to support v4 of the S3 api be sure to pass the -Dcom.amazonaws.services.s3.enableV4 driver options for the config key spark.driver.extraJavaOptions\nFor instructions on how to configure s3n:// check the hadoop documentation: s3n authentication properties"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_json.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_json.html#see-also",
    "title": "Read a JSON file into a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compile.html",
    "href": "packages/sparklyr/latest/reference/spark_compile.html",
    "title": "Compile Scala sources into a Java Archive",
    "section": "",
    "text": "R/spark_compile.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compile.html#spark_compile",
    "href": "packages/sparklyr/latest/reference/spark_compile.html#spark_compile",
    "title": "Compile Scala sources into a Java Archive",
    "section": "spark_compile",
    "text": "spark_compile"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compile.html#description",
    "href": "packages/sparklyr/latest/reference/spark_compile.html#description",
    "title": "Compile Scala sources into a Java Archive",
    "section": "Description",
    "text": "Description\nGiven a set of scala source files, compile them into a Java Archive (jar)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compile.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_compile.html#usage",
    "title": "Compile Scala sources into a Java Archive",
    "section": "Usage",
    "text": "Usage\nspark_compile( \n  jar_name, \n  spark_home = NULL, \n  filter = NULL, \n  scalac = NULL, \n  jar = NULL, \n  jar_dep = NULL, \n  embedded_srcs = \"embedded_sources.R\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compile.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_compile.html#arguments",
    "title": "Compile Scala sources into a Java Archive",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nspark_home\nThe path to the Spark sources to be used alongside compilation.\n\n\nfilter\nAn optional function, used to filter out discovered scalafiles during compilation. This can be used to ensure that e.g. certain files are only compiled with certain versions of Spark, and so on.\n\n\nscalac\nThe path to the scalac program to be used, for compilation of scala files.\n\n\njar\nThe path to the jar program to be used, for generating of the resulting jar.\n\n\njar_dep\nAn optional list of additional jar dependencies.\n\n\nembedded_srcs\nEmbedded source file(s) under &lt;R package root&gt;/java to be included in the root of the resulting jar file as resources\n\n\nname\nThe name to assign to the target jar."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/unnest.html",
    "href": "packages/sparklyr/latest/reference/unnest.html",
    "title": "Unnest",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/unnest.html#unnest",
    "href": "packages/sparklyr/latest/reference/unnest.html#unnest",
    "title": "Unnest",
    "section": "unnest",
    "text": "unnest"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/unnest.html#description",
    "href": "packages/sparklyr/latest/reference/unnest.html#description",
    "title": "Unnest",
    "section": "Description",
    "text": "Description\nSee unnest for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_source.html",
    "href": "packages/sparklyr/latest/reference/spark_write_source.html",
    "title": "Writes a Spark DataFrame into a generic source",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_source.html#spark_write_source",
    "href": "packages/sparklyr/latest/reference/spark_write_source.html#spark_write_source",
    "title": "Writes a Spark DataFrame into a generic source",
    "section": "spark_write_source",
    "text": "spark_write_source"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_source.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_source.html#description",
    "title": "Writes a Spark DataFrame into a generic source",
    "section": "Description",
    "text": "Description\nWrites a Spark DataFrame into a generic source."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_source.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_source.html#usage",
    "title": "Writes a Spark DataFrame into a generic source",
    "section": "Usage",
    "text": "Usage\nspark_write_source( \n  x, \n  source, \n  mode = NULL, \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_source.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_source.html#arguments",
    "title": "Writes a Spark DataFrame into a generic source",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nsource\nA data source capable of reading data.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_source.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_source.html#see-also",
    "title": "Writes a Spark DataFrame into a generic source",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency_fallback.html",
    "href": "packages/sparklyr/latest/reference/spark_dependency_fallback.html",
    "title": "Fallback to Spark Dependency",
    "section": "",
    "text": "R/spark_extensions.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#spark_dependency_fallback",
    "href": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#spark_dependency_fallback",
    "title": "Fallback to Spark Dependency",
    "section": "spark_dependency_fallback",
    "text": "spark_dependency_fallback"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#description",
    "href": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#description",
    "title": "Fallback to Spark Dependency",
    "section": "Description",
    "text": "Description\nHelper function to assist falling back to previous Spark versions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#usage",
    "title": "Fallback to Spark Dependency",
    "section": "Usage",
    "text": "Usage\nspark_dependency_fallback(spark_version, supported_versions)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#arguments",
    "title": "Fallback to Spark Dependency",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nspark_version\nThe Spark version being requested in spark_dependencies.\n\n\nsupported_versions\nThe Spark versions that are supported by this extension."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#value",
    "href": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#value",
    "title": "Fallback to Spark Dependency",
    "section": "Value",
    "text": "Value\nA Spark version to use."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_describe.html",
    "href": "packages/sparklyr/latest/reference/sdf_describe.html",
    "title": "Compute summary statistics for columns of a data frame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_describe.html#sdf_describe",
    "href": "packages/sparklyr/latest/reference/sdf_describe.html#sdf_describe",
    "title": "Compute summary statistics for columns of a data frame",
    "section": "sdf_describe",
    "text": "sdf_describe"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_describe.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_describe.html#description",
    "title": "Compute summary statistics for columns of a data frame",
    "section": "Description",
    "text": "Description\nCompute summary statistics for columns of a data frame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_describe.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_describe.html#usage",
    "title": "Compute summary statistics for columns of a data frame",
    "section": "Usage",
    "text": "Usage\nsdf_describe(x, cols = colnames(x))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_describe.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_describe.html#arguments",
    "title": "Compute summary statistics for columns of a data frame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercible to a Spark DataFrame\n\n\ncols\nColumns to compute statistics for, given as a character vector"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_memory.html",
    "href": "packages/sparklyr/latest/reference/stream_write_memory.html",
    "title": "Write Memory Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_memory.html#stream_write_memory",
    "href": "packages/sparklyr/latest/reference/stream_write_memory.html#stream_write_memory",
    "title": "Write Memory Stream",
    "section": "stream_write_memory",
    "text": "stream_write_memory"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_memory.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_memory.html#description",
    "title": "Write Memory Stream",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into a memory stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_memory.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_memory.html#usage",
    "title": "Write Memory Stream",
    "section": "Usage",
    "text": "Usage\nstream_write_memory( \n  x, \n  name = random_string(\"sparklyr_tmp_\"), \n  mode = c(\"append\", \"complete\", \"update\"), \n  trigger = stream_trigger_interval(), \n  checkpoint = file.path(\"checkpoints\", name, random_string(\"\")), \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_memory.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_memory.html#arguments",
    "title": "Write Memory Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nname\nThe name to assign to the newly generated stream.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\".\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous.\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_memory.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_memory.html#examples",
    "title": "Write Memory Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\") \ndir.create(\"csv-in\") \nwrite.csv(iris, \"csv-in/data.csv\", row.names = FALSE) \ncsv_path &lt;- file.path(\"file://\", getwd(), \"csv-in\") \nstream &lt;- stream_read_csv(sc, csv_path) %&gt;% stream_write_memory(\"csv-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_memory.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_memory.html#see-also",
    "title": "Write Memory Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_kafka.html",
    "href": "packages/sparklyr/latest/reference/stream_write_kafka.html",
    "title": "Write Kafka Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_kafka.html#stream_write_kafka",
    "href": "packages/sparklyr/latest/reference/stream_write_kafka.html#stream_write_kafka",
    "title": "Write Kafka Stream",
    "section": "stream_write_kafka",
    "text": "stream_write_kafka"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_kafka.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_kafka.html#description",
    "title": "Write Kafka Stream",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into an kafka stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_kafka.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_kafka.html#usage",
    "title": "Write Kafka Stream",
    "section": "Usage",
    "text": "Usage\nstream_write_kafka( \n  x, \n  mode = c(\"append\", \"complete\", \"update\"), \n  trigger = stream_trigger_interval(), \n  checkpoint = file.path(\"checkpoints\", random_string(\"\")), \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_kafka.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_kafka.html#arguments",
    "title": "Write Kafka Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\".\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous.\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_kafka.html#details",
    "href": "packages/sparklyr/latest/reference/stream_write_kafka.html#details",
    "title": "Write Kafka Stream",
    "section": "Details",
    "text": "Details\nPlease note that Kafka requires installing the appropriate package by setting the packages parameter to \"kafka\" in spark_connect()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_kafka.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_kafka.html#examples",
    "title": "Write Kafka Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\", version = \"2.3\", packages = \"kafka\") \nread_options &lt;- list(kafka.bootstrap.servers = \"localhost:9092\", subscribe = \"topic1\") \nwrite_options &lt;- list(kafka.bootstrap.servers = \"localhost:9092\", topic = \"topic2\") \nstream &lt;- stream_read_kafka(sc, options = read_options) %&gt;% \n  stream_write_kafka(options = write_options) \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_kafka.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_kafka.html#see-also",
    "title": "Write Kafka Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-saveload.html",
    "href": "packages/sparklyr/latest/reference/sdf-saveload.html",
    "title": "Save / Load a Spark DataFrame",
    "section": "",
    "text": "R/sdf_saveload.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-saveload.html#sdf-saveload",
    "href": "packages/sparklyr/latest/reference/sdf-saveload.html#sdf-saveload",
    "title": "Save / Load a Spark DataFrame",
    "section": "sdf-saveload",
    "text": "sdf-saveload"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-saveload.html#description",
    "href": "packages/sparklyr/latest/reference/sdf-saveload.html#description",
    "title": "Save / Load a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRoutines for saving and loading Spark DataFrames."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-saveload.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf-saveload.html#usage",
    "title": "Save / Load a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_save_table(x, name, overwrite = FALSE, append = FALSE) \n\nsdf_load_table(sc, name) \n\nsdf_save_parquet(x, path, overwrite = FALSE, append = FALSE) \n\nsdf_load_parquet(sc, path)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-saveload.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf-saveload.html#arguments",
    "title": "Save / Load a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nname\nThe table name to assign to the saved Spark DataFrame.\n\n\noverwrite\nBoolean; overwrite a pre-existing table of the same name?\n\n\nappend\nBoolean; append to a pre-existing table of the same name?\n\n\nsc\nA spark_connection object.\n\n\npath\nThe path where the Spark DataFrame should be saved."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sample.html",
    "href": "packages/sparklyr/latest/reference/sdf_sample.html",
    "title": "Randomly Sample Rows from a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sample.html#sdf_sample",
    "href": "packages/sparklyr/latest/reference/sdf_sample.html#sdf_sample",
    "title": "Randomly Sample Rows from a Spark DataFrame",
    "section": "sdf_sample",
    "text": "sdf_sample"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sample.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_sample.html#description",
    "title": "Randomly Sample Rows from a Spark DataFrame",
    "section": "Description",
    "text": "Description\nDraw a random sample of rows (with or without replacement) from a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sample.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_sample.html#usage",
    "title": "Randomly Sample Rows from a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_sample(x, fraction = 1, replacement = TRUE, seed = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sample.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_sample.html#arguments",
    "title": "Randomly Sample Rows from a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\nfraction\nThe fraction to sample.\n\n\nreplacement\nBoolean; sample with replacement?\n\n\nseed\nAn (optional) integer seed."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sample.html#section",
    "href": "packages/sparklyr/latest/reference/sdf_sample.html#section",
    "title": "Randomly Sample Rows from a Spark DataFrame",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sample.html#transforming-spark-dataframes",
    "href": "packages/sparklyr/latest/reference/sdf_sample.html#transforming-spark-dataframes",
    "title": "Randomly Sample Rows from a Spark DataFrame",
    "section": "Transforming Spark DataFrames",
    "text": "Transforming Spark DataFrames\nThe family of functions prefixed with sdf_ generally access the Scala Spark DataFrame API directly, as opposed to the dplyr interface which uses Spark SQL. These functions will ‘force’ any pending SQL in a dplyr pipeline, such that the resulting tbl_spark object returned will no longer have the attached ‘lazy’ SQL operations. Note that the underlying Spark DataFrame does execute its operations lazily, so that even though the pending set of operations (currently) are not exposed at the R level, these operations will only be executed when you explicitly collect() the table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sample.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_sample.html#see-also",
    "title": "Randomly Sample Rows from a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_copy_to(), sdf_distinct(), sdf_random_split(), sdf_register(), sdf_sort(), sdf_weighted_sample()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc.html",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc.html",
    "title": "Spark ML – LinearSVC",
    "section": "",
    "text": "R/ml_classification_linear_svc.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc.html#ml_linear_svc",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc.html#ml_linear_svc",
    "title": "Spark ML – LinearSVC",
    "section": "ml_linear_svc",
    "text": "ml_linear_svc"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc.html#description",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc.html#description",
    "title": "Spark ML – LinearSVC",
    "section": "Description",
    "text": "Description\nPerform classification using linear support vector machines (SVM). This binary classifier optimizes the Hinge Loss using the OWLQN optimizer. Only supports L2 regularization currently."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc.html#usage",
    "title": "Spark ML – LinearSVC",
    "section": "Usage",
    "text": "Usage\n \nml_linear_svc( \n  x, \n  formula = NULL, \n  fit_intercept = TRUE, \n  reg_param = 0, \n  max_iter = 100, \n  standardization = TRUE, \n  weight_col = NULL, \n  tol = 1e-06, \n  threshold = 0, \n  aggregation_depth = 2, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  raw_prediction_col = \"rawPrediction\", \n  uid = random_string(\"linear_svc_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc.html#arguments",
    "title": "Spark ML – LinearSVC",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nfit_intercept\nBoolean; should the model be fit with an intercept term?\n\n\nreg_param\nRegularization parameter (aka lambda)\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nstandardization\nWhether to standardize the training features before fitting the model.\n\n\nweight_col\nThe name of the column to use as weights for the model fit.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nthreshold\nin binary classification prediction, in range [0, 1].\n\n\naggregation_depth\n(Spark 2.1.0+) Suggested depth for treeAggregate (&gt;= 2).\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc.html#details",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc.html#details",
    "title": "Spark ML – LinearSVC",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc.html#value",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc.html#value",
    "title": "Spark ML – LinearSVC",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc.html#examples",
    "title": "Spark ML – LinearSVC",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nlibrary(dplyr) \n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \npartitions &lt;- iris_tbl %&gt;% \n  filter(Species != \"setosa\") %&gt;% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \n \niris_training &lt;- partitions$training \niris_test &lt;- partitions$test \n \nsvc_model &lt;- iris_training %&gt;% \n  ml_linear_svc(Species ~ .) \n \npred &lt;- ml_predict(svc_model, iris_test) \n \nml_binary_classification_evaluator(pred) \n#&gt; [1] 0.9545455"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc.html#see-also",
    "title": "Spark ML – LinearSVC",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms. Other ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_filter.html",
    "href": "packages/sparklyr/latest/reference/hof_map_filter.html",
    "title": "Filters a map",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_filter.html#hof_map_filter",
    "href": "packages/sparklyr/latest/reference/hof_map_filter.html#hof_map_filter",
    "title": "Filters a map",
    "section": "hof_map_filter",
    "text": "hof_map_filter"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_filter.html#description",
    "href": "packages/sparklyr/latest/reference/hof_map_filter.html#description",
    "title": "Filters a map",
    "section": "Description",
    "text": "Description\nFilters entries in a map using the function specified (this is essentially a dplyr wrapper to the map_filter(expr, func) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_filter.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_map_filter.html#usage",
    "title": "Filters a map",
    "section": "Usage",
    "text": "Usage\n \nhof_map_filter(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_filter.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_map_filter.html#arguments",
    "title": "Filters a map",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\nfunc\nThe filter function to apply (it should take (key, value) as arguments and return a boolean value, with FALSE indicating the key-value pair should be discarded and TRUE otherwise)\n\n\nexpr\nThe map being filtered, could be any SQL expression evaluating to a map (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the filtered result (default: expr)\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_filter.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_map_filter.html#examples",
    "title": "Filters a map",
    "section": "Examples",
    "text": "Examples\n\n \n \nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\", version = \"3.0.0\") \nsdf &lt;- sdf_len(sc, 1) %&gt;% dplyr::mutate(m = map(1, 0, 2, 2, 3, -1)) \nfiltered_sdf &lt;- sdf %&gt;% hof_map_filter(~ .x &gt; .y)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_source.html",
    "href": "packages/sparklyr/latest/reference/spark_read_source.html",
    "title": "Read from a generic source into a Spark DataFrame.",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_source.html#spark_read_source",
    "href": "packages/sparklyr/latest/reference/spark_read_source.html#spark_read_source",
    "title": "Read from a generic source into a Spark DataFrame.",
    "section": "spark_read_source",
    "text": "spark_read_source"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_source.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_source.html#description",
    "title": "Read from a generic source into a Spark DataFrame.",
    "section": "Description",
    "text": "Description\nRead from a generic source into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_source.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_source.html#usage",
    "title": "Read from a generic source into a Spark DataFrame.",
    "section": "Usage",
    "text": "Usage\nspark_read_source( \n  sc, \n  name = NULL, \n  path = name, \n  source, \n  options = list(), \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE, \n  columns = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_source.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_source.html#arguments",
    "title": "Read from a generic source into a Spark DataFrame.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nsource\nA data source capable of reading data.\n\n\noptions\nA list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_source.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_source.html#see-also",
    "title": "Read from a generic source into a Spark DataFrame.",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_render.html",
    "href": "packages/sparklyr/latest/reference/stream_render.html",
    "title": "Render Stream",
    "section": "",
    "text": "R/stream_view.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_render.html#stream_render",
    "href": "packages/sparklyr/latest/reference/stream_render.html#stream_render",
    "title": "Render Stream",
    "section": "stream_render",
    "text": "stream_render"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_render.html#description",
    "href": "packages/sparklyr/latest/reference/stream_render.html#description",
    "title": "Render Stream",
    "section": "Description",
    "text": "Description\nCollects streaming statistics to render the stream as an ‘htmlwidget’."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_render.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_render.html#usage",
    "title": "Render Stream",
    "section": "Usage",
    "text": "Usage\nstream_render(stream = NULL, collect = 10, stats = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_render.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_render.html#arguments",
    "title": "Render Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nstream\nThe stream to render\n\n\ncollect\nThe interval in seconds to collect data before rendering the ‘htmlwidget’.\n\n\nstats\nOptional stream statistics collected using stream_stats(), when specified, stream should be omitted.\n\n\n…\nAdditional optional arguments."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_render.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_render.html#examples",
    "title": "Render Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\") \ndir.create(\"iris-in\") \nwrite.csv(iris, \"iris-in/iris.csv\", row.names = FALSE) \nstream &lt;- stream_read_csv(sc, \"iris-in/\") %&gt;% \n  stream_write_csv(\"iris-out/\") \nstream_render(stream) \n\n\n\n\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/news.html",
    "href": "packages/sparklyr/latest/news.html",
    "title": "Sparklyr 1.7.8",
    "section": "",
    "text": "Adds new metric extraction functions: ml_metrics_binary(), ml_metrics_regression() and ml_metrics_multiclass(). They work closer to how yardstick metric extraction functions work. They expect a table with the predictions and actual values, and returns a concise tibble with the metrics. (#3281)\nAdds new spark_insert_table() function. This allows one to insert data into an existing table definition without redefining the table, even when overwriting the existing data. (#3272 @jimhester)\n\n\n\n\n\nRestores “validator” functions to regression models. Removing them in a previous version broke ml_cross_validator() for regression models. (#3273)\n\n\n\n\n\nAdds support to Spark 3.3 local installation. This includes the ability to enable and setup log4j version 2. (#3269)\nUpdates the JSON file that sparklyr uses to find and download Spark for local use. It is worth mentioning that starting with Spark 3.3, the Hadoop version number is no longer using a minor version for its download link. So, instead of requesting 3.2, the version to request is 3.\n\n\n\n\n\nRemoves workaround for older versions of arrow. Bumps arrow version dependency, from 0.14.0 to 0.17.0 (#3283 @nealrichardson)\nRemoves code related to backwards compatibility with dbplyr. sparklyr requires dbplyr version 2.2.1 or above, so the code is no longer needed. (#3277)\nBegins centralizing ML parameter validation into a single function that will run the proper cast function for each Spark parameter. It also starts using S3 methods, instead of searching for a concatenated function name, to find the proper parameter validator. Regression models are the first ones to use this new method. (#3279)\nsparklyr compilation routines have been improved and simplified.\nspark_compile() now provides more informative output when used. It also adds tests to compilation to make sure. It also adds a step to install Scala in the corresponding GHAs. This is so that the new JAR build tests are able to run. (#3275)\nStops using package environment variables directly. Any package level variable will be handled by a genv prefixed function to set and retrieve values. This avoids the risk of having the exact same variable initialized on more than on R script. (#3274)\nAdds more tests to improve coverage.\n\n\n\n\n\nAddresses new CRAN HTML check NOTEs. It also adds a new GHA action to run the same checks to make sure we avoid new issues with this in the future.",
    "crumbs": [
      "News",
      "Package News"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/news.html#distributed-r-7",
    "href": "packages/sparklyr/latest/news.html#distributed-r-7",
    "title": "Sparklyr 1.7.8",
    "section": "Distributed R",
    "text": "Distributed R\n\nThe memory parameter in spark_apply() now defaults to FALSE when the name parameter is not specified.",
    "crumbs": [
      "News",
      "Package News"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/news.html#other",
    "href": "packages/sparklyr/latest/news.html#other",
    "title": "Sparklyr 1.7.8",
    "section": "Other",
    "text": "Other\n\nRemoved dreprecated sdf_mutate().\nRemove exported ensure_ functions which were deprecated.\nFixed missing Hive tables not rendering under some Spark distributions (#1823).\nRemove dependency on broom.\nFixed re-entrancy job progress issues when running RStudio 1.2.\nTables with periods supported by setting sparklyr.dplyr.period.splits to FALSE.\nsdf_len(), sdf_along() and sdf_seq() default to 32 bit integers but allow support for 64 bits through bits parameter.\nSupport for detecting Spark version using spark-submit.",
    "crumbs": [
      "News",
      "Package News"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/news.html#batches-1",
    "href": "packages/sparklyr/latest/news.html#batches-1",
    "title": "Sparklyr 1.7.8",
    "section": "Batches",
    "text": "Batches\n\nAdded support for spark_submit() to assist submitting non-interactive Spark jobs.\n\n\nSpark ML\n\n(Breaking change) The formula API for ML classification algorithms no longer indexes numeric labels, to avoid the confusion of 0 being mapped to \"1\" and vice versa. This means that if the largest numeric label is N, Spark will fit a N+1-class classification model, regardless of how many distinct labels there are in the provided training set (#1591).\nFix retrieval of coefficients in ml_logistic_regression() (@shabbybanks, #1596).\n(Breaking change) For model objects, lazy val and def attributes have been converted to closures, so they are not evaluated at object instantiation (#1453).\nInput and output column names are no longer required to construct pipeline objects to be consistent with Spark (#1513).\nVector attributes of pipeline stages are now printed correctly (#1618).\nDeprecate various aliases favoring method names in Spark.\n\nml_binary_classification_eval()\nml_classification_eval()\nml_multilayer_perceptron()\nml_survival_regression()\nml_als_factorization()\n\nDeprecate incompatible signatures for sdf_transform() and ml_transform() families of methods; the former should take a tbl_spark as the first argument while the latter should take a model object as the first argument.\nInput and output column names are no longer required to construct pipeline objects to be consistent with Spark (#1513).\n\n\n\nData\n\nImplemented support for DBI::db_explain() (#1623).\nFixed for timestamp fields when using copy_to() (#1312, @yutannihilation).\nAdded support to read and write ORC files using spark_read_orc() and spark_write_orc() (#1548).\n\n\n\nLivy\n\nFixed must share the same src error for sdf_broadcast() and other functions when using Livy connections.\nAdded support for logging sparklyr server events and logging sparklyr invokes as comments in the Livy UI.\nAdded support to open the Livy UI from the connections viewer while using RStudio.\nImprove performance in Livy for long execution queries, fixed livy.session.command.timeout and support for livy.session.command.interval to control max polling while waiting for command response (#1538).\nFixed Livy version with MapR distributions.\nRemoved install column from livy_available_versions().\n\n\n\nDistributed R\n\nAdded name parameter to spark_apply() to optionally name resulting table.\nFix to spark_apply() to retain column types when NAs are present (#1665).\nspark_apply() now supports rlang anonymous functions. For example, sdf_len(sc, 3) %&gt;% spark_apply(~.x+1).\nBreaking Change: spark_apply() no longer defaults to the input column names when the columns parameter is nos specified.\nSupport for reading column names from the R data frame returned by spark_apply().\nFix to support retrieving empty data frames in grouped spark_apply() operations (#1505).\nAdded support for sparklyr.apply.packages to configure default behavior for spark_apply() parameters (#1530).\nAdded support for spark.r.libpaths to configure package library in spark_apply() (#1530).\n\n\n\nConnections\n\nDefault to Spark 2.3.1 for installation and local connections (#1680).\nml_load() no longer keeps extraneous table views which was cluttering up the RStudio Connections pane (@randomgambit, #1549).\nAvoid preparing windows environment in non-local connections.\n\n\n\nExtensions\n\nThe ensure_* family of functions is deprecated in favor of forge which doesn’t use NSE and provides more informative errors messages for debugging (#1514).\nSupport for sparklyr.invoke.trace and sparklyr.invoke.trace.callstack configuration options to trace all invoke() calls.\nSupport to invoke methods with char types using single character strings (@lawremi, #1395).\n\n\n\nSerialization\n\nFixed collection of Date types to support correct local JVM timezone to UTC ().\n\n\n\nDocumentation\n\nMany new examples for ft_binarizer(), ft_bucketizer(), ft_min_max_scaler, ft_max_abs_scaler(), ft_standard_scaler(), ml_kmeans(), ml_pca(), ml_bisecting_kmeans(), ml_gaussian_mixture(), ml_naive_bayes(), ml_decision_tree(), ml_random_forest(), ml_multilayer_perceptron_classifier(), ml_linear_regression(), ml_logistic_regression(), ml_gradient_boosted_trees(), ml_generalized_linear_regression(), ml_cross_validator(), ml_evaluator(), ml_clustering_evaluator(), ml_corr(), ml_chisquare_test() and sdf_pivot() (@samuelmacedo83).\n\n\n\nBroom\n\nImplemented tidy(), augment(), and glance() for ml_aft_survival_regression(), ml_isotonic_regression(), ml_naive_bayes(), ml_logistic_regression(), ml_decision_tree(), ml_random_forest(), ml_gradient_boosted_trees(), ml_bisecting_kmeans(), ml_kmeans()and ml_gaussian_mixture() models (@samuelmacedo83)\n\n\n\nConfiguration\n\nDeprecated configuration option sparklyr.dplyr.compute.nocache.\nAdded spark_config_settings() to list all sparklyr configuration settings and describe them, cleaned all settings and grouped by area while maintaining support for previous settings.\nStatic SQL configuration properties are now respected for Spark 2.3, and spark.sql.catalogImplementation defaults to hive to maintain Hive support (#1496, #415).\nspark_config() values can now also be specified as options().\nSupport for functions as values in entries to spark_config() to enable advanced configuration workflows.",
    "crumbs": [
      "News",
      "Package News"
    ]
  },
  {
    "objectID": "deployment/databricks-connect-udfs.html",
    "href": "deployment/databricks-connect-udfs.html",
    "title": "Run R inside Databricks Connect",
    "section": "",
    "text": "Last updated: Fri Apr 19 08:47:30 2024",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Run R code in Databricks"
    ]
  },
  {
    "objectID": "deployment/databricks-connect-udfs.html#intro",
    "href": "deployment/databricks-connect-udfs.html#intro",
    "title": "Run R inside Databricks Connect",
    "section": "Intro",
    "text": "Intro\nSupport for spark_apply() is available starting with the following package versions:\n\nsparklyr - 1.8.5\npysparklyr - 0.1.4\n\nDatabricks Connect is now able to run regular Python code inside Spark. sparklyr takes advantage of this capability by having Python transport and run the R code. It does this via the rpy2 Python library. Using this library also guarantees Arrow support.\n\n\n\n\n\n\n\n\nflowchart LR\n  subgraph mm[My machine]\n    sp[R &lt;br&gt; **********  &lt;br&gt;sparklyr]\n    rp[Python&lt;br&gt; **************** &lt;br&gt;rpy2 'packages'&lt;br&gt; the R code]\n  end\n  subgraph db[Databricks]\n    subgraph sr[Spark]\n      pt[Python&lt;br&gt; ********************* &lt;br&gt;rpy2 runs the R code]\n    end\n  end\n\nsp --&gt; rp\nrp --&gt; sr\n\nstyle mm   fill:#fff,stroke:#666,color:#000\nstyle sp   fill:#fff,stroke:#666,color:#000\nstyle rp   fill:#fff,stroke:#666,color:#000\nstyle db   fill:#fff,stroke:#666,color:#000\nstyle sr   fill:#fff,stroke:#666,color:#000\nstyle pt   fill:#fff,stroke:#666,color:#000\n\n\n\n\n\n\n\n\nFigure 1: How sparklyr uses rpy2 to run R code in Databricks Connect",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Run R code in Databricks"
    ]
  },
  {
    "objectID": "deployment/databricks-connect-udfs.html#getting-started",
    "href": "deployment/databricks-connect-udfs.html#getting-started",
    "title": "Run R inside Databricks Connect",
    "section": "Getting started",
    "text": "Getting started\nIf you have been using sparklyr with Databricks Connect v2 already, then after upgrading the packages, you will be prompted to install rpy2 in your Python environment. The prompt will occur the first time you use spark_apply() in an interactive R session. If this is the first time you are using sparklyr with Databricks Connect v2, please refer to our intro article“Databricks Connect v2” to learn how to setup your environment.\nAs shown in the diagram on the previous section, rpy2 is needed on the Databricks cluster you plan to use. This means that you will need to “manually” install the library in the cluster. This is a simple operation that is done via your Databricks web portal. Here are the instructions that shows you how to do that: Databricks - Cluster Libraries.",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Run R code in Databricks"
    ]
  },
  {
    "objectID": "deployment/databricks-connect-udfs.html#what-is-supported-in-spark_apply---at-a-glance",
    "href": "deployment/databricks-connect-udfs.html#what-is-supported-in-spark_apply---at-a-glance",
    "title": "Run R inside Databricks Connect",
    "section": "What is supported in spark_apply() - At a glance",
    "text": "What is supported in spark_apply() - At a glance\n\n\n\nArgument\nSupported?\nNotes\n\n\n\n\nx\nYes\n\n\n\nf\nYes\n\n\n\ncolumns\nYes\nRequires a string entry that contains the name of the column and its Spark variable type. Accepted values are: long, decimal, string, datetime and bool. Example: columns = \"x long, y string\". If not provided, sparklyr will automatically create one, by examining the first 10 records of x, and it will provide a columns spec you can use when running spark_apply() again. See: Providing a schema\n\n\nmemory\nYes\n\n\n\ngroup_by\nYes\n\n\n\npackages\nNo\nYou will need to pre-install the needed R packages in your cluster via the Databricks web portal, see R packages\n\n\ncontext\nNo\n\n\n\nname\nYes\n\n\n\nbarrier\nYes\nSupport only on ungrouped data. In other words, it is valid when the group_by argument is used.\n\n\nfetch_result_as_sdf\nYes\nAt this time, spark_apply() inside Databricks Connect only supports rectangular data, so seeing to FALSE will always return a data frame.\n\n\npartition_index_param\nNo\n\n\n\narrow_max_records_per_batch\nYes\nSupport only on ungrouped data. In other words, it is valid when the group_by argument is used.\n\n\nauto_deps\nNo\n\n\n\n...",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Run R code in Databricks"
    ]
  },
  {
    "objectID": "deployment/databricks-connect-udfs.html#r-packages",
    "href": "deployment/databricks-connect-udfs.html#r-packages",
    "title": "Run R inside Databricks Connect",
    "section": "R packages",
    "text": "R packages\nIf your spark_apply() call uses specific R packages, you will need to pre-install those specific packages in your target cluster. This is a simple operation, because you can do this via your Databricks web portal, please see Databricks - Cluster Libraries to learn how to do this.\n\n\n\n\n\n\nOnly CRAN packages supported\n\n\n\nThe Databricks cluster library interface is able to source packages from CRAN only. This means that packages installed from GitHub, or another alternative sources, will not be available.\n\n\n\nAdditional background\nIn previous implementation, spark_apply() was able to easily copy the locally installed R packages in order to ensure that your code will run in the cluster. This was possible because R, and RStudio, was running in one of the matching servers in the Spark cluster. Because sparklyr is running on a remote machine, more likely a laptop, this is no longer an option. In the vast majority of cases, the remote machine will be on different a Operating System than the cluster. Additionally, transmitting the unpacked, compiled, R packages would take a long time over a broadband Internet connection.",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Run R code in Databricks"
    ]
  },
  {
    "objectID": "deployment/databricks-connect-udfs.html#providing-a-schema",
    "href": "deployment/databricks-connect-udfs.html#providing-a-schema",
    "title": "Run R inside Databricks Connect",
    "section": "Providing a schema",
    "text": "Providing a schema\nPassing a schema in columns will makespark_apply() run faster. Because if not provided, sparklyr has to collect the first 10 rows, and run the R code in order to try and determine the names and types of your resulting data set. As a convenience, sparklyr will output a message with the schema it used as the schema. If you are going to rerun your spark_apply() command again, you can copy and paste the output of the message to you code.\n\nspark_apply(\n  tbl_mtcars,\n  nrow,\n  group_by = \"am\"\n)\n#&gt; To increase performance, use the following schema:\n#&gt; columns = \"am double, x long\"\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_b84460ea_b1d3_471b_9cef_b13f339819b6`&gt; [2 x 2]\n#&gt; # Database: spark_connection\n#&gt;      am     x\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     0    19\n#&gt; 2     1    13\n\nPassing the columns argument, silences the message:\n\nspark_apply(\n  tbl_mtcars,\n  nrow,\n  group_by = \"am\", \n  columns = \"am double, x long\"\n)\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_e2b75205_e82e_43c1_ad5b_60944ed8ed65`&gt; [2 x 2]\n#&gt; # Database: spark_connection\n#&gt;      am     x\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     0    19\n#&gt; 2     1    13",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Run R code in Databricks"
    ]
  },
  {
    "objectID": "deployment/databricks-connect-udfs.html#partition-data",
    "href": "deployment/databricks-connect-udfs.html#partition-data",
    "title": "Run R inside Databricks Connect",
    "section": "Partition data",
    "text": "Partition data\nTypically, with un-grouped data, the number of parallel jobs will correspond with the number of partitions of the data. For Databricks connections, sparklyr will, by default, attempt to use Apache Arrow. The Databricks Connect clusters come with Arrow installed. This approach also changes how Spark will partition your data. Instead of the number of partitions, Spark will use the value in the “Arrow Max Records per Bach” option. This option can be controlled directly in the spark_apply() call by setting the arrow_max_records_per_batch.\n\nspark_apply(tbl_mtcars, nrow, arrow_max_records_per_batch = 4, columns = \"x long\")\n#&gt; Changing spark.sql.execution.arrow.maxRecordsPerBatch to: 4\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_cb0e87af_2c9a_459d_9dd0_05a7522c4c21`&gt; [8 x 1]\n#&gt; # Database: spark_connection\n#&gt;       x\n#&gt;   &lt;dbl&gt;\n#&gt; 1     4\n#&gt; 2     4\n#&gt; 3     4\n#&gt; 4     4\n#&gt; 5     4\n#&gt; 6     4\n#&gt; 7     4\n#&gt; 8     4\n\nIf you pass a different Arrow Batch size than what the option is set to currently, sparklyr will change the value of that option, and will notify you of that:\n\nspark_apply(tbl_mtcars, nrow, arrow_max_records_per_batch = 2, columns = \"x long\")\n#&gt; Changing spark.sql.execution.arrow.maxRecordsPerBatch to: 2\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_c3e3e281_a330_4f32_91cd_ffe27c456247`&gt; [?? x 1]\n#&gt; # Database: spark_connection\n#&gt;        x\n#&gt;    &lt;dbl&gt;\n#&gt;  1     2\n#&gt;  2     2\n#&gt;  3     2\n#&gt;  4     2\n#&gt;  5     2\n#&gt;  6     2\n#&gt;  7     2\n#&gt;  8     2\n#&gt;  9     2\n#&gt; 10     2\n#&gt; # ℹ more rows",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Run R code in Databricks"
    ]
  },
  {
    "objectID": "deployment/databricks-connect-udfs.html#limitations",
    "href": "deployment/databricks-connect-udfs.html#limitations",
    "title": "Run R inside Databricks Connect",
    "section": "Limitations",
    "text": "Limitations\nspark_apply() will only work on Databricks “Single Access” mode. “Shared Access” mode does not currently support mapInPandas(), and applyInPandas() (see Databricks - Access mode limitations). These are the Python functions that sparklyr uses to run the Python code, which in turn runs the R code via rpy2.",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Run R code in Databricks"
    ]
  },
  {
    "objectID": "deployment/stand-alone-aws.html",
    "href": "deployment/stand-alone-aws.html",
    "title": "Spark Standalone Deployment in AWS",
    "section": "",
    "text": "The plan is to launch 4 identical EC2 server instances. One server will be the Master node and the other 3 the worker nodes. In one of the worker nodes, we will install RStudio server.\nWhat makes a server the Master node is only the fact that it is running the master service, while the other machines are running the slave service and are pointed to that first master. This simple setup, allows us to install the same Spark components on all 4 servers and then just add RStudio to one of them.\nThe topology will look something like this:",
    "crumbs": [
      "Deployment",
      "Stand Alone Clusters",
      "Standalone cluster using AWS EC2"
    ]
  },
  {
    "objectID": "deployment/stand-alone-aws.html#overview",
    "href": "deployment/stand-alone-aws.html#overview",
    "title": "Spark Standalone Deployment in AWS",
    "section": "",
    "text": "The plan is to launch 4 identical EC2 server instances. One server will be the Master node and the other 3 the worker nodes. In one of the worker nodes, we will install RStudio server.\nWhat makes a server the Master node is only the fact that it is running the master service, while the other machines are running the slave service and are pointed to that first master. This simple setup, allows us to install the same Spark components on all 4 servers and then just add RStudio to one of them.\nThe topology will look something like this:",
    "crumbs": [
      "Deployment",
      "Stand Alone Clusters",
      "Standalone cluster using AWS EC2"
    ]
  },
  {
    "objectID": "deployment/stand-alone-aws.html#aws-ec-instances",
    "href": "deployment/stand-alone-aws.html#aws-ec-instances",
    "title": "Spark Standalone Deployment in AWS",
    "section": "AWS EC Instances",
    "text": "AWS EC Instances\nHere are the details of the EC2 instance, just deploy one at this point:\n\nType: t2.medium\nOS: Ubuntu 16.04 LTS\nDisk space: At least 20GB\nSecurity group: Open the following ports: 8080 (Spark UI), 4040 (Spark Worker UI), 8088 (sparklyr UI) and 8787 (RStudio). Also open All TCP ports for the machines inside the security group.",
    "crumbs": [
      "Deployment",
      "Stand Alone Clusters",
      "Standalone cluster using AWS EC2"
    ]
  },
  {
    "objectID": "deployment/stand-alone-aws.html#spark",
    "href": "deployment/stand-alone-aws.html#spark",
    "title": "Spark Standalone Deployment in AWS",
    "section": "Spark",
    "text": "Spark\nPerform the steps in this section on all of the servers that will be part of the cluster.\n\nInstall Java 8\n\nWe will add the Java 8 repository, install it and set it as default\n\n\nsudo apt-add-repository ppa:webupd8team/java\nsudo apt-get update\nsudo apt-get install oracle-java8-installer\nsudo apt-get install oracle-java8-set-default\nsudo apt-get update\nor alternatively, run\nsudo apt install openjdk-8-jdk\nto install Open JDK version 8.\n\n\nDownload Spark\n\nDownload and unpack a pre-compiled version of Spark. Here’s is the link to the official Spark download page\n\n\nwget http://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-hadoop2.7.tgz\ntar -xvzf spark-2.1.0-bin-hadoop2.7.tgz\ncd spark-2.1.0-bin-hadoop2.7\n\n\nCreate and launch AMI\n\nWe will create an image of the server. In Amazon, these are called AMIs, for information please see the User Guide.\nLaunch 3 instances of the AMI",
    "crumbs": [
      "Deployment",
      "Stand Alone Clusters",
      "Standalone cluster using AWS EC2"
    ]
  },
  {
    "objectID": "deployment/stand-alone-aws.html#rstudio-server",
    "href": "deployment/stand-alone-aws.html#rstudio-server",
    "title": "Spark Standalone Deployment in AWS",
    "section": "RStudio Server",
    "text": "RStudio Server\nSelect one of the nodes to execute this section. Please check the RStudio download page for the latest version\n\nInstall R\n\nIn order to get the latest R core, we will need to update the source list in Ubuntu.\n\n\nsudo sh -c 'echo \"deb http://cran.rstudio.com/bin/linux/ubuntu xenial/\" &gt;&gt; /etc/apt/sources.list'\ngpg --keyserver keyserver.ubuntu.com --recv-key 0x517166190x51716619e084dab9\ngpg -a --export 0x517166190x51716619e084dab9 | sudo apt-key add -\nsudo apt-get update\n\nNow we can install R\n\n\nsudo apt-get install r-base\nsudo apt-get install gdebi-core\n\n\nInstall RStudio\n\nWe will download and install 1.044 of RStudio Server. To find the latest version, please visit the RStudio website. In order to get the enhanced integration with Spark, RStudio version 1.044 or later will be needed.\n\n\nwget https://download2.rstudio.org/rstudio-server-1.0.153-amd64.deb\nsudo gdebi rstudio-server-1.0.153-amd64.deb\n\n\nInstall dependencies\n\nRun the following commands\n\n\nsudo apt-get -y install libcurl4-gnutls-dev\nsudo apt-get -y install libssl-dev\nsudo apt-get -y install libxml2-dev\n\n\nAdd default user\n\nRun the following command to add a default user\n\n\nsudo adduser rstudio-user\n\n\nStart the Master node\n\nSelect one of the servers to become your Master node\nRun the command that starts the master service\n\n\nsudo spark-2.1.0-bin-hadoop2.7/sbin/start-master.sh\n\nClose the terminal connection (optional)\n\n\n\nStart Worker nodes\n\nStart the “slave” service. Important: Use dots not dashes as separators for the Spark Master node’s address\n\n\nsudo spark-2.1.0-bin-hadoop2.7/sbin/start-slave.sh spark://[Master node's IP address]:7077\nsudo spark-2.1.0-bin-hadoop2.7/sbin/start-slave.sh spark://ip-172-30-1-94.us-west-2.compute.internal:7077\n\nClose the terminal connection (optional)\n\n\n\nPre-load pacakges\n\nLog into RStudio (port 8787)\nUse ‘rstudio-user’\n\ninstall.packages(\"sparklyr\")\n\n\nConnect to the Spark Master\n\nNavigate to the Spark Master’s UI, typically on port 8080\n\n\n\nNote the Spark Master URL\nLogon to RStudio\nRun the following code\n\nlibrary(sparklyr)\n\nconf &lt;- spark_config()\nconf$spark.executor.memory &lt;- \"2GB\"\nconf$spark.memory.fraction &lt;- 0.9\n\nsc &lt;- spark_connect(master=\"[Spark Master URL]\",\n              version = \"2.1.0\",\n              config = conf,\n              spark_home = \"/home/ubuntu/spark-2.1.0-bin-hadoop2.7/\"\n              )",
    "crumbs": [
      "Deployment",
      "Stand Alone Clusters",
      "Standalone cluster using AWS EC2"
    ]
  },
  {
    "objectID": "deployment/data-lakes.html",
    "href": "deployment/data-lakes.html",
    "title": "Data Lakes",
    "section": "",
    "text": "This article explains how to take advantage of Apache Spark at organizations that have a Hadoop based Big Data Lake.",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Understanding Data Lakes"
    ]
  },
  {
    "objectID": "deployment/data-lakes.html#audience",
    "href": "deployment/data-lakes.html#audience",
    "title": "Data Lakes",
    "section": "",
    "text": "This article explains how to take advantage of Apache Spark at organizations that have a Hadoop based Big Data Lake.",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Understanding Data Lakes"
    ]
  },
  {
    "objectID": "deployment/data-lakes.html#introduction",
    "href": "deployment/data-lakes.html#introduction",
    "title": "Data Lakes",
    "section": "Introduction",
    "text": "Introduction\nWe have noticed that the types of questions we field after a demo of sparklyr to our customers were more about high-level architecture than how the package works. To answer those questions, we put together a set of slides that illustrate and discuss important concepts, to help customers see where Spark, R, and sparklyr fit in a Big Data Platform implementation. In this article, we’ll review those slides and provide a narrative that will help you better envision how you can take advantage of our products.",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Understanding Data Lakes"
    ]
  },
  {
    "objectID": "deployment/data-lakes.html#r-for-data-science",
    "href": "deployment/data-lakes.html#r-for-data-science",
    "title": "Data Lakes",
    "section": "R for Data Science",
    "text": "R for Data Science\nIt is very important to preface the Use Case review with some background information about where RStudio focuses its efforts when developing packages and products. Many vendors offer R integration, but in most cases, what this means is that they will add a model built in R to their pipeline or interface, and pass new inputs to that model to generate outputs that can be used in the next step in the pipeline, or in a calculation for the interface.\nIn contrast, our focus is on the process that happens before that: the discipline that produces the model, meaning Data Science.\n\nIn their R for Data Science book, Hadley Wickham and Garrett Grolemund provide a great diagram that nicely illustrates the Data Science process:\n\nWe import data into memory with R\nClean and tidy the data\nDive into a cyclical process called understand, which helps us to get to know our data, and hopefully find the answer to the question we started with. This cycle typically involves making transformations to our tidied data, using the transformed data to fit models, and visualizing results.\nOnce we find an answer to our question, we then communicate the results.\n\nData Scientists like using R because it allows them to complete a Data Science project from beginning to end inside the R environment, and in memory.",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Understanding Data Lakes"
    ]
  },
  {
    "objectID": "deployment/data-lakes.html#hadoop-as-a-data-source",
    "href": "deployment/data-lakes.html#hadoop-as-a-data-source",
    "title": "Data Lakes",
    "section": "Hadoop as a Data Source",
    "text": "Hadoop as a Data Source\nWhat happens when the data that needs to be analyzed is very large, like the data sets found in a Hadoop cluster? It would be impossible to fit these in memory, so workarounds are normally used. Possible workarounds include using a comparatively minuscule data sample, or download as much data as possible. This becomes disruptive to Data Scientists because either the small sample may not be representative, or they have to wait a long time in every iteration of importing a lot of data, exploring a lot of data, and modeling a lot of data.",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Understanding Data Lakes"
    ]
  },
  {
    "objectID": "deployment/data-lakes.html#spark-as-an-analysis-engine",
    "href": "deployment/data-lakes.html#spark-as-an-analysis-engine",
    "title": "Data Lakes",
    "section": "Spark as an Analysis Engine",
    "text": "Spark as an Analysis Engine\nWe noticed that a very important mental leap to make is to see Spark not just as a gateway to Hadoop (or worse, as an additional data source), but as a computing engine. As such, it is an excellent vehicle to scale our analytics. Spark has many capabilities that makes it ideal for Data Science in a data lake, such as close integration with Hadoop and Hive, the ability to cache data into memory across multiple nodes, data transformers, and its Machine Learning libraries.\nThe approach, then, is to push as much compute to the cluster as possible, using R primarily as an interface to Spark for the Data Scientist, which will then collect as few results as possible back into R memory, mostly to visualize and communicate. As shown in the slide, the more import, tidy, transform and modeling work we can push to Spark, the faster we can analyze very large data sets.",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Understanding Data Lakes"
    ]
  },
  {
    "objectID": "deployment/data-lakes.html#cluster-setup",
    "href": "deployment/data-lakes.html#cluster-setup",
    "title": "Data Lakes",
    "section": "Cluster Setup",
    "text": "Cluster Setup\nHere is an illustration of how R, RStudio, and sparklyr can be added to the YARN managed cluster. The highlights are:\n\nR, RStudio, and sparklyr need to be installed on one node only, typically an edge node\nThe Data Scientist can access R, Spark, and the cluster via a web browser by navigating to the RStudio IDE inside the edge node",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Understanding Data Lakes"
    ]
  },
  {
    "objectID": "deployment/data-lakes.html#considerations",
    "href": "deployment/data-lakes.html#considerations",
    "title": "Data Lakes",
    "section": "Considerations",
    "text": "Considerations\nThere are some important considerations to keep in mind when combining your Data Lake and R for large scale analytics:\n\nSpark’s Machine Learning libraries may not contain specific models that a Data Scientist needs. For those cases, workarounds would include using a sparklyr extension such as H2O, or collecting a sample of the data into R memory for modeling.\nSpark does not have visualization functionality; currently, the best approach is to collect pre-calculated data into R for plotting. A good way to drastically reduce the number of rows being brought back into memory is to push as much computation as possible to Spark, and return just the results to be plotted. For example, the bins of a Histogram can be calculated in Spark, so that only the final bucket values would be returned to R for visualization.\nA particular use case may require a different way of scaling analytics. We have published an article that provides a very good overview of the options that are available: R for Enterprise: How to Scale Your Analytics Using R",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Understanding Data Lakes"
    ]
  },
  {
    "objectID": "deployment/data-lakes.html#r-for-data-science-toolchain-with-spark",
    "href": "deployment/data-lakes.html#r-for-data-science-toolchain-with-spark",
    "title": "Data Lakes",
    "section": "R for Data Science Toolchain with Spark",
    "text": "R for Data Science Toolchain with Spark\nWith sparklyr, the Data Scientist will be able to access the Data Lake’s data, and also gain an additional, very powerful understand layer via Spark. sparklyr, along with the RStudio IDE and the tidyverse packages, provides the Data Scientist with an excellent toolbox to analyze data, big and small.",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Understanding Data Lakes"
    ]
  },
  {
    "objectID": "deployment/index.html",
    "href": "deployment/index.html",
    "title": "Deployment",
    "section": "",
    "text": "YARN (Hadoop)\n\nUnderstanding Data Lakes\nSetting up an AWS EMR Cluster\nSetting up a Cloudera cluster in AWS\n\n\n\n\nStand Alone\n\nQubole cluster\nSetting up a Standalone Cluster in AWS EC2\n\n\n\nDatabricks Connevt (v2)\n\nGetting Started\n\nRun R code in Databricks\nDeploying to Posit Connect",
    "crumbs": [
      "Deployment",
      "Overview"
    ]
  },
  {
    "objectID": "deployment/qubole-cluster.html",
    "href": "deployment/qubole-cluster.html",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "",
    "text": "Qubole users can request access to RStudio Server Pro. This allows users to use sparklyr to interact directly with Spark from within the Qubole cluster."
  },
  {
    "objectID": "deployment/qubole-cluster.html#overview",
    "href": "deployment/qubole-cluster.html#overview",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "",
    "text": "Qubole users can request access to RStudio Server Pro. This allows users to use sparklyr to interact directly with Spark from within the Qubole cluster."
  },
  {
    "objectID": "deployment/qubole-cluster.html#advantages-and-limitations",
    "href": "deployment/qubole-cluster.html#advantages-and-limitations",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Advantages and limitations",
    "text": "Advantages and limitations\nAdvantages:\n\nAbility for users to connect sparklyr directly to Spark within Qubole\nProvides a high-bandwidth connection between R and the Spark JVM processes because they are running on the same machine\nCan load data from the cluster directly into an R session since RStudio Workbench is installed within the Qubole cluster\nA unique, persistent home directory for each user\n\nLimitations:\n\nPersistent packages must be managed using Qubole Environments, not directly from within RStudio\nRStudio Workbench installed within a Qubole cluster will be limited to the compute resources and lifecycle of that particular Spark cluster\nNon-Spark jobs will use CPU and RAM resources within the Qubole cluster"
  },
  {
    "objectID": "deployment/qubole-cluster.html#access-rstudio-workbench",
    "href": "deployment/qubole-cluster.html#access-rstudio-workbench",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Access RStudio Workbench",
    "text": "Access RStudio Workbench\nRStudio Workbench can be accessed from the cluster resources menu:"
  },
  {
    "objectID": "deployment/qubole-cluster.html#use-sparklyr",
    "href": "deployment/qubole-cluster.html#use-sparklyr",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Use sparklyr",
    "text": "Use sparklyr\nUse the following R code to establish a connection from sparklyr to the Qubole cluster:\nlibrary(sparklyr)\nsc &lt;- spark_connect(method = \"qubole\")"
  },
  {
    "objectID": "deployment/qubole-cluster.html#additional-information",
    "href": "deployment/qubole-cluster.html#additional-information",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Additional information",
    "text": "Additional information\nFor more information on using RStudio Workbench inside of Qubole, refer to the Qubole documentation."
  },
  {
    "objectID": "guides/distributed-r.html",
    "href": "guides/distributed-r.html",
    "title": "Distributing R Computations",
    "section": "",
    "text": "sparklyr provides support to run arbitrary R code at scale within your Spark Cluster through spark_apply(). This is especially useful where there is a need to use functionality available only in R or R packages that is not available in Apache Spark nor Spark Packages.\nspark_apply() applies an R function to a Spark object (typically, a Spark DataFrame). Spark objects are partitioned so they can be distributed across a cluster. You can use spark_apply() with the default partitions or you can define your own partitions with the group_by() argument. Your R function must return another Spark DataFrame. spark_apply() will run your R function on each partition and output a single Spark DataFrame.\n\n\nLets run a simple example. We will apply the identify function, I(), over a list of numbers we created with the sdf_len() function.\n\nlibrary(sparklyr)\n\nsc &lt;- spark_connect(master = \"local\")\n\nsdf_len(sc, 5, repartition = 1) %&gt;%\n  spark_apply(function(e) I(e))\n#&gt; # Source: spark&lt;?&gt; [?? x 1]\n#&gt;      id\n#&gt;   &lt;int&gt;\n#&gt; 1     1\n#&gt; 2     2\n#&gt; 3     3\n#&gt; 4     4\n#&gt; 5     5\n\nYour R function should be designed to operate on an R data frame. The R function passed to spark_apply() expects a DataFrame and will return an object that can be cast as a DataFrame. We can use the class function to verify the class of the data.\n\nsdf_len(sc, 10, repartition = 1) %&gt;%\n  spark_apply(function(e) class(e))\n#&gt; # Source: spark&lt;?&gt; [?? x 1]\n#&gt;   result    \n#&gt;   &lt;chr&gt;     \n#&gt; 1 tbl_df    \n#&gt; 2 tbl       \n#&gt; 3 data.frame\n\nSpark will partition your data by hash or range so it can be distributed across a cluster. In the following example we create two partitions and count the number of rows in each partition. Then we print the first record in each partition.\n\ntrees_tbl &lt;- sdf_copy_to(sc, trees, repartition = 2)\n\nspark_apply(\n  trees_tbl,\n  function(e) nrow(e), names = \"n\"\n  )\n#&gt; # Source: spark&lt;?&gt; [?? x 1]\n#&gt;       n\n#&gt;   &lt;int&gt;\n#&gt; 1    15\n#&gt; 2    16\n\n\nspark_apply(trees_tbl, function(e) head(e, 1))\n#&gt; # Source: spark&lt;?&gt; [?? x 3]\n#&gt;   Girth Height Volume\n#&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1   8.3     70   10.3\n#&gt; 2  12.9     74   22.2\n\nWe can apply any arbitrary function to the partitions in the Spark DataFrame. For instance, we can scale or jitter the columns. Notice that spark_apply() applies the R function to all partitions and returns a single DataFrame.\n\nspark_apply(trees_tbl, function(e) scale(e))\n#&gt; # Source: spark&lt;?&gt; [?? x 3]\n#&gt;      Girth Height  Volume\n#&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 -2.05   -0.607 -1.69  \n#&gt;  2 -1.79   -1.43  -1.69  \n#&gt;  3 -1.62   -1.76  -1.71  \n#&gt;  4 -0.134  -0.276 -0.339 \n#&gt;  5  0.0407  1.21   0.191 \n#&gt;  6  0.128   1.54   0.390 \n#&gt;  7  0.302  -1.27  -0.515 \n#&gt;  8  0.302   0.221  0.0589\n#&gt;  9  0.389   1.05   1.03  \n#&gt; 10  0.477   0.221  0.434 \n#&gt; # … with more rows\n\n\nspark_apply(trees_tbl, function(e) lapply(e, jitter))\n#&gt; # Source: spark&lt;?&gt; [?? x 3]\n#&gt;    Girth Height Volume\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1  8.29   70.0   10.3\n#&gt;  2  8.60   65.0   10.3\n#&gt;  3  8.82   63.0   10.2\n#&gt;  4 10.5    72.1   16.4\n#&gt;  5 10.7    80.8   18.8\n#&gt;  6 10.8    83.0   19.7\n#&gt;  7 11.0    66.1   15.6\n#&gt;  8 11.0    75.1   18.2\n#&gt;  9 11.1    80.2   22.6\n#&gt; 10 11.2    75.2   19.9\n#&gt; # … with more rows\n\nBy default spark_apply() derives the column names from the input Spark data frame. Use the names argument to rename or add new columns.\n\nspark_apply(\n  trees_tbl,\n  function(e) data.frame(2.54 * e$Girth, e), names = c(\"Girth(cm)\", colnames(trees))\n  )\n#&gt; # Source: spark&lt;?&gt; [?? x 4]\n#&gt;    `Girth(cm)` Girth Height Volume\n#&gt;          &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1        21.1   8.3     70   10.3\n#&gt;  2        21.8   8.6     65   10.3\n#&gt;  3        22.4   8.8     63   10.2\n#&gt;  4        26.7  10.5     72   16.4\n#&gt;  5        27.2  10.7     81   18.8\n#&gt;  6        27.4  10.8     83   19.7\n#&gt;  7        27.9  11       66   15.6\n#&gt;  8        27.9  11       75   18.2\n#&gt;  9        28.2  11.1     80   22.6\n#&gt; 10        28.4  11.2     75   19.9\n#&gt; # … with more rows\n\n\n\n\nIn some cases you may want to apply your R function to specific groups in your data. For example, suppose you want to compute regression models against specific subgroups. To solve this, you can specify a group_by() argument. This example counts the number of rows in iris by species and then fits a simple linear model for each species.\n\niris_tbl &lt;- sdf_copy_to(sc, iris)\n\nspark_apply(iris_tbl, nrow, group_by = \"Species\")\n#&gt; # Source: spark&lt;?&gt; [?? x 2]\n#&gt;   Species    result\n#&gt;   &lt;chr&gt;       &lt;int&gt;\n#&gt; 1 versicolor     50\n#&gt; 2 virginica      50\n#&gt; 3 setosa         50\n\n\niris_tbl %&gt;%\n  spark_apply(\n    function(e) summary(lm(Petal_Length ~ Petal_Width, e))$r.squared,\n    names = \"r.squared\",\n    group_by = \"Species\"\n    )\n#&gt; # Source: spark&lt;?&gt; [?? x 2]\n#&gt;   Species    r.squared\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 versicolor     0.619\n#&gt; 2 virginica      0.104\n#&gt; 3 setosa         0.110",
    "crumbs": [
      "Guides",
      "Advanced",
      "Run R code inside Spark"
    ]
  },
  {
    "objectID": "guides/distributed-r.html#overview",
    "href": "guides/distributed-r.html#overview",
    "title": "Distributing R Computations",
    "section": "",
    "text": "sparklyr provides support to run arbitrary R code at scale within your Spark Cluster through spark_apply(). This is especially useful where there is a need to use functionality available only in R or R packages that is not available in Apache Spark nor Spark Packages.\nspark_apply() applies an R function to a Spark object (typically, a Spark DataFrame). Spark objects are partitioned so they can be distributed across a cluster. You can use spark_apply() with the default partitions or you can define your own partitions with the group_by() argument. Your R function must return another Spark DataFrame. spark_apply() will run your R function on each partition and output a single Spark DataFrame.\n\n\nLets run a simple example. We will apply the identify function, I(), over a list of numbers we created with the sdf_len() function.\n\nlibrary(sparklyr)\n\nsc &lt;- spark_connect(master = \"local\")\n\nsdf_len(sc, 5, repartition = 1) %&gt;%\n  spark_apply(function(e) I(e))\n#&gt; # Source: spark&lt;?&gt; [?? x 1]\n#&gt;      id\n#&gt;   &lt;int&gt;\n#&gt; 1     1\n#&gt; 2     2\n#&gt; 3     3\n#&gt; 4     4\n#&gt; 5     5\n\nYour R function should be designed to operate on an R data frame. The R function passed to spark_apply() expects a DataFrame and will return an object that can be cast as a DataFrame. We can use the class function to verify the class of the data.\n\nsdf_len(sc, 10, repartition = 1) %&gt;%\n  spark_apply(function(e) class(e))\n#&gt; # Source: spark&lt;?&gt; [?? x 1]\n#&gt;   result    \n#&gt;   &lt;chr&gt;     \n#&gt; 1 tbl_df    \n#&gt; 2 tbl       \n#&gt; 3 data.frame\n\nSpark will partition your data by hash or range so it can be distributed across a cluster. In the following example we create two partitions and count the number of rows in each partition. Then we print the first record in each partition.\n\ntrees_tbl &lt;- sdf_copy_to(sc, trees, repartition = 2)\n\nspark_apply(\n  trees_tbl,\n  function(e) nrow(e), names = \"n\"\n  )\n#&gt; # Source: spark&lt;?&gt; [?? x 1]\n#&gt;       n\n#&gt;   &lt;int&gt;\n#&gt; 1    15\n#&gt; 2    16\n\n\nspark_apply(trees_tbl, function(e) head(e, 1))\n#&gt; # Source: spark&lt;?&gt; [?? x 3]\n#&gt;   Girth Height Volume\n#&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1   8.3     70   10.3\n#&gt; 2  12.9     74   22.2\n\nWe can apply any arbitrary function to the partitions in the Spark DataFrame. For instance, we can scale or jitter the columns. Notice that spark_apply() applies the R function to all partitions and returns a single DataFrame.\n\nspark_apply(trees_tbl, function(e) scale(e))\n#&gt; # Source: spark&lt;?&gt; [?? x 3]\n#&gt;      Girth Height  Volume\n#&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 -2.05   -0.607 -1.69  \n#&gt;  2 -1.79   -1.43  -1.69  \n#&gt;  3 -1.62   -1.76  -1.71  \n#&gt;  4 -0.134  -0.276 -0.339 \n#&gt;  5  0.0407  1.21   0.191 \n#&gt;  6  0.128   1.54   0.390 \n#&gt;  7  0.302  -1.27  -0.515 \n#&gt;  8  0.302   0.221  0.0589\n#&gt;  9  0.389   1.05   1.03  \n#&gt; 10  0.477   0.221  0.434 \n#&gt; # … with more rows\n\n\nspark_apply(trees_tbl, function(e) lapply(e, jitter))\n#&gt; # Source: spark&lt;?&gt; [?? x 3]\n#&gt;    Girth Height Volume\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1  8.29   70.0   10.3\n#&gt;  2  8.60   65.0   10.3\n#&gt;  3  8.82   63.0   10.2\n#&gt;  4 10.5    72.1   16.4\n#&gt;  5 10.7    80.8   18.8\n#&gt;  6 10.8    83.0   19.7\n#&gt;  7 11.0    66.1   15.6\n#&gt;  8 11.0    75.1   18.2\n#&gt;  9 11.1    80.2   22.6\n#&gt; 10 11.2    75.2   19.9\n#&gt; # … with more rows\n\nBy default spark_apply() derives the column names from the input Spark data frame. Use the names argument to rename or add new columns.\n\nspark_apply(\n  trees_tbl,\n  function(e) data.frame(2.54 * e$Girth, e), names = c(\"Girth(cm)\", colnames(trees))\n  )\n#&gt; # Source: spark&lt;?&gt; [?? x 4]\n#&gt;    `Girth(cm)` Girth Height Volume\n#&gt;          &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1        21.1   8.3     70   10.3\n#&gt;  2        21.8   8.6     65   10.3\n#&gt;  3        22.4   8.8     63   10.2\n#&gt;  4        26.7  10.5     72   16.4\n#&gt;  5        27.2  10.7     81   18.8\n#&gt;  6        27.4  10.8     83   19.7\n#&gt;  7        27.9  11       66   15.6\n#&gt;  8        27.9  11       75   18.2\n#&gt;  9        28.2  11.1     80   22.6\n#&gt; 10        28.4  11.2     75   19.9\n#&gt; # … with more rows\n\n\n\n\nIn some cases you may want to apply your R function to specific groups in your data. For example, suppose you want to compute regression models against specific subgroups. To solve this, you can specify a group_by() argument. This example counts the number of rows in iris by species and then fits a simple linear model for each species.\n\niris_tbl &lt;- sdf_copy_to(sc, iris)\n\nspark_apply(iris_tbl, nrow, group_by = \"Species\")\n#&gt; # Source: spark&lt;?&gt; [?? x 2]\n#&gt;   Species    result\n#&gt;   &lt;chr&gt;       &lt;int&gt;\n#&gt; 1 versicolor     50\n#&gt; 2 virginica      50\n#&gt; 3 setosa         50\n\n\niris_tbl %&gt;%\n  spark_apply(\n    function(e) summary(lm(Petal_Length ~ Petal_Width, e))$r.squared,\n    names = \"r.squared\",\n    group_by = \"Species\"\n    )\n#&gt; # Source: spark&lt;?&gt; [?? x 2]\n#&gt;   Species    r.squared\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 versicolor     0.619\n#&gt; 2 virginica      0.104\n#&gt; 3 setosa         0.110",
    "crumbs": [
      "Guides",
      "Advanced",
      "Run R code inside Spark"
    ]
  },
  {
    "objectID": "guides/distributed-r.html#distributing-packages",
    "href": "guides/distributed-r.html#distributing-packages",
    "title": "Distributing R Computations",
    "section": "Distributing Packages",
    "text": "Distributing Packages\nWith spark_apply() you can use any R package inside Spark. For instance, you can use the broom package to create a tidy data frame from linear regression output.\n\nspark_apply(\n  iris_tbl,\n  function(e) broom::tidy(lm(Petal_Length ~ Petal_Width, e)),\n  names = c(\"term\", \"estimate\", \"std.error\", \"statistic\", \"p.value\"),\n  group_by = \"Species\"\n  )\n#&gt; # Source: spark&lt;?&gt; [?? x 6]\n#&gt;   Species    term        estimate std.error stati…¹  p.value\n#&gt;   &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 versicolor (Intercept)    1.78     0.284     6.28 9.48e- 8\n#&gt; 2 versicolor Petal_Width    1.87     0.212     8.83 1.27e-11\n#&gt; 3 virginica  (Intercept)    4.24     0.561     7.56 1.04e- 9\n#&gt; 4 virginica  Petal_Width    0.647    0.275     2.36 2.25e- 2\n#&gt; 5 setosa     (Intercept)    1.33     0.0600   22.1  7.68e-27\n#&gt; 6 setosa     Petal_Width    0.546    0.224     2.44 1.86e- 2\n#&gt; # … with abbreviated variable name ¹​statistic\n\nTo use R packages inside Spark, your packages must be installed on the worker nodes. The first time you call spark_apply() all of the contents in your local .libPaths() will be copied into each Spark worker node via the SparkConf.addFile() function. Packages will only be copied once and will persist as long as the connection remains open. It’s not uncommon for R libraries to be several gigabytes in size, so be prepared for a one-time tax while the R packages are copied over to your Spark cluster. You can disable package distribution by setting packages = FALSE. Note: packages are not copied in local mode (master=\"local\") because the packages already exist on the system.",
    "crumbs": [
      "Guides",
      "Advanced",
      "Run R code inside Spark"
    ]
  },
  {
    "objectID": "guides/distributed-r.html#handling-errors",
    "href": "guides/distributed-r.html#handling-errors",
    "title": "Distributing R Computations",
    "section": "Handling Errors",
    "text": "Handling Errors\nIt can be more difficult to troubleshoot R issues in a cluster than in local mode. For instance, the following R code causes the distributed execution to fail and suggests you check the logs for details.\n\nspark_apply(iris_tbl, function(e) stop(\"Make this fail\"))\n\nIt is worth mentioning that different cluster providers and platforms expose worker logs in different ways. Specific documentation for your environment will point out how to retrieve these logs.",
    "crumbs": [
      "Guides",
      "Advanced",
      "Run R code inside Spark"
    ]
  },
  {
    "objectID": "guides/distributed-r.html#requirements",
    "href": "guides/distributed-r.html#requirements",
    "title": "Distributing R Computations",
    "section": "Requirements",
    "text": "Requirements\nThe R Runtime is expected to be pre-installed in the cluster for spark_apply() to function. Failure to install the cluster will trigger a Cannot run program, no such file or directory error while attempting to use spark_apply(). Contact your cluster administrator to consider making the R runtime available throughout the entire cluster.\nA Homogeneous Cluster is required since the driver node distributes, and potentially compiles, packages to the workers. For instance, the driver and workers must have the same processor architecture, system libraries, etc.",
    "crumbs": [
      "Guides",
      "Advanced",
      "Run R code inside Spark"
    ]
  },
  {
    "objectID": "guides/distributed-r.html#configuration",
    "href": "guides/distributed-r.html#configuration",
    "title": "Distributing R Computations",
    "section": "Configuration",
    "text": "Configuration\nThe following table describes relevant parameters while making use of spark_apply.\n\n\n\n\n\n\n\nValue\nDescription\n\n\n\n\nspark.r.command\nThe path to the R binary. Useful to select from multiple R versions.\n\n\nsparklyr.worker.gateway.address\nThe gateway address to use under each worker node. Defaults to sparklyr.gateway.address.\n\n\nsparklyr.worker.gateway.port\nThe gateway port to use under each worker node. Defaults to sparklyr.gateway.port.\n\n\n\nFor example, one could make use of an specific R version by running:\n\nconfig &lt;- spark_config()\nconfig[[\"spark.r.command\"]] &lt;- \"&lt;path-to-r-version&gt;\"\nsc &lt;- spark_connect(master = \"local\", config = config)\n\nsdf_len(sc, 10) %&gt;% spark_apply(function(e) e)",
    "crumbs": [
      "Guides",
      "Advanced",
      "Run R code inside Spark"
    ]
  },
  {
    "objectID": "guides/distributed-r.html#limitations",
    "href": "guides/distributed-r.html#limitations",
    "title": "Distributing R Computations",
    "section": "Limitations",
    "text": "Limitations\n\nClosures\nClosures are serialized using serialize, which is described as “A simple low-level interface for serializing to connections.”. One of the current limitations of serialize is that it wont serialize objects being referenced outside of it’s environment. For instance, the following function will error out since the closures references external_value:\n\nexternal_value &lt;- 1\nspark_apply(iris_tbl, function(e) e + external_value)\n\n\n\nLivy\nCurrently, Livy connections do not support distributing packages since the client machine where the libraries are pre-compiled might not have the same processor architecture, not operating systems that the cluster machines.\n\n\nComputing over Groups\nWhile performing computations over groups, spark_apply() will provide partitions over the selected column; however, this implies that each partition can fit into a worker node, if this is not the case an exception will be thrown. To perform operations over groups that exceed the resources of a single node, one can consider partitioning to smaller units or use dplyr::do() which is currently optimized for large partitions.\n\n\nPackage Installation\nSince packages are copied only once for the duration of the spark_connect() connection, installing additional packages is not supported while the connection is active. Therefore, if a new package needs to be installed, spark_disconnect() the connection, modify packages and reconnect.",
    "crumbs": [
      "Guides",
      "Advanced",
      "Run R code inside Spark"
    ]
  },
  {
    "objectID": "guides/extensions.html",
    "href": "guides/extensions.html",
    "title": "Creating Extensions for sparklyr",
    "section": "",
    "text": "library(sparklyr)\nsc &lt;- spark_connect(master = \"local\", version = \"2.0.0\")",
    "crumbs": [
      "Guides",
      "Advanced",
      "Create `sparklyr` extensions"
    ]
  },
  {
    "objectID": "guides/extensions.html#introduction",
    "href": "guides/extensions.html#introduction",
    "title": "Creating Extensions for sparklyr",
    "section": "Introduction",
    "text": "Introduction\nThe sparklyr package provides a dplyr interface to Spark DataFrames as well as an R interface to Spark’s distributed machine learning pipelines. However, since Spark is a general-purpose cluster computing system there are many other R interfaces that could be built (e.g. interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).\nThe facilities used internally by sparklyr for its dplyr and machine learning interfaces are available to extension packages. This guide describes how you can use these tools to create your own custom R interfaces to Spark.\n\nExamples\nHere’s an example of an extension function that calls the text file line counting function available via the SparkContext:\n\nlibrary(sparklyr)\ncount_lines &lt;- function(sc, file) {\n  spark_context(sc) %&gt;% \n    invoke(\"textFile\", file, 1L) %&gt;% \n    invoke(\"count\")\n}\n\nThe count_lines function takes a spark_connection (sc) argument which enables it to obtain a reference to the SparkContext object, and in turn call the textFile().count() method.\nYou can use this function with an existing sparklyr connection as follows:\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\")\ncount_lines(sc, \"hdfs://path/data.csv\")\n\nHere are links to some additional examples of extension packages:\n\n\n\nPackage\nDescription\n\n\n\n\nspark.sas7bdat\nRead in SAS data in parallel into Apache Spark.\n\n\nrsparkling\nExtension for using H2O machine learning algorithms against Spark Data Frames.\n\n\nsparkhello\nSimple example of including a custom JAR file within an extension package.\n\n\nrddlist\nImplements some methods of an R list as a Spark RDD (resilient distributed dataset).\n\n\nsparkwarc\nLoad WARC files into Apache Spark with sparklyr.\n\n\nsparkavro\nLoad Avro data into Spark with sparklyr. It is a wrapper of spark-avro\n\n\ncrassy\nConnect to Cassandra with sparklyr using the Spark-Cassandra-Connector.\n\n\nsparklygraphs\nR interface for GraphFrames which aims to provide the functionality of GraphX.\n\n\nsparklyr.nested\nExtension for working with nested data.\n\n\nsparklyudf\nSimple example registering an Scala UDF within an extension package.\n\n\nmleap\nR Interface to MLeap.\n\n\nsparkbq\nSparklyr extension package to connect to Google BigQuery.\n\n\nsparkgeo\nSparklyr extension package providing geospatial analytics capabilities.\n\n\nsparklytd\nSpaklyr plugin for td-spark to connect TD from R.\n\n\nsparkts\nExtensions for the spark-timeseries framework.\n\n\nsparkxgb\nR interface for XGBoost on Spark.\n\n\nsparktf\nR interface to Spark TensorFlow Connector.\n\n\ngeospark\nR interface to GeoSpark to perform spatial analysis in Spark.\n\n\nsynapseml\nSimplifies the creation of scalable machine learning pipelines. SynapseML builds on Apache Spark and SparkML to enable new kinds of machine learning, analytics, and model deployment workflows.",
    "crumbs": [
      "Guides",
      "Advanced",
      "Create `sparklyr` extensions"
    ]
  },
  {
    "objectID": "guides/extensions.html#core-types",
    "href": "guides/extensions.html#core-types",
    "title": "Creating Extensions for sparklyr",
    "section": "Core Types",
    "text": "Core Types\nThree classes are defined for representing the fundamental types of the R to Java bridge:\n\n\n\nFunction\nDescription\n\n\n\n\nspark_connection\nConnection between R and the Spark shell process\n\n\nspark_jobj\nInstance of a remote Spark object\n\n\nspark_dataframe\nInstance of a remote Spark DataFrame object\n\n\n\nS3 methods are defined for each of these classes so they can be easily converted to or from objects that contain or wrap them. Note that for any given spark_jobj it’s possible to discover the underlying spark_connection.",
    "crumbs": [
      "Guides",
      "Advanced",
      "Create `sparklyr` extensions"
    ]
  },
  {
    "objectID": "guides/extensions.html#calling-spark-from-r",
    "href": "guides/extensions.html#calling-spark-from-r",
    "title": "Creating Extensions for sparklyr",
    "section": "Calling Spark from R",
    "text": "Calling Spark from R\nThere are several functions available for calling the methods of Java objects and static methods of Java classes:\n\n\n\nFunction\nDescription\n\n\n\n\ninvoke\nCall a method on an object\n\n\ninvoke_new\nCreate a new object by invoking a constructor\n\n\ninvoke_static\nCall a static method on an object\n\n\n\nFor example, to create a new instance of the java.math.BigInteger class and then call the longValue() method on it you would use code like this:\n\nbillionBigInteger &lt;- invoke_new(sc, \"java.math.BigInteger\", \"1000000000\")\nbillion &lt;- invoke(billionBigInteger, \"longValue\")\n\nNote the sc argument: that’s the spark_connection object which is provided by the front-end package (e.g. sparklyr).\nThe previous example can be re-written to be more compact and clear using magrittr pipes:\n\nbillion &lt;- sc %&gt;% \n  invoke_new(\"java.math.BigInteger\", \"1000000000\") %&gt;%\n    invoke(\"longValue\")\n\nThis syntax is similar to the method-chaining syntax often used with Scala code so is generally preferred.\nCalling a static method of a class is also straightforward. For example, to call the Math::hypot() static function you would use this code:\n\nhypot &lt;- sc %&gt;% \n  invoke_static(\"java.lang.Math\", \"hypot\", 10, 20)",
    "crumbs": [
      "Guides",
      "Advanced",
      "Create `sparklyr` extensions"
    ]
  },
  {
    "objectID": "guides/extensions.html#wrapper-functions",
    "href": "guides/extensions.html#wrapper-functions",
    "title": "Creating Extensions for sparklyr",
    "section": "Wrapper Functions",
    "text": "Wrapper Functions\nCreating an extension typically consists of writing R wrapper functions for a set of Spark services. In this section we’ll describe the typical form of these functions as well as how to handle special types like Spark DataFrames.\nHere’s the wrapper function for textFile().count() which we defined earlier:\n\ncount_lines &lt;- function(sc, file) {\n  spark_context(sc) %&gt;% \n    invoke(\"textFile\", file, 1L) %&gt;% \n      invoke(\"count\")\n}\n\nThe count_lines function takes a spark_connection (sc) argument which enables it to obtain a reference to the SparkContext object, and in turn call the textFile().count() method.\nThe following functions are useful for implementing wrapper functions of various kinds:\n\n\n\nFunction\nDescription\n\n\n\n\nspark_connection\nGet the Spark connection associated with an object (S3)\n\n\nspark_jobj\nGet the Spark jobj associated with an object (S3)\n\n\nspark_dataframe\nGet the Spark DataFrame associated with an object (S3)\n\n\nspark_context\nGet the SparkContext for a spark_connection\n\n\nhive_context\nGet the HiveContext for a spark_connection\n\n\nspark_version\nGet the version of Spark (as a numeric_version) for a spark_connection\n\n\n\nThe use of these functions is illustrated in this simple example:\n\nanalyze &lt;- function(x, features) {\n  \n  # normalize whatever we were passed (e.g. a dplyr tbl) into a DataFrame\n  df &lt;- spark_dataframe(x)\n  \n  # get the underlying connection so we can create new objects\n  sc &lt;- spark_connection(df)\n  \n  # create an object to do the analysis and call its `analyze` and `summary`\n  # methods (note that the df and features are passed to the analyze function)\n  summary &lt;- sc %&gt;%  \n    invoke_new(\"com.example.tools.Analyzer\") %&gt;% \n      invoke(\"analyze\", df, features) %&gt;% \n      invoke(\"summary\")\n\n  # return the results\n  summary\n}\n\nThe first argument is an object that can be accessed using the Spark DataFrame API (this might be an actual reference to a DataFrame or could rather be a dplyr tbl which has a DataFrame reference inside it).\nAfter using the spark_dataframe function to normalize the reference, we extract the underlying Spark connection associated with the data frame using the spark_connection function. Finally, we create a new Analyzer object, call it’s analyze method with the DataFrame and list of features, and then call the summary method on the results of the analysis.\nAccepting a spark_jobj or spark_dataframe as the first argument of a function makes it very easy to incorporate into magrittr pipelines so this pattern is highly recommended when possible.",
    "crumbs": [
      "Guides",
      "Advanced",
      "Create `sparklyr` extensions"
    ]
  },
  {
    "objectID": "guides/extensions.html#dependencies",
    "href": "guides/extensions.html#dependencies",
    "title": "Creating Extensions for sparklyr",
    "section": "Dependencies",
    "text": "Dependencies\nWhen creating R packages which implement interfaces to Spark you may need to include additional dependencies. Your dependencies might be a set of Spark Packages or might be a custom JAR file. In either case, you’ll need a way to specify that these dependencies should be included during the initialization of a Spark session. A Spark dependency is defined using the spark_dependency function:\n\n\n\nFunction\nDescription\n\n\n\n\nspark_dependency\nDefine a Spark dependency consisting of JAR files and Spark packages\n\n\n\nYour extension package can specify it’s dependencies by implementing a function named spark_dependencies within the package (this function should not be publicly exported). For example, let’s say you were creating an extension package named sparkds that needs to include a custom JAR as well as the Redshift and Apache Avro packages:\n\nspark_dependencies &lt;- function(spark_version, scala_version, ...) {\n  spark_dependency(\n    jars = c(\n      system.file(\n        sprintf(\"java/sparkds-%s-%s.jar\", spark_version, scala_version), \n        package = \"sparkds\"\n      )\n    ),\n    packages = c(\n      sprintf(\"com.databricks:spark-redshift_%s:0.6.0\", scala_version),\n      sprintf(\"com.databricks:spark-avro_%s:2.0.1\", scala_version)\n    )\n  )\n}\n\n.onLoad &lt;- function(libname, pkgname) {\n  sparklyr::register_extension(pkgname)\n}\n\nThe spark_version argument is provided so that a package can support multiple Spark versions for it’s JARs. Note that the argument will include just the major and minor versions (e.g. 1.6 or 2.0) and will not include the patch level (as JARs built for a given major/minor version are expected to work for all patch levels).\nThe scala_version argument is provided so that a single package can support multiple Scala compiler versions for it’s JARs and packages (currently Scala 1.6 downloadable binaries are compiled with Scala 2.10 and Scala 2.0 downloadable binaries are compiled with Scala 2.11).\nThe ... argument is unused but nevertheless should be included to ensure compatibility if new arguments are added to spark_dependencies in the future.\nThe .onLoad function registers your extension package so that it’s spark_dependencies function will be automatically called when new connections to Spark are made via spark_connect:\n\nlibrary(sparklyr)\nlibrary(sparkds)\nsc &lt;- spark_connect(master = \"local\")\n\n\nCompiling JARs\nThe sparklyr package includes a utility function (compile_package_jars) that will automatically compile a JAR file from your Scala source code for the required permutations of Spark and Scala compiler versions. To use the function just invoke it from the root directory of your R package as follows:\n\nsparklyr::compile_package_jars()\n\nNote that a prerequisite to calling compile_package_jars is the installation of the Scala 2.10 and 2.11 compilers to one of the following paths:\n\n/opt/scala\n/opt/local/scala\n/usr/local/scala\n~/scala (Windows-only)\n\nSee the sparkhello repository for a complete example of including a custom JAR within an extension package.\n\nCRAN\nWhen including a JAR file within an R package distributed on CRAN, you should follow the guidelines provided in Writing R Extensions:\n\nJava code is a special case: except for very small programs, .java files should be byte-compiled (to a .class file) and distributed as part of a .jar file: the conventional location for the .jar file(s) is inst/java. It is desirable (and required under an Open Source license) to make the Java source files available: this is best done in a top-level java directory in the package – the source files should not be installed.",
    "crumbs": [
      "Guides",
      "Advanced",
      "Create `sparklyr` extensions"
    ]
  },
  {
    "objectID": "guides/extensions.html#data-types",
    "href": "guides/extensions.html#data-types",
    "title": "Creating Extensions for sparklyr",
    "section": "Data Types",
    "text": "Data Types\nThe ensure_* family of functions can be used to enforce specific data types that are passed to a Spark routine. For example, Spark routines that require an integer will not accept an R numeric element. Use these functions ensure certain parameters are scalar integers, or scalar doubles, and so on.\n\nensure_scalar_integer\nensure_scalar_double\nensure_scalar_boolean\nensure_scalar_character\n\nIn order to match the correct data types while calling Scala code from R, or retrieving results from Scala back to R, consider the following types mapping:\n\n\n\nFrom R\nScala\nTo R\n\n\n\n\nNULL\nvoid\nNULL\n\n\ninteger\nInt\ninteger\n\n\ncharacter\nString\ncharacter\n\n\nlogical\nBoolean\nlogical\n\n\ndouble\nDouble\ndouble\n\n\nnumeric\nDouble\ndouble\n\n\n\nFloat\ndouble\n\n\n\nDecimal\ndouble\n\n\n\nLong\ndouble\n\n\nraw\nArray[Byte]\nraw\n\n\nDate\nDate\nDate\n\n\nPOSIXlt\nTime\n\n\n\nPOSIXct\nTime\nPOSIXct\n\n\nlist\nArray[T]\nlist\n\n\nenvironment\nMap[String, T]\n\n\n\njobj\nObject\njobj",
    "crumbs": [
      "Guides",
      "Advanced",
      "Create `sparklyr` extensions"
    ]
  },
  {
    "objectID": "guides/extensions.html#compiling",
    "href": "guides/extensions.html#compiling",
    "title": "Creating Extensions for sparklyr",
    "section": "Compiling",
    "text": "Compiling\nMost Spark extensions won’t need to define their own compilation specification, and can instead rely on the default behavior of compile_package_jars. For users who would like to take more control over where the scalac compilers should be looked up, use the spark_compilation_spec fucnction. The Spark compilation specification is used when compiling Spark extension Java Archives, and defines which versions of Spark, as well as which versions of Scala, should be used for compilation.",
    "crumbs": [
      "Guides",
      "Advanced",
      "Create `sparklyr` extensions"
    ]
  },
  {
    "objectID": "guides/caching.html",
    "href": "guides/caching.html",
    "title": "Understanding Spark Caching",
    "section": "",
    "text": "Spark also supports pulling data sets into a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small dataset or when running an iterative algorithm like random forests. Since operations in Spark are lazy, caching can help force computation. sparklyr tools can be used to cache and un-cache DataFrames. The Spark UI will tell you which DataFrames and what percentages are in memory.\nBy using a reproducible example, we will review some of the main configuration settings, commands and command arguments that can be used that can help you get the best out of Spark’s memory management options.",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Managing Spark cache"
    ]
  },
  {
    "objectID": "guides/caching.html#introduction",
    "href": "guides/caching.html#introduction",
    "title": "Understanding Spark Caching",
    "section": "",
    "text": "Spark also supports pulling data sets into a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small dataset or when running an iterative algorithm like random forests. Since operations in Spark are lazy, caching can help force computation. sparklyr tools can be used to cache and un-cache DataFrames. The Spark UI will tell you which DataFrames and what percentages are in memory.\nBy using a reproducible example, we will review some of the main configuration settings, commands and command arguments that can be used that can help you get the best out of Spark’s memory management options.",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Managing Spark cache"
    ]
  },
  {
    "objectID": "guides/caching.html#preparation",
    "href": "guides/caching.html#preparation",
    "title": "Understanding Spark Caching",
    "section": "Preparation",
    "text": "Preparation\n\nDownload Test Data\nBecause of their size, we will use trip data provided by the NYC Taxi & Limousine Commission. Each file represents a month’s worth of trips. We will download two files, the ones for January and February 2020.\n\nif(!file.exists(\"jan_2020.parquet\")) {\n  download.file(\n    \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-01.parquet\",\n    \"jan_2020.parquet\",\n    mode = \"wb\"\n  )  \n}\n\nif(!file.exists(\"feb_2020.parquet\")) {\n  download.file(\n    \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-02.parquet\",\n    \"feb_2020.parquet\",\n    mode = \"wb\"\n  )  \n}\n\n\n\nStart a Spark session\nA local deployment will be used for this example.\n\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n# Customize the connection configuration\nconf &lt;- spark_config()\nconf$`sparklyr.shell.driver-memory` &lt;- \"16G\"\n\n# Connect to Spark\nsc &lt;- spark_connect(master = \"local\", config = conf)",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Managing Spark cache"
    ]
  },
  {
    "objectID": "guides/caching.html#the-memory-argument",
    "href": "guides/caching.html#the-memory-argument",
    "title": "Understanding Spark Caching",
    "section": "The Memory Argument",
    "text": "The Memory Argument\nIn the spark_read_… functions, the memory argument controls if the data will be loaded into memory as an RDD. Setting it to FALSE means that Spark will essentially map the file, but not make a copy of it in memory. This makes the spark_read_parquet() command run faster, but the trade off is that any data transformation operations will take much longer.\n\nspark_read_parquet(\n  sc, \n  \"taxi_jan_2020\", \n  \"jan_2020.parquet\", \n  memory = FALSE\n  )\n#&gt; # Source: spark&lt;taxi_jan_2020&gt; [?? x 19]\n#&gt;    VendorID tpep_pickup_datetime tpep_dropoff_date…¹ passe…²\n#&gt;       &lt;dbl&gt; &lt;dttm&gt;               &lt;dttm&gt;                &lt;dbl&gt;\n#&gt;  1        1 2019-12-31 18:28:15  2019-12-31 18:33:03       1\n#&gt;  2        1 2019-12-31 18:35:39  2019-12-31 18:43:04       1\n#&gt;  3        1 2019-12-31 18:47:41  2019-12-31 18:53:52       1\n#&gt;  4        1 2019-12-31 18:55:23  2019-12-31 19:00:14       1\n#&gt;  5        2 2019-12-31 18:01:58  2019-12-31 18:04:16       1\n#&gt;  6        2 2019-12-31 18:09:44  2019-12-31 18:10:37       1\n#&gt;  7        2 2019-12-31 18:39:25  2019-12-31 18:39:29       1\n#&gt;  8        2 2019-12-18 09:27:49  2019-12-18 09:28:59       1\n#&gt;  9        2 2019-12-18 09:30:35  2019-12-18 09:31:35       4\n#&gt; 10        1 2019-12-31 18:29:01  2019-12-31 18:40:28       2\n#&gt; # … with more rows, 15 more variables: trip_distance &lt;dbl&gt;,\n#&gt; #   RatecodeID &lt;dbl&gt;, store_and_fwd_flag &lt;chr&gt;,\n#&gt; #   PULocationID &lt;dbl&gt;, DOLocationID &lt;dbl&gt;,\n#&gt; #   payment_type &lt;dbl&gt;, fare_amount &lt;dbl&gt;, extra &lt;dbl&gt;,\n#&gt; #   mta_tax &lt;dbl&gt;, tip_amount &lt;dbl&gt;, tolls_amount &lt;dbl&gt;,\n#&gt; #   improvement_surcharge &lt;dbl&gt;, total_amount &lt;dbl&gt;,\n#&gt; #   congestion_surcharge &lt;dbl&gt;, airport_fee &lt;int&gt;, and …\n\nIn the RStudio IDE, the taxi_jan_2020 table now shows up in the Spark tab.\n\nTo access the Spark Web UI, click the Spark button in the RStudio Spark Tab. As expected, the Storage page shows no tables loaded into memory.",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Managing Spark cache"
    ]
  },
  {
    "objectID": "guides/caching.html#loading-less-data-into-memory",
    "href": "guides/caching.html#loading-less-data-into-memory",
    "title": "Understanding Spark Caching",
    "section": "Loading Less Data into Memory",
    "text": "Loading Less Data into Memory\nUsing the pre-processing capabilities of Spark, the data will be transformed before being loaded into memory. In this section, we will continue to build on the example started in the previous section\n\nLazy Transform\nThe following dplyr script will not be immediately run, so the code is processed quickly. There are some check-ups made, but for the most part it is building a Spark SQL statement in the background.\n\ntrips_table &lt;- tbl(sc,\"taxi_jan_2020\") %&gt;%\n  filter(trip_distance &gt; 20) %&gt;% \n  select(VendorID, passenger_count, trip_distance)\n\n\n\nRegister in Spark\nsdf_register() will register the resulting Spark SQL in Spark. The results will show up as a table called trip_spark. But a table of the same name is still not loaded into memory in Spark.\n\nsdf_register(trips_table, \"trips_spark\")\n#&gt; # Source: spark&lt;trips_spark&gt; [?? x 3]\n#&gt;    VendorID passenger_count trip_distance\n#&gt;       &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n#&gt;  1        2               1          23.5\n#&gt;  2        1               2          22.8\n#&gt;  3        2               2          37.6\n#&gt;  4        2               4          20.3\n#&gt;  5        1               2          29.4\n#&gt;  6        2               1          25.9\n#&gt;  7        2               3          22.1\n#&gt;  8        2               1          21.0\n#&gt;  9        2               3          20.1\n#&gt; 10        2               1          32.5\n#&gt; # … with more rows\n\n\n\n\nCache into Memory\nThe tbl_cache() command loads the results into an Spark RDD in memory, so any analysis from there on will not need to re-read and re-transform the original file. The resulting Spark RDD is smaller than the original file because the transformations created a smaller data set than the original file.\n\ntbl_cache(sc, \"trips_spark\")\n\n\n\n\nDriver Memory\nIn the Executors page of the Spark Web UI, we can see that the Storage Memory is at about half of the 16 gigabytes requested. This is mainly because of a Spark setting called spark.memory.fraction, which reserves by default 40% of the memory requested.",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Managing Spark cache"
    ]
  },
  {
    "objectID": "guides/caching.html#process-on-the-fly",
    "href": "guides/caching.html#process-on-the-fly",
    "title": "Understanding Spark Caching",
    "section": "Process on the fly",
    "text": "Process on the fly\nThe plan for this exercise is to read the January file, combine it with the February file and summarize the data without bringing either file fully into memory.\n\nspark_read_parquet(sc, \"taxi_feb_2020\" , \"feb_2020.parquet\", memory = FALSE)\n#&gt; # Source: spark&lt;taxi_feb_2020&gt; [?? x 19]\n#&gt;    VendorID tpep_pickup_datetime tpep_dropoff_date…¹ passe…²\n#&gt;       &lt;dbl&gt; &lt;dttm&gt;               &lt;dttm&gt;                &lt;dbl&gt;\n#&gt;  1        1 2020-01-31 18:17:35  2020-01-31 18:30:32       1\n#&gt;  2        1 2020-01-31 18:32:47  2020-01-31 19:05:36       1\n#&gt;  3        1 2020-01-31 18:31:44  2020-01-31 18:43:28       1\n#&gt;  4        2 2020-01-31 18:07:35  2020-01-31 18:31:39       1\n#&gt;  5        2 2020-01-31 18:51:43  2020-01-31 19:01:29       1\n#&gt;  6        1 2020-01-31 18:15:49  2020-01-31 18:20:48       2\n#&gt;  7        1 2020-01-31 18:25:31  2020-01-31 18:50:22       2\n#&gt;  8        1 2020-01-31 18:11:15  2020-01-31 18:24:29       1\n#&gt;  9        2 2020-01-31 18:58:26  2020-01-31 19:02:26       1\n#&gt; 10        2 2020-01-31 18:03:57  2020-01-31 18:48:10       1\n#&gt; # … with more rows, 15 more variables: trip_distance &lt;dbl&gt;,\n#&gt; #   RatecodeID &lt;dbl&gt;, store_and_fwd_flag &lt;chr&gt;,\n#&gt; #   PULocationID &lt;dbl&gt;, DOLocationID &lt;dbl&gt;,\n#&gt; #   payment_type &lt;dbl&gt;, fare_amount &lt;dbl&gt;, extra &lt;dbl&gt;,\n#&gt; #   mta_tax &lt;dbl&gt;, tip_amount &lt;dbl&gt;, tolls_amount &lt;dbl&gt;,\n#&gt; #   improvement_surcharge &lt;dbl&gt;, total_amount &lt;dbl&gt;,\n#&gt; #   congestion_surcharge &lt;dbl&gt;, airport_fee &lt;int&gt;, and …\n\n\nUnion and Transform\nThe union() command is akin to the dplyr::bind_rows() command. It will allow us to append the February file to the January file, and as with the previous transform, this script will be evaluated lazily.\n\npassenger_count &lt;- tbl(sc, \"taxi_jan_2020\") %&gt;%\n  union(tbl(sc, \"taxi_feb_2020\")) %&gt;%\n  mutate(pickup_date = as.Date(tpep_pickup_datetime)) %&gt;% \n  count(pickup_date)\n\n\n\nCollect into R\nWhen receiving a collect() command, Spark will execute the SQL statement and send the results back to R in a data frame. In this case, R only loads 51 observations into a data frame called passenger_count.\n\npassenger_count &lt;- passenger_count %&gt;%\n  collect()\n\n\n\nPlot in R\nNow the smaller data set can be plotted\n\npassenger_count %&gt;% \n  filter(pickup_date &gt;= \"2020-01-01\", pickup_date &lt;= \"2020-02-28\") %&gt;% \n  ggplot() +\n  geom_line(aes(pickup_date, n)) +\n  theme_minimal() +\n  labs(title = \"Daily Trip Volume\", \n       subtitle = \"NYC Yellow Cab - January and February 2020\",\n       y = \"Number of Trips\",\n       x = \"\"\n       )\n\n\n\n\n\n\n\n\n\nspark_disconnect(sc)",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Managing Spark cache"
    ]
  },
  {
    "objectID": "guides/index.html",
    "href": "guides/index.html",
    "title": "Guides",
    "section": "",
    "text": "Interacting with Spark\n\nUsing dplyr commands\n\nUnderstanding Spark Caching\nSpark connection options\nAccess AWS S3 Buckets\n\n\n\nModeling and Machine Learning\n\nSpark ML Overview\nUsing tidymodels\nML Pipelines\nText Modeling\nModel Tuning\nUsing H2O in sparklyr\n\n\n\n\n\n\n\n\n\n\nNon-rectangular Data\n\nStream Data\nText Mining\n\n\n\nAdvanced\n\nRun R code in Spark\nCreate sparklyr extensions",
    "crumbs": [
      "Guides",
      "Overview"
    ]
  },
  {
    "objectID": "guides/dplyr.html",
    "href": "guides/dplyr.html",
    "title": "Manipulating Data with dplyr",
    "section": "",
    "text": "dplyr is an R package for working with structured data both in and outside of R. dplyr makes data manipulation for R users easy, consistent, and performant. With dplyr as an interface to manipulating Spark DataFrames, you can:\n\nSelect, filter, and aggregate data\nUse window functions (e.g. for sampling)\nPerform joins on DataFrames\nCollect data from Spark into R\n\nStatements in dplyr can be chained together using pipes defined by the magrittr R package. dplyr also supports non-standard evalution of its arguments. For more information on dplyr, see the introduction, a guide for connecting to databases, and a variety of vignettes.\n\n\nThis guide will demonstrate some of the basic data manipulation verbs of dplyr by using data from the nycflights13 R package. This package contains data for all 336,776 flights departing New York City in 2013. It also includes useful metadata on airlines, airports, weather, and planes. The data comes from the US Bureau of Transportation Statistics, and is documented in ?nycflights13\nConnect to the cluster and copy the flights data using the copy_to() function. Caveat: The flight data in nycflights13 is convenient for dplyr demonstrations because it is small, but in practice large data should rarely be copied directly from R objects.\n\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nsc &lt;- spark_connect(master=\"local\")\n\nflights_tbl &lt;- copy_to(sc, nycflights13::flights, \"flights\")\n\nairlines_tbl &lt;- copy_to(sc, nycflights13::airlines, \"airlines\")",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Using `dplyr`"
    ]
  },
  {
    "objectID": "guides/dplyr.html#overview",
    "href": "guides/dplyr.html#overview",
    "title": "Manipulating Data with dplyr",
    "section": "",
    "text": "dplyr is an R package for working with structured data both in and outside of R. dplyr makes data manipulation for R users easy, consistent, and performant. With dplyr as an interface to manipulating Spark DataFrames, you can:\n\nSelect, filter, and aggregate data\nUse window functions (e.g. for sampling)\nPerform joins on DataFrames\nCollect data from Spark into R\n\nStatements in dplyr can be chained together using pipes defined by the magrittr R package. dplyr also supports non-standard evalution of its arguments. For more information on dplyr, see the introduction, a guide for connecting to databases, and a variety of vignettes.\n\n\nThis guide will demonstrate some of the basic data manipulation verbs of dplyr by using data from the nycflights13 R package. This package contains data for all 336,776 flights departing New York City in 2013. It also includes useful metadata on airlines, airports, weather, and planes. The data comes from the US Bureau of Transportation Statistics, and is documented in ?nycflights13\nConnect to the cluster and copy the flights data using the copy_to() function. Caveat: The flight data in nycflights13 is convenient for dplyr demonstrations because it is small, but in practice large data should rarely be copied directly from R objects.\n\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nsc &lt;- spark_connect(master=\"local\")\n\nflights_tbl &lt;- copy_to(sc, nycflights13::flights, \"flights\")\n\nairlines_tbl &lt;- copy_to(sc, nycflights13::airlines, \"airlines\")",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Using `dplyr`"
    ]
  },
  {
    "objectID": "guides/dplyr.html#dplyr-verbs",
    "href": "guides/dplyr.html#dplyr-verbs",
    "title": "Manipulating Data with dplyr",
    "section": "dplyr Verbs",
    "text": "dplyr Verbs\nVerbs are dplyr commands for manipulating data. When connected to a Spark DataFrame, dplyr translates the commands into Spark SQL statements. Remote data sources use exactly the same five verbs as local data sources. Here are the five verbs with their corresponding SQL commands:\n\nselect() ~ SELECT\nfilter() ~ WHERE\narrange() ~ ORDER\nsummarise() ~ aggregators: sum, min, sd, etc.\nmutate() ~ operators: +, *, log, etc.\n\n\nselect(flights_tbl, year:day, arr_delay, dep_delay)\n#&gt; # Source: spark&lt;?&gt; [?? x 5]\n#&gt;     year month   day arr_delay dep_delay\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1  2013     1     1        11         2\n#&gt;  2  2013     1     1        20         4\n#&gt;  3  2013     1     1        33         2\n#&gt;  4  2013     1     1       -18        -1\n#&gt;  5  2013     1     1       -25        -6\n#&gt;  6  2013     1     1        12        -4\n#&gt;  7  2013     1     1        19        -5\n#&gt;  8  2013     1     1       -14        -3\n#&gt;  9  2013     1     1        -8        -3\n#&gt; 10  2013     1     1         8        -2\n#&gt; # … with more rows\n\n\nfilter(flights_tbl, dep_delay &gt; 1000)\n#&gt; # Source: spark&lt;?&gt; [?? x 19]\n#&gt;    year month   day dep_time sched…¹ dep_d…² arr_t…³ sched…⁴\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;   &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;\n#&gt; 1  2013     1     9      641     900    1301    1242    1530\n#&gt; 2  2013     1    10     1121    1635    1126    1239    1810\n#&gt; 3  2013     6    15     1432    1935    1137    1607    2120\n#&gt; 4  2013     7    22      845    1600    1005    1044    1815\n#&gt; 5  2013     9    20     1139    1845    1014    1457    2210\n#&gt; # … with 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;,\n#&gt; #   flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;,\n#&gt; #   air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,\n#&gt; #   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated\n#&gt; #   variable names ¹​sched_dep_time, ²​dep_delay, ³​arr_time,\n#&gt; #   ⁴​sched_arr_time\n\n\narrange(flights_tbl, desc(dep_delay))\n#&gt; # Source:     spark&lt;?&gt; [?? x 19]\n#&gt; # Ordered by: desc(dep_delay)\n#&gt;     year month   day dep_t…¹ sched…² dep_d…³ arr_t…⁴ sched…⁵\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;\n#&gt;  1  2013     1     9     641     900    1301    1242    1530\n#&gt;  2  2013     6    15    1432    1935    1137    1607    2120\n#&gt;  3  2013     1    10    1121    1635    1126    1239    1810\n#&gt;  4  2013     9    20    1139    1845    1014    1457    2210\n#&gt;  5  2013     7    22     845    1600    1005    1044    1815\n#&gt;  6  2013     4    10    1100    1900     960    1342    2211\n#&gt;  7  2013     3    17    2321     810     911     135    1020\n#&gt;  8  2013     6    27     959    1900     899    1236    2226\n#&gt;  9  2013     7    22    2257     759     898     121    1026\n#&gt; 10  2013    12     5     756    1700     896    1058    2020\n#&gt; # … with more rows, 11 more variables: arr_delay &lt;dbl&gt;,\n#&gt; #   carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;,\n#&gt; #   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#&gt; #   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;,\n#&gt; #   time_hour &lt;dttm&gt;, and abbreviated variable names\n#&gt; #   ¹​dep_time, ²​sched_dep_time, ³​dep_delay, ⁴​arr_time,\n#&gt; #   ⁵​sched_arr_time\n\n\nsummarise(\n  flights_tbl, \n  mean_dep_delay = mean(dep_delay, na.rm = TRUE)\n  )\n#&gt; # Source: spark&lt;?&gt; [?? x 1]\n#&gt;   mean_dep_delay\n#&gt;            &lt;dbl&gt;\n#&gt; 1           12.6\n\n\nmutate(flights_tbl, speed = distance / air_time * 60)\n#&gt; # Source: spark&lt;?&gt; [?? x 20]\n#&gt;     year month   day dep_t…¹ sched…² dep_d…³ arr_t…⁴ sched…⁵\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;\n#&gt;  1  2013     1     1     517     515       2     830     819\n#&gt;  2  2013     1     1     533     529       4     850     830\n#&gt;  3  2013     1     1     542     540       2     923     850\n#&gt;  4  2013     1     1     544     545      -1    1004    1022\n#&gt;  5  2013     1     1     554     600      -6     812     837\n#&gt;  6  2013     1     1     554     558      -4     740     728\n#&gt;  7  2013     1     1     555     600      -5     913     854\n#&gt;  8  2013     1     1     557     600      -3     709     723\n#&gt;  9  2013     1     1     557     600      -3     838     846\n#&gt; 10  2013     1     1     558     600      -2     753     745\n#&gt; # … with more rows, 12 more variables: arr_delay &lt;dbl&gt;,\n#&gt; #   carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;,\n#&gt; #   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#&gt; #   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;,\n#&gt; #   time_hour &lt;dttm&gt;, speed &lt;dbl&gt;, and abbreviated variable\n#&gt; #   names ¹​dep_time, ²​sched_dep_time, ³​dep_delay,\n#&gt; #   ⁴​arr_time, ⁵​sched_arr_time",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Using `dplyr`"
    ]
  },
  {
    "objectID": "guides/dplyr.html#laziness",
    "href": "guides/dplyr.html#laziness",
    "title": "Manipulating Data with dplyr",
    "section": "Laziness",
    "text": "Laziness\nWhen working with databases, dplyr tries to be as lazy as possible:\n\nIt never pulls data into R unless you explicitly ask for it.\nIt delays doing any work until the last possible moment: it collects together everything you want to do and then sends it to the database in one step.\n\nFor example, take the following code:\n\nc1 &lt;- filter(\n  flights_tbl, \n  day == 17, month == 5, carrier %in% c('UA', 'WN', 'AA', 'DL')\n  )\n\nc2 &lt;- select(c1, year, month, day, carrier, dep_delay, air_time, distance)\n\nc3 &lt;- mutate(c2, air_time_hours = air_time / 60)\n\nc4 &lt;- arrange(c3, year, month, day, carrier)\n\nThis sequence of operations never actually touches the database. It’s not until you ask for the data (e.g. by printing c4) that dplyr requests the results from the database.\n\nc4\n#&gt; # Source:     spark&lt;?&gt; [?? x 8]\n#&gt; # Ordered by: year, month, day, carrier\n#&gt;     year month   day carrier dep_d…¹ air_t…² dista…³ air_t…⁴\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1  2013     5    17 AA           -7     142    1089    2.37\n#&gt;  2  2013     5    17 AA           -9     186    1389    3.1 \n#&gt;  3  2013     5    17 AA           -6     143    1096    2.38\n#&gt;  4  2013     5    17 AA           -7     119     733    1.98\n#&gt;  5  2013     5    17 AA           -4     114     733    1.9 \n#&gt;  6  2013     5    17 AA           -2     146    1085    2.43\n#&gt;  7  2013     5    17 AA           -2     185    1372    3.08\n#&gt;  8  2013     5    17 AA           -3     193    1598    3.22\n#&gt;  9  2013     5    17 AA           -7     137     944    2.28\n#&gt; 10  2013     5    17 AA           -1     195    1389    3.25\n#&gt; # … with more rows, and abbreviated variable names\n#&gt; #   ¹​dep_delay, ²​air_time, ³​distance, ⁴​air_time_hours",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Using `dplyr`"
    ]
  },
  {
    "objectID": "guides/dplyr.html#piping",
    "href": "guides/dplyr.html#piping",
    "title": "Manipulating Data with dplyr",
    "section": "Piping",
    "text": "Piping\nYou can use magrittr pipes to write cleaner syntax. Using the same example from above, you can write a much cleaner version like this:\n\nc4 &lt;- flights_tbl %&gt;%\n  filter(month == 5, day == 17, carrier %in% c('UA', 'WN', 'AA', 'DL')) %&gt;%\n  select(carrier, dep_delay, air_time, distance) %&gt;%\n  mutate(air_time_hours = air_time / 60) %&gt;% \n  arrange(carrier)",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Using `dplyr`"
    ]
  },
  {
    "objectID": "guides/dplyr.html#grouping",
    "href": "guides/dplyr.html#grouping",
    "title": "Manipulating Data with dplyr",
    "section": "Grouping",
    "text": "Grouping\nThe group_by() function corresponds to the GROUP BY statement in SQL.\n\nflights_tbl %&gt;% \n  group_by(carrier) %&gt;%\n  summarize(\n    count = n(), \n    mean_dep_delay = mean(dep_delay, na.rm = FALSE)\n    )\n#&gt; Warning: Missing values are always removed in SQL aggregation functions.\n#&gt; Use `na.rm = TRUE` to silence this warning\n#&gt; This warning is displayed once every 8 hours.\n#&gt; # Source: spark&lt;?&gt; [?? x 3]\n#&gt;    carrier count mean_dep_delay\n#&gt;    &lt;chr&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n#&gt;  1 WN      12275          17.7 \n#&gt;  2 VX       5162          12.9 \n#&gt;  3 YV        601          19.0 \n#&gt;  4 DL      48110           9.26\n#&gt;  5 OO         32          12.6 \n#&gt;  6 B6      54635          13.0 \n#&gt;  7 F9        685          20.2 \n#&gt;  8 EV      54173          20.0 \n#&gt;  9 US      20536           3.78\n#&gt; 10 UA      58665          12.1 \n#&gt; # … with more rows",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Using `dplyr`"
    ]
  },
  {
    "objectID": "guides/dplyr.html#collecting-to-r",
    "href": "guides/dplyr.html#collecting-to-r",
    "title": "Manipulating Data with dplyr",
    "section": "Collecting to R",
    "text": "Collecting to R\nYou can copy data from Spark into R’s memory by using collect().\n\ncarrierhours &lt;- collect(c4)\n\ncollect() executes the Spark query and returns the results to R for further analysis and visualization.\n\n# Test the significance of pairwise differences and plot the results\n\nwith(carrierhours, pairwise.t.test(air_time, carrier))\n#&gt; \n#&gt;  Pairwise comparisons using t tests with pooled SD \n#&gt; \n#&gt; data:  air_time and carrier \n#&gt; \n#&gt;    AA      DL      UA     \n#&gt; DL 0.25057 -       -      \n#&gt; UA 0.07957 0.00044 -      \n#&gt; WN 0.07957 0.23488 0.00041\n#&gt; \n#&gt; P value adjustment method: holm\n\n\ncarrierhours %&gt;% \n  ggplot() + \n  geom_boxplot(aes(carrier, air_time_hours))",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Using `dplyr`"
    ]
  },
  {
    "objectID": "guides/dplyr.html#sql-translation",
    "href": "guides/dplyr.html#sql-translation",
    "title": "Manipulating Data with dplyr",
    "section": "SQL Translation",
    "text": "SQL Translation\nIt’s relatively straightforward to translate R code to SQL (or indeed to any programming language) when doing simple mathematical operations of the form you normally use when filtering, mutating and summarizing. dplyr knows how to convert the following R functions to Spark SQL:\n# Basic math operators\n+, -, *, /, %%, ^\n  \n# Math functions\nabs, acos, asin, asinh, atan, atan2, ceiling, cos, cosh, exp, floor, log, \nlog10, round, sign, sin, sinh, sqrt, tan, tanh\n\n# Logical comparisons\n&lt;, &lt;=, !=, &gt;=, &gt;, ==, %in%\n\n# Boolean operations\n&, &&, |, ||, !\n\n# Character functions\npaste, tolower, toupper, nchar\n\n# Casting\nas.double, as.integer, as.logical, as.character, as.date\n\n# Basic aggregations\nmean, sum, min, max, sd, var, cor, cov, n\ndplyr supports Spark SQL window functions. Window functions are used in conjunction with mutate and filter to solve a wide range of problems. You can compare the dplyr syntax to the query it has generated by using dplyr::show_query().\n\n# Rank each flight within a daily\nranked &lt;- flights_tbl %&gt;%\n  group_by(year, month, day) %&gt;%\n  select(dep_delay) %&gt;% \n  mutate(rank = rank(desc(dep_delay)))\n#&gt; Adding missing grouping variables: `year`, `month`, and `day`\n\ndplyr::show_query(ranked)\n#&gt; &lt;SQL&gt;\n#&gt; SELECT\n#&gt;   `year`,\n#&gt;   `month`,\n#&gt;   `day`,\n#&gt;   `dep_delay`,\n#&gt;   RANK() OVER (PARTITION BY `year`, `month`, `day` ORDER BY `dep_delay` DESC) AS `rank`\n#&gt; FROM `flights`\n\n\nranked \n#&gt; # Source: spark&lt;?&gt; [?? x 5]\n#&gt; # Groups: year, month, day\n#&gt;     year month   day dep_delay  rank\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;\n#&gt;  1  2013     1     1       853     1\n#&gt;  2  2013     1     1       379     2\n#&gt;  3  2013     1     1       290     3\n#&gt;  4  2013     1     1       285     4\n#&gt;  5  2013     1     1       260     5\n#&gt;  6  2013     1     1       255     6\n#&gt;  7  2013     1     1       216     7\n#&gt;  8  2013     1     1       192     8\n#&gt;  9  2013     1     1       157     9\n#&gt; 10  2013     1     1       155    10\n#&gt; # … with more rows",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Using `dplyr`"
    ]
  },
  {
    "objectID": "guides/dplyr.html#peforming-joins",
    "href": "guides/dplyr.html#peforming-joins",
    "title": "Manipulating Data with dplyr",
    "section": "Peforming Joins",
    "text": "Peforming Joins\nIt’s rare that a data analysis involves only a single table of data. In practice, you’ll normally have many tables that contribute to an analysis, and you need flexible tools to combine them. In dplyr, there are three families of verbs that work with two tables at a time:\n\nMutating joins, which add new variables to one table from matching rows in another.\nFiltering joins, which filter observations from one table based on whether or not they match an observation in the other table.\nSet operations, which combine the observations in the data sets as if they were set elements.\n\nAll two-table verbs work similarly. The first two arguments are x and y, and provide the tables to combine. The output is always a new table with the same type as x.\n\nflights_tbl %&gt;% \n  left_join(airlines_tbl, by = \"carrier\") %&gt;% \n  select(name, flight, dep_time)\n#&gt; # Source: spark&lt;?&gt; [?? x 3]\n#&gt;    name           flight dep_time\n#&gt;    &lt;chr&gt;           &lt;int&gt;    &lt;int&gt;\n#&gt;  1 Virgin America    399      658\n#&gt;  2 Virgin America     11      729\n#&gt;  3 Virgin America    407      859\n#&gt;  4 Virgin America    251      932\n#&gt;  5 Virgin America     23     1031\n#&gt;  6 Virgin America    409     1133\n#&gt;  7 Virgin America     25     1203\n#&gt;  8 Virgin America    411     1327\n#&gt;  9 Virgin America     27     1627\n#&gt; 10 Virgin America    413     1655\n#&gt; # … with more rows",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Using `dplyr`"
    ]
  },
  {
    "objectID": "guides/dplyr.html#sampling",
    "href": "guides/dplyr.html#sampling",
    "title": "Manipulating Data with dplyr",
    "section": "Sampling",
    "text": "Sampling\nYou can use sample_n() and sample_frac() to take a random sample of rows: use sample_n() for a fixed number and sample_frac() for a fixed fraction.\n\nsample_n(flights_tbl, 10) %&gt;% \n  select(1:4)\n#&gt; # Source: spark&lt;?&gt; [?? x 4]\n#&gt;     year month   day dep_time\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;\n#&gt;  1  2013     6    21     1258\n#&gt;  2  2013     9    19     1947\n#&gt;  3  2013     8    12     1442\n#&gt;  4  2013    11     9     1412\n#&gt;  5  2013     9     7      826\n#&gt;  6  2013     4     4     1605\n#&gt;  7  2013     6    16     1306\n#&gt;  8  2013     9    15     1501\n#&gt;  9  2013     8    14     1324\n#&gt; 10  2013     9    30     1258\n\n\nsample_frac(flights_tbl, 0.01) %&gt;% \n  count()\n#&gt; # Source: spark&lt;?&gt; [?? x 1]\n#&gt;       n\n#&gt;   &lt;dbl&gt;\n#&gt; 1  3368",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Using `dplyr`"
    ]
  },
  {
    "objectID": "guides/dplyr.html#hive-functions",
    "href": "guides/dplyr.html#hive-functions",
    "title": "Manipulating Data with dplyr",
    "section": "Hive Functions",
    "text": "Hive Functions\nMany of Hive’s built-in functions (UDF) and built-in aggregate functions (UDAF) can be called inside dplyr’s mutate and summarize. The Languange Reference UDF page provides the list of available functions.\nThe following example uses the datediff and current_date Hive UDFs to figure the difference between the flight_date and the current system date:\n\nflights_tbl %&gt;% \n  mutate(\n    flight_date = paste(year,month,day,sep=\"-\"),\n    days_since = datediff(current_date(), flight_date)\n    ) %&gt;%\n  group_by(flight_date,days_since) %&gt;%\n  count() %&gt;%\n  arrange(-days_since)\n#&gt; # Source:     spark&lt;?&gt; [?? x 3]\n#&gt; # Groups:     flight_date\n#&gt; # Ordered by: -days_since\n#&gt;    flight_date days_since     n\n#&gt;    &lt;chr&gt;            &lt;int&gt; &lt;dbl&gt;\n#&gt;  1 2013-1-1          3527   842\n#&gt;  2 2013-1-2          3526   943\n#&gt;  3 2013-1-3          3525   914\n#&gt;  4 2013-1-4          3524   915\n#&gt;  5 2013-1-5          3523   720\n#&gt;  6 2013-1-6          3522   832\n#&gt;  7 2013-1-7          3521   933\n#&gt;  8 2013-1-8          3520   899\n#&gt;  9 2013-1-9          3519   902\n#&gt; 10 2013-1-10         3518   932\n#&gt; # … with more rows\n\n\nspark_disconnect(sc)",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Using `dplyr`"
    ]
  },
  {
    "objectID": "guides/h2o.html",
    "href": "guides/h2o.html",
    "title": "Sparkling Water (H2O) Machine Learning",
    "section": "",
    "text": "The rsparkling extension package provides bindings to H2O’s distributed machine learning algorithms via sparklyr. In particular, rsparkling allows you to access the machine learning routines provided by the Sparkling Water Spark package.\nTogether with sparklyr’s dplyr interface, you can easily create and tune H2O machine learning workflows on Spark, orchestrated entirely within R.\nrsparkling provides a few simple conversion functions that allow the user to transfer data between Spark DataFrames and H2O Frames. Once the Spark DataFrames are available as H2O Frames, the h2o R interface can be used to train H2O machine learning algorithms on the data.\nA typical machine learning pipeline with rsparkling might be composed of the following stages. To fit a model, you might need to:\n\nPerform SQL queries through the sparklyr dplyr interface,\nUse the sdf_* and ft_* family of functions to generate new columns, or partition your data set,\nConvert your training, validation and/or test data frames into H2O Frames using the as_h2o_frame function,\nChoose an appropriate H2O machine learning algorithm to model your data,\nInspect the quality of your model fit, and use it to make predictions with new data.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "H2O models"
    ]
  },
  {
    "objectID": "guides/h2o.html#overview",
    "href": "guides/h2o.html#overview",
    "title": "Sparkling Water (H2O) Machine Learning",
    "section": "",
    "text": "The rsparkling extension package provides bindings to H2O’s distributed machine learning algorithms via sparklyr. In particular, rsparkling allows you to access the machine learning routines provided by the Sparkling Water Spark package.\nTogether with sparklyr’s dplyr interface, you can easily create and tune H2O machine learning workflows on Spark, orchestrated entirely within R.\nrsparkling provides a few simple conversion functions that allow the user to transfer data between Spark DataFrames and H2O Frames. Once the Spark DataFrames are available as H2O Frames, the h2o R interface can be used to train H2O machine learning algorithms on the data.\nA typical machine learning pipeline with rsparkling might be composed of the following stages. To fit a model, you might need to:\n\nPerform SQL queries through the sparklyr dplyr interface,\nUse the sdf_* and ft_* family of functions to generate new columns, or partition your data set,\nConvert your training, validation and/or test data frames into H2O Frames using the as_h2o_frame function,\nChoose an appropriate H2O machine learning algorithm to model your data,\nInspect the quality of your model fit, and use it to make predictions with new data.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "H2O models"
    ]
  },
  {
    "objectID": "guides/h2o.html#installation",
    "href": "guides/h2o.html#installation",
    "title": "Sparkling Water (H2O) Machine Learning",
    "section": "Installation",
    "text": "Installation\nYou can install the rsparkling package from CRAN as follows:\ninstall.packages(\"rsparkling\")\nThen set the Sparkling Water version for rsparkling.:\noptions(rsparkling.sparklingwater.version = \"2.1.14\")\nFor Spark 2.0.x set rsparkling.sparklingwater.version to 2.0.3 instead, for Spark 1.6.2 use 1.6.8.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "H2O models"
    ]
  },
  {
    "objectID": "guides/h2o.html#using-h2o",
    "href": "guides/h2o.html#using-h2o",
    "title": "Sparkling Water (H2O) Machine Learning",
    "section": "Using H2O",
    "text": "Using H2O\nNow let’s walk through a simple example to demonstrate the use of H2O’s machine learning algorithms within R. We’ll use h2o.glm to fit a linear regression model. Using the built-in mtcars dataset, we’ll try to predict a car’s fuel consumption (mpg) based on its weight (wt), and the number of cylinders the engine contains (cyl).\nFirst, we will initialize a local Spark connection, and copy the mtcars dataset into Spark.\nlibrary(rsparkling)\nlibrary(sparklyr)\nlibrary(h2o)\nlibrary(dplyr)\n\nsc &lt;- spark_connect(\"local\", version = \"2.1.0\")\n\nmtcars_tbl &lt;- copy_to(sc, mtcars, \"mtcars\")\nNow, let’s perform some simple transformations – we’ll\n\nRemove all cars with horsepower less than 100,\nProduce a column encoding whether a car has 8 cylinders or not,\nPartition the data into separate training and test data sets,\nFit a model to our training data set,\nEvaluate our predictive performance on our test dataset.\n\n# transform our data set, and then partition into 'training', 'test'\npartitions &lt;- mtcars_tbl %&gt;%\n  filter(hp &gt;= 100) %&gt;%\n  mutate(cyl8 = cyl == 8) %&gt;%\n  sdf_partition(training = 0.5, test = 0.5, seed = 1099)\nNow, we convert our training and test sets into H2O Frames using rsparkling conversion functions. We have already split the data into training and test frames using dplyr.\ntraining &lt;- as_h2o_frame(sc, partitions$training, strict_version_check = FALSE)\ntest &lt;- as_h2o_frame(sc, partitions$test, strict_version_check = FALSE)\nAlternatively, we can use the h2o.splitFrame() function instead of sdf_partition() to partition the data within H2O instead of Spark (e.g. partitions &lt;- h2o.splitFrame(as_h2o_frame(mtcars_tbl), 0.5))\n# fit a linear model to the training dataset\nglm_model &lt;- h2o.glm(x = c(\"wt\", \"cyl\"), \n                     y = \"mpg\", \n                     training_frame = training,\n                     lambda_search = TRUE)\nFor linear regression models produced by H2O, we can use either print() or summary() to learn a bit more about the quality of our fit. The summary() method returns some extra information about scoring history and variable importance.\nglm_model\n## Model Details:\n## ==============\n## \n## H2ORegressionModel: glm\n## Model ID:  GLM_model_R_1510348062048_1 \n## GLM Model: summary\n##     family     link                               regularization\n## 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.05468 )\n##                                                                 lambda_search\n## 1 nlambda = 100, lambda.max = 5.4682, lambda.min = 0.05468, lambda.1se = -1.0\n##   number_of_predictors_total number_of_active_predictors\n## 1                          2                           2\n##   number_of_iterations                                training_frame\n## 1                  100 frame_rdd_32_929e407384e0082416acd4c9897144a0\n## \n## Coefficients: glm coefficients\n##       names coefficients standardized_coefficients\n## 1 Intercept    32.997281                 16.625000\n## 2       cyl    -0.906688                 -1.349195\n## 3        wt    -2.712562                 -2.282649\n## \n## H2ORegressionMetrics: glm\n## ** Reported on training data. **\n## \n## MSE:  2.03293\n## RMSE:  1.425808\n## MAE:  1.306314\n## RMSLE:  0.08238032\n## Mean Residual Deviance :  2.03293\n## R^2 :  0.8265696\n## Null Deviance :93.775\n## Null D.o.F. :7\n## Residual Deviance :16.26344\n## Residual D.o.F. :5\n## AIC :36.37884\nThe output suggests that our model is a fairly good fit, and that both a cars weight, as well as the number of cylinders in its engine, will be powerful predictors of its average fuel consumption. (The model suggests that, on average, heavier cars consume more fuel.)\nLet’s use our H2O model fit to predict the average fuel consumption on our test data set, and compare the predicted response with the true measured fuel consumption. We’ll build a simple ggplot2 plot that will allow us to inspect the quality of our predictions.\nlibrary(ggplot2)\n\n# compute predicted values on our test dataset\npred &lt;- h2o.predict(glm_model, newdata = test)\n# convert from H2O Frame to Spark DataFrame\npredicted &lt;- as_spark_dataframe(sc, pred, strict_version_check = FALSE)\n\n# extract the true 'mpg' values from our test dataset\nactual &lt;- partitions$test %&gt;%\n  select(mpg) %&gt;%\n  collect() %&gt;%\n  `[[`(\"mpg\")\n\n# produce a data.frame housing our predicted + actual 'mpg' values\ndata &lt;- data.frame(\n  predicted = predicted,\n  actual    = actual\n)\n# a bug in data.frame does not set colnames properly; reset here \nnames(data) &lt;- c(\"predicted\", \"actual\")\n\n# plot predicted vs. actual values\nggplot(data, aes(x = actual, y = predicted)) +\n  geom_abline(lty = \"dashed\", col = \"red\") +\n  geom_point() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  coord_fixed(ratio = 1) +\n  labs(\n    x = \"Actual Fuel Consumption\",\n    y = \"Predicted Fuel Consumption\",\n    title = \"Predicted vs. Actual Fuel Consumption\"\n  )\n\nAlthough simple, our model appears to do a fairly good job of predicting a car’s average fuel consumption.\nAs you can see, we can easily and effectively combine dplyr data transformation pipelines with the machine learning algorithms provided by H2O’s Sparkling Water.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "H2O models"
    ]
  },
  {
    "objectID": "guides/h2o.html#algorithms",
    "href": "guides/h2o.html#algorithms",
    "title": "Sparkling Water (H2O) Machine Learning",
    "section": "Algorithms",
    "text": "Algorithms\nOnce the H2OContext is made available to Spark (as demonstrated below), all of the functions in the standard h2o R interface can be used with H2O Frames (converted from Spark DataFrames). Here is a table of the available algorithms:\n\n\n\nFunction\nDescription\n\n\n\n\nh2o.glm\nGeneralized Linear Model\n\n\nh2o.deeplearning\nMultilayer Perceptron\n\n\nh2o.randomForest\nRandom Forest\n\n\nh2o.gbm\nGradient Boosting Machine\n\n\nh2o.naiveBayes\nNaive-Bayes\n\n\nh2o.prcomp\nPrincipal Components Analysis\n\n\nh2o.svd\nSingular Value Decomposition\n\n\nh2o.glrm\nGeneralized Low Rank Model\n\n\nh2o.kmeans\nK-Means Clustering\n\n\nh2o.anomaly\nAnomaly Detection via Deep Learning Autoencoder\n\n\n\nAdditionally, the h2oEnsemble R package can be used to generate Super Learner ensembles of H2O algorithms:\n\n\n\nFunction\nDescription\n\n\n\n\nh2o.ensemble\nSuper Learner / Stacking\n\n\nh2o.stack\nSuper Learner / Stacking",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "H2O models"
    ]
  },
  {
    "objectID": "guides/h2o.html#transformers",
    "href": "guides/h2o.html#transformers",
    "title": "Sparkling Water (H2O) Machine Learning",
    "section": "Transformers",
    "text": "Transformers\nA model is often fit not on a dataset as-is, but instead on some transformation of that dataset. Spark provides feature transformers, facilitating many common transformations of data within a Spark DataFrame, and sparklyr exposes these within the ft_* family of functions. Transformers can be used on Spark DataFrames, and the final training set can be sent to the H2O cluster for machine learning.\n\n\n\n\n\n\n\n\nFunction\n\n\nDescription\n\n\n\n\n\n\nft_binarizer\n\n\nThreshold numerical features to binary (0/1) feature\n\n\n\n\nft_bucketizer\n\n\nBucketizer transforms a column of continuous features to a column of feature buckets\n\n\n\n\nft_discrete_cosine_transform\n\n\nTransforms a length NN real-valued sequence in the time domain into another length NN real-valued sequence in the frequency domain\n\n\n\n\nft_elementwise_product\n\n\nMultiplies each input vector by a provided weight vector, using element-wise multiplication.\n\n\n\n\nft_index_to_string\n\n\nMaps a column of label indices back to a column containing the original labels as strings\n\n\n\n\nft_quantile_discretizer\n\n\nTakes a column with continuous features and outputs a column with binned categorical features\n\n\n\n\nft_sql_transformer\n\n\nImplements the transformations which are defined by a SQL statement\n\n\n\n\nft_string_indexer\n\n\nEncodes a string column of labels to a column of label indices\n\n\n\n\nft_vector_assembler\n\n\nCombines a given list of columns into a single vector column",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "H2O models"
    ]
  },
  {
    "objectID": "guides/h2o.html#examples",
    "href": "guides/h2o.html#examples",
    "title": "Sparkling Water (H2O) Machine Learning",
    "section": "Examples",
    "text": "Examples\nWe will use the iris data set to examine a handful of learning algorithms and transformers. The iris data set measures attributes for 150 flowers in 3 different species of iris.\niris_tbl &lt;- copy_to(sc, iris, \"iris\", overwrite = TRUE)\niris_tbl\n## # Source:   table&lt;iris&gt; [?? x 5]\n## # Database: spark_connection\n##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species\n##           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;   &lt;chr&gt;\n##  1          5.1         3.5          1.4         0.2  setosa\n##  2          4.9         3.0          1.4         0.2  setosa\n##  3          4.7         3.2          1.3         0.2  setosa\n##  4          4.6         3.1          1.5         0.2  setosa\n##  5          5.0         3.6          1.4         0.2  setosa\n##  6          5.4         3.9          1.7         0.4  setosa\n##  7          4.6         3.4          1.4         0.3  setosa\n##  8          5.0         3.4          1.5         0.2  setosa\n##  9          4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## # ... with more rows\nConvert to an H2O Frame:\niris_hf &lt;- as_h2o_frame(sc, iris_tbl, strict_version_check = FALSE)\n\nK-Means Clustering\nUse H2O’s K-means clustering to partition a dataset into groups. K-means clustering partitions points into k groups, such that the sum of squares from points to the assigned cluster centers is minimized.\nkmeans_model &lt;- h2o.kmeans(training_frame = iris_hf, \n                           x = 3:4,\n                           k = 3,\n                           seed = 1)\nTo look at particular metrics of the K-means model, we can use h2o.centroid_stats() and h2o.centers() or simply print out all the model metrics using print(kmeans_model).\n# print the cluster centers\nh2o.centers(kmeans_model)\n##   petal_length petal_width\n## 1     1.462000     0.24600\n## 2     5.566667     2.05625\n## 3     4.296154     1.32500\n# print the centroid statistics\nh2o.centroid_stats(kmeans_model)\n## Centroid Statistics: \n##   centroid     size within_cluster_sum_of_squares\n## 1        1 50.00000                       1.41087\n## 2        2 48.00000                       9.29317\n## 3        3 52.00000                       7.20274\n\n\nPCA\nUse H2O’s Principal Components Analysis (PCA) to perform dimensionality reduction. PCA is a statistical method to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate in turn has the largest variance possible.\npca_model &lt;- h2o.prcomp(training_frame = iris_hf,\n                        x = 1:4,\n                        k = 4,\n                        seed = 1)\n## Warning in doTryCatch(return(expr), name, parentenv, handler): _train:\n## Dataset used may contain fewer number of rows due to removal of rows with\n## NA/missing values. If this is not desirable, set impute_missing argument in\n## pca call to TRUE/True/true/... depending on the client language.\npca_model\n## Model Details:\n## ==============\n## \n## H2ODimReductionModel: pca\n## Model ID:  PCA_model_R_1510348062048_3 \n## Importance of components: \n##                             pc1      pc2      pc3      pc4\n## Standard deviation     7.861342 1.455041 0.283531 0.154411\n## Proportion of Variance 0.965303 0.033069 0.001256 0.000372\n## Cumulative Proportion  0.965303 0.998372 0.999628 1.000000\n## \n## \n## H2ODimReductionMetrics: pca\n## \n## No model metrics available for PCA\n\n\nRandom Forest\nUse H2O’s Random Forest to perform regression or classification on a dataset. We will continue to use the iris dataset as an example for this problem.\nAs usual, we define the response and predictor variables using the x and y arguments. Since we’d like to do a classification, we need to ensure that the response column is encoded as a factor (enum) column.\ny &lt;- \"Species\"\nx &lt;- setdiff(names(iris_hf), y)\niris_hf[,y] &lt;- as.factor(iris_hf[,y])\nWe can split the iris_hf H2O Frame into a train and test set (the split defaults to 75/25 train/test).\nsplits &lt;- h2o.splitFrame(iris_hf, seed = 1)\nThen we can train a Random Forest model:\nrf_model &lt;- h2o.randomForest(x = x, \n                             y = y,\n                             training_frame = splits[[1]],\n                             validation_frame = splits[[2]],\n                             nbins = 32,\n                             max_depth = 5,\n                             ntrees = 20,\n                             seed = 1)\nSince we passed a validation frame, the validation metrics will be calculated. We can retrieve individual metrics using functions such as h2o.mse(rf_model, valid = TRUE). The confusion matrix can be printed using the following:\nh2o.confusionMatrix(rf_model, valid = TRUE)\n## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n##            setosa versicolor virginica  Error     Rate\n## setosa          7          0         0 0.0000 =  0 / 7\n## versicolor      0         13         0 0.0000 = 0 / 13\n## virginica       0          1        10 0.0909 = 1 / 11\n## Totals          7         14        10 0.0323 = 1 / 31\nTo view the variable importance computed from an H2O model, you can use either the h2o.varimp() or h2o.varimp_plot() functions:\nh2o.varimp_plot(rf_model)\n\n\n\nGradient Boosting Machine\nThe Gradient Boosting Machine (GBM) is one of H2O’s most popular algorithms, as it works well on many types of data. We will continue to use the iris dataset as an example for this problem.\nUsing the same dataset and x and y from above, we can train a GBM:\ngbm_model &lt;- h2o.gbm(x = x, \n                     y = y,\n                     training_frame = splits[[1]],\n                     validation_frame = splits[[2]],                     \n                     ntrees = 20,\n                     max_depth = 3,\n                     learn_rate = 0.01,\n                     col_sample_rate = 0.7,\n                     seed = 1)\nSince this is a multi-class problem, we may be interested in inspecting the confusion matrix on a hold-out set. Since we passed along a validatin_frame at train time, the validation metrics are already computed and we just need to retreive them from the model object.\nh2o.confusionMatrix(gbm_model, valid = TRUE)\n## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n##            setosa versicolor virginica  Error     Rate\n## setosa          7          0         0 0.0000 =  0 / 7\n## versicolor      0         13         0 0.0000 = 0 / 13\n## virginica       0          1        10 0.0909 = 1 / 11\n## Totals          7         14        10 0.0323 = 1 / 31\n\n\nDeep Learning\nUse H2O’s Deep Learning to perform regression or classification on a dataset, extact non-linear features generated by the deep neural network, and/or detect anomalies using a deep learning model with auto-encoding.\nIn this example, we will use the prostate dataset available within the h2o package:\npath &lt;- system.file(\"extdata\", \"prostate.csv\", package = \"h2o\")\nprostate_df &lt;- spark_read_csv(sc, \"prostate\", path)\nhead(prostate_df)\n## # Source:   lazy query [?? x 9]\n## # Database: spark_connection\n##      ID CAPSULE   AGE  RACE DPROS DCAPS   PSA   VOL GLEASON\n##   &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n## 1     1       0    65     1     2     1   1.4   0.0       6\n## 2     2       0    72     1     3     2   6.7   0.0       7\n## 3     3       0    70     1     1     2   4.9   0.0       6\n## 4     4       0    76     2     2     1  51.2  20.0       7\n## 5     5       0    69     1     1     1  12.3  55.9       6\n## 6     6       1    71     1     3     2   3.3   0.0       8\nOnce we’ve done whatever data manipulation is required to run our model we’ll get a reference to it as an h2o frame then split it into training and test sets using the h2o.splitFrame function:\nprostate_hf &lt;- as_h2o_frame(sc, prostate_df, strict_version_check = FALSE)\nsplits &lt;- h2o.splitFrame(prostate_hf, seed = 1)\nNext we define the response and predictor columns.\ny &lt;- \"VOL\"\n#remove response and ID cols\nx &lt;- setdiff(names(prostate_hf), c(\"ID\", y))\nNow we can train a deep neural net.\ndl_fit &lt;- h2o.deeplearning(x = x, y = y,\n                           training_frame = splits[[1]],\n                           epochs = 15,\n                           activation = \"Rectifier\",\n                           hidden = c(10, 5, 10),\n                           input_dropout_ratio = 0.7)\nEvaluate performance on a test set:\nh2o.performance(dl_fit, newdata = splits[[2]])\n## H2ORegressionMetrics: deeplearning\n## \n## MSE:  253.7022\n## RMSE:  15.92803\n## MAE:  12.90077\n## RMSLE:  1.885052\n## Mean Residual Deviance :  253.7022\nNote that the above metrics are not reproducible when H2O’s Deep Learning is run on multiple cores, however, the metrics should be fairly stable across repeat runs.\n\n\nGrid Search\nH2O’s grid search capabilities currently supports traditional (Cartesian) grid search and random grid search. Grid search in R provides the following capabilities:\n\nH2OGrid class: Represents the results of the grid search\nh2o.getGrid(&lt;grid_id&gt;, sort_by, decreasing): Display the specified grid\nh2o.grid: Start a new grid search parameterized by\n\nmodel builder name (e.g., algorithm = \"gbm\")\nmodel parameters (e.g., ntrees = 100)\nhyper_parameters: attribute for passing a list of hyper parameters (e.g., list(ntrees=c(1,100), learn_rate=c(0.1,0.001)))\nsearch_criteria: optional attribute for specifying more a advanced search strategy\n\n\n\nCartesian Grid Search\nBy default, h2o.grid() will train a Cartesian grid search – meaning, all possible models in the specified grid. In this example, we will re-use the prostate data as an example dataset for a regression problem.\nsplits &lt;- h2o.splitFrame(prostate_hf, seed = 1)\n\ny &lt;- \"VOL\"\n#remove response and ID cols\nx &lt;- setdiff(names(prostate_hf), c(\"ID\", y))\nAfter prepping the data, we define a grid and execute the grid search.\n# GBM hyperparamters\ngbm_params1 &lt;- list(learn_rate = c(0.01, 0.1),\n                    max_depth = c(3, 5, 9),\n                    sample_rate = c(0.8, 1.0),\n                    col_sample_rate = c(0.2, 0.5, 1.0))\n\n# Train and validate a grid of GBMs\ngbm_grid1 &lt;- h2o.grid(\"gbm\", x = x, y = y,\n                      grid_id = \"gbm_grid1\",\n                      training_frame = splits[[1]],\n                      validation_frame = splits[[1]],\n                      ntrees = 100,\n                      seed = 1,\n                      hyper_params = gbm_params1)\n\n# Get the grid results, sorted by validation MSE\ngbm_gridperf1 &lt;- h2o.getGrid(grid_id = \"gbm_grid1\", \n                             sort_by = \"mse\", \n                             decreasing = FALSE)\ngbm_gridperf1\n## H2O Grid Details\n## ================\n## \n## Grid ID: gbm_grid1 \n## Used hyper parameters: \n##   -  col_sample_rate \n##   -  learn_rate \n##   -  max_depth \n##   -  sample_rate \n## Number of models: 36 \n## Number of failed models: 0 \n## \n## Hyper-Parameter Search Summary: ordered by increasing mse\n##   col_sample_rate learn_rate max_depth sample_rate          model_ids\n## 1             1.0        0.1         9         1.0 gbm_grid1_model_35\n## 2             0.5        0.1         9         1.0 gbm_grid1_model_34\n## 3             1.0        0.1         9         0.8 gbm_grid1_model_17\n## 4             0.5        0.1         9         0.8 gbm_grid1_model_16\n## 5             1.0        0.1         5         0.8 gbm_grid1_model_11\n##                  mse\n## 1  88.10947523138782\n## 2  102.3118989994892\n## 3 102.78632321923726\n## 4  126.4217260351778\n## 5  149.6066650109763\n## \n## ---\n##    col_sample_rate learn_rate max_depth sample_rate          model_ids\n## 31             0.5       0.01         3         0.8  gbm_grid1_model_1\n## 32             0.2       0.01         5         1.0 gbm_grid1_model_24\n## 33             0.5       0.01         3         1.0 gbm_grid1_model_19\n## 34             0.2       0.01         5         0.8  gbm_grid1_model_6\n## 35             0.2       0.01         3         1.0 gbm_grid1_model_18\n## 36             0.2       0.01         3         0.8  gbm_grid1_model_0\n##                   mse\n## 31  324.8117304723162\n## 32 325.10992525687294\n## 33 325.27898443785045\n## 34 329.36983845305735\n## 35 338.54411936919456\n## 36  339.7744828617712\n\n\nRandom Grid Search\nH2O’s Random Grid Search samples from the given parameter space until a set of constraints is met. The user can specify the total number of desired models using (e.g. max_models = 40), the amount of time (e.g. max_runtime_secs = 1000), or tell the grid to stop after performance stops improving by a specified amount. Random Grid Search is a practical way to arrive at a good model without too much effort.\nThe example below is set to run fairly quickly – increase max_runtime_secs or max_models to cover more of the hyperparameter space in your grid search. Also, you can expand the hyperparameter space of each of the algorithms by modifying the definition of hyper_param below.\n# GBM hyperparamters\ngbm_params2 &lt;- list(learn_rate = seq(0.01, 0.1, 0.01),\n                    max_depth = seq(2, 10, 1),\n                    sample_rate = seq(0.5, 1.0, 0.1),\n                    col_sample_rate = seq(0.1, 1.0, 0.1))\nsearch_criteria2 &lt;- list(strategy = \"RandomDiscrete\", \n                         max_models = 50)\n\n# Train and validate a grid of GBMs\ngbm_grid2 &lt;- h2o.grid(\"gbm\", x = x, y = y,\n                      grid_id = \"gbm_grid2\",\n                      training_frame = splits[[1]],\n                      validation_frame = splits[[2]],\n                      ntrees = 100,\n                      seed = 1,\n                      hyper_params = gbm_params2,\n                      search_criteria = search_criteria2)\n\n# Get the grid results, sorted by validation MSE\ngbm_gridperf2 &lt;- h2o.getGrid(grid_id = \"gbm_grid2\", \n                             sort_by = \"mse\", \n                             decreasing = FALSE)\nTo get the best model, as measured by validation MSE, we simply grab the first row of the gbm_gridperf2@summary_table object, since this table is already sorted such that the lowest MSE model is on top.\ngbm_gridperf2@summary_table[1,]\n## Hyper-Parameter Search Summary: ordered by increasing mse\n##   col_sample_rate learn_rate max_depth sample_rate          model_ids\n## 1             0.8       0.01         2         0.7 gbm_grid2_model_35\n##                  mse\n## 1 244.61196951586288\nIn the examples above, we generated two different grids, specified by grid_id. The first grid was called grid_id = \"gbm_grid1\" and the second was called grid_id = \"gbm_grid2\". However, if we are using the same dataset & algorithm in two grid searches, it probably makes more sense just to add the results of the second grid search to the first. If you want to add models to an existing grid, rather than create a new one, you simply re-use the same grid_id.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "H2O models"
    ]
  },
  {
    "objectID": "guides/h2o.html#exporting-models",
    "href": "guides/h2o.html#exporting-models",
    "title": "Sparkling Water (H2O) Machine Learning",
    "section": "Exporting Models",
    "text": "Exporting Models\nThere are two ways of exporting models from H2O – saving models as a binary file, or saving models as pure Java code.\n\nBinary Models\nThe more traditional method is to save a binary model file to disk using the h2o.saveModel() function. To load the models using h2o.loadModel(), the same version of H2O that generated the models is required. This method is commonly used when H2O is being used in a non-production setting.\nA binary model can be saved as follows:\nh2o.saveModel(my_model, path = \"/Users/me/h2omodels\")\n\n\nJava (POJO) Models\nOne of the most valuable features of H2O is it’s ability to export models as pure Java code, or rather, a “Plain Old Java Object” (POJO). You can learn more about H2O POJO models in this POJO quickstart guide. The POJO method is used most commonly when a model is deployed in a production setting. POJO models are ideal for when you need very fast prediction response times, and minimal requirements – the POJO is a standalone Java class with no dependencies on the full H2O stack.\nTo generate the POJO for your model, use the following command:\nh2o.download_pojo(my_model, path = \"/Users/me/h2omodels\")\nFinally, disconnect with:\nspark_disconnect_all()\n## [1] 1\nYou can learn more about how to take H2O models to production in the productionizing H2O models section of the H2O docs.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "H2O models"
    ]
  },
  {
    "objectID": "guides/h2o.html#additional-resources",
    "href": "guides/h2o.html#additional-resources",
    "title": "Sparkling Water (H2O) Machine Learning",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nMain documentation site for Sparkling Water (and all H2O software projects)\nH2O.ai website\n\nIf you are new to H2O for machine learning, we recommend you start with the Intro to H2O Tutorial, followed by the H2O Grid Search & Model Selection Tutorial. There are a number of other H2O R tutorials and demos available, as well as the H2O World 2015 Training Gitbook, and the Machine Learning with R and H2O Booklet (pdf).",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "H2O models"
    ]
  },
  {
    "objectID": "guides/mlib.html",
    "href": "guides/mlib.html",
    "title": "Spark Machine Learning Library (MLlib)",
    "section": "",
    "text": "sparklyr provides bindings to Spark’s distributed machine learning library. In particular, sparklyr allows you to access the machine learning routines provided by the spark.ml package. Together with sparklyr’s dplyr interface, you can easily create and tune machine learning workflows on Spark, orchestrated entirely within R.\nsparklyr provides three families of functions that you can use with Spark machine learning:\n\nMachine learning algorithms for analyzing data (ml_*)\nFeature transformers for manipulating individual features (ft_*)\nFunctions for manipulating Spark DataFrames (sdf_*)\n\nAn analytic workflow with sparklyr might be composed of the following stages. For an example see Example Workflow.\n\nPerform SQL queries through the sparklyr dplyr interface\nUse the sdf_* and ft_* family of functions to generate new columns, or partition your data set\nChoose an appropriate machine learning algorithm from the ml_* family of functions to model your data\nInspect the quality of your model fit, and use it to make predictions with new data.\nCollect the results for visualization and further analysis in R",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Spark ML Overview"
    ]
  },
  {
    "objectID": "guides/mlib.html#overview",
    "href": "guides/mlib.html#overview",
    "title": "Spark Machine Learning Library (MLlib)",
    "section": "",
    "text": "sparklyr provides bindings to Spark’s distributed machine learning library. In particular, sparklyr allows you to access the machine learning routines provided by the spark.ml package. Together with sparklyr’s dplyr interface, you can easily create and tune machine learning workflows on Spark, orchestrated entirely within R.\nsparklyr provides three families of functions that you can use with Spark machine learning:\n\nMachine learning algorithms for analyzing data (ml_*)\nFeature transformers for manipulating individual features (ft_*)\nFunctions for manipulating Spark DataFrames (sdf_*)\n\nAn analytic workflow with sparklyr might be composed of the following stages. For an example see Example Workflow.\n\nPerform SQL queries through the sparklyr dplyr interface\nUse the sdf_* and ft_* family of functions to generate new columns, or partition your data set\nChoose an appropriate machine learning algorithm from the ml_* family of functions to model your data\nInspect the quality of your model fit, and use it to make predictions with new data.\nCollect the results for visualization and further analysis in R",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Spark ML Overview"
    ]
  },
  {
    "objectID": "guides/mlib.html#algorithms",
    "href": "guides/mlib.html#algorithms",
    "title": "Spark Machine Learning Library (MLlib)",
    "section": "Algorithms",
    "text": "Algorithms\nSpark’s machine learning library can be accessed from sparklyr through the ml_* set of functions. Visit the sparklyr reference page to see the complete list of available algorithms: Reference - Spark Machine Learning\n\nFormulas\nThe ml_* functions take the arguments response and features. But features can also be a formula with main effects (it currently does not accept interaction terms). The intercept term can be omitted by using -1.\nThe following two statements are equivalent:\nml_linear_regression(z ~ -1 + x + y)\nml_linear_regression(intercept = FALSE, response = \"z\", features = c(\"x\", \"y\"))\n\n\nOptions\nThe Spark model output can be modified with the ml_options argument in the ml_* functions. The ml_options is an experts only interface for tweaking the model output. For example, model.transform can be used to mutate the Spark model object before the fit is performed.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Spark ML Overview"
    ]
  },
  {
    "objectID": "guides/mlib.html#transformers",
    "href": "guides/mlib.html#transformers",
    "title": "Spark Machine Learning Library (MLlib)",
    "section": "Transformers",
    "text": "Transformers\nA model is often fit not on a data set as-is, but instead on some transformation of that data set. Spark provides feature transformers, facilitating many common transformations of data within a Spark DataFrame, and sparklyr exposes these within the ft_* family of functions. These routines generally take one or more input columns, and generate a new output column formed as a transformation of those columns. Visit the sparklyr reference page to see the complete list of available transformers: Reference - Feature Transformers",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Spark ML Overview"
    ]
  },
  {
    "objectID": "guides/mlib.html#examples",
    "href": "guides/mlib.html#examples",
    "title": "Spark Machine Learning Library (MLlib)",
    "section": "Examples",
    "text": "Examples\nWe will use the iris data set to examine a handful of learning algorithms and transformers. The iris data set measures attributes for 150 flowers in 3 different species of iris.\n\nlibrary(sparklyr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nsc &lt;- spark_connect(master = \"local\")\n\niris_tbl &lt;- copy_to(sc, iris, \"iris\", overwrite = TRUE)\n\niris_tbl\n#&gt; # Source: spark&lt;iris&gt; [?? x 5]\n#&gt;    Sepal_Length Sepal_Width Petal_Length Petal_Width Species\n#&gt;           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  \n#&gt;  1          5.1         3.5          1.4         0.2 setosa \n#&gt;  2          4.9         3            1.4         0.2 setosa \n#&gt;  3          4.7         3.2          1.3         0.2 setosa \n#&gt;  4          4.6         3.1          1.5         0.2 setosa \n#&gt;  5          5           3.6          1.4         0.2 setosa \n#&gt;  6          5.4         3.9          1.7         0.4 setosa \n#&gt;  7          4.6         3.4          1.4         0.3 setosa \n#&gt;  8          5           3.4          1.5         0.2 setosa \n#&gt;  9          4.4         2.9          1.4         0.2 setosa \n#&gt; 10          4.9         3.1          1.5         0.1 setosa \n#&gt; # … with more rows\n\n\nK-Means Clustering\nUse Spark’s K-means clustering to partition a dataset into groups. K-means clustering partitions points into k groups, such that the sum of squares from points to the assigned cluster centers is minimized.\n\nkmeans_model &lt;- iris_tbl %&gt;%\n  ml_kmeans(k = 3, features = c(\"Petal_Length\", \"Petal_Width\"))\n\nkmeans_model\n#&gt; K-means clustering with 3 clusters\n#&gt; \n#&gt; Cluster centers:\n#&gt;   Petal_Length Petal_Width\n#&gt; 1     5.626087    2.047826\n#&gt; 2     1.462000    0.246000\n#&gt; 3     4.292593    1.359259\n#&gt; \n#&gt; Within Set Sum of Squared Errors =  not computed.\n\nRun and collect predictions into R:\n\npredicted &lt;- ml_predict(kmeans_model, iris_tbl) %&gt;%\n  collect()\n\ntable(predicted$Species, predicted$prediction)\n#&gt;             \n#&gt;               0  1  2\n#&gt;   setosa      0 50  0\n#&gt;   versicolor  2  0 48\n#&gt;   virginica  44  0  6\n\nUse the collected data to plot the results:\n\npredicted %&gt;%\n  ggplot(aes(Petal_Length, Petal_Width)) +\n  geom_point(aes(Petal_Width, Petal_Length, col = factor(prediction + 1)),\n    size = 2, alpha = 0.5\n  ) +\n  geom_point(\n    data = kmeans_model$centers, aes(Petal_Width, Petal_Length),\n    col = scales::muted(c(\"red\", \"green\", \"blue\")),\n    pch = \"x\", size = 12\n  ) +\n  scale_color_discrete(\n    name = \"Predicted Cluster\",\n    labels = paste(\"Cluster\", 1:3)\n  ) +\n  labs(\n    x = \"Petal Length\",\n    y = \"Petal Width\",\n    title = \"K-Means Clustering\",\n    subtitle = \"Use Spark.ML to predict cluster membership with the iris dataset.\"\n  )\n\n\n\n\n\n\n\n\n\n\nLinear Regression\nUse Spark’s linear regression to model the linear relationship between a response variable and one or more explanatory variables.\n\nlm_model &lt;- iris_tbl %&gt;%\n  ml_linear_regression(Petal_Length ~ Petal_Width)\n\nExtract the slope and the intercept into discrete R variables. We will use them to plot:\n\nspark_slope &lt;- coef(lm_model)[[\"Petal_Width\"]]\nspark_intercept &lt;- coef(lm_model)[[\"(Intercept)\"]]\n\n\niris_tbl %&gt;%\n  select(Petal_Width, Petal_Length) %&gt;%\n  collect() %&gt;%\n  ggplot(aes(Petal_Length, Petal_Width)) +\n  geom_point(aes(Petal_Width, Petal_Length), size = 2, alpha = 0.5) +\n  geom_abline(aes(\n    slope = spark_slope,\n    intercept = spark_intercept\n  ),\n  color = \"red\"\n  ) +\n  labs(\n    x = \"Petal Width\",\n    y = \"Petal Length\",\n    title = \"Linear Regression: Petal Length ~ Petal Width\",\n    subtitle = \"Use Spark.ML linear regression to predict petal length as a function of petal width.\"\n  )\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\nUse Spark’s logistic regression to perform logistic regression, modeling a binary outcome as a function of one or more explanatory variables.\n\nglm_model &lt;- iris_tbl %&gt;% \n  mutate(is_setosa = ifelse(Species == \"setosa\", 1, 0)) %&gt;% \n  select_if(is.numeric) %&gt;% \n  ml_logistic_regression(is_setosa ~.)\n\nsummary(glm_model)\n#&gt; Coefficients:\n#&gt;  (Intercept) Sepal_Length  Sepal_Width Petal_Length  Petal_Width \n#&gt;  -0.02904898  -7.23312634  28.56334798  -9.02580864 -20.62238442\n\n\nml_predict(glm_model, iris_tbl) %&gt;% \n  count(Species, prediction) \n#&gt; # Source: spark&lt;?&gt; [?? x 3]\n#&gt; # Groups: Species\n#&gt;   Species    prediction     n\n#&gt;   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 virginica           0    50\n#&gt; 2 versicolor          0    50\n#&gt; 3 setosa              1    50\n\n\n\nPCA\nUse Spark’s Principal Components Analysis (PCA) to perform dimensionality reduction. PCA is a statistical method to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate in turn has the largest variance possible.\n\npca_model &lt;- tbl(sc, \"iris\") %&gt;%\n  select(-Species) %&gt;%\n  ml_pca()\n\npca_model\n#&gt; Explained variance:\n#&gt; \n#&gt;         PC1         PC2         PC3         PC4 \n#&gt; 0.924618723 0.053066483 0.017102610 0.005212184 \n#&gt; \n#&gt; Rotation:\n#&gt;                      PC1         PC2         PC3        PC4\n#&gt; Sepal_Length -0.36138659 -0.65658877  0.58202985  0.3154872\n#&gt; Sepal_Width   0.08452251 -0.73016143 -0.59791083 -0.3197231\n#&gt; Petal_Length -0.85667061  0.17337266 -0.07623608 -0.4798390\n#&gt; Petal_Width  -0.35828920  0.07548102 -0.54583143  0.7536574\n\n\n\nRandom Forest\nUse Spark’s Random Forest to perform regression or multiclass classification.\n\nrf_model &lt;- iris_tbl %&gt;%\n  ml_random_forest(\n    Species ~ Petal_Length + Petal_Width, type = \"classification\"\n    )\n\nUse ml_predict() to use the apply the new model back to the data.\n\nrf_predict &lt;- ml_predict(rf_model, iris_tbl) \n\nglimpse(rf_predict)\n#&gt; Rows: ??\n#&gt; Columns: 14\n#&gt; Database: spark_connection\n#&gt; $ Sepal_Length           &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.…\n#&gt; $ Sepal_Width            &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.…\n#&gt; $ Petal_Length           &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.…\n#&gt; $ Petal_Width            &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.…\n#&gt; $ Species                &lt;chr&gt; \"setosa\", \"setosa\", \"setosa…\n#&gt; $ features               &lt;list&gt; &lt;1.4, 0.2&gt;, &lt;1.4, 0.2&gt;, &lt;1…\n#&gt; $ label                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ rawPrediction          &lt;list&gt; &lt;20, 0, 0&gt;, &lt;20, 0, 0&gt;, &lt;2…\n#&gt; $ probability            &lt;list&gt; &lt;1, 0, 0&gt;, &lt;1, 0, 0&gt;, &lt;1, …\n#&gt; $ prediction             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ predicted_label        &lt;chr&gt; \"setosa\", \"setosa\", \"setosa…\n#&gt; $ probability_setosa     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n#&gt; $ probability_versicolor &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#&gt; $ probability_virginica  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\nTo get an idea of the model effectiveness, use count() to compare species against the prediction. ml_predict() created a variable called predicted_label. That variable contains the string value of the prediction:\n\nrf_predict %&gt;% \n  count(Species, predicted_label) \n#&gt; # Source: spark&lt;?&gt; [?? x 3]\n#&gt; # Groups: Species\n#&gt;   Species    predicted_label     n\n#&gt;   &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n#&gt; 1 setosa     setosa             50\n#&gt; 2 versicolor virginica           1\n#&gt; 3 virginica  virginica          50\n#&gt; 4 versicolor versicolor         49\n\n\n\nFT String Indexing\nUse ft_string_indexer() and ft_index_to_string() to convert a character column into a numeric column and back again.\n\nft_string2idx &lt;- iris_tbl %&gt;%\n  ft_string_indexer(\"Species\", \"Species_idx\") %&gt;%\n  ft_index_to_string(\"Species_idx\", \"Species_remap\") %&gt;% \n  select(Species, Species_remap, Species_idx)\n\nTo see the value assigned to each value in Species, we can pull the aggregates of all the species, re-mapped species and index combinations:\n\nft_string2idx %&gt;% \n  group_by_all() %&gt;% \n  summarise(count = n(), .groups = \"keep\")\n#&gt; # Source: spark&lt;?&gt; [?? x 4]\n#&gt; # Groups: Species, Species_remap, Species_idx\n#&gt;   Species    Species_remap Species_idx count\n#&gt;   &lt;chr&gt;      &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 setosa     setosa                  0    50\n#&gt; 2 versicolor versicolor              1    50\n#&gt; 3 virginica  virginica               2    50\n\n\n\nSDF Partitioning\nSplit a Spark DataFrame into “training” and “test” datasets.\n\npartitions &lt;- iris_tbl %&gt;%\n  sdf_random_split(training = 0.75, test = 0.25, seed = 1099)\n\nThe partitions variable is now a list with two elements called training and test. It does not contain any data. It is just a pointer to where Spark has separated the data, so nothing is downloaded into R. Use partitions$training to access the data the Spark has separated for that purpose.\n\nfit &lt;- partitions$training %&gt;%\n  ml_linear_regression(Petal_Length ~ Petal_Width)\n\nUse ml_predict() to then calculate the mse of the “test” data:\n\nml_predict(fit, partitions$test) %&gt;%\n  mutate(resid = Petal_Length - prediction) %&gt;%\n  summarize(mse = mean(resid ^ 2, na.rm = TRUE)) \n#&gt; # Source: spark&lt;?&gt; [?? x 1]\n#&gt;     mse\n#&gt;   &lt;dbl&gt;\n#&gt; 1 0.212\n\n\n\nDisconnect from Spark\nLastly, cleanup your session by disconnecting Spark:\n\nspark_disconnect(sc)",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Spark ML Overview"
    ]
  },
  {
    "objectID": "guides/streaming.html",
    "href": "guides/streaming.html",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "",
    "text": "As stated in the Spark’s official site, Spark Streaming makes it easy to build scalable fault-tolerant streaming applications. Because is part of the Spark API, it is possible to re-use query code that queries the current state of the stream, as well as joining the streaming data with historical data. Please see Spark’s official documentation for a deeper look into Spark Streaming.\nThe sparklyr interface provides the following:\n\nAbility to run dplyr, SQL, spark_apply(), and PipelineModels against a stream\nRead in multiple formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc\nWrite stream results to Spark memory and the following file formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc\nAn out-of-the box graph visualization to monitor the stream\nA new reactiveSpark() function, that allows Shiny apps to poll the contents of the stream create Shiny apps that are able to read the contents of the stream",
    "crumbs": [
      "Guides",
      "Non-rectangular Data",
      "Streaming Data"
    ]
  },
  {
    "objectID": "guides/streaming.html#the-sparklyr-interface",
    "href": "guides/streaming.html#the-sparklyr-interface",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "",
    "text": "As stated in the Spark’s official site, Spark Streaming makes it easy to build scalable fault-tolerant streaming applications. Because is part of the Spark API, it is possible to re-use query code that queries the current state of the stream, as well as joining the streaming data with historical data. Please see Spark’s official documentation for a deeper look into Spark Streaming.\nThe sparklyr interface provides the following:\n\nAbility to run dplyr, SQL, spark_apply(), and PipelineModels against a stream\nRead in multiple formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc\nWrite stream results to Spark memory and the following file formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc\nAn out-of-the box graph visualization to monitor the stream\nA new reactiveSpark() function, that allows Shiny apps to poll the contents of the stream create Shiny apps that are able to read the contents of the stream",
    "crumbs": [
      "Guides",
      "Non-rectangular Data",
      "Streaming Data"
    ]
  },
  {
    "objectID": "guides/streaming.html#interacting-with-a-stream",
    "href": "guides/streaming.html#interacting-with-a-stream",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Interacting with a stream",
    "text": "Interacting with a stream\nA good way of looking at the way how Spark streams update is as a three stage operation:\n\nInput - Spark reads the data inside a given folder. The folder is expected to contain multiple data files, with new files being created containing the most current stream data.\nProcessing - Spark applies the desired operations on top of the data. These operations could be data manipulations (dplyr, SQL), data transformations (sdf operations, PipelineModel predictions), or native R manipulations (spark_apply()).\nOutput - The results of processing the input files are saved in a different folder.\n\nIn the same way all of the read and write operations in sparklyr for Spark Standalone, or in sparklyr’s local mode, the input and output folders are actual OS file system folders. For Hadoop clusters, these will be folder locations inside the HDFS.",
    "crumbs": [
      "Guides",
      "Non-rectangular Data",
      "Streaming Data"
    ]
  },
  {
    "objectID": "guides/streaming.html#example-1---inputoutput",
    "href": "guides/streaming.html#example-1---inputoutput",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 1 - Input/Output",
    "text": "Example 1 - Input/Output\nThe first intro example is a small script that can be used with a local master. The result should be to see the stream_view() app showing live the number of records processed for each iteration of test data being sent to the stream.\n\nlibrary(future)\nlibrary(sparklyr)\n\nsc &lt;- spark_connect(master = \"local\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\nstream_generate_test(iterations = 1)\nread_folder &lt;- stream_read_csv(sc, \"source\") \nwrite_output &lt;- stream_write_csv(read_folder, \"source-out\")\ninvisible(future(stream_generate_test(interval = 0.5)))\n\nstream_view(write_output)\n\n\n\nstream_stop(write_output)\nspark_disconnect(sc)\n\n\nCode breakdown\n\nOpen the Spark connection\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\")\n\nOptional step. This resets the input and output folders. It makes it easier to run the code multiple times in a clean manner.\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\nProduces a single test file inside the “source” folder. This allows the “read” function to infer CSV file definition.\n\nstream_generate_test(iterations = 1)\nlist.files(\"source\")\n\n[1] \"stream_1.csv\"\nPoints the stream reader to the folder where the streaming files will be placed. Since it is primed with a single CSV file, it will use as the expected layout of subsequent files. By default, stream_read_csv() creates a single integer variable data frame.\n\nread_folder &lt;- stream_read_csv(sc, \"source\")\n\nThe output writer is what starts the streaming job. It will start monitoring the input folder, and then write the new results in the “source-out” folder. So as new records stream in, new files will be created in the “source-out” folder. Since there are no operations on the incoming data at this time, the output files will have the same exact raw data as the input files. The only difference is that the files and sub folders within “source-out” will be structured how Spark structures data folders.\n\nwrite_output &lt;- stream_write_csv(read_folder, \"source-out\")\nlist.files(\"source-out\")\n\n[1] \"_spark_metadata\"                                     \"checkpoint\"\n[3] \"part-00000-1f29719a-2314-40e1-b93d-a647a3d57154-c000.csv\"\nThe test generation function will run 100 files every 0.2 seconds. To run the tests “out-of-sync” with the current R session, the future package is used.\n\nlibrary(future)\n\ninvisible(\n  future(\n    stream_generate_test(interval = 0.2, iterations = 100)\n    )\n  )\n\nThe stream_view() function can be used before the 50 tests are complete because of the use of the future package. It will monitor the status of the job that write_output is pointing to and provide information on the amount of data coming into the “source” folder and going out into the “source-out” folder.\n\nstream_view(write_output)\n\nThe monitor will continue to run even after the tests are complete. To end the experiment, stop the Shiny app and then use the following to stop the stream and close the Spark session.\n\nstream_stop(write_output)\n\nspark_disconnect(sc)",
    "crumbs": [
      "Guides",
      "Non-rectangular Data",
      "Streaming Data"
    ]
  },
  {
    "objectID": "guides/streaming.html#example-2---processing",
    "href": "guides/streaming.html#example-2---processing",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 2 - Processing",
    "text": "Example 2 - Processing\nThe second example builds on the first. It adds a processing step that manipulates the input data before saving it to the output folder. In this case, a new binary field is added indicating if the value from x is over 400 or not. This time, while run the second code chunk in this example a few times during the stream tests to see the aggregated values change.\n\nlibrary(future)\nlibrary(sparklyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nsc &lt;- spark_connect(master = \"local\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\nstream_generate_test(iterations = 1)\nread_folder &lt;- stream_read_csv(sc, \"source\") \n\nprocess_stream &lt;- read_folder %&gt;%\n  mutate(x = as.double(x)) %&gt;%\n  ft_binarizer(\n    input_col = \"x\",\n    output_col = \"over\",\n    threshold = 400\n  )\n\nwrite_output &lt;- stream_write_csv(process_stream, \"source-out\")\ninvisible(future(stream_generate_test(interval = 0.2, iterations = 100)))\n\nRun this code a few times during the experiment:\n\nspark_read_csv(sc, \"stream\", \"source-out\", memory = FALSE) %&gt;%\n  group_by(over) %&gt;%\n  tally()\n\nThe results would look similar to this. The n totals will increase as the experiment progresses.\n# Source:   lazy query [?? x 2]\n# Database: spark_connection\n   over     n\n  &lt;dbl&gt; &lt;dbl&gt;\n1     0 40215\n2     1 60006\nClean up after the experiment\n\nstream_stop(write_output)\nspark_disconnect(sc)\n\n\nCode breakdown\n\nThe processing starts with the read_folder variable that contains the input stream. It coerces the integer field x, into a type double. This is because the next function, ft_binarizer() does not accept integers. The binarizer determines if x is over 400 or not. This is a good illustration of how dplyr can help simplify the manipulation needed during the processing stage.\n\nprocess_stream &lt;- read_folder %&gt;%\n  mutate(x = as.double(x)) %&gt;%\n  ft_binarizer(\n    input_col = \"x\",\n    output_col = \"over\",\n    threshold = 400\n  )\n\nThe output now needs to write-out the processed data instead of the raw input data. Swap read_folder with process_stream.\n\nwrite_output &lt;- stream_write_csv(process_stream, \"source-out\")\n\nThe “source-out” folder can be treated as a if it was a single table within Spark. Using spark_read_csv(), the data can be mapped, but not brought into memory (memory = FALSE). This allows the current results to be further analyzed using regular dplyr commands.\n\nspark_read_csv(sc, \"stream\", \"source-out\", memory = FALSE) %&gt;%\n  group_by(over) %&gt;%\n  tally()",
    "crumbs": [
      "Guides",
      "Non-rectangular Data",
      "Streaming Data"
    ]
  },
  {
    "objectID": "guides/streaming.html#example-3---aggregate-in-process-and-output-to-memory",
    "href": "guides/streaming.html#example-3---aggregate-in-process-and-output-to-memory",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 3 - Aggregate in process and output to memory",
    "text": "Example 3 - Aggregate in process and output to memory\nAnother option is to save the results of the processing into a in-memory Spark table. Unless intentionally saving it to disk, the table and its data will only exist while the Spark session is active.\nThe biggest advantage of using Spark memory as the target, is that it will allow for aggregation to happen during processing. This is an advantage because aggregation is not allowed for any file output, expect Kafka, on the input/process stage.\nUsing example 2 as the base, this example code will perform some aggregations to the current stream input and save only those summarized results into Spark memory:\n\nlibrary(future)\nlibrary(sparklyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nsc &lt;- spark_connect(master = \"local\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\n\nstream_generate_test(iterations = 1)\nread_folder &lt;- stream_read_csv(sc, \"source\") \n\nprocess_stream &lt;- read_folder %&gt;%\n  stream_watermark() %&gt;%\n  group_by(timestamp) %&gt;%\n  summarise(\n    max_x = max(x, na.rm = TRUE),\n    min_x = min(x, na.rm = TRUE),\n    count = n()\n  )\n\nwrite_output &lt;- stream_write_memory(process_stream, name = \"stream\")\n\ninvisible(future(stream_generate_test()))\n\nRun this command a different times while the experiment is running:\n\ntbl(sc, \"stream\") \n\nClean up after the experiment\n\nstream_stop(write_output)\n\nspark_disconnect(sc)\n\n\nCode breakdown\n\nThe stream_watermark() functions add a new timestamp variable that is then used in the group_by() command. This is required by Spark Stream to accept summarized results as output of the stream. The second step is to simply decide what kinds of aggregations we need to perform. In this case, a simply max, min and count are performed.\n\nprocess_stream &lt;- read_folder %&gt;%\n  stream_watermark() %&gt;%\n  group_by(timestamp) %&gt;%\n  summarise(\n    max_x = max(x, na.rm = TRUE),\n    min_x = min(x, na.rm = TRUE),\n    count = n()\n  )\n\nThe spark_write_memory() function is used to write the output to Spark memory. The results will appear as a table of the Spark session with the name assigned in the name argument, in this case the name selected is: “stream”.\n\nwrite_output &lt;- stream_write_memory(process_stream, name = \"stream\")\n\nTo query the current data in the “stream” table can be queried by using the dplyr tbl() command.\n\ntbl(sc, \"stream\")",
    "crumbs": [
      "Guides",
      "Non-rectangular Data",
      "Streaming Data"
    ]
  },
  {
    "objectID": "guides/streaming.html#example-4---shiny-integration",
    "href": "guides/streaming.html#example-4---shiny-integration",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 4 - Shiny integration",
    "text": "Example 4 - Shiny integration\nsparklyr provides a new Shiny function called reactiveSpark(). It can take a Spark data frame, in this case the one created as a result of the stream processing, and then creates a Spark memory stream table, the same way a table is created in example 3.\n\nlibrary(future)\nlibrary(sparklyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(ggplot2)\n\nsc &lt;- spark_connect(master = \"local\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\nstream_generate_test(iterations = 1)\nread_folder &lt;- stream_read_csv(sc, \"source\") \n\nprocess_stream &lt;- read_folder %&gt;%\n  stream_watermark() %&gt;%\n  group_by(timestamp) %&gt;%\n  summarise(\n    max_x = max(x, na.rm = TRUE),\n    min_x = min(x, na.rm = TRUE),\n    count = n()\n  )\n\ninvisible(future(stream_generate_test(interval = 0.2, iterations = 100)))\n\nlibrary(shiny)\nui &lt;- function(){\n  tableOutput(\"table\")\n}\nserver &lt;- function(input, output, session){\n  \n  ps &lt;- reactiveSpark(process_stream)\n  \n  output$table &lt;- renderTable({\n    ps() %&gt;%\n      mutate(timestamp = as.character(timestamp)) \n    })\n}\nrunGadget(ui, server)\n\n\n\nCode breakdown\n\nNotice that there is no stream_write_... command. The reason is that reactiveSpark() function contains the stream_write_memory() function.\nThis very basic Shiny app simply displays the output of a table in the ui section\n\nlibrary(shiny)\n\nui &lt;- function(){\n  tableOutput(\"table\")\n}\n\nIn the server section, the reactiveSpark() function will update every time there’s a change to the stream and return a data frame. The results are saved to a variable called ps() in this script. Treat the ps() variable as a regular table that can be piped from, as shown in the example. In this case, the timestamp variable is converted to string for to make it easier to read.\n\nserver &lt;- function(input, output, session){\n\n  ps &lt;- reactiveSpark(process_stream)\n\n  output$table &lt;- renderTable({\n    ps() %&gt;%\n      mutate(timestamp = as.character(timestamp)) \n  })\n}\n\nUse runGadget() to display the Shiny app in the Viewer pane. This is optional, the app can be run using normal Shiny run functions.\n\nrunGadget(ui, server)",
    "crumbs": [
      "Guides",
      "Non-rectangular Data",
      "Streaming Data"
    ]
  },
  {
    "objectID": "guides/streaming.html#example-5---ml-pipeline-model",
    "href": "guides/streaming.html#example-5---ml-pipeline-model",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 5 - ML Pipeline Model",
    "text": "Example 5 - ML Pipeline Model\nThis example uses a fitted Pipeline Model to process the input, and saves the predictions to the output. This approach would be used to apply Machine Learning on top of streaming data.\n\nlibrary(sparklyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nsc &lt;- spark_connect(master = \"local\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\ndf &lt;- data.frame(x = rep(1:1000), y = rep(2:1001))\n\nstream_generate_test(df = df, iteration = 1)\n\nmodel_sample &lt;- spark_read_csv(sc, \"sample\", \"source\")\n\npipeline &lt;- sc %&gt;%\n  ml_pipeline() %&gt;%\n  ft_r_formula(x ~ y) %&gt;%\n  ml_linear_regression()\n\nfitted_pipeline &lt;- ml_fit(pipeline, model_sample)\n\nml_stream &lt;- stream_read_csv(\n    sc = sc, \n    path = \"source\", \n    columns = c(x = \"integer\", y = \"integer\")\n  )  %&gt;%\n  ml_transform(fitted_pipeline, .)  %&gt;%\n  select(- features) %&gt;%\n  stream_write_csv(\"source-out\")\n\nstream_generate_test(df = df, interval = 0.5)\n\n\nspark_read_csv(sc, \"stream\", \"source-out\", memory = FALSE) \n\n### Source: spark&lt;stream&gt; [?? x 4]\n##       x     y label prediction\n## * &lt;int&gt; &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt;\n## 1   276   277   276       276.\n## 2   277   278   277       277.\n## 3   278   279   278       278.\n## 4   279   280   279       279.\n## 5   280   281   280       280.\n## 6   281   282   281       281.\n## 7   282   283   282       282.\n## 8   283   284   283       283.\n## 9   284   285   284       284.\n##10   285   286   285       285.\n### ... with more rows\n\nstream_stop(ml_stream)\nspark_disconnect(sc)\n\n\nCode Breakdown\n\nCreates and fits a pipeline\n\ndf &lt;- data.frame(x = rep(1:1000), y = rep(2:1001))\nstream_generate_test(df = df, iteration = 1)\nmodel_sample &lt;- spark_read_csv(sc, \"sample\", \"source\")\n\npipeline &lt;- sc %&gt;%\n  ml_pipeline() %&gt;%\n  ft_r_formula(x ~ y) %&gt;%\n  ml_linear_regression()\n\nfitted_pipeline &lt;- ml_fit(pipeline, model_sample)\n\nThis example pipelines the input, process and output in a single code segment. The ml_transform() function is used to create the predictions. Because the CSV format does not support list type fields, the features column is removed before the results are sent to the output.\n\nml_stream &lt;- stream_read_csv(\n    sc = sc, \n    path = \"source\", \n    columns = c(x = \"integer\", y = \"integer\")\n  )  %&gt;%\n  ml_transform(fitted_pipeline, .)  %&gt;%\n  select(- features) %&gt;%\n  stream_write_csv(\"source-out\")\n\n\n\nstream_stop(write_output)\nspark_disconnect(sc)",
    "crumbs": [
      "Guides",
      "Non-rectangular Data",
      "Streaming Data"
    ]
  },
  {
    "objectID": "guides/connections.html",
    "href": "guides/connections.html",
    "title": "Configuring Spark Connections",
    "section": "",
    "text": "Local mode is an excellent way to learn and experiment with Spark. Local mode also provides a convenient development environment for analyses, reports, and applications that you plan to eventually deploy to a multi-node Spark cluster.\nTo work in local mode, you should first install a version of Spark for local use. You can do this using the spark_install() function, for example:\n\n\nThe following are the recommended Spark properties to set when connecting via R:\n\nsparklyr.cores.local - It defaults to using all of the available cores. Not a necessary property to set, unless there’s a reason to use less cores than available for a given Spark session.\nsparklyr.shell.driver-memory - The limit is the amount of RAM available in the computer minus what would be needed for OS operations.\nspark.memory.fraction - The default is set to 60% of the requested memory per executor. For more information, please see this Memory Management Overview page in the official Spark website.\n\n\n\n\nconf$`sparklyr.cores.local` &lt;- 4\nconf$`sparklyr.shell.driver-memory` &lt;- \"16G\"\nconf$spark.memory.fraction &lt;- 0.9\n\nsc &lt;- spark_connect(master = \"local\", \n                    version = \"2.1.0\",\n                    config = conf)\n\n\nTo see how the requested configuration affected the Spark connection, go to the Executors page in the Spark Web UI available in http://localhost:4040/storage/",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Spark connection options"
    ]
  },
  {
    "objectID": "guides/connections.html#local-mode",
    "href": "guides/connections.html#local-mode",
    "title": "Configuring Spark Connections",
    "section": "",
    "text": "Local mode is an excellent way to learn and experiment with Spark. Local mode also provides a convenient development environment for analyses, reports, and applications that you plan to eventually deploy to a multi-node Spark cluster.\nTo work in local mode, you should first install a version of Spark for local use. You can do this using the spark_install() function, for example:\n\n\nThe following are the recommended Spark properties to set when connecting via R:\n\nsparklyr.cores.local - It defaults to using all of the available cores. Not a necessary property to set, unless there’s a reason to use less cores than available for a given Spark session.\nsparklyr.shell.driver-memory - The limit is the amount of RAM available in the computer minus what would be needed for OS operations.\nspark.memory.fraction - The default is set to 60% of the requested memory per executor. For more information, please see this Memory Management Overview page in the official Spark website.\n\n\n\n\nconf$`sparklyr.cores.local` &lt;- 4\nconf$`sparklyr.shell.driver-memory` &lt;- \"16G\"\nconf$spark.memory.fraction &lt;- 0.9\n\nsc &lt;- spark_connect(master = \"local\", \n                    version = \"2.1.0\",\n                    config = conf)\n\n\nTo see how the requested configuration affected the Spark connection, go to the Executors page in the Spark Web UI available in http://localhost:4040/storage/",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Spark connection options"
    ]
  },
  {
    "objectID": "guides/connections.html#customizing-connections",
    "href": "guides/connections.html#customizing-connections",
    "title": "Configuring Spark Connections",
    "section": "Customizing connections",
    "text": "Customizing connections\nA connection to Spark can be customized by setting the values of certain Spark properties. In sparklyr, Spark properties can be set by using the config argument in the spark_connect() function.\nBy default, spark_connect() uses spark_config() as the default configuration. But that can be customized as shown in the example code below. Because of the unending number of possible combinations, spark_config() contains only a basic configuration, so it will be very likely that additional settings will be needed to properly connect to the cluster.\nconf &lt;- spark_config()   # Load variable with spark_config()\n\nconf$spark.executor.memory &lt;- \"16G\" # Use `$` to add or set values\n\nsc &lt;- spark_connect(master = \"yarn-client\", \n                    config = conf)  # Pass the conf variable \n\nSpark definitions\nIt may be useful to provide some simple definitions for the Spark nomenclature:\n\nNode: A server\nWorker Node: A server that is part of the cluster and are available to run Spark jobs\nMaster Node: The server that coordinates the Worker nodes.\nExecutor: A sort of virtual machine inside a node. One Node can have multiple Executors.\nDriver Node: The Node that initiates the Spark session. Typically, this will be the server where sparklyr is located.\nDriver (Executor): The Driver Node will also show up in the Executor list.\n\n\n\nUseful concepts\n\nSpark configuration properties passed by R are just requests - In most cases, the cluster has the final say regarding the resources apportioned to a given Spark session.\nThe cluster overrides ‘silently’ - Many times, no errors are returned when more resources than allowed are requested, or if an attempt is made to change a setting fixed by the cluster.",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Spark connection options"
    ]
  },
  {
    "objectID": "guides/connections.html#yarn",
    "href": "guides/connections.html#yarn",
    "title": "Configuring Spark Connections",
    "section": "YARN",
    "text": "YARN\n\nBackground\nUsing Spark and R inside a Hadoop based Data Lake is becoming a common practice at companies. Currently, there is no good way to manage user connections to the Spark service centrally. There are some caps and settings that can be applied, but in most cases there are configurations that the R user will need to customize.\nThe Running on YARN page in Spark’s official website is the best place to start for configuration settings reference, please bookmark it. Cluster administrators and users can benefit from this document. If Spark is new to the company, the YARN tunning article, courtesy of Cloudera, does a great job at explaining how the Spark/YARN architecture works.\n\n\nRecommended properties\nThe following are the recommended Spark properties to set when connecting via R:\n\nspark.executor.memory - The maximum possible is managed by the YARN cluster. See the Executor Memory Error\nspark.executor.cores - Number of cores assigned per Executor.\nspark.executor.instances - Number of executors to start. This property is acknowledged by the cluster if spark.dynamicAllocation.enabled is set to “false”.\nspark.dynamicAllocation.enabled - Overrides the mechanism that Spark provides to dynamically adjust resources. Disabling it provides more control over the number of the Executors that can be started, which in turn impact the amount of storage available for the session. For more information, please see the Dynamic Resource Allocation page in the official Spark website.\n\n\n\nClient mode\nUsing yarn-client as the value for the master argument in spark_connect() will make the server in which R is running to be the Spark’s session driver. Here is a sample connection:\nconf &lt;- spark_config()\n\nconf$spark.executor.memory &lt;- \"300M\"\nconf$spark.executor.cores &lt;- 2\nconf$spark.executor.instances &lt;- 3\nconf$spark.dynamicAllocation.enabled &lt;- \"false\"\n\nsc &lt;- spark_connect(master = \"yarn-client\", \n                    spark_home = \"/usr/lib/spark/\",\n                    version = \"1.6.0\",\n                    config = conf)\n\nExecutors page\nTo see how the requested configuration affected the Spark connection, go to the Executors page in the Spark Web UI. Typically, the Spark Web UI can be found using the exact same URL used for RStudio but on port 4040.\nNotice that 155.3MB per executor are assigned instead of the 300MB requested. This is because the spark.memory.fraction has been fixed by the cluster, plus, there is fixed amount of memory designated for overhead.\n\n\n\n\nCluster mode\nRunning in cluster mode means that YARN will choose where the driver of the Spark session will run. This means that the server where R is running may not necessarily be the driver for that session. Here is a good write-up explaining how running Spark applications work: Running Spark on YARN\nThe server will need to have copies of at least two files: yarn-site.xml and hive-site.xml. There may be other files needed based on your cluster’s individual setup.\nThis is an example of connecting to a Cloudera cluster:\nlibrary(sparklyr)\n\nSys.setenv(JAVA_HOME=\"/usr/lib/jvm/java-7-oracle-cloudera/\")\nSys.setenv(SPARK_HOME = '/opt/cloudera/parcels/CDH/lib/spark')\nSys.setenv(YARN_CONF_DIR = '/opt/cloudera/parcels/CDH/lib/spark/conf/yarn-conf')\n\nconf$spark.executor.memory &lt;- \"300M\"\nconf$spark.executor.cores &lt;- 2\nconf$spark.executor.instances &lt;- 3\nconf$spark.dynamicAllocation.enabled &lt;- \"false\"\nconf &lt;- spark_config()\n\nsc &lt;- spark_connect(master = \"yarn-cluster\", \n                    config = conf)\n\n\nExecutor memory error\nRequesting more memory or CPUs for Executors than allowed will return an error. This is one of the exceptions to the cluster’s ‘silent’ overrides. It will return a message similar to this:\n    Failed during initialize_connection: java.lang.IllegalArgumentException: Required executor memory (16384+1638 MB) is above the max threshold (8192 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'\nA cluster’s administrator is the only person who can make changes to the settings mentioned in the error. If the cluster is supported by a vendor, like Cloudera or Hortonworks, then the change can be made using the cluster’s web UI. Otherwise, changes to those settings are done directly in the yarn-default.xml file.\n\n\nKerberos\nThere are two options to access a “kerberized” data lake:\n\nUse kinit to get and cache the ticket. After kinit is installed and configured. After kinit is setup, it can used in R via a system() call prior to connecting to the cluster:\n\nsystem(\"echo '&lt;password&gt;' | kinit &lt;username&gt;\")\nFor more information visit this site: Apache - Authenticate with kinit\n\nA preferred option may be to use the out-of-the-box integration with Kerberos that the commercial version of RStudio Server offers.",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Spark connection options"
    ]
  },
  {
    "objectID": "guides/connections.html#standalone-mode",
    "href": "guides/connections.html#standalone-mode",
    "title": "Configuring Spark Connections",
    "section": "Standalone mode",
    "text": "Standalone mode\n\nRecommended properties\nThe following are the recommended Spark properties to set when connecting via R:\nThe default behavior in Standalone mode is to create one executor per worker. So in a 3 worker node cluster, there will be 3 executors setup. The basic properties that can be set are:\n\nspark.executor.memory - The requested memory cannot exceed the actual RAM available.\nspark.memory.fraction - The default is set to 60% of the requested memory per executor. For more information, please see this Memory Management Overview page in the official Spark website.\nspark.executor.cores - The requested cores cannot be higher than the cores available in each worker.\n\n\nDynamic Allocation\nIf dynamic allocation is disabled, then Spark will attempt to assign all of the available cores evenly across the cluster. The property used is spark.dynamicAllocation.enabled.\nFor example, the Standalone cluster used for this article has 3 worker nodes. Each node has 14.7GB in RAM and 4 cores. This means that there are a total of 12 cores (3 workers with 4 cores) and 44.1GB in RAM (3 workers with 14.7GB in RAM each).\nIf the spark.executor.cores property is set to 2, and dynamic allocation is disabled, then Spark will spawn 6 executors. The spark.executor.memory property should be set to a level that when the value is multiplied by 6 (number of executors) it will not be over total available RAM. In this case, the value can be safely set to 7GB so that the total memory requested will be 42GB, which is under the available 44.1GB.\n\n\n\nConnection example\nconf &lt;- spark_config()\nconf$spark.executor.memory &lt;- \"7GB\"\nconf$spark.memory.fraction &lt;- 0.9\nconf$spark.executor.cores &lt;- 2\nconf$spark.dynamicAllocation.enabled &lt;- \"false\"\n\nsc &lt;- spark_connect(master=\"spark://master-url:7077\", \n              version = \"2.1.0\",\n              config = conf,\n              spark_home = \"/home/ubuntu/spark-2.1.0-bin-hadoop2.7/\")\n\nExecutors page\nTo see how the requested configuration affected the Spark connection, go to the Executors page in the Spark Web UI. Typically, the Spark Web UI can be found using the exact same URL used for RStudio but on port 4040:",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Spark connection options"
    ]
  },
  {
    "objectID": "source/guides/model_tuning_text.html",
    "href": "source/guides/model_tuning_text.html",
    "title": "sparklyr",
    "section": "",
    "text": "library(sparklyr) library(dplyr) library(modeldata)\ndata(“small_fine_foods”)\nsc &lt;- spark_connect(master = “local”, version = “3.3”)\nsff_training_data &lt;- copy_to(sc, training_data) sff_testing_data &lt;- copy_to(sc, testing_data)\nsff_pipeline &lt;- ml_pipeline(sc) %&gt;% ft_tokenizer( input_col = “review”, output_col = “word_list” ) %&gt;% ft_stop_words_remover( input_col = “word_list”, output_col = “wo_stop_words” ) %&gt;% ft_hashing_tf( input_col = “wo_stop_words”, output_col = “hashed_features”, binary = TRUE, num_features = 1024 ) %&gt;% ft_normalizer( input_col = “hashed_features”, output_col = “normal_features” ) %&gt;% ft_r_formula(score ~ normal_features) %&gt;% ml_logistic_regression()\nsff_pipeline\nsff_grid &lt;- list( hashing_tf = list( num_features = 2^c(8, 10, 12)\n), logistic_regression = list( elastic_net_param = 10^seq(-3, 0, length = 20), reg_param = seq(0, 1, length = 5)\n) )\nsff_grid\nsff_evaluator &lt;- ml_binary_classification_evaluator(sc)\nsff_cv &lt;- ml_cross_validator( x = sc, estimator = sff_pipeline, estimator_param_maps = sff_grid, evaluator = sff_evaluator, num_folds = 3, parallelism = 4, seed = 100 )\nsff_cv\nsff_model &lt;- ml_fit( x = sff_cv, dataset = sff_training_data )\nsff_metrics &lt;- ml_validation_metrics(sff_model)\nlibrary(dplyr)\nsff_metrics %&gt;% arrange(desc(areaUnderROC)) %&gt;% head()\nlibrary(ggplot2)\nsff_metrics %&gt;% mutate(reg_param_1 = as.factor(reg_param_1)) %&gt;% ggplot(aes( x = elastic_net_param_1, y = areaUnderROC, color = reg_param_1 )) + geom_line() + geom_point(size = 0.5) + scale_x_continuous(trans = “log10”) + facet_wrap(~ num_features_2) + theme_light(base_size = 9)\nnew_sff_pipeline &lt;- ml_pipeline(sc) %&gt;% ft_tokenizer( input_col = “review”, output_col = “word_list” ) %&gt;% ft_stop_words_remover( input_col = “word_list”, output_col = “wo_stop_words” ) %&gt;% ft_hashing_tf( input_col = “wo_stop_words”, output_col = “hashed_features”, binary = TRUE, num_features = 4096\n) %&gt;% ft_normalizer( input_col = “hashed_features”, output_col = “normal_features” ) %&gt;% ft_r_formula(score ~ normal_features) %&gt;% ml_logistic_regression( elastic_net_param = 0.05, reg_param = 0.25\n)\nnew_sff_fitted &lt;- new_sff_pipeline %&gt;% ml_fit(sff_training_data)\nnew_sff_fitted %&gt;% ml_transform(sff_testing_data) %&gt;% ml_metrics_binary()\nspark_disconnect(sc)"
  },
  {
    "objectID": "site-news.html",
    "href": "site-news.html",
    "title": "Site News",
    "section": "",
    "text": "Date\nEvent\nArticle / Page\n\n\n\n\n9/15/22\nNewly added\nText Modeling |\n\n\n8/31/22\nUpdated\nUnderstanding Spark Caching\n\n\n3/2/22\nNewly added\ntidymodels with Spark\n\n\n1/25/22\nNewly added\nGet Started - Model Data\n\n\n1/25/22\nNewly added\nGet Started - Prepare Data\n\n\n1/25/22\nUpdated\nSpark Machine Learning Library (MLlib)\n\n\n1/25/22\nUpdated\nManipulating Data with dplyr\n\n\n1/24/22\nNewly added\nGet Started - Read Data\n\n\n1/24/22\nNewly added\nGet Started - Install\n\n\n1/21/22\nUpdated\nDistributing R Computations\n\n\n1/21/22\nUpdated\nIntro to Spark Streaming with sparklyr\n\n\n1/20/22\nUpdated\nText mining with Spark & sparklyr\n\n\n1/20/22\nUpdated\nSpark ML Pipelines",
    "crumbs": [
      "News",
      "Site News"
    ]
  },
  {
    "objectID": "guides/model_tuning_text.html",
    "href": "guides/model_tuning_text.html",
    "title": "Grid Search Tuning",
    "section": "",
    "text": "In this article, will cover the following four points:",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Grid Search Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning_text.html#grid-search",
    "href": "guides/model_tuning_text.html#grid-search",
    "title": "Grid Search Tuning",
    "section": "Grid Search",
    "text": "Grid Search\nThe main goal of hyper-parameter tuning is to find the ideal set of model parameter values. For example, finding out the ideal number of trees to use for a model. We use model tuning to try several, and increasing values. That will tell us at what point a increasing the number of trees does not improve the model’s performance.\nIn Grid Search, we provide a provide a set of specific parameters, and specific values to test for each parameter. The total number of combinations will be the product of all the specific values of each parameter.\nFor example, suppose we are going to try two parameters. For the first parameter we provide 5 values to try, and for the second parameter we provide 10 values to try. The total number of combinations will be 50. The number of combinations grows quickly the more parameters we use. Adding a third parameter, with only 2 values, will mean that the number of combinations would double to 100 (5x10x2).\nThe model tuning returns a performance metric for each combination. We can compare the results, and decide which model to use.\nIn Spark, we use an ML Pipeline, and a list of the parameters and the values to try (Grid). We also specify the metric it should use to measure performance (See Figure 1). Spark will then take care of figuring out the combinations, and fits the corresponding models.\n\n\n\n\n\n\n\n\nflowchart LR\n  subgraph id1 [ ]\n    subgraph id2 [ ]\n      subgraph id3 [ML Pipeline]\n        d[Prepare] --&gt; m[Model]\n      end\n      subgraph id4 [Grid Search Tuning]\n        m --&gt; gv[Grid]\n        gv-- Combo 1 --&gt;ft1[Fit Models]\n        ft1 --&gt; ev1[Metric]\n        gv-- Combo 2 --&gt;ft2[Fit Models]\n        ft2 --&gt; ev2[Metric]     \n        gv-- Combo 3 --&gt;ft3[Fit Models]\n        ft3 --&gt; ev3[Metric]  \n        gv-- Combo n --&gt;ft4[Fit Models]\n        ft4 --&gt; ev4[Metric]          \n      end\n    end\n  end\n  style id1 fill:#eee,stroke:#eee\n  style id2 fill:#eee,stroke:#eee\n  style d fill:#99ccff,stroke#666\n  style m fill:#99ffcc,stroke:#666\n  style gv fill:#ffcc99,stroke:#666\n  style ft1 fill:#ccff99,stroke:#666\n  style ft2 fill:#ccff99,stroke:#666\n  style ft3 fill:#ccff99,stroke:#666\n  style ft4 fill:#ccff99,stroke:#666\n\n\n\n\n\n\n\n\nFigure 1: Grid Search Tuning in Spark",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Grid Search Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning_text.html#cross-validation",
    "href": "guides/model_tuning_text.html#cross-validation",
    "title": "Grid Search Tuning",
    "section": "Cross Validation",
    "text": "Cross Validation\nIn Cross Validation, multiple models are fitted with the same combination of parameters. The difference is the data used for training, and validation. These are called folds. The training data, and the validation data is different for each fold, that is called re-sampling.\nThe model is fitted with the current fold’s training data, and then it is evaluated using the validation data. The average of the evaluation results become the official metric value of the combination of parameters. Figure 2, is a “zoomed” look of what happens inside Fit Models of Figure 1.\nThe total number of models, will be the total number of combinations times the number of folds. For example, if we use 3 parameters, with 5 values each, that would be 125 combinations. If tuning with 3 folds, Cross Validation will fit, and validate, a total of 375 models.\nIn Spark, running the 375 discrete models can be distributed across the entire cluster, thus significantly reducing the amount of time we would have to wait to see the results.\n\n\n\n\n\n\n\n\nflowchart LR\n  subgraph id1 [ ]\n    subgraph id2 [ ]\n      gv[Grid] -- Combo n --&gt;re[Resample]\n      re -- Fold 1 --&gt; ft1[Fit]\n      subgraph id4 [Fit Models with Cross Validation]\n        re -- Fold 2 --&gt; ft2[Fit]\n        re -- Fold 3 --&gt; ft3[Fit]\n        ft1 --&gt; ev1[Evaluate]\n        ft2 --&gt; ev2[Evaluate]     \n        ft3 --&gt; ev3[Evaluate]  \n        ev1 --&gt; eva[Avgerage]\n        ev2 --&gt; eva\n        ev3 --&gt; eva\n      end\n      eva --&gt; mt[Metric]\n    end\n  end\n  style id1 fill:#eee,stroke:#eee\n  style id4 fill:#ccff99,stroke:#666\n  style gv fill:#ffcc99,stroke:#666\n  style ft1 fill:#ccffff,stroke:#666\n  style ft2 fill:#ccffff,stroke:#666\n  style ft3 fill:#ccffff,stroke:#666\n  style re fill:#ffccff,stroke:#666\n  style ev1 fill:#ffff99,stroke:#666\n  style ev2 fill:#ffff99,stroke:#666\n  style ev3 fill:#ffff99,stroke:#666\n  style eva fill:#ffff66,stroke:#666\n\n\n\n\n\n\n\n\nFigure 2: Cross Validation in Spark",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Grid Search Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning_text.html#reproducing-tuning-text-analysis-in-spark",
    "href": "guides/model_tuning_text.html#reproducing-tuning-text-analysis-in-spark",
    "title": "Grid Search Tuning",
    "section": "Reproducing “Tuning Text Analysis” in Spark",
    "text": "Reproducing “Tuning Text Analysis” in Spark\nIn this article, we will reproduce the Grid Search tuning example found in the tune package’s website: Tuning Text Analysis. That example analyzes Amazon’s Fine Food Reviews text data. The goal is to tune the model using the exact same tuning parameters, and values, that were used in the tune’s website example.\n\n\n\n\n\n\nTip\n\n\n\nThis article builds on the knowledge of two previous articles, Text Modeling and Intro to Model Tuning. We encourage you to familiarize yourself with the concepts and code from those articles.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Grid Search Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning_text.html#spark-and-data-setup",
    "href": "guides/model_tuning_text.html#spark-and-data-setup",
    "title": "Grid Search Tuning",
    "section": "Spark and Data Setup",
    "text": "Spark and Data Setup\nFor this example, we will start a local Spark session, and then copy the Fine Food Reviews data to it. For more information about the data, please see the Data section of the Text Modeling article.\n\nlibrary(sparklyr)\nlibrary(modeldata)\n\ndata(\"small_fine_foods\")\n\nsc &lt;- spark_connect(master = \"local\", version = \"3.3\")\n\nsff_training_data &lt;- copy_to(sc, training_data)\nsff_testing_data &lt;- copy_to(sc, testing_data)",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Grid Search Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning_text.html#ml-pipeline",
    "href": "guides/model_tuning_text.html#ml-pipeline",
    "title": "Grid Search Tuning",
    "section": "ML Pipeline",
    "text": "ML Pipeline\nAs mentioned before, the data preparation and modeling in Text Modeling are based on the same example from the tune website’s article. The recipe steps, and parsnip model are recreated with Feature Transformers, and an ML model respectively.\nUnlike tidymodels, there is no need to “pre-define” the arguments that will need tuning. At execution, Spark will automatically override the parameters specified in the grid. This means that it doesn’t matter that we use the exact same code for developing, and tuning the pipeline. We can literally copy-paste, and run the resulting pipeline code from Text Modeling.\n\nsff_pipeline &lt;- ml_pipeline(sc) %&gt;% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %&gt;% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %&gt;% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 1024\n    ) %&gt;%\n  ft_normalizer(\n    input_col = \"hashed_features\", \n    output_col = \"normal_features\"\n    ) %&gt;% \n  ft_r_formula(score ~ normal_features) %&gt;% \n  ml_logistic_regression()\n\nsff_pipeline\n#&gt; Pipeline (Estimator) with 6 stages\n#&gt; &lt;pipeline__1e8eb529_63ad_4182_b716_a32aa659c037&gt; \n#&gt;   Stages \n#&gt;   |--1 Tokenizer (Transformer)\n#&gt;   |    &lt;tokenizer__da641107_67cf_46bc_9d9b_075f5beb3dc7&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      input_col: review\n#&gt;   |      output_col: word_list\n#&gt;   |--2 StopWordsRemover (Transformer)\n#&gt;   |    &lt;stop_words_remover__24642e4c_01bf_48d4_b1d2_e3d439bdcdf6&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      input_col: word_list\n#&gt;   |      output_col: wo_stop_words\n#&gt;   |--3 HashingTF (Transformer)\n#&gt;   |    &lt;hashing_tf__bd5aebaa_864d_4ece_97e1_30fa0bbc515a&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      input_col: wo_stop_words\n#&gt;   |      output_col: hashed_features\n#&gt;   |--4 Normalizer (Transformer)\n#&gt;   |    &lt;normalizer__fd807e7a_0a8e_490f_b554_a19329337637&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      input_col: hashed_features\n#&gt;   |      output_col: normal_features\n#&gt;   |--5 RFormula (Estimator)\n#&gt;   |    &lt;r_formula__02516e56_4219_4277_91d1_7e5c0f4574bd&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      features_col: features\n#&gt;   |      label_col: label\n#&gt;   |     (Parameters)\n#&gt;   |      force_index_label: FALSE\n#&gt;   |      formula: score ~ normal_features\n#&gt;   |      handle_invalid: error\n#&gt;   |      stringIndexerOrderType: frequencyDesc\n#&gt;   |--6 LogisticRegression (Estimator)\n#&gt;   |    &lt;logistic_regression__89201a87_4f60_4ad3_a84c_d8e982c0988d&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      features_col: features\n#&gt;   |      label_col: label\n#&gt;   |      prediction_col: prediction\n#&gt;   |      probability_col: probability\n#&gt;   |      raw_prediction_col: rawPrediction\n#&gt;   |     (Parameters)\n#&gt;   |      aggregation_depth: 2\n#&gt;   |      elastic_net_param: 0\n#&gt;   |      family: auto\n#&gt;   |      fit_intercept: TRUE\n#&gt;   |      max_iter: 100\n#&gt;   |      maxBlockSizeInMB: 0\n#&gt;   |      reg_param: 0\n#&gt;   |      standardization: TRUE\n#&gt;   |      threshold: 0.5\n#&gt;   |      tol: 1e-06\n\nIt is also worth pointing out that in a”real life” exercise, sff_pipeline would probably already be loaded into our environment. That is because we just finished modeling and, decided to test to see if we could tune the model (See Figure 3). Spark can re-use the exact same ML Pipeline object for the cross validation step.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Grid Search Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning_text.html#grid",
    "href": "guides/model_tuning_text.html#grid",
    "title": "Grid Search Tuning",
    "section": "Grid",
    "text": "Grid\nThere is a big advantage to transforming, and modeling the data in a single ML Pipeline. It opens the door for Spark to also alter parameters used for data transformation, in addition to the model’s parameters. This means that we can include the parameters of the tokenization, cleaning, hashing, and normalization steps as possible candidates for the model tuning.\nThe Tuning Text Analysis article uses three tuning parameters. Two parameters are in the model, and one is in the hashing step. Here are the parameters, and how they map between tidymodels and sparklyr:\n\n\n\n\n\n\n\n\nParameter\ntidymodels\nsparklyr\n\n\n\n\nNumber of Terms to Hash\nnum_terms\nnum_features\n\n\nAmount of regularization in the model\npenalty\nelastic_net_param\n\n\nProportion of pure vs ridge Lasso\nmixture\nreg_param\n\n\n\nUsing partial name matching, we map the parameters to the steps we want to tune:\n\nhashing_ft will be the name of the list object containing the num_features values\nlogistic_regression will be the of the list object with the model parameters\n\nFor more about partial name matching, see in the Intro Model Tuning article. For the parameters values, we can copy the exact same values from the tune website\n\nsff_grid &lt;-  list(\n    hashing_tf = list(\n      num_features = 2^c(8, 10, 12)  \n    ),\n    logistic_regression = list(\n      elastic_net_param = 10^seq(-3, 0, length = 20), \n      reg_param = seq(0, 1, length = 5)    \n    )\n  )\n\nsff_grid\n#&gt; $hashing_tf\n#&gt; $hashing_tf$num_features\n#&gt; [1]  256 1024 4096\n#&gt; \n#&gt; \n#&gt; $logistic_regression\n#&gt; $logistic_regression$elastic_net_param\n#&gt;  [1] 0.001000000 0.001438450 0.002069138 0.002976351 0.004281332 0.006158482\n#&gt;  [7] 0.008858668 0.012742750 0.018329807 0.026366509 0.037926902 0.054555948\n#&gt; [13] 0.078475997 0.112883789 0.162377674 0.233572147 0.335981829 0.483293024\n#&gt; [19] 0.695192796 1.000000000\n#&gt; \n#&gt; $logistic_regression$reg_param\n#&gt; [1] 0.00 0.25 0.50 0.75 1.00",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Grid Search Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning_text.html#evaluate",
    "href": "guides/model_tuning_text.html#evaluate",
    "title": "Grid Search Tuning",
    "section": "Evaluate",
    "text": "Evaluate\nIn the tune website’s article, ROC AUC is used to measure performance. The is the default metric of ml_binary_classification_evaluator() , so we only need to pass the connection variable to the evaluator function.\n\nsff_evaluator &lt;- ml_binary_classification_evaluator(sc)",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Grid Search Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning_text.html#model-tuning",
    "href": "guides/model_tuning_text.html#model-tuning",
    "title": "Grid Search Tuning",
    "section": "Model Tuning",
    "text": "Model Tuning\nWe will use ml_cross_validator() to prepare a tuning specification inside Spark. Spark will automatically create the parameter combinations when tuning the model. In this case, sff_grid contains three parameters:\n\nnum_features with 3 values\nelastic_net_param with 20 values\nreg_parm with 5 values\n\nThis means that there will be 300 combinations for the tuning parameters (3 x 20 x 5). Because we set the number of folds to 3 (num_folds), Spark will run a total of 900 models (3 x 300).\n\nsff_cv &lt;- ml_cross_validator(\n  x = sc,\n  estimator = sff_pipeline, \n  estimator_param_maps = sff_grid,\n  evaluator = sff_evaluator,\n  num_folds = 3,\n  parallelism = 4,\n  seed = 100\n)\n\nsff_cv\n#&gt; CrossValidator (Estimator)\n#&gt; &lt;cross_validator__e3d09026_aa6f_4bf8_b1be_91dadd0c0b51&gt; \n#&gt;  (Parameters -- Tuning)\n#&gt;   estimator: Pipeline\n#&gt;              &lt;pipeline__1e8eb529_63ad_4182_b716_a32aa659c037&gt; \n#&gt;   evaluator: BinaryClassificationEvaluator\n#&gt;              &lt;binary_classification_evaluator__ca7cff64_45f7_4da6_9068_d0ec0c0f1de8&gt; \n#&gt;     with metric areaUnderROC \n#&gt;   num_folds: 3 \n#&gt;   [Tuned over 300 hyperparameter sets]\n\n\n\n\n\n\n\nTip\n\n\n\nWe recommend to set the seed argument in order to increase reproducibility.\n\n\nThis is the step that will take the longest time. The ml_fit() function will run the 900 models using the training data. There is no need to pre-prepare the re-sampling folds, Spark will take care of that.\n\nsff_model &lt;- ml_fit(\n  x = sff_cv, \n  dataset = sff_training_data\n  )",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Grid Search Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning_text.html#metrics",
    "href": "guides/model_tuning_text.html#metrics",
    "title": "Grid Search Tuning",
    "section": "Metrics",
    "text": "Metrics\nWe can now extract the metrics from sff_model using ml_validation_metrics(). The ROC AUC values will be in a column called areaUnderROC. We can then take a look at the best performing models using dplyr.\n\nsff_metrics &lt;- ml_validation_metrics(sff_model)\n\nlibrary(dplyr)\n\nsff_metrics %&gt;% \n  arrange(desc(areaUnderROC)) %&gt;% \n  head()\n#&gt;   areaUnderROC elastic_net_param_1 reg_param_1 num_features_2\n#&gt; 1    0.7858727          0.05455595        0.25           4096\n#&gt; 2    0.7847232          0.02636651        0.50           4096\n#&gt; 3    0.7835850          0.01832981        0.75           4096\n#&gt; 4    0.7830411          0.01832981        0.50           4096\n#&gt; 5    0.7830230          0.03792690        0.25           4096\n#&gt; 6    0.7828651          0.01274275        0.75           4096\n\nWe will now plot the results. We will match the approach used in the Grid Search section of the Tuning Text Analysis article.\n\nlibrary(ggplot2)\n\nsff_metrics %&gt;% \n  mutate(reg_param_1 = as.factor(reg_param_1)) %&gt;% \n  ggplot(aes(\n    x = elastic_net_param_1, \n    y = areaUnderROC, \n    color = reg_param_1\n    )) +\n  geom_line() +\n  geom_point(size = 0.5) +\n  scale_x_continuous(trans = \"log10\") +\n  facet_wrap(~ num_features_2) +\n  theme_light(base_size = 9)\n\n\n\n\n\n\n\n\nIn the plot, we can see the effects of the three parameters, and the values that look to be the best. These effects are very similar to the original tune’s website article.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Grid Search Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning_text.html#model-selection",
    "href": "guides/model_tuning_text.html#model-selection",
    "title": "Grid Search Tuning",
    "section": "Model selection",
    "text": "Model selection\nWe can create a new ML Pipeline using the same code as the original pipeline. We only need to change the 3 parameters values, with values that performed best.\n\nnew_sff_pipeline &lt;- ml_pipeline(sc) %&gt;% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %&gt;% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %&gt;% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 4096      \n    ) %&gt;%\n  ft_normalizer(\n    input_col = \"hashed_features\", \n    output_col = \"normal_features\"\n    ) %&gt;% \n  ft_r_formula(score ~ normal_features) %&gt;% \n  ml_logistic_regression(\n    elastic_net_param = 0.05,\n    reg_param = 0.25  \n    )\n\nNow, we create a final model using the new ML Pipeline.\n\nnew_sff_fitted &lt;- new_sff_pipeline %&gt;% \n  ml_fit(sff_training_data)\n\nThe test data set is now used to confirm that the performance gains hold. We use it to run predictions with the new ML Pipeline Model.\n\nnew_sff_fitted %&gt;% \n  ml_transform(sff_testing_data) %&gt;% \n  ml_metrics_binary()\n#&gt; # A tibble: 2 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.783\n#&gt; 2 pr_auc  binary         0.653",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Grid Search Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning_text.html#benefits-of-ml-pipelines-for-everyday-work",
    "href": "guides/model_tuning_text.html#benefits-of-ml-pipelines-for-everyday-work",
    "title": "Grid Search Tuning",
    "section": "Benefits of ML Pipelines for everyday work",
    "text": "Benefits of ML Pipelines for everyday work\nIn the previous section, the metrics show an increase performance compared to the model in the Text Modeling article.\nThe gains in performance were easy to obtain. We literally took the exact same pipeline we used in developing the model, and ran it through the tuning process. All we had to create was a simple grid, and a provide the metric function.\nThis highlights an advantage of using ML Pipelines. Because transitioning from modeling to tuning in Spark, will be a simple operation. An operation that has the potential to yield great benefits, with little cost of effort. (Figure 3)\n\n\n\n\n\n\n\n\nflowchart LR\n  subgraph id1 [ ]\n  subgraph id2 [ ]\n    subgraph si [ML Pipeline]\n      dm[Data prep&lt;br&gt; & model] \n    end\n    dm -- Development&lt;br&gt;cycle --&gt; t[Test] \n    t --&gt; dm\n    t -- Selected&lt;br&gt;ML Pipeline  --&gt; tn[Tune]\n    gm[Grid & Metric] --&gt; tn\n    subgraph sp [ML Pipeline]\n      fm[Final model] \n    end\n    tn -- Best&lt;br&gt;parameters --&gt; fm\n  end\n  end\n  style t fill:#ffcc00,stroke:#000\n  style tn fill:#ffcc99,stroke:#000\n  style dm fill:#ffff99,stroke:#000\n  style fm fill:#ffff99,stroke:#000\n  style id1 fill:#eee,stroke:#eee\n  style id2 fill:#eee,stroke:#eee\n  style gm fill:#fff,stroke:#000\n\n\n\n\n\n\n\n\nFigure 3: Developing models with ML Pipelines",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Grid Search Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning_text.html#accelerate-model-tuning-with-spark",
    "href": "guides/model_tuning_text.html#accelerate-model-tuning-with-spark",
    "title": "Grid Search Tuning",
    "section": "Accelerate model tuning with Spark",
    "text": "Accelerate model tuning with Spark\nAs highlighter in the previous section, Spark, and sparklyr, provide an easy way to go from exploration, to modeling, and tuning. Even without a “formal” Spark cluster, it is possible to take advantage of these capabilities right from our personal computer.\nIf we add an actual cluster to the mix, the advantage of using Spark raises dramatically. Usually, we talk about Spark for “big data” analysis, but in this case, we can leverage it to “parallelize” hundreds of models across multiple machines. The ability to distribute the models across the cluster will cut down the tuning processing time (Figure 4). The resources available to the cluster, and the given Spark session, will also determine the the amount of time saved. There is really no other open-source technology that is capable of this.\n\n\n\n\n\n\n\n\nclassDiagram\n  class Driver {\n  }\n  class Node1{\n    Job 1 - Model 1\n    Job 2 - Model 2\n    Job 3 - Model 3\n    Job 4 - Model 4    \n  }\n  class Node2{\n    Job 1 - Model 5\n    Job 2 - Model 6\n    Job 3 - Model 7\n    Job 4 - Model 8       \n  }\n  class Node3{\n    Job 1 - Model 9\n    Job 2 - Model 10\n    Job 3 - Model 11\n    Job 4 - Model 12      \n  }  \n  class Node4{\n    Job 1 - Model 13\n    Job 2 - Model 14\n    Job 3 - Model 15\n    Job 4 - Model 16      \n  }    \n  Driver --&gt; Node1\n  Driver --&gt; Node2\n  Driver --&gt; Node3\n  Driver --&gt; Node4\n\n\n\n\n\n\n\n\n\nFigure 4: Model tuning in a Spark cluster",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Grid Search Tuning"
    ]
  },
  {
    "objectID": "guides/textmining.html",
    "href": "guides/textmining.html",
    "title": "Text mining with Spark & sparklyr",
    "section": "",
    "text": "This article focuses on a set of functions that can be used for text mining with Spark and sparklyr. The main goal is to illustrate how to perform most of the data preparation and analysis with commands that will run inside the Spark cluster, as opposed to locally in R. Because of that, the amount of data used will be small.",
    "crumbs": [
      "Guides",
      "Non-rectangular Data",
      "Text Data"
    ]
  },
  {
    "objectID": "guides/textmining.html#data-import",
    "href": "guides/textmining.html#data-import",
    "title": "Text mining with Spark & sparklyr",
    "section": "Data Import",
    "text": "Data Import\n\nConnect to Spark\nAn additional goal of this article is to encourage the reader to try it out, so a simple Spark local mode session is used.\n\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc &lt;- spark_connect(master = \"local\")\n\n\n\nspark_read_text()\nThe spark_read_text() is a new function which works like readLines() but for sparklyr. It comes in handy when non-structured data, such as lines in a book, is what is available for analysis.\n\n# Imports Mark Twain's file\n\ntwain_path &lt;- paste0(\"file:///\", here::here(), \"/mark_twain.txt\")\ntwain &lt;-  spark_read_text(sc, \"twain\", twain_path)\n\n\n# Imports Sir Arthur Conan Doyle's file\ndoyle_path &lt;- paste0(\"file:///\", here::here(), \"/arthur_doyle.txt\")\ndoyle &lt;-  spark_read_text(sc, \"doyle\", doyle_path)",
    "crumbs": [
      "Guides",
      "Non-rectangular Data",
      "Text Data"
    ]
  },
  {
    "objectID": "guides/textmining.html#data-transformation",
    "href": "guides/textmining.html#data-transformation",
    "title": "Text mining with Spark & sparklyr",
    "section": "Data transformation",
    "text": "Data transformation\nThe objective is to end up with a tidy table inside Spark with one row per word used. The steps will be:\n\nThe needed data transformations apply to the data from both authors. The data sets will be appended to one another\nPunctuation will be removed\nThe words inside each line will be separated, or tokenized\nFor a cleaner analysis, stop words will be removed\nTo tidy the data, each word in a line will become its own row\nThe results will be saved to Spark memory\n\n\nsdf_bind_rows()\n\nsdf_bind_rows() appends the doyle Spark Dataframe to the twain Spark Dataframe. This function can be used in lieu of a dplyr::bind_rows() wrapper function. For this exercise, the column author is added to differentiate between the two bodies of work.\n\n\nall_words &lt;- doyle %&gt;%\n  mutate(author = \"doyle\") %&gt;%\n  sdf_bind_rows({\n    twain %&gt;%\n      mutate(author = \"twain\")\n  }) %&gt;%\n  filter(nchar(line) &gt; 0)\n\n\n\nregexp_replace()\n\nThe Hive UDF, regexp_replace, is used as a sort of gsub() that works inside Spark. In this case it is used to remove punctuation. The usual [:punct:] regular expression did not work well during development, so a custom list is provided. For more information, see the Hive Functions section in the dplyr page.\n\n\nall_words &lt;- all_words %&gt;%\n  mutate(line = regexp_replace(line, \"[_\\\"\\'():;,.!?\\\\-]\", \" \"))\n\n\n\nft_tokenizer()\n\nft_tokenizer() uses the Spark API to separate each word. It creates a new list column with the results.\n\n\nall_words &lt;- all_words %&gt;%\n    ft_tokenizer(\n      input_col = \"line\",\n      output_col = \"word_list\"\n      )\n\nhead(all_words, 4)\n\n# Source: spark&lt;?&gt; [?? x 3]\n  line                          author word_list \n  &lt;chr&gt;                         &lt;chr&gt;  &lt;list&gt;    \n1 cover                         doyle  &lt;list [1]&gt;\n2 The Return of Sherlock Holmes doyle  &lt;list [5]&gt;\n3 by Sir Arthur Conan Doyle     doyle  &lt;list [5]&gt;\n4 Contents                      doyle  &lt;list [1]&gt;\n\n\n\n\nft_stop_words_remover()\n\nft_stop_words_remover() is a new function that, as its name suggests, takes care of removing stop words from the previous transformation. It expects a list column, so it is important to sequence it correctly after a ft_tokenizer() command. In the sample results, notice that the new wo_stop_words column contains less items than word_list.\n\n\nall_words &lt;- all_words %&gt;%\n  ft_stop_words_remover(\n    input_col = \"word_list\",\n    output_col = \"wo_stop_words\"\n    )\n\nhead(all_words, 4)\n\n# Source: spark&lt;?&gt; [?? x 4]\n  line                          author word_list  wo_stop_words\n  &lt;chr&gt;                         &lt;chr&gt;  &lt;list&gt;     &lt;list&gt;       \n1 cover                         doyle  &lt;list [1]&gt; &lt;list [1]&gt;   \n2 The Return of Sherlock Holmes doyle  &lt;list [5]&gt; &lt;list [3]&gt;   \n3 by Sir Arthur Conan Doyle     doyle  &lt;list [5]&gt; &lt;list [4]&gt;   \n4 Contents                      doyle  &lt;list [1]&gt; &lt;list [1]&gt;   \n\n\n\n\nexplode()\n\nThe Hive UDF explode performs the job of unnesting the tokens into their own row. Some further filtering and field selection is done to reduce the size of the dataset.\n\n\nall_words &lt;- all_words %&gt;%\n  mutate(word = explode(wo_stop_words)) %&gt;%\n  select(word, author) %&gt;%\n  filter(nchar(word) &gt; 2)\n\nhead(all_words, 4)\n\n# Source: spark&lt;?&gt; [?? x 2]\n  word     author\n  &lt;chr&gt;    &lt;chr&gt; \n1 cover    doyle \n2 return   doyle \n3 sherlock doyle \n4 holmes   doyle \n\n\n\n\ncompute()\n\ncompute() will operate this transformation and cache the results in Spark memory. It is a good idea to pass a name to compute() to make it easier to identify it inside the Spark environment. In this case the name will be all_words\n\n\nall_words &lt;- all_words %&gt;%\n  compute(\"all_words\")\n\n\n\nFull code\nThis is what the code would look like on an actual analysis:\n\nall_words &lt;- doyle %&gt;%\n  mutate(author = \"doyle\") %&gt;%\n  sdf_bind_rows({\n    twain %&gt;%\n      mutate(author = \"twain\")\n  }) %&gt;%\n  filter(nchar(line) &gt; 0) %&gt;%\n  mutate(line = regexp_replace(line, \"[_\\\"\\'():;,.!?\\\\-]\", \" \")) %&gt;%\n  ft_tokenizer(\n    input_col = \"line\",\n    output_col = \"word_list\"\n  ) %&gt;%\n  ft_stop_words_remover(\n    input_col = \"word_list\",\n    output_col = \"wo_stop_words\"\n  ) %&gt;%\n  mutate(word = explode(wo_stop_words)) %&gt;%\n  select(word, author) %&gt;%\n  filter(nchar(word) &gt; 2) %&gt;%\n  compute(\"all_words\")",
    "crumbs": [
      "Guides",
      "Non-rectangular Data",
      "Text Data"
    ]
  },
  {
    "objectID": "guides/textmining.html#data-analysis",
    "href": "guides/textmining.html#data-analysis",
    "title": "Text mining with Spark & sparklyr",
    "section": "Data Analysis",
    "text": "Data Analysis\n\nWords used the most\n\nword_count &lt;- all_words %&gt;%\n  count(author, word) %&gt;% \n  ungroup()\n\nword_count\n\n# Source: spark&lt;?&gt; [?? x 3]\n   author word              n\n   &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;\n 1 doyle  empty           398\n 2 doyle  students        109\n 3 doyle  golden          303\n 4 doyle  abbey           164\n 5 doyle  grange           18\n 6 doyle  year            866\n 7 doyle  world          1520\n 8 doyle  circumstances   284\n 9 doyle  particulars      49\n10 doyle  crime           357\n# … with more rows\n\n\n\n\nWords used by Doyle and not Twain\n\ndoyle_unique &lt;- filter(word_count, author == \"doyle\") %&gt;%\n  anti_join(\n    filter(word_count, author == \"twain\"), \n    by = \"word\"\n    ) %&gt;%\n  compute(\"doyle_unique\")\n\ndoyle_unique %&gt;% \n  arrange(-n)\n\n# Source:     spark&lt;?&gt; [?? x 3]\n# Ordered by: -n\n   author word          n\n   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n 1 doyle  nigel       972\n 2 doyle  alleyne     500\n 3 doyle  ezra        421\n 4 doyle  maude       337\n 5 doyle  aylward     336\n 6 doyle  lestrade    311\n 7 doyle  catinat     301\n 8 doyle  sharkey     281\n 9 doyle  summerlee   248\n10 doyle  congo       211\n# … with more rows\n\n\n\ndoyle_unique %&gt;%\n  arrange(-n) %&gt;%\n  head(100) %&gt;%\n  collect() %&gt;%\n  with(wordcloud::wordcloud(\n    word,\n    n,\n    colors = c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#56B4E9\")\n  ))\n\n\n\n\n\n\n\n\n\n\nTwain and Sherlock\nThe word cloud highlighted something interesting. The word “lestrade” is listed as one of the words used by Doyle but not Twain. Lestrade is the last name of a major character in the Sherlock Holmes books. It makes sense that the word “sherlock” appears considerably more times than “lestrade” in Doyle’s books, so why is Sherlock not in the word cloud? Did Mark Twain use the word “sherlock” in his writings?\n\nall_words %&gt;%\n  filter(\n    author == \"twain\",\n    word == \"sherlock\"\n    ) %&gt;%\n  count()\n\n# Source: spark&lt;?&gt; [?? x 1]\n      n\n  &lt;dbl&gt;\n1    16\n\n\nThe all_words table contains 16 instances of the word sherlock in the words used by Twain in his works. The instr Hive UDF is used to extract the lines that contain that word in the twain table. This Hive function works can be used instead of base::grep() or stringr::str_detect(). To account for any word capitalization, the lower command will be used in mutate() to make all words in the full text lower cap.\n\n\ninstr() & lower()\nMost of these lines are in a short story by Mark Twain called A Double Barrelled Detective Story. As per the Wikipedia page about this story, this is a satire by Twain on the mystery novel genre, published in 1902.\n\ntwain %&gt;%\n  mutate(line = lower(line)) %&gt;%\n  filter(instr(line, \"sherlock\") &gt; 0) %&gt;%\n  pull(line)\n\n [1] \"late sherlock holmes, and yet discernible by a member of a race charged\" \n [2] \"sherlock holmes.\"                                                        \n [3] \"“uncle sherlock! the mean luck of it!--that he should come just\"         \n [4] \"another trouble presented itself. “uncle sherlock 'll be wanting to talk\"\n [5] \"flint buckner's cabin in the frosty gloom. they were sherlock holmes and\"\n [6] \"“uncle sherlock's got some work to do, gentlemen, that 'll keep him till\"\n [7] \"“by george, he's just a duke, boys! three cheers for sherlock holmes,\"   \n [8] \"he brought sherlock holmes to the billiard-room, which was jammed with\"  \n [9] \"of interest was there--sherlock holmes. the miners stood silent and\"     \n[10] \"the room; the chair was on it; sherlock holmes, stately, imposing,\"      \n[11] \"“you have hunted me around the world, sherlock holmes, yet god is my\"    \n[12] \"“if it's only sherlock holmes that's troubling you, you needn't worry\"   \n[13] \"they sighed; then one said: “we must bring sherlock holmes. he can be\"   \n[14] \"i had small desire that sherlock holmes should hang for my deeds, as you\"\n[15] \"“my name is sherlock holmes, and i have not been doing anything.”\"       \n[16] \"late sherlock holmes, and yet discernible by a member of a race charged\" \n\n\n\nspark_disconnect(sc)",
    "crumbs": [
      "Guides",
      "Non-rectangular Data",
      "Text Data"
    ]
  },
  {
    "objectID": "guides/textmining.html#appendix",
    "href": "guides/textmining.html#appendix",
    "title": "Text mining with Spark & sparklyr",
    "section": "Appendix",
    "text": "Appendix\n\ngutenbergr package\nThis is an example of how the data for this article was pulled from the Gutenberg site:\nlibrary(gutenbergr)\n\ngutenberg_works()  %&gt;%\n  filter(author == \"Twain, Mark\") %&gt;%\n  pull(gutenberg_id) %&gt;%\n  gutenberg_download(mirror = \"http://mirrors.xmission.com/gutenberg/\") %&gt;%\n  pull(text) %&gt;%\n  writeLines(\"mark_twain.txt\")",
    "crumbs": [
      "Guides",
      "Non-rectangular Data",
      "Text Data"
    ]
  },
  {
    "objectID": "guides/textmodeling.html",
    "href": "guides/textmodeling.html",
    "title": "Text modeling",
    "section": "",
    "text": "This article builds on the concepts and techniques contained in other articles found on this site. The example contained here goes beyond the descriptive analysis found in the Text Mining article. It shows how to pre-process, and then model text data. This article also expands on ML Pipelines, by providing more “real life” scenario of how and why to use pipelines.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Text modeling"
    ]
  },
  {
    "objectID": "guides/textmodeling.html#data",
    "href": "guides/textmodeling.html#data",
    "title": "Text modeling",
    "section": "Data",
    "text": "Data\nThis article uses text data from the modeldata package. The Fine foods example data contains reviews of fine foods from Amazon. The package contains a training and a test set. The data consist of a product code, the text of the review, and the score. The score has two values: “great”, and “other”.\n\nlibrary(modeldata)\n\ndata(\"small_fine_foods\")\n\ntraining_data %&gt;% \n  head(1) %&gt;% \n  as.list()\n#&gt; $product\n#&gt; [1] \"B000J0LSBG\"\n#&gt; \n#&gt; $review\n#&gt; [1] \"this stuff is  not stuffing  its  not good at all  save your money\"\n#&gt; \n#&gt; $score\n#&gt; [1] other\n#&gt; Levels: great other\n\nWe will start by starting a local session of Spark, and then copying both data sets to our new session.\n\nlibrary(sparklyr)\n\nsc &lt;- spark_connect(master = \"local\", version = \"3.3\")\n\nsff_training_data &lt;- copy_to(sc, training_data)\n\nsff_testing_data &lt;- copy_to(sc, testing_data)",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Text modeling"
    ]
  },
  {
    "objectID": "guides/textmodeling.html#text-transformers",
    "href": "guides/textmodeling.html#text-transformers",
    "title": "Text modeling",
    "section": "Text transformers",
    "text": "Text transformers\n\nSplit into words (tokenizer)\nWe will split each review into individual words, or tokens. The ft_tokenizer() function returns a in-line list containing the individual words.\n\nsff_training_data %&gt;% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %&gt;% \n  select(3:4)\n#&gt; # Source: spark&lt;?&gt; [?? x 2]\n#&gt;    score word_list   \n#&gt;    &lt;chr&gt; &lt;list&gt;      \n#&gt;  1 other &lt;list [17]&gt; \n#&gt;  2 great &lt;list [100]&gt;\n#&gt;  3 great &lt;list [106]&gt;\n#&gt;  4 great &lt;list [36]&gt; \n#&gt;  5 great &lt;list [18]&gt; \n#&gt;  6 great &lt;list [30]&gt; \n#&gt;  7 other &lt;list [87]&gt; \n#&gt;  8 great &lt;list [54]&gt; \n#&gt;  9 great &lt;list [59]&gt; \n#&gt; 10 great &lt;list [44]&gt; \n#&gt; # … with more rows\n\n\n\nClean-up words (stop words)\nThere are words very common in text, words such as: “the”, “and”, “or”, etc. These are called “stop words”. Most often, stop words are not useful in analysis and modeling, so it is necessary to remove them. That is exactly what ft_stop_words_remover() does. In addition to English, Spark has lists of stop words for several other languages. In the resulting table, notice that the number of words in the wo_stop_words is lower than the word_list.\n\nsff_training_data %&gt;% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %&gt;% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %&gt;% \n  select(3:5) \n#&gt; # Source: spark&lt;?&gt; [?? x 3]\n#&gt;    score word_list    wo_stop_words\n#&gt;    &lt;chr&gt; &lt;list&gt;       &lt;list&gt;       \n#&gt;  1 other &lt;list [17]&gt;  &lt;list [9]&gt;   \n#&gt;  2 great &lt;list [100]&gt; &lt;list [61]&gt;  \n#&gt;  3 great &lt;list [106]&gt; &lt;list [67]&gt;  \n#&gt;  4 great &lt;list [36]&gt;  &lt;list [20]&gt;  \n#&gt;  5 great &lt;list [18]&gt;  &lt;list [9]&gt;   \n#&gt;  6 great &lt;list [30]&gt;  &lt;list [17]&gt;  \n#&gt;  7 other &lt;list [87]&gt;  &lt;list [58]&gt;  \n#&gt;  8 great &lt;list [54]&gt;  &lt;list [33]&gt;  \n#&gt;  9 great &lt;list [59]&gt;  &lt;list [36]&gt;  \n#&gt; 10 great &lt;list [44]&gt;  &lt;list [24]&gt;  \n#&gt; # … with more rows\n\n\n\nIndex words (hash)\nText hashing maps a sequence of words, or “terms”, to their frequencies. The number of terms that are mapped can be controlled using the num_features argument in ft_hashing_ft(). Because we are eventually going to use a logistic regression model, we will need to override the frequencies from their original value to 1. This is accomplished by setting the binary argument to TRUE.\n\nsff_training_data %&gt;%\n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %&gt;% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %&gt;% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 1024\n    ) %&gt;%\n  select(3:6) \n#&gt; # Source: spark&lt;?&gt; [?? x 4]\n#&gt;    score word_list    wo_stop_words hashed_features\n#&gt;    &lt;chr&gt; &lt;list&gt;       &lt;list&gt;        &lt;list&gt;         \n#&gt;  1 other &lt;list [17]&gt;  &lt;list [9]&gt;    &lt;dbl [1,024]&gt;  \n#&gt;  2 great &lt;list [100]&gt; &lt;list [61]&gt;   &lt;dbl [1,024]&gt;  \n#&gt;  3 great &lt;list [106]&gt; &lt;list [67]&gt;   &lt;dbl [1,024]&gt;  \n#&gt;  4 great &lt;list [36]&gt;  &lt;list [20]&gt;   &lt;dbl [1,024]&gt;  \n#&gt;  5 great &lt;list [18]&gt;  &lt;list [9]&gt;    &lt;dbl [1,024]&gt;  \n#&gt;  6 great &lt;list [30]&gt;  &lt;list [17]&gt;   &lt;dbl [1,024]&gt;  \n#&gt;  7 other &lt;list [87]&gt;  &lt;list [58]&gt;   &lt;dbl [1,024]&gt;  \n#&gt;  8 great &lt;list [54]&gt;  &lt;list [33]&gt;   &lt;dbl [1,024]&gt;  \n#&gt;  9 great &lt;list [59]&gt;  &lt;list [36]&gt;   &lt;dbl [1,024]&gt;  \n#&gt; 10 great &lt;list [44]&gt;  &lt;list [24]&gt;   &lt;dbl [1,024]&gt;  \n#&gt; # … with more rows\n\n\n\nNormalize results\nFinally, we normalize the hashed column using ft_normalizer() .\n\nsff_training_data %&gt;% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %&gt;% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %&gt;% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 1024\n    ) %&gt;%\n  ft_normalizer(\n    input_col = \"hashed_features\", \n    output_col = \"normal_features\"\n    ) %&gt;% \n  select(3:7) \n#&gt; # Source: spark&lt;?&gt; [?? x 5]\n#&gt;    score word_list    wo_stop_words hashed_features normal_features\n#&gt;    &lt;chr&gt; &lt;list&gt;       &lt;list&gt;        &lt;list&gt;          &lt;list&gt;         \n#&gt;  1 other &lt;list [17]&gt;  &lt;list [9]&gt;    &lt;dbl [1,024]&gt;   &lt;dbl [1,024]&gt;  \n#&gt;  2 great &lt;list [100]&gt; &lt;list [61]&gt;   &lt;dbl [1,024]&gt;   &lt;dbl [1,024]&gt;  \n#&gt;  3 great &lt;list [106]&gt; &lt;list [67]&gt;   &lt;dbl [1,024]&gt;   &lt;dbl [1,024]&gt;  \n#&gt;  4 great &lt;list [36]&gt;  &lt;list [20]&gt;   &lt;dbl [1,024]&gt;   &lt;dbl [1,024]&gt;  \n#&gt;  5 great &lt;list [18]&gt;  &lt;list [9]&gt;    &lt;dbl [1,024]&gt;   &lt;dbl [1,024]&gt;  \n#&gt;  6 great &lt;list [30]&gt;  &lt;list [17]&gt;   &lt;dbl [1,024]&gt;   &lt;dbl [1,024]&gt;  \n#&gt;  7 other &lt;list [87]&gt;  &lt;list [58]&gt;   &lt;dbl [1,024]&gt;   &lt;dbl [1,024]&gt;  \n#&gt;  8 great &lt;list [54]&gt;  &lt;list [33]&gt;   &lt;dbl [1,024]&gt;   &lt;dbl [1,024]&gt;  \n#&gt;  9 great &lt;list [59]&gt;  &lt;list [36]&gt;   &lt;dbl [1,024]&gt;   &lt;dbl [1,024]&gt;  \n#&gt; 10 great &lt;list [44]&gt;  &lt;list [24]&gt;   &lt;dbl [1,024]&gt;   &lt;dbl [1,024]&gt;  \n#&gt; # … with more rows\n\n\n\n\n\n\n\nImportant concept\n\n\n\nThe ft_hashing_tf() outputs the index and frequency of each term. This can be thought of as how “dummy variables” are created for each discrete value of a categorical variable. This means that for modeling, we will only need to use only one “column”, hashed_features. But, we will use normal_features for the model because it is derived from hashed_features.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Text modeling"
    ]
  },
  {
    "objectID": "guides/textmodeling.html#prepare-the-model-with-an-ml-pipeline",
    "href": "guides/textmodeling.html#prepare-the-model-with-an-ml-pipeline",
    "title": "Text modeling",
    "section": "Prepare the model with an ML Pipeline",
    "text": "Prepare the model with an ML Pipeline\nThe same set of complex transformations are needed for both modeling, and predictions. This means that we will have to duplicate the code for both. This is not ideal when developing, because any change in the transformation will have to be copied to both sets of code. This makes a compelling argument for using ML Pipelines.\nWe can initialize a pipeline (using ml_pipeline()), and then pass the same exact steps used in the previous section. We then append the model via ft_r_formula() and then the model function, in this case ml_logistic_regression()\n\nsff_pipeline &lt;- ml_pipeline(sc) %&gt;% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %&gt;% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %&gt;% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 1024\n    ) %&gt;%\n  ft_normalizer(\n    input_col = \"hashed_features\", \n    output_col = \"normal_features\"\n    ) %&gt;% \n  ft_r_formula(score ~ normal_features) %&gt;% \n  ml_logistic_regression()  \n\nsff_pipeline\n#&gt; Pipeline (Estimator) with 6 stages\n#&gt; &lt;pipeline__87caaa39_2fa9_4708_a1e1_20ab570c8917&gt; \n#&gt;   Stages \n#&gt;   |--1 Tokenizer (Transformer)\n#&gt;   |    &lt;tokenizer__e3cf3ba6_f7e9_4a05_a41d_11963d70fd6c&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      input_col: review\n#&gt;   |      output_col: word_list\n#&gt;   |--2 StopWordsRemover (Transformer)\n#&gt;   |    &lt;stop_words_remover__3fc0bf48_9fa0_441a_9bb3_5a19ec72be0f&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      input_col: word_list\n#&gt;   |      output_col: wo_stop_words\n#&gt;   |--3 HashingTF (Transformer)\n#&gt;   |    &lt;hashing_tf__3fa3d087_39e8_4668_9921_28150a53412c&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      input_col: wo_stop_words\n#&gt;   |      output_col: hashed_features\n#&gt;   |--4 Normalizer (Transformer)\n#&gt;   |    &lt;normalizer__6d4d9c1c_7488_4a4d_8d42_d9830de4ee2f&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      input_col: hashed_features\n#&gt;   |      output_col: normal_features\n#&gt;   |--5 RFormula (Estimator)\n#&gt;   |    &lt;r_formula__4ae7b190_ce59_4d5f_b75b_cbd623e1a790&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      features_col: features\n#&gt;   |      label_col: label\n#&gt;   |     (Parameters)\n#&gt;   |      force_index_label: FALSE\n#&gt;   |      formula: score ~ normal_features\n#&gt;   |      handle_invalid: error\n#&gt;   |      stringIndexerOrderType: frequencyDesc\n#&gt;   |--6 LogisticRegression (Estimator)\n#&gt;   |    &lt;logistic_regression__46c6e5fb_7c70_44f2_a366_f0a7f94801e1&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      features_col: features\n#&gt;   |      label_col: label\n#&gt;   |      prediction_col: prediction\n#&gt;   |      probability_col: probability\n#&gt;   |      raw_prediction_col: rawPrediction\n#&gt;   |     (Parameters)\n#&gt;   |      aggregation_depth: 2\n#&gt;   |      elastic_net_param: 0\n#&gt;   |      family: auto\n#&gt;   |      fit_intercept: TRUE\n#&gt;   |      max_iter: 100\n#&gt;   |      maxBlockSizeInMB: 0\n#&gt;   |      reg_param: 0\n#&gt;   |      standardization: TRUE\n#&gt;   |      threshold: 0.5\n#&gt;   |      tol: 1e-06",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Text modeling"
    ]
  },
  {
    "objectID": "guides/textmodeling.html#fit-and-predict",
    "href": "guides/textmodeling.html#fit-and-predict",
    "title": "Text modeling",
    "section": "Fit and predict",
    "text": "Fit and predict\nsff_pipeline is an ML Pipeline, which is essentially a set of steps to take, can be think of akin to a recipe. In order to actually process de model we use ml_fit(). This executes all of the transformations, and then fits the model. In other words, ml_fit() runs all of the steps in the pipeline. The output will be considered an ML Pipeline Model.\n\nsff_pipeline_model &lt;- ml_fit(sff_pipeline, sff_training_data)\n\nsff_pipeline_model is more than just a “fitted” model. It also contains all of the pre-processing steps. So any new data passed through it, will go through the same transformations before running the predictions. To execute the pipeline model on against the test data, we use ml_transform()\n\nsff_test_predictions &lt;- sff_pipeline_model %&gt;% \n  ml_transform(sff_testing_data) \n\nglimpse(sff_test_predictions)\n#&gt; Rows: ??\n#&gt; Columns: 12\n#&gt; Database: spark_connection\n#&gt; $ product         &lt;chr&gt; \"B005GXFP60\", \"B000G7V394\", \"B004WJAULO\", \"B003D4MBOS\"…\n#&gt; $ review          &lt;chr&gt; \"These are the best tasting gummy fruits I have ever e…\n#&gt; $ score           &lt;chr&gt; \"great\", \"great\", \"other\", \"other\", \"great\", \"other\", …\n#&gt; $ word_list       &lt;list&gt; [\"these\", \"are\", \"the\", \"best\", \"tasting\", \"gummy\", \"…\n#&gt; $ wo_stop_words   &lt;list&gt; [\"best\", \"tasting\", \"gummy\", \"fruits\", \"ever\", \"eaten…\n#&gt; $ hashed_features &lt;list&gt; &lt;0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#&gt; $ normal_features &lt;list&gt; &lt;0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.000000…\n#&gt; $ features        &lt;list&gt; &lt;0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.000000…\n#&gt; $ label           &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, …\n#&gt; $ rawPrediction   &lt;list&gt; &lt;8.570594, -8.570594&gt;, &lt;-0.1648486, 0.1648486&gt;, &lt;-1.9…\n#&gt; $ probability     &lt;list&gt; &lt;0.9998104359, 0.0001895641&gt;, &lt;0.4588809, 0.5411191&gt;,…\n#&gt; $ prediction      &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, …\n\nUsing ml_metrics_binary(), we can see how well the model performed.\n\nml_metrics_binary(sff_test_predictions)\n#&gt; # A tibble: 2 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.706\n#&gt; 2 pr_auc  binary         0.567",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Text modeling"
    ]
  },
  {
    "objectID": "guides/textmodeling.html#tune-the-model-optional",
    "href": "guides/textmodeling.html#tune-the-model-optional",
    "title": "Text modeling",
    "section": "Tune the model (optional)",
    "text": "Tune the model (optional)\nThe performance of the model may be acceptable, but there could be a desire to improve it. Hyper parameter tuning can be applied to figure if there are better function arguments to use. A big advantage of using an ML Pipeline for the initial model, is that we can literally use the exact same pipeline code to perform the tuning. The Grid Search Tuning article shows how to do this.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Text modeling"
    ]
  },
  {
    "objectID": "guides/model_tuning.html",
    "href": "guides/model_tuning.html",
    "title": "Intro to Model Tuning",
    "section": "",
    "text": "Hyper parameter tuning is possible within Spark. sparklyr provides an interface that makes it possible to setup, and run this kind of model tuning.\nThis article walks through the basics of setting up and running a cross validation tuning run using the cells data from the modeldata package.\ndata(cells, package = \"modeldata\")\nThe goal of the experiments in the example is to see at what point does the number of trees in the model stop improving the accuracy of the predictions. We will have Spark run multiple iterations of the same model with an increasing number of trees.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Intro to Model Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning.html#data-setup",
    "href": "guides/model_tuning.html#data-setup",
    "title": "Intro to Model Tuning",
    "section": "Data setup",
    "text": "Data setup\nFor this example, we will use a local connection, using Spark 3.3.\n\nlibrary(sparklyr)\n\nsc &lt;- spark_connect(master = \"local\", version = \"3.3\")\n\nThe cells data is copied to the Spark session.\n\ntbl_cells &lt;- copy_to(sc, cells, name = \"cells_tbl\")\n\nWe will split the data into two sets, “training” and “test”. The test split will be treated as the “holdout” data to be used at the end of the process to confirm that we did not over fit the model.\n\ntbl_cells_split &lt;- tbl_cells %&gt;% \n  select(-case) %&gt;% \n  sdf_random_split(\n    training = 0.8, \n    test = 0.2, \n    seed = 100\n    )",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Intro to Model Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning.html#cross-validator-prep",
    "href": "guides/model_tuning.html#cross-validator-prep",
    "title": "Intro to Model Tuning",
    "section": "Cross validator prep",
    "text": "Cross validator prep\nPreparing the cross validator requires three elements:\n\nAn ML Pipeline\nA list object containing the “grid”, meaning the different parameters to test\nAn “evaluator” that will calculate the metrics of each model run\n\n\nPipeline\nIn this example we will use a very simple pipeline. It will contain a formula step and a Random Forest model.\n\ncells_pipeline &lt;- sc %&gt;% \n  ml_pipeline() %&gt;%\n  ft_r_formula(class ~ .) %&gt;%\n  ml_random_forest_classifier(seed = 207336481)\n\nEach step within a pipeline receives a unique identifier. This identifier is made up of the name of the step and a UID. The UID will change every time a new pipeline is created. Here is an example of the out put for cells_pipeline:\n#&gt; Pipeline (Estimator) with 2 stages\n#&gt; &lt;pipeline__a1c04c2f_b955_4917_89b6_67cb576a779b&gt; \n#&gt;   Stages \n#&gt;   |--1 RFormula (Estimator)\n#&gt;   |    &lt;r_formula__043dd75a_7fd6_48bb_85c5_f20b321b97cb&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      features_col: features\n#&gt;   |      label_col: label\n#&gt;   |     (Parameters)\n#&gt;   |      force_index_label: FALSE\n#&gt;   |      formula: class ~ .\n#&gt;   |      handle_invalid: error\n#&gt;   |      stringIndexerOrderType: frequencyDesc\n#&gt;   |--2 RandomForestClassifier (Estimator)\n#&gt;   |    &lt;random_forest_classifier__52fc33ff_c4dc_44ac_b842_b873957db90a&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      features_col: features\n#&gt;   |      label_col: label\n#&gt;   |      prediction_col: prediction\n#&gt;   |      probability_col: probability\n#&gt;   |      raw_prediction_col: rawPrediction\n#&gt;   |     (Parameters)\n#&gt;   |      bootstrap: TRUE\n#&gt;   |      cache_node_ids: FALSE\n#&gt;   |      checkpoint_interval: 10\n#&gt;   |      feature_subset_strategy: auto\n#&gt;   |      impurity: gini\n#&gt;   |      leafCol: \n#&gt;   |      max_bins: 32\n#&gt;   |      max_depth: 5\n#&gt;   |      max_memory_in_mb: 256\n#&gt;   |      min_info_gain: 0\n#&gt;   |      min_instances_per_node: 1\n#&gt;   |      minWeightFractionPerNode: 0\n#&gt;   |      num_trees: 20\n#&gt;   |      seed: 207336481\n#&gt;   |      subsampling_rate: 1\n\n\nGrid\nThe way we can pass the parameters to try is via a simple list object. sparklyr performs partial name matching to assign the list’s entries to the pipeline steps and the parameters.\nThe idea is to modify the number of trees for each model run. From the output of the ML Pipeline above, we see that we need to modify the following:\n#&gt;   |    &lt;random_forest_classifier__52fc33ff_c4dc_44ac_b842_b873957db90a&gt; \n#&gt;   |                    ...\n#&gt;   |     (Parameters)\n#&gt;   |                    ...\n#&gt;   |      num_trees: 20\nIn R, we create the grid spec using the following:\n\ncells_grid &lt;- list(\n  random_forest_classifier = list(  \n    num_trees = 1:20 * 5\n  )\n)\n\ncells_grid\n#&gt; $random_forest_classifier\n#&gt; $random_forest_classifier$num_trees\n#&gt;  [1]   5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95\n#&gt; [20] 100\n\nTwo things to highlight about the grid spec:\n\nrandom_forest_classifier is used to partially match to random_forest_classifier__52fc33ff_c4dc_44ac_b842_b873957db90a in the pipeline. It is possible to pass the entire name, but that may prevent it from working if a new pipeline is used. A new pipeline will have a different UID.\nFor num_trees we passed a vector of 20 values to test with\n\n\n\n“Evaluator”\nA metric will have to be calculated for each validation set in the folds. The model tuning function requires for that to be explicitly defined as an argument (ml_cross_validator()). There are multiple outcomes on the class field in the tbl_cells table, this means that we will use ml_multiclass_classification_evaluator() for our validation function.\n\ncells_evaluator &lt;- ml_multiclass_classification_evaluator(\n  x = sc,\n  metric_name = \"accuracy\"\n  )\n\nThe “evaluator” function to use is based on the type of model that is being used for tuning. Here is the list of the available “evaluator” functions in sparklyr:\n\nml_binary_classification_evaluator()\nml_binary_classification_eval()\nml_multiclass_classification_evaluator()\nml_classification_eval()\nml_regression_evaluator()",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Intro to Model Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning.html#model-tuning",
    "href": "guides/model_tuning.html#model-tuning",
    "title": "Intro to Model Tuning",
    "section": "Model tuning",
    "text": "Model tuning\nAll the preparations steps come together now as arguments of ml_cross_validator(). There are two additional arguments to consider:\n\nnum_folds: The number of folder for the cross validation. The higher the number, the longer it will take to complete.\nparallelism: The number of threads to use when running parallel algorithms. Default is 1 for serial execution.\n\nIn this example, cells_pipeline, cells_grid, and cells_evaluator are passed to the respective arguments.\n\ncells_cv &lt;- ml_cross_validator(\n  x = sc,\n  estimator = cells_pipeline, \n  estimator_param_maps = cells_grid,\n  evaluator = cells_evaluator,\n  num_folds = 5,\n  parallelism = 4\n)\n\ncells_cv\n#&gt; CrossValidator (Estimator)\n#&gt; &lt;cross_validator__5efce189_b365_4d96_af6e_14172c733372&gt; \n#&gt;  (Parameters -- Tuning)\n#&gt;   estimator: Pipeline\n#&gt;              &lt;pipeline__70e03d4c_ae3f_4b78_970e_99b209fce2fb&gt; \n#&gt;   evaluator: MulticlassClassificationEvaluator\n#&gt;              &lt;multiclass_classification_evaluator__add09dd3_1143_49e6_a923_c8bff6fb942f&gt; \n#&gt;     with metric accuracy \n#&gt;   num_folds: 5 \n#&gt;   [Tuned over 20 hyperparameter sets]\n\nThe ml_fit() function will actually run the model tuning. This is where the training split of the data is used.\n\nmodel_cv &lt;- ml_fit(\n  x = cells_cv, \n  dataset = tbl_cells_split$training\n  )",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Intro to Model Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning.html#validation-metrics",
    "href": "guides/model_tuning.html#validation-metrics",
    "title": "Intro to Model Tuning",
    "section": "Validation metrics",
    "text": "Validation metrics\nThe ml_validation_metrics() function will extract the metrics from each of the values passed for number of trees parameter. If more than one parameter would have been used, the number of results would be the total number of combinations.\n\ncv_metrics &lt;- ml_validation_metrics(model_cv)\n\ncv_metrics\n#&gt;     accuracy num_trees_1\n#&gt; 1  0.8114324           5\n#&gt; 2  0.8157122          10\n#&gt; 3  0.8176925          15\n#&gt; 4  0.8150990          20\n#&gt; 5  0.8241812          25\n#&gt; 6  0.8212221          30\n#&gt; 7  0.8221358          35\n#&gt; 8  0.8310173          40\n#&gt; 9  0.8290158          45\n#&gt; 10 0.8296345          50\n#&gt; 11 0.8335203          55\n#&gt; 12 0.8301165          60\n#&gt; 13 0.8234423          65\n#&gt; 14 0.8257469          70\n#&gt; 15 0.8258069          75\n#&gt; 16 0.8340482          80\n#&gt; 17 0.8321753          85\n#&gt; 18 0.8276960          90\n#&gt; 19 0.8223973          95\n#&gt; 20 0.8294530         100\n\nFor easier selection, we can use a quick plot to visualize how accuracy improves as more trees are used, and when does the benefit plateau.\n\nlibrary(ggplot2)\n\ncv_metrics %&gt;% \n  ggplot(aes(num_trees_1, accuracy)) +\n  geom_line() +\n  geom_smooth()",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Intro to Model Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning.html#model-selection",
    "href": "guides/model_tuning.html#model-selection",
    "title": "Intro to Model Tuning",
    "section": "Model selection",
    "text": "Model selection\nAs seen in the previous section, 50 trees seems to be a good number to use. To finalize, a new model is fit, using that number for num_trees.\nA Model pipeline or a regular model could be used to do this. For this example we will just use the single step of fitting a new model using ml_random_forest_classifier() directly.\n\ncell_model &lt;- ml_random_forest_classifier(\n  tbl_cells_split$training, \n  class ~ ., \n  num_trees = 50\n  )",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Intro to Model Tuning"
    ]
  },
  {
    "objectID": "guides/model_tuning.html#test-data-metrics",
    "href": "guides/model_tuning.html#test-data-metrics",
    "title": "Intro to Model Tuning",
    "section": "Test data metrics",
    "text": "Test data metrics\nThe final step is to confirm that the model is not over-fitted. We use the new model against the test split, and then piping it to ml_metrics_multiclass() to confirm that the accuracy is within the expected range.\n\ncell_model %&gt;% \n  ml_predict(tbl_cells_split$test) %&gt;% \n  ml_metrics_multiclass()\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy multiclass     0.839",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Intro to Model Tuning"
    ]
  },
  {
    "objectID": "guides/caching.knit.html",
    "href": "guides/caching.knit.html",
    "title": "Understanding Spark Caching",
    "section": "",
    "text": "Spark also supports pulling data sets into a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small dataset or when running an iterative algorithm like random forests. Since operations in Spark are lazy, caching can help force computation. sparklyr tools can be used to cache and un-cache DataFrames. The Spark UI will tell you which DataFrames and what percentages are in memory.\nBy using a reproducible example, we will review some of the main configuration settings, commands and command arguments that can be used that can help you get the best out of Spark’s memory management options."
  },
  {
    "objectID": "guides/caching.knit.html#introduction",
    "href": "guides/caching.knit.html#introduction",
    "title": "Understanding Spark Caching",
    "section": "",
    "text": "Spark also supports pulling data sets into a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small dataset or when running an iterative algorithm like random forests. Since operations in Spark are lazy, caching can help force computation. sparklyr tools can be used to cache and un-cache DataFrames. The Spark UI will tell you which DataFrames and what percentages are in memory.\nBy using a reproducible example, we will review some of the main configuration settings, commands and command arguments that can be used that can help you get the best out of Spark’s memory management options."
  },
  {
    "objectID": "guides/caching.knit.html#preparation",
    "href": "guides/caching.knit.html#preparation",
    "title": "Understanding Spark Caching",
    "section": "Preparation",
    "text": "Preparation\n\nDownload Test Data\nBecause of their size, we will use trip data provided by the NYC Taxi & Limousine Commission. Each file represents a month’s worth of trips. We will download two files, the ones for January and February 2020.\n\nif(!file.exists(\"jan_2020.parquet\")) {\n  download.file(\n    \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-01.parquet\",\n    \"jan_2020.parquet\",\n    mode = \"wb\"\n  )  \n}\n\nif(!file.exists(\"feb_2020.parquet\")) {\n  download.file(\n    \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-02.parquet\",\n    \"feb_2020.parquet\",\n    mode = \"wb\"\n  )  \n}\n\n\n\nStart a Spark session\nA local deployment will be used for this example.\n\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n# Customize the connection configuration\nconf &lt;- spark_config()\nconf$`sparklyr.shell.driver-memory` &lt;- \"16G\"\n\n# Connect to Spark\nsc &lt;- spark_connect(master = \"local\", config = conf)"
  },
  {
    "objectID": "guides/caching.knit.html#the-memory-argument",
    "href": "guides/caching.knit.html#the-memory-argument",
    "title": "Understanding Spark Caching",
    "section": "The Memory Argument",
    "text": "The Memory Argument\nIn the spark_read_… functions, the memory argument controls if the data will be loaded into memory as an RDD. Setting it to FALSE means that Spark will essentially map the file, but not make a copy of it in memory. This makes the spark_read_parquet() command run faster, but the trade off is that any data transformation operations will take much longer.\n\nspark_read_parquet(\n  sc, \n  \"taxi_jan_2020\", \n  \"jan_2020.parquet\", \n  memory = FALSE\n  )\n#&gt; # Source: spark&lt;taxi_jan_2020&gt; [?? x 19]\n#&gt;    VendorID tpep_pickup_datetime tpep_dropoff_date…¹ passe…²\n#&gt;       &lt;dbl&gt; &lt;dttm&gt;               &lt;dttm&gt;                &lt;dbl&gt;\n#&gt;  1        1 2019-12-31 18:28:15  2019-12-31 18:33:03       1\n#&gt;  2        1 2019-12-31 18:35:39  2019-12-31 18:43:04       1\n#&gt;  3        1 2019-12-31 18:47:41  2019-12-31 18:53:52       1\n#&gt;  4        1 2019-12-31 18:55:23  2019-12-31 19:00:14       1\n#&gt;  5        2 2019-12-31 18:01:58  2019-12-31 18:04:16       1\n#&gt;  6        2 2019-12-31 18:09:44  2019-12-31 18:10:37       1\n#&gt;  7        2 2019-12-31 18:39:25  2019-12-31 18:39:29       1\n#&gt;  8        2 2019-12-18 09:27:49  2019-12-18 09:28:59       1\n#&gt;  9        2 2019-12-18 09:30:35  2019-12-18 09:31:35       4\n#&gt; 10        1 2019-12-31 18:29:01  2019-12-31 18:40:28       2\n#&gt; # … with more rows, 15 more variables: trip_distance &lt;dbl&gt;,\n#&gt; #   RatecodeID &lt;dbl&gt;, store_and_fwd_flag &lt;chr&gt;,\n#&gt; #   PULocationID &lt;dbl&gt;, DOLocationID &lt;dbl&gt;,\n#&gt; #   payment_type &lt;dbl&gt;, fare_amount &lt;dbl&gt;, extra &lt;dbl&gt;,\n#&gt; #   mta_tax &lt;dbl&gt;, tip_amount &lt;dbl&gt;, tolls_amount &lt;dbl&gt;,\n#&gt; #   improvement_surcharge &lt;dbl&gt;, total_amount &lt;dbl&gt;,\n#&gt; #   congestion_surcharge &lt;dbl&gt;, airport_fee &lt;int&gt;, and …\n\nIn the RStudio IDE, the taxi_jan_2020 table now shows up in the Spark tab.\n\nTo access the Spark Web UI, click the Spark button in the RStudio Spark Tab. As expected, the Storage page shows no tables loaded into memory."
  },
  {
    "objectID": "guides/caching.knit.html#loading-less-data-into-memory",
    "href": "guides/caching.knit.html#loading-less-data-into-memory",
    "title": "Understanding Spark Caching",
    "section": "Loading Less Data into Memory",
    "text": "Loading Less Data into Memory\nUsing the pre-processing capabilities of Spark, the data will be transformed before being loaded into memory. In this section, we will continue to build on the example started in the previous section\n\nLazy Transform\nThe following dplyr script will not be immediately run, so the code is processed quickly. There are some check-ups made, but for the most part it is building a Spark SQL statement in the background.\n\ntrips_table &lt;- tbl(sc,\"taxi_jan_2020\") %&gt;%\n  filter(trip_distance &gt; 20) %&gt;% \n  select(VendorID, passenger_count, trip_distance)\n\n\n\nRegister in Spark\nsdf_register() will register the resulting Spark SQL in Spark. The results will show up as a table called trip_spark. But a table of the same name is still not loaded into memory in Spark.\n\nsdf_register(trips_table, \"trips_spark\")\n#&gt; # Source: spark&lt;trips_spark&gt; [?? x 3]\n#&gt;    VendorID passenger_count trip_distance\n#&gt;       &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n#&gt;  1        2               1          23.5\n#&gt;  2        1               2          22.8\n#&gt;  3        2               2          37.6\n#&gt;  4        2               4          20.3\n#&gt;  5        1               2          29.4\n#&gt;  6        2               1          25.9\n#&gt;  7        2               3          22.1\n#&gt;  8        2               1          21.0\n#&gt;  9        2               3          20.1\n#&gt; 10        2               1          32.5\n#&gt; # … with more rows\n\n\n\n\nCache into Memory\nThe tbl_cache() command loads the results into an Spark RDD in memory, so any analysis from there on will not need to re-read and re-transform the original file. The resulting Spark RDD is smaller than the original file because the transformations created a smaller data set than the original file.\n\ntbl_cache(sc, \"trips_spark\")\n\n\n\n\nDriver Memory\nIn the Executors page of the Spark Web UI, we can see that the Storage Memory is at about half of the 16 gigabytes requested. This is mainly because of a Spark setting called spark.memory.fraction, which reserves by default 40% of the memory requested."
  },
  {
    "objectID": "guides/caching.knit.html#process-on-the-fly",
    "href": "guides/caching.knit.html#process-on-the-fly",
    "title": "Understanding Spark Caching",
    "section": "Process on the fly",
    "text": "Process on the fly\nThe plan for this exercise is to read the January file, combine it with the February file and summarize the data without bringing either file fully into memory.\n\nspark_read_parquet(sc, \"taxi_feb_2020\" , \"feb_2020.parquet\", memory = FALSE)\n#&gt; # Source: spark&lt;taxi_feb_2020&gt; [?? x 19]\n#&gt;    VendorID tpep_pickup_datetime tpep_dropoff_date…¹ passe…²\n#&gt;       &lt;dbl&gt; &lt;dttm&gt;               &lt;dttm&gt;                &lt;dbl&gt;\n#&gt;  1        1 2020-01-31 18:17:35  2020-01-31 18:30:32       1\n#&gt;  2        1 2020-01-31 18:32:47  2020-01-31 19:05:36       1\n#&gt;  3        1 2020-01-31 18:31:44  2020-01-31 18:43:28       1\n#&gt;  4        2 2020-01-31 18:07:35  2020-01-31 18:31:39       1\n#&gt;  5        2 2020-01-31 18:51:43  2020-01-31 19:01:29       1\n#&gt;  6        1 2020-01-31 18:15:49  2020-01-31 18:20:48       2\n#&gt;  7        1 2020-01-31 18:25:31  2020-01-31 18:50:22       2\n#&gt;  8        1 2020-01-31 18:11:15  2020-01-31 18:24:29       1\n#&gt;  9        2 2020-01-31 18:58:26  2020-01-31 19:02:26       1\n#&gt; 10        2 2020-01-31 18:03:57  2020-01-31 18:48:10       1\n#&gt; # … with more rows, 15 more variables: trip_distance &lt;dbl&gt;,\n#&gt; #   RatecodeID &lt;dbl&gt;, store_and_fwd_flag &lt;chr&gt;,\n#&gt; #   PULocationID &lt;dbl&gt;, DOLocationID &lt;dbl&gt;,\n#&gt; #   payment_type &lt;dbl&gt;, fare_amount &lt;dbl&gt;, extra &lt;dbl&gt;,\n#&gt; #   mta_tax &lt;dbl&gt;, tip_amount &lt;dbl&gt;, tolls_amount &lt;dbl&gt;,\n#&gt; #   improvement_surcharge &lt;dbl&gt;, total_amount &lt;dbl&gt;,\n#&gt; #   congestion_surcharge &lt;dbl&gt;, airport_fee &lt;int&gt;, and …\n\n\nUnion and Transform\nThe union() command is akin to the dplyr::bind_rows() command. It will allow us to append the February file to the January file, and as with the previous transform, this script will be evaluated lazily.\n\npassenger_count &lt;- tbl(sc, \"taxi_jan_2020\") %&gt;%\n  union(tbl(sc, \"taxi_feb_2020\")) %&gt;%\n  mutate(pickup_date = as.Date(tpep_pickup_datetime)) %&gt;% \n  count(pickup_date)\n\n\n\nCollect into R\nWhen receiving a collect() command, Spark will execute the SQL statement and send the results back to R in a data frame. In this case, R only loads 51 observations into a data frame called passenger_count.\n\npassenger_count &lt;- passenger_count %&gt;%\n  collect()\n\n\n\nPlot in R\nNow the smaller data set can be plotted\n\npassenger_count %&gt;% \n  filter(pickup_date &gt;= \"2020-01-01\", pickup_date &lt;= \"2020-02-28\") %&gt;% \n  ggplot() +\n  geom_line(aes(pickup_date, n)) +\n  theme_minimal() +\n  labs(title = \"Daily Trip Volume\", \n       subtitle = \"NYC Yellow Cab - January and February 2020\",\n       y = \"Number of Trips\",\n       x = \"\"\n       )\n\n\n\n\n\n\n\n\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "guides/tidymodels.html",
    "href": "guides/tidymodels.html",
    "title": "tidymodels and Spark",
    "section": "",
    "text": "tidymodels is a collection of packages for modeling and machine learning. Just like sparklyr, tidymodels uses tidyverse principles.\nsparklyr allows us to use dplyr verbs to manipulate data. We use the same commands in R when manipulating local data or Spark data. Similarly, sparklyr and some packages in the tidymodels ecosystem offer integration.\nAs with any evolving framework, the integration does not apply to all functions. This article aims at enumerating what is available today, and why should we consider using the tidymodels implementation in our day-to-day work with Spark.\nOur expectation is that this article will be constantly updated as the integration between tidymodels and sparklyr grows and improves.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Using `tidymodels`"
    ]
  },
  {
    "objectID": "guides/tidymodels.html#intro",
    "href": "guides/tidymodels.html#intro",
    "title": "tidymodels and Spark",
    "section": "",
    "text": "tidymodels is a collection of packages for modeling and machine learning. Just like sparklyr, tidymodels uses tidyverse principles.\nsparklyr allows us to use dplyr verbs to manipulate data. We use the same commands in R when manipulating local data or Spark data. Similarly, sparklyr and some packages in the tidymodels ecosystem offer integration.\nAs with any evolving framework, the integration does not apply to all functions. This article aims at enumerating what is available today, and why should we consider using the tidymodels implementation in our day-to-day work with Spark.\nOur expectation is that this article will be constantly updated as the integration between tidymodels and sparklyr grows and improves.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Using `tidymodels`"
    ]
  },
  {
    "objectID": "guides/tidymodels.html#model-specification-with-parsnip",
    "href": "guides/tidymodels.html#model-specification-with-parsnip",
    "title": "tidymodels and Spark",
    "section": "Model specification with parsnip",
    "text": "Model specification with parsnip\nparsnip provides a common interface to models. This enables us to run the same model against multiple engines. parsnip contains translation for each of these packages, so we do not have to remember, or find out, how to setup each argument in the respective package.\n\nWhy use in Spark?\nIn some cases, it is better to try out model parameters on a smaller, local, data set in R. Once we are happy with the parameters, we can then run the model over the entire data set in Spark.\nFor example, doing this for a linear regression model, we would use lm() locally, and then we would have to re-write the model using ml_linear_regression(). Both of these functions have different sets of function arguments that we would need to set.\nparsnip allows us to use the exact same set of functions and arguments when running against either back-end. With a couple of small changes, we can change the target engine (R vs Spark) and the target data set (local vs remote). Here is an example of what the model fitting looks like locally in R:\n\nlinear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%           # &lt;&lt; Engine set to `lm`\n  fit(mpg ~ ., data = mtcars)    # &lt;&lt; Local `mtcars`\n\nTo switch to Spark, we just need to change the engine to spark, and the training data set to the remote Spark data set:\n\nlinear_reg() %&gt;%\n  set_engine(\"spark\") %&gt;%           # &lt;&lt; Engine set to `spark`\n  fit(mpg ~ ., data = spark_mtcars) # &lt;&lt; Remote `mtcars`\n\n\n\nList of supported models\nThere are six parsnip models that currently support sparklyr equivalent models. Here is the list:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nparsnip function\nClassification\nRegression\n\n\n\n\nBoosted trees\n\nboost_tree()\n\nYes\nYes\n\n\nDecision trees\n\ndecision_tree()\n\nYes\nYes\n\n\nLinear regression\n\nlinear_reg()\n\n\nYes\n\n\nLogistic regression\n\nlogistic_reg()\n\nYes\n\n\n\nMultinomial regression\n\nmultinom_reg()\n\nYes\n\n\n\nRandom forest\n\nrand_forest()\n\nYes\nYes\n\n\n\n\n\n\n\n\n\n\nExamples\nThis article will use the same Spark session in all the examples.\n\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc &lt;- spark_connect(\"local\")\n\nWe will upload the mtcars data set to the Spark session:\n\nspark_mtcars &lt;- copy_to(sc, mtcars)\n\nA linear regression model is trained with spark_mtcars:\n\nlibrary(parsnip)\n\nmtcars_model &lt;- linear_reg() %&gt;%\n  set_engine(\"spark\") %&gt;%\n  fit(mpg ~ ., data = spark_mtcars)\n\nmtcars_model\n#&gt; parsnip model object\n#&gt; \n#&gt; Formula: mpg ~ .\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)         cyl        disp          hp        drat          wt \n#&gt; 12.30337416 -0.11144048  0.01333524 -0.02148212  0.78711097 -3.71530393 \n#&gt;        qsec          vs          am        gear        carb \n#&gt;  0.82104075  0.31776281  2.52022689  0.65541302 -0.19941925\n\nIt is also possible to see how parsnip plans to translate the model against the given engine. Use translate() so view the translation:\n\nlinear_reg() %&gt;%\n  set_engine(\"spark\") %&gt;%\n  translate()\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: spark \n#&gt; \n#&gt; Model fit template:\n#&gt; sparklyr::ml_linear_regression(x = missing_arg(), formula = missing_arg(), \n#&gt;     weights = missing_arg())\n\nNow, we will show an example with a classification model. We will fit a random forest model. To start, we will copy the iris data set to the Spark session:\n\nspark_iris &lt;- copy_to(sc, iris)\n\nWe can prepare the model by piping the initial setup of 100 trees, then then to set the mode to “classification”, and then the engine to “spark” and lastly, fit the model:\n\niris_model &lt;- rand_forest(trees = 100) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"spark\") %&gt;% \n  fit(Species ~., data = spark_iris)\n\niris_model\n#&gt; parsnip model object\n#&gt; \n#&gt; Formula: Species ~ .\n#&gt; \n#&gt; RandomForestClassificationModel: uid=random_forest__1a784501_52cb_47a2_8b01_283d1b5e321a, numTrees=100, numClasses=3, numFeatures=4",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Using `tidymodels`"
    ]
  },
  {
    "objectID": "guides/tidymodels.html#model-results-with-broom",
    "href": "guides/tidymodels.html#model-results-with-broom",
    "title": "tidymodels and Spark",
    "section": "Model results with broom",
    "text": "Model results with broom\nThe broom package offers great ways to get summarized information about a fitted model. There is support for three broom functions in sparklyr:\n\ntidy() - Summarizes information about the components of a model. A model component might be a single term in a regression, a single hypothesis, a cluster, or a class.\nglance() - Returns a data frame with exactly one row of model summaries. The summaries are typically goodness of fit measures, p-values for hypothesis tests on residuals, or model convergence information.\naugment() - Adds the prediction columns to the data set. This function is similar to ml_predict(), but instead of returning only a vector of predictions (like predict()), it adds the new column(s) to the data set. augment()\n\n\nWhy use in Spark?\ntidy() and glance() offer a very good, concise way to view the model results in a rectangular data frame. This is very helpful when we want to compare different model runs side-by-side.\n\n\nList of supported models\nCurrently, 20 Spark models support broom via sparklyr. Here is the current list of models and the corresponding sparklyr function:\n\n\n\n\n\n  \n    \n      Models that support glance(), tidy(), and augment()\n    \n    \n  \n  \n    \n      Model\n      Function\n    \n  \n  \n     ALS\nml_als()\n\n     Bisecting K-Means Clustering\nml_bisecting_kmeans()\n\n     Decision Trees\nml_decision_tree()\n\n     Gaussian Mixture clustering.\nml_gaussian_mixture()\n\n     Generalized Linear Regression\nml_generalized_linear_regression()\n\n     Gradient Boosted Trees\nml_gradient_boosted_trees()\n\n     Isotonic Regression\nml_isotonic_regression()\n\n     K-Means Clustering\nml_kmeans()\n\n     Latent Dirichlet Allocation\nml_lda()\n\n     Linear Regression\nml_linear_regression()\n\n     LinearSVC\nml_linear_svc()\n\n     Logistic Regression\nml_logistic_regression()\n\n     Multilayer Perceptron\nml_multilayer_perceptron()\n\n     Naive-Bayes\nml_naive_bayes()\n\n     Random Forest\nml_random_forest()\n\n     Survival Regression\nml_aft_survival_regression()\n\n    PCA (Estimator)\nml_pca()\n\n  \n  \n  \n\n\n\n\n\n\nExamples\nUsing the same Spark session and models created in the previous section we start by loading broom:\n\nlibrary(broom)\n\nTo view the estimates for each term simply pass mtcars_model to the tidy() function:\n\ntidy(mtcars_model)\n#&gt; # A tibble: 11 × 5\n#&gt;    term        estimate std.error statistic p.value\n#&gt;    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 (Intercept)  12.3      18.7        0.657  0.518 \n#&gt;  2 cyl          -0.111     1.05      -0.107  0.916 \n#&gt;  3 disp          0.0133    0.0179     0.747  0.463 \n#&gt;  4 hp           -0.0215    0.0218    -0.987  0.335 \n#&gt;  5 drat          0.787     1.64       0.481  0.635 \n#&gt;  6 wt           -3.72      1.89      -1.96   0.0633\n#&gt;  7 qsec          0.821     0.731      1.12   0.274 \n#&gt;  8 vs            0.318     2.10       0.151  0.881 \n#&gt;  9 am            2.52      2.06       1.23   0.234 \n#&gt; 10 gear          0.655     1.49       0.439  0.665 \n#&gt; 11 carb         -0.199     0.829     -0.241  0.812\n\nglance() returns the the models R Squared, error means, and variance:\n\nglance(mtcars_model)\n#&gt; # A tibble: 1 × 5\n#&gt;   explained.variance mean.absolute…¹ mean.…² r.squ…³ root.…⁴\n#&gt;                &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1               30.6            1.72    4.61   0.869    2.15\n#&gt; # … with abbreviated variable names ¹​mean.absolute.error,\n#&gt; #   ²​mean.squared.error, ³​r.squared,\n#&gt; #   ⁴​root.mean.squared.error\n\n\naugment(mtcars_model)\n\nFor our classification model, tidy() returns each feature’s importance:\n\ntidy(iris_model)\n#&gt; # A tibble: 4 × 2\n#&gt;   feature      importance\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;\n#&gt; 1 Petal_Length     0.455 \n#&gt; 2 Petal_Width      0.403 \n#&gt; 3 Sepal_Length     0.126 \n#&gt; 4 Sepal_Width      0.0160\n\nThe glance() model returns the number of trees, nodes depth, sub-sampling rate and impurtiy mode:\n\nglance(iris_model)\n#&gt; # A tibble: 1 × 5\n#&gt;   num_trees total_num_nodes max_depth impurity subsampling…¹\n#&gt;       &lt;int&gt;           &lt;int&gt;     &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt;\n#&gt; 1       100            1468         5 gini                 1\n#&gt; # … with abbreviated variable name ¹​subsampling_rate",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Using `tidymodels`"
    ]
  },
  {
    "objectID": "guides/tidymodels.html#yardstick-like-metrics",
    "href": "guides/tidymodels.html#yardstick-like-metrics",
    "title": "tidymodels and Spark",
    "section": "yardstick-like metrics",
    "text": "yardstick-like metrics\nThe metrics() function, from the yardstick package, provides an easy to read tibble with the relevant metrics. It automatically detects the type of model and it decides which metrics to show.\n\nWhy use in Spark?\nIn sparklyr, the family of ml_metrics... functions outputs a tibble with the same structure as yardstick::metrics(). The functions also expect the same base arguments of x, truth and estimate. In sparklyr, model detection is not available yet, so based on the type of model, there are three functions to choose from.\nThe ml_metrics... functions expect a tbl_spark that was created by the ml_predict() function. These functions provide a metrics argument that allows us to change the metrics to calculate. All of the metrics that have an equivalent in yardstick can be called using the same value, such as f_meas. For others, they can be requested using Spark’s designation. For more information, see the help file of the specific ml_metrics... function.\n\nml_metrics_binary()\nml_metrics_multiclass()\nml_metrics_regression()\n\n\nHow are they different form ml_evaluate()?\nIt is true that both sets of functions return metrics based on the results. The difference is that ml_evaluate() requires the original Spark model object in order to work. ml_metrics... only required a table with the predictions, preferably, predictions created by ml_predict().\n\n\n\nExample\nUsing sdf_random_split(), split the data into training and test. And then fit a model. In this case it will be a logistic regression model.\n\nprep_iris &lt;- tbl(sc, \"iris\") %&gt;%\n  mutate(is_setosa = ifelse(Species == \"setosa\", 1, 0))\n\niris_split &lt;- sdf_random_split(prep_iris, training = 0.5, test = 0.5)\n\nmodel &lt;- ml_logistic_regression(iris_split$training, \"is_setosa ~ Sepal_Length\")\n\nWith ml_predict(), create a new tbl_spark that contains the original data and additional columns needed created by the prediction process.\n\ntbl_predictions &lt;- ml_predict(model, iris_split$test)\n\nThe ml_metrics_binary() outputs a tibble with the ROC and PR AUC.\n\nml_metrics_binary(tbl_predictions)\n#&gt; # A tibble: 2 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.962\n#&gt; 2 pr_auc  binary         0.913",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Using `tidymodels`"
    ]
  },
  {
    "objectID": "guides/tidymodels.html#correlations-using-corrr",
    "href": "guides/tidymodels.html#correlations-using-corrr",
    "title": "tidymodels and Spark",
    "section": "Correlations using corrr",
    "text": "Correlations using corrr\nThe corrr package helps with exploring data correlations in R. It returns a data frame with all of the correlations.\n\nWhy use in Spark?\nFor sparklyr, corrr wraps the ml_cor() function, and returns a data frame with the exact same format as if the correlation would have been calculated in R. This allows us to use all the other functions inside corrr, such as filtering, and plotting without having to re-run the correlation inside Spark.\n\n\nExample\nWe start by loading the package corrr:\n\nlibrary(corrr)\n\nWe will pipe spark_mtcars into the correlate() function. That runs the correlations inside Spark, and returning the results into R. Those results are saved into a data frame:\n\ncorr_mtcars &lt;- spark_mtcars %&gt;% \n  correlate()\n\nThe corr_mtcars variable is now a local data set. So we do not need to go back to Spark if we wish to use it for other things that corrr can do:\n\ncorr_mtcars\n#&gt; # A tibble: 11 × 12\n#&gt;    term     mpg    cyl   disp     hp    drat     wt    qsec\n#&gt;    &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 mpg   NA     -0.852 -0.848 -0.776  0.681  -0.868  0.419 \n#&gt;  2 cyl   -0.852 NA      0.902  0.832 -0.700   0.782 -0.591 \n#&gt;  3 disp  -0.848  0.902 NA      0.791 -0.710   0.888 -0.434 \n#&gt;  4 hp    -0.776  0.832  0.791 NA     -0.449   0.659 -0.708 \n#&gt;  5 drat   0.681 -0.700 -0.710 -0.449 NA      -0.712  0.0912\n#&gt;  6 wt    -0.868  0.782  0.888  0.659 -0.712  NA     -0.175 \n#&gt;  7 qsec   0.419 -0.591 -0.434 -0.708  0.0912 -0.175 NA     \n#&gt;  8 vs     0.664 -0.811 -0.710 -0.723  0.440  -0.555  0.745 \n#&gt;  9 am     0.600 -0.523 -0.591 -0.243  0.713  -0.692 -0.230 \n#&gt; 10 gear   0.480 -0.493 -0.556 -0.126  0.700  -0.583 -0.213 \n#&gt; 11 carb  -0.551  0.527  0.395  0.750 -0.0908  0.428 -0.656 \n#&gt; # … with 4 more variables: vs &lt;dbl&gt;, am &lt;dbl&gt;, gear &lt;dbl&gt;,\n#&gt; #   carb &lt;dbl&gt;\n\nFor example, shave() removes the duplicate correlations from the data set, making it easier to read:\n\ncorr_mtcars %&gt;% \n  shave()\n#&gt; # A tibble: 11 × 12\n#&gt;    term     mpg    cyl   disp     hp    drat     wt   qsec\n#&gt;    &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 mpg   NA     NA     NA     NA     NA      NA     NA    \n#&gt;  2 cyl   -0.852 NA     NA     NA     NA      NA     NA    \n#&gt;  3 disp  -0.848  0.902 NA     NA     NA      NA     NA    \n#&gt;  4 hp    -0.776  0.832  0.791 NA     NA      NA     NA    \n#&gt;  5 drat   0.681 -0.700 -0.710 -0.449 NA      NA     NA    \n#&gt;  6 wt    -0.868  0.782  0.888  0.659 -0.712  NA     NA    \n#&gt;  7 qsec   0.419 -0.591 -0.434 -0.708  0.0912 -0.175 NA    \n#&gt;  8 vs     0.664 -0.811 -0.710 -0.723  0.440  -0.555  0.745\n#&gt;  9 am     0.600 -0.523 -0.591 -0.243  0.713  -0.692 -0.230\n#&gt; 10 gear   0.480 -0.493 -0.556 -0.126  0.700  -0.583 -0.213\n#&gt; 11 carb  -0.551  0.527  0.395  0.750 -0.0908  0.428 -0.656\n#&gt; # … with 4 more variables: vs &lt;dbl&gt;, am &lt;dbl&gt;, gear &lt;dbl&gt;,\n#&gt; #   carb &lt;dbl&gt;\n\nrplot() provides a nice way to visualize the correlations. Again, because corr_mtcars’s data it is currently locally in R, plotting requires no extra steps:\n\ncorr_mtcars %&gt;% \n  rplot()",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "Using `tidymodels`"
    ]
  },
  {
    "objectID": "guides/pipelines.html",
    "href": "guides/pipelines.html",
    "title": "Spark ML Pipelines",
    "section": "",
    "text": "Spark’s ML Pipelines provide a way to easily combine multiple transformations and algorithms into a single workflow, or pipeline.\nFor R users, the insights gathered during the interactive sessions with Spark can now be converted to a formal pipeline. This makes the hand-off from Data Scientists to Big Data Engineers a lot easier, this is because there should not be additional changes needed to be made by the later group.\nThe final list of selected variables, data manipulation, feature transformations and modeling can be easily re-written into a ml_pipeline() object, saved, and ultimately placed into a Production environment. The sparklyr output of a saved Spark ML Pipeline object is in Scala code, which means that the code can be added to the scheduled Spark ML jobs, and without any dependencies in R.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "ML Pipelines"
    ]
  },
  {
    "objectID": "guides/pipelines.html#introduction-to-ml-pipelines",
    "href": "guides/pipelines.html#introduction-to-ml-pipelines",
    "title": "Spark ML Pipelines",
    "section": "Introduction to ML Pipelines",
    "text": "Introduction to ML Pipelines\nThe official Apache Spark site contains a more complete overview of ML Pipelines. This article will focus in introducing the basic concepts and steps to work with ML Pipelines via sparklyr.\nThere are two important stages in building an ML Pipeline. The first one is creating a Pipeline. A good way to look at it, or call it, is as an “empty” pipeline. This step just builds the steps that the data will go through. This is the somewhat equivalent of doing this in R:\n\nlibrary(dplyr)\n\nr_pipeline &lt;-  . %&gt;% mutate(cyl = paste0(\"c\", cyl)) %&gt;% lm(am ~ cyl + mpg, data = .)\nr_pipeline\n#&gt; Functional sequence with the following components:\n#&gt; \n#&gt;  1. mutate(., cyl = paste0(\"c\", cyl))\n#&gt;  2. lm(am ~ cyl + mpg, data = .)\n#&gt; \n#&gt; Use 'functions' to extract the individual functions.\n\nThe r_pipeline object has all the steps needed to transform and fit the model, but it has not yet transformed any data. The second step, is to pass data through the pipeline, which in turn will output a fitted model. That is called a PipelineModel. The PipelineModel can then be used to produce predictions.\n\nr_model &lt;- r_pipeline(mtcars)\nr_model\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = am ~ cyl + mpg, data = .)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)        cylc6        cylc8          mpg  \n#&gt;    -0.54388      0.03124     -0.03313      0.04767\n\n\nTaking advantage of Pipelines and PipelineModels\nThe two stage ML Pipeline approach produces two final data products:\n\nA PipelineModel that can be added to the daily Spark jobs which will produce new predictions for the incoming data, and again, with no R dependencies.\nA Pipeline that can be easily re-fitted on a regular interval, say every month. All that is needed is to pass a new sample to obtain the new coefficients.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "ML Pipelines"
    ]
  },
  {
    "objectID": "guides/pipelines.html#pipeline",
    "href": "guides/pipelines.html#pipeline",
    "title": "Spark ML Pipelines",
    "section": "Pipeline",
    "text": "Pipeline\nAn additional goal of this article is that the reader can follow along, so the data, transformations and Spark connection in this example will be kept as easy to reproduce as possible.\n\nlibrary(nycflights13)\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc &lt;- spark_connect(master = \"local\")\n\nspark_flights &lt;- copy_to(sc, flights)\n\n\nFeature Transformers\nPipelines make heavy use of Feature Transformers. If new to Spark, and sparklyr, it would be good to review what these transformers do. These functions use the Spark API directly to transform the data, and may be faster at making the data manipulations that a dplyr (SQL) transformation.\nIn sparklyr the ft functions are essentially are wrappers to original Spark feature transformer.\n\n\nft_dplyr_transformer\nThis example will start with dplyr transformations, which are ultimately SQL transformations, loaded into the df variable.\nIn sparklyr, there is one feature transformer that is not available in Spark, ft_dplyr_transformer(). The goal of this function is to convert the dplyr code to a SQL Feature Transformer that can then be used in a Pipeline.\n\ndf &lt;- spark_flights %&gt;%\n  filter(!is.na(dep_delay)) %&gt;%\n  mutate(\n    month = paste0(\"m\", month),\n    day = paste0(\"d\", day)\n  ) %&gt;%\n  select(dep_delay, sched_dep_time, month, day, distance) \n\nThis is the resulting pipeline stage produced from the dplyr code:\n\nft_dplyr_transformer(sc, df)\n#&gt; SQLTransformer (Transformer)\n#&gt; &lt;dplyr_transformer__18f79b59_a1d4_4d16_a7a9_0c501ff89afd&gt; \n#&gt;  (Parameters -- Column Names)\n\nUse the ml_param() function to extract the “statement” attribute. That attribute contains the finalized SQL statement. Notice that the flights table name has been replace with __THIS__. This allows the pipeline to accept different table names as its source, making the pipeline very modular.\n\nft_dplyr_transformer(sc, df) %&gt;%\n  ml_param(\"statement\")\n#&gt; [1] \"SELECT\\n  `dep_delay`,\\n  `sched_dep_time`,\\n  CONCAT(\\\"m\\\", `month`) AS `month`,\\n  CONCAT(\\\"d\\\", `day`) AS `day`,\\n  `distance`\\nFROM `__THIS__`\\nWHERE (NOT((`dep_delay` IS NULL)))\"\n\n\n\nCreating the Pipeline\nThe following step will create a 5 stage pipeline:\n\nSQL transformer - Resulting from the ft_dplyr_transformer() transformation\nBinarizer - To determine if the flight should be considered delay. The eventual outcome variable.\nBucketizer - To split the day into specific hour buckets\nR Formula - To define the model’s formula\nLogistic Model\n\n\nflights_pipeline &lt;- ml_pipeline(sc) %&gt;%\n  ft_dplyr_transformer(\n    tbl = df\n    ) %&gt;%\n  ft_binarizer(\n    input_col = \"dep_delay\",\n    output_col = \"delayed\",\n    threshold = 15\n  ) %&gt;%\n  ft_bucketizer(\n    input_col = \"sched_dep_time\",\n    output_col = \"hours\",\n    splits = c(400, 800, 1200, 1600, 2000, 2400)\n  )  %&gt;%\n  ft_r_formula(delayed ~ month + day + hours + distance) %&gt;% \n  ml_logistic_regression()\n\nAnother nice feature for ML Pipelines in sparklyr, is the print-out. It makes it really easy to how each stage is setup:\n\nflights_pipeline\n#&gt; Pipeline (Estimator) with 5 stages\n#&gt; &lt;pipeline__874aaafe_ea0f_4ba3_a163_156a88ec9b52&gt; \n#&gt;   Stages \n#&gt;   |--1 SQLTransformer (Transformer)\n#&gt;   |    &lt;dplyr_transformer__3190d26f_ab76_449c_8440_81c53aa0fb63&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |--2 Binarizer (Transformer)\n#&gt;   |    &lt;binarizer__1c70c2db_cdfb_44a4_a02c_115f01d81f9d&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      input_col: dep_delay\n#&gt;   |      output_col: delayed\n#&gt;   |--3 Bucketizer (Transformer)\n#&gt;   |    &lt;bucketizer__3b5948a2_c7e4_480b_a835_deaa503bf84f&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      input_col: sched_dep_time\n#&gt;   |      output_col: hours\n#&gt;   |--4 RFormula (Estimator)\n#&gt;   |    &lt;r_formula__48d64927_5c6e_47d1_950f_7b6a8bf7ab22&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      features_col: features\n#&gt;   |      label_col: label\n#&gt;   |     (Parameters)\n#&gt;   |      force_index_label: FALSE\n#&gt;   |      formula: delayed ~ month + day + hours + distance\n#&gt;   |      handle_invalid: error\n#&gt;   |      stringIndexerOrderType: frequencyDesc\n#&gt;   |--5 LogisticRegression (Estimator)\n#&gt;   |    &lt;logistic_regression__66f928cb_efdd_488d_9e77_dff0ca92a140&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      features_col: features\n#&gt;   |      label_col: label\n#&gt;   |      prediction_col: prediction\n#&gt;   |      probability_col: probability\n#&gt;   |      raw_prediction_col: rawPrediction\n#&gt;   |     (Parameters)\n#&gt;   |      aggregation_depth: 2\n#&gt;   |      elastic_net_param: 0\n#&gt;   |      family: auto\n#&gt;   |      fit_intercept: TRUE\n#&gt;   |      max_iter: 100\n#&gt;   |      reg_param: 0\n#&gt;   |      standardization: TRUE\n#&gt;   |      threshold: 0.5\n#&gt;   |      tol: 1e-06\n\nNotice that there are no coefficients defined yet. That’s because no data has been actually processed. Even though df uses spark_flights(), recall that the final SQL transformer makes that name, so there’s no data to process yet.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "ML Pipelines"
    ]
  },
  {
    "objectID": "guides/pipelines.html#pipelinemodel",
    "href": "guides/pipelines.html#pipelinemodel",
    "title": "Spark ML Pipelines",
    "section": "PipelineModel",
    "text": "PipelineModel\nA quick partition of the data is created for this exercise.\n\npartitioned_flights &lt;- sdf_random_split(\n  spark_flights,\n  training = 0.01,\n  testing = 0.01,\n  rest = 0.98\n)\n\nThe ml_fit() function produces the PipelineModel. The training partition of the partitioned_flights data is used to train the model:\n\nfitted_pipeline &lt;- ml_fit(\n  flights_pipeline,\n  partitioned_flights$training\n)\nfitted_pipeline\n#&gt; PipelineModel (Transformer) with 5 stages\n#&gt; &lt;pipeline__874aaafe_ea0f_4ba3_a163_156a88ec9b52&gt; \n#&gt;   Stages \n#&gt;   |--1 SQLTransformer (Transformer)\n#&gt;   |    &lt;dplyr_transformer__3190d26f_ab76_449c_8440_81c53aa0fb63&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |--2 Binarizer (Transformer)\n#&gt;   |    &lt;binarizer__1c70c2db_cdfb_44a4_a02c_115f01d81f9d&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      input_col: dep_delay\n#&gt;   |      output_col: delayed\n#&gt;   |--3 Bucketizer (Transformer)\n#&gt;   |    &lt;bucketizer__3b5948a2_c7e4_480b_a835_deaa503bf84f&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      input_col: sched_dep_time\n#&gt;   |      output_col: hours\n#&gt;   |--4 RFormulaModel (Transformer)\n#&gt;   |    &lt;r_formula__48d64927_5c6e_47d1_950f_7b6a8bf7ab22&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      features_col: features\n#&gt;   |      label_col: label\n#&gt;   |     (Transformer Info)\n#&gt;   |      formula:  chr \"delayed ~ month + day + hours + distance\" \n#&gt;   |--5 LogisticRegressionModel (Transformer)\n#&gt;   |    &lt;logistic_regression__66f928cb_efdd_488d_9e77_dff0ca92a140&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      features_col: features\n#&gt;   |      label_col: label\n#&gt;   |      prediction_col: prediction\n#&gt;   |      probability_col: probability\n#&gt;   |      raw_prediction_col: rawPrediction\n#&gt;   |     (Transformer Info)\n#&gt;   |      coefficient_matrix:  num [1, 1:43] 0.6214 0.0372 -0.2004 0.4301 0.6641 ... \n#&gt;   |      coefficients:  num [1:43] 0.6214 0.0372 -0.2004 0.4301 0.6641 ... \n#&gt;   |      intercept:  num -3.22 \n#&gt;   |      intercept_vector:  num -3.22 \n#&gt;   |      num_classes:  int 2 \n#&gt;   |      num_features:  int 43 \n#&gt;   |      threshold:  num 0.5 \n#&gt;   |      thresholds:  num [1:2] 0.5 0.5\n\nNotice that the print-out for the fitted pipeline now displays the model’s coefficients.\nThe ml_transform() function can be used to run predictions, in other words it is used instead of predict() or sdf_predict().\n\npredictions &lt;- ml_transform(\n  fitted_pipeline,\n  partitioned_flights$testing\n)\n\npredictions %&gt;%\n  count(delayed, prediction) \n#&gt; # Source: spark&lt;?&gt; [?? x 3]\n#&gt; # Groups: delayed\n#&gt;   delayed prediction     n\n#&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1       0          1    57\n#&gt; 2       0          0  2667\n#&gt; 3       1          0   632\n#&gt; 4       1          1    56",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "ML Pipelines"
    ]
  },
  {
    "objectID": "guides/pipelines.html#save-the-pipelines-to-disk",
    "href": "guides/pipelines.html#save-the-pipelines-to-disk",
    "title": "Spark ML Pipelines",
    "section": "Save the pipelines to disk",
    "text": "Save the pipelines to disk\nThe ml_save() command can be used to save the Pipeline and PipelineModel to disk. The resulting output is a folder with the selected name, which contains all of the necessary Scala scripts:\n\n\nml_save(\n  flights_pipeline,\n  \"flights_pipeline\",\n  overwrite = TRUE\n)\n#&gt; Model successfully saved.\n\nml_save(\n  fitted_pipeline,\n  \"flights_model\",\n  overwrite = TRUE\n)\n#&gt; Model successfully saved.",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "ML Pipelines"
    ]
  },
  {
    "objectID": "guides/pipelines.html#use-an-existing-pipelinemodel",
    "href": "guides/pipelines.html#use-an-existing-pipelinemodel",
    "title": "Spark ML Pipelines",
    "section": "Use an existing PipelineModel",
    "text": "Use an existing PipelineModel\nThe ml_load() command can be used to re-load Pipelines and PipelineModels. The saved ML Pipeline files can only be loaded into an open Spark session.\n\nreloaded_model &lt;- ml_load(sc, \"flights_model\")\n\nA simple query can be used as the table that will be used to make the new predictions. This of course, does not have to done in R, at this time the “flights_model” can be loaded into an independent Spark session outside of R.\n\nnew_df &lt;- spark_flights %&gt;%\n  filter(\n    month == 7,\n    day == 5\n  )\n\nml_transform(reloaded_model, new_df)\n#&gt; # Source: spark&lt;?&gt; [?? x 12]\n#&gt;    dep_d…¹ sched…² month day   dista…³ delayed hours featu…⁴\n#&gt;      &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; \n#&gt;  1      39    2359 m7    d5       1617       1     4 &lt;dbl&gt;  \n#&gt;  2     141    2245 m7    d5       2475       1     4 &lt;dbl&gt;  \n#&gt;  3       0     500 m7    d5        529       0     0 &lt;dbl&gt;  \n#&gt;  4      -5     536 m7    d5       1400       0     0 &lt;dbl&gt;  \n#&gt;  5      -2     540 m7    d5       1089       0     0 &lt;dbl&gt;  \n#&gt;  6      -7     545 m7    d5       1416       0     0 &lt;dbl&gt;  \n#&gt;  7      -3     545 m7    d5       1576       0     0 &lt;dbl&gt;  \n#&gt;  8      -7     600 m7    d5       1076       0     0 &lt;dbl&gt;  \n#&gt;  9      -7     600 m7    d5         96       0     0 &lt;dbl&gt;  \n#&gt; 10      -6     600 m7    d5        937       0     0 &lt;dbl&gt;  \n#&gt; # … with more rows, 4 more variables: label &lt;dbl&gt;,\n#&gt; #   rawPrediction &lt;list&gt;, probability &lt;list&gt;,\n#&gt; #   prediction &lt;dbl&gt;, and abbreviated variable names\n#&gt; #   ¹​dep_delay, ²​sched_dep_time, ³​distance, ⁴​features",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "ML Pipelines"
    ]
  },
  {
    "objectID": "guides/pipelines.html#re-fit-an-existing-pipeline",
    "href": "guides/pipelines.html#re-fit-an-existing-pipeline",
    "title": "Spark ML Pipelines",
    "section": "Re-fit an existing Pipeline",
    "text": "Re-fit an existing Pipeline\nFirst, reload the pipeline into an open Spark session:\n\nreloaded_pipeline &lt;- ml_load(sc, \"flights_pipeline\")\n\nUse ml_fit() again to pass new data, in this case, sample_frac() is used instead of sdf_partition() to provide the new data. The idea being that the re-fitting would happen at a later date than when the model was initially fitted.\n\nnew_model &lt;-  ml_fit(reloaded_pipeline, sample_frac(spark_flights, 0.01))\n\nnew_model\n#&gt; PipelineModel (Transformer) with 5 stages\n#&gt; &lt;pipeline__874aaafe_ea0f_4ba3_a163_156a88ec9b52&gt; \n#&gt;   Stages \n#&gt;   |--1 SQLTransformer (Transformer)\n#&gt;   |    &lt;dplyr_transformer__3190d26f_ab76_449c_8440_81c53aa0fb63&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |--2 Binarizer (Transformer)\n#&gt;   |    &lt;binarizer__1c70c2db_cdfb_44a4_a02c_115f01d81f9d&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      input_col: dep_delay\n#&gt;   |      output_col: delayed\n#&gt;   |--3 Bucketizer (Transformer)\n#&gt;   |    &lt;bucketizer__3b5948a2_c7e4_480b_a835_deaa503bf84f&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      input_col: sched_dep_time\n#&gt;   |      output_col: hours\n#&gt;   |--4 RFormulaModel (Transformer)\n#&gt;   |    &lt;r_formula__48d64927_5c6e_47d1_950f_7b6a8bf7ab22&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      features_col: features\n#&gt;   |      label_col: label\n#&gt;   |     (Transformer Info)\n#&gt;   |      formula:  chr \"delayed ~ month + day + hours + distance\" \n#&gt;   |--5 LogisticRegressionModel (Transformer)\n#&gt;   |    &lt;logistic_regression__66f928cb_efdd_488d_9e77_dff0ca92a140&gt; \n#&gt;   |     (Parameters -- Column Names)\n#&gt;   |      features_col: features\n#&gt;   |      label_col: label\n#&gt;   |      prediction_col: prediction\n#&gt;   |      probability_col: probability\n#&gt;   |      raw_prediction_col: rawPrediction\n#&gt;   |     (Transformer Info)\n#&gt;   |      coefficient_matrix:  num [1, 1:43] 0.5949 0.2643 0.3657 -0.0494 -0.4228 ... \n#&gt;   |      coefficients:  num [1:43] 0.5949 0.2643 0.3657 -0.0494 -0.4228 ... \n#&gt;   |      intercept:  num -2.89 \n#&gt;   |      intercept_vector:  num -2.89 \n#&gt;   |      num_classes:  int 2 \n#&gt;   |      num_features:  int 43 \n#&gt;   |      threshold:  num 0.5 \n#&gt;   |      thresholds:  num [1:2] 0.5 0.5\n\nThe new model can be saved using ml_save(). A new name is used in this case, but the same name as the existing PipelineModel to replace it.\n\nml_save(new_model, \"new_flights_model\", overwrite = TRUE)\n#&gt; Model successfully saved.\n\nFinally, this example is complete by closing the Spark session.\n\nspark_disconnect(sc)",
    "crumbs": [
      "Guides",
      "Machine Learning",
      "ML Pipelines"
    ]
  },
  {
    "objectID": "guides/troubleshooting.html",
    "href": "guides/troubleshooting.html",
    "title": "Troubleshooting",
    "section": "",
    "text": "For general programming questions with sparklyr, please ask on Stack Overflow.",
    "crumbs": [
      "Guides",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "guides/troubleshooting.html#help-with-code-debugging",
    "href": "guides/troubleshooting.html#help-with-code-debugging",
    "title": "Troubleshooting",
    "section": "",
    "text": "For general programming questions with sparklyr, please ask on Stack Overflow.",
    "crumbs": [
      "Guides",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "guides/troubleshooting.html#code-does-not-work-after-upgrading-to-the-latest-sparklyr-version",
    "href": "guides/troubleshooting.html#code-does-not-work-after-upgrading-to-the-latest-sparklyr-version",
    "title": "Troubleshooting",
    "section": "Code does not work after upgrading to the latest sparklyr version",
    "text": "Code does not work after upgrading to the latest sparklyr version\nPlease refer to the NEWS section of the sparklyr package to find out if any of the updates listed may have changed the way your code needs to work.\nIf it seems that current version of the package has a bug, or the new functionality does not perform as stated, please refer to the sparklyr ISSUES page. If no existing issue matches to what your problem is, please open a new issue.",
    "crumbs": [
      "Guides",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "guides/troubleshooting.html#not-able-to-connect-or-the-jobs-take-a-long-time-when-working-with-a-data-lake",
    "href": "guides/troubleshooting.html#not-able-to-connect-or-the-jobs-take-a-long-time-when-working-with-a-data-lake",
    "title": "Troubleshooting",
    "section": "Not able to connect, or the jobs take a long time when working with a Data Lake",
    "text": "Not able to connect, or the jobs take a long time when working with a Data Lake\nThe Configuration connections contains an overview and recommendations for requesting resources form the cluster.\nThe articles in the Guides section provide best-practice information about specific operations that may match to the intent of your code.\nTo verify your infrastructure, please review the Deployment Examples section.",
    "crumbs": [
      "Guides",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "guides/aws-s3.html",
    "href": "guides/aws-s3.html",
    "title": "Using Spark with AWS S3 buckets",
    "section": "",
    "text": "AWS Access Keys are needed to access S3 data. To learn how to setup a new keys, please review the AWS documentation: http://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html .We then pass the keys to R via Environment Variables:\nSys.setenv(AWS_ACCESS_KEY_ID=\"[Your access key]\")\nSys.setenv(AWS_SECRET_ACCESS_KEY=\"[Your secret access key]\")",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Access S3 buckets"
    ]
  },
  {
    "objectID": "guides/aws-s3.html#aws-access-keys",
    "href": "guides/aws-s3.html#aws-access-keys",
    "title": "Using Spark with AWS S3 buckets",
    "section": "",
    "text": "AWS Access Keys are needed to access S3 data. To learn how to setup a new keys, please review the AWS documentation: http://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html .We then pass the keys to R via Environment Variables:\nSys.setenv(AWS_ACCESS_KEY_ID=\"[Your access key]\")\nSys.setenv(AWS_SECRET_ACCESS_KEY=\"[Your secret access key]\")",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Access S3 buckets"
    ]
  },
  {
    "objectID": "guides/aws-s3.html#connecting-to-spark",
    "href": "guides/aws-s3.html#connecting-to-spark",
    "title": "Using Spark with AWS S3 buckets",
    "section": "Connecting to Spark",
    "text": "Connecting to Spark\nThere are four key settings needed to connect to Spark and use S3:\n\nA Hadoop-AWS package\nExecutor memory (key but not critical)\nThe master URL\nThe Spark Home\n\n\nHadoop-AWS package:\nA Spark connection can be enhanced by using packages, please note that these are not R packages. For example, there are packages that tells Spark how to read CSV files, Hadoop or Hadoop in AWS.\nIn order to read S3 buckets, our Spark connection will need a package called hadoop-aws. If needed, multiple packages can be used. We experimented with many combinations of packages, and determined that for reading data in S3 we only need one. The version we used, 3.3.1, refers to the latest Hadoop version, so as this article ages, please visit this site to make sure that you are using the latest version: Hadoop AWS Maven Repository\n\nlibrary(sparklyr)\n\nconf &lt;- spark_config()\n\nconf$sparklyr.defaultPackages &lt;- \"org.apache.hadoop:hadoop-aws:3.3.1\"\n\nsc &lt;- spark_connect(maste = \"local\", config = conf)",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Access S3 buckets"
    ]
  },
  {
    "objectID": "guides/aws-s3.html#data-importwrangle-approach",
    "href": "guides/aws-s3.html#data-importwrangle-approach",
    "title": "Using Spark with AWS S3 buckets",
    "section": "Data Import/Wrangle approach",
    "text": "Data Import/Wrangle approach\nWe experimented with multiple approaches. Most of the factors for settling on a recommended approach were made based on the speed of each step. The premise is that we would rather wait longer during Data Import, if it meant that we can much faster Register and Cache our data subsets during Data Wrangling, especially since we would expect to end up with many subsets as we explore and model.The selected combination was the second slowest during the Import stage, but the fastest when caching a subset, by a lot.\nIn our original tests, it took 72 seconds to read and cache the 29 columns of the 41 million rows of data, the slowest was 77 seconds. But when it comes to registering and caching a considerably sizable subset of 3 columns and almost all of the 41 million records, this approach was 17X faster than the second fastest approach. It took 1/3 of a second to register and cache the subset, the second fastest was 5 seconds.\nTo implement this approach, we need to set three arguments in the spark_csv_read() step:\n\nmemory\ninfer_schema\ncolumns\n\nAgain, this is a recommended approach. The columns argument is needed only if infer_schema is set to FALSE. When memory is set to TRUE it makes Spark load the entire dataset into memory, and setting infer_schema to FALSE prevents Spark from trying to figure out what the schema of the files are. By trying different combinations the memory and infer_schema arguments you may be able to find an approach that may better fits your needs.\n\nReading the schema\nSurprisingly, another critical detail that can easily be overlooked is choosing the right s3 URI scheme. There are two options: s3n and s3a. In most examples and tutorials I found, there was no reason give of why or when to use which one. The article the finally clarified it was this one: https://wiki.apache.org/hadoop/AmazonS3\nThe gist of it is that s3a is the recommended one going forward, especially for Hadoop versions 2.7 and above. This means that if we copy from older examples that used Hadoop 2.6 we would more likely also used s3n thus making data import much, much slower.",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Access S3 buckets"
    ]
  },
  {
    "objectID": "guides/aws-s3.html#data-import",
    "href": "guides/aws-s3.html#data-import",
    "title": "Using Spark with AWS S3 buckets",
    "section": "Data Import",
    "text": "Data Import\nAfter the long introduction in the previous section, there is only one point to add about the following code chunk. If there are any NA values in numeric fields, then define the column as character and then convert it on later subsets using dplyr. The data import will fail if it finds any NA values on numeric fields. This is a small trade off in this approach because the next fastest one does not have this issue but is 17X slower at caching subsets.\n\nflights &lt;- spark_read_csv(sc, \"flights_spark\", \n                          path =  \"s3a://flights-data/full\", \n                          memory = TRUE, \n                          columns = list(\n                            Year = \"character\",\n                            Month = \"character\",\n                            DayofMonth = \"character\",\n                            DayOfWeek = \"character\",\n                            DepTime = \"character\",\n                            CRSDepTime = \"character\",\n                            ArrTime = \"character\",\n                            CRSArrTime = \"character\",\n                            UniqueCarrier = \"character\",\n                            FlightNum = \"character\",\n                            TailNum = \"character\",\n                            ActualElapsedTime = \"character\",\n                            CRSElapsedTime = \"character\",\n                            AirTime = \"character\",\n                            ArrDelay = \"character\",\n                            DepDelay = \"character\",\n                            Origin = \"character\",\n                            Dest = \"character\",\n                            Distance = \"character\",\n                            TaxiIn = \"character\",\n                            TaxiOut = \"character\",\n                            Cancelled = \"character\",\n                            CancellationCode = \"character\",\n                            Diverted = \"character\",\n                            CarrierDelay = \"character\",\n                            WeatherDelay = \"character\",\n                            NASDelay = \"character\",\n                            SecurityDelay = \"character\",\n                            LateAircraftDelay = \"character\"), \n                         infer_schema = FALSE)",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Access S3 buckets"
    ]
  },
  {
    "objectID": "guides/aws-s3.html#data-wrangle",
    "href": "guides/aws-s3.html#data-wrangle",
    "title": "Using Spark with AWS S3 buckets",
    "section": "Data Wrangle",
    "text": "Data Wrangle\nThere are a few points we need to highlight about the following simple dyplr code:\nBecause there were NAs in the original fields, we have to mutate them to a number. Try coercing any variable as integer instead of numeric, this will save a lot of space when cached to Spark memory. The sdf_register command can be piped at the end of the code. After running the code, a new table will appear in the RStudio IDE’s Spark tab\ntidy_flights &lt;- tbl(sc, \"flights_spark\") %&gt;%\n  mutate(ArrDelay = as.integer(ArrDelay),\n         DepDelay = as.integer(DepDelay),\n         Distance = as.integer(Distance)) %&gt;%\n  filter(!is.na(ArrDelay)) %&gt;%\n  select(DepDelay, ArrDelay, Distance) %&gt;%\n  sdf_register(\"tidy_spark\")\nAfter we use tbl_cache() to load the tidy_spark table into Spark memory. We can see the new table in the Storage page of our Spark session.\ntbl_cache(sc, \"tidy_spark\")",
    "crumbs": [
      "Guides",
      "Interacting with Spark",
      "Access S3 buckets"
    ]
  },
  {
    "objectID": "deployment/cloudera-aws.html",
    "href": "deployment/cloudera-aws.html",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "",
    "text": "This document demonstrates how to use sparklyr with an Cloudera Hadoop & Spark cluster. Data are downloaded from the web and stored in Hive tables on HDFS across multiple worker nodes. RStudio Server is installed on the master node and orchestrates the analysis in spark.",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Cloudera cluster"
    ]
  },
  {
    "objectID": "deployment/cloudera-aws.html#cdh-5",
    "href": "deployment/cloudera-aws.html#cdh-5",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "CDH 5",
    "text": "CDH 5\nWe will start with a Cloudera cluster CDH version 5.8.2 (free version) with an underlaying Ubuntu Linux distribution.",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Cloudera cluster"
    ]
  },
  {
    "objectID": "deployment/cloudera-aws.html#spark-1.6",
    "href": "deployment/cloudera-aws.html#spark-1.6",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Spark 1.6",
    "text": "Spark 1.6\nThe default Spark 1.6.0 parcel is in installed and running",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Cloudera cluster"
    ]
  },
  {
    "objectID": "deployment/cloudera-aws.html#hive-data",
    "href": "deployment/cloudera-aws.html#hive-data",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Hive data",
    "text": "Hive data\nFor this demo, we have created and populated 3 tables in Hive. The table names are: flights, airlines and airports. Using Hue, we can see the loaded tables. For the links to the data files and their Hive import scripts please see Appendix A.",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Cloudera cluster"
    ]
  },
  {
    "objectID": "deployment/cloudera-aws.html#cache-the-tables-into-memory",
    "href": "deployment/cloudera-aws.html#cache-the-tables-into-memory",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Cache the tables into memory",
    "text": "Cache the tables into memory\nUse tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame.\n\n# Cache flights Hive table into Spark\ntbl_cache(sc, 'flights')\nflights_tbl &lt;- tbl(sc, 'flights')\n\n# Cache airlines Hive table into Spark\ntbl_cache(sc, 'airlines')\nairlines_tbl &lt;- tbl(sc, 'airlines')\n\n# Cache airports Hive table into Spark\ntbl_cache(sc, 'airports')\nairports_tbl &lt;- tbl(sc, 'airports')",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Cloudera cluster"
    ]
  },
  {
    "objectID": "deployment/cloudera-aws.html#create-a-model-data-set",
    "href": "deployment/cloudera-aws.html#create-a-model-data-set",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Create a model data set",
    "text": "Create a model data set\nFilter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight.\n\n# Filter records and create target variable 'gain'\nmodel_data &lt;- flights_tbl %&gt;%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %&gt;%\n  filter(depdelay &gt; 15 & depdelay &lt; 240) %&gt;%\n  filter(arrdelay &gt; -60 & arrdelay &lt; 360) %&gt;%\n  filter(year &gt;= 2003 & year &lt;= 2007) %&gt;%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %&gt;%\n  mutate(gain = depdelay - arrdelay) %&gt;%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain)\n\n# Summarize data by carrier\nmodel_data %&gt;%\n  group_by(uniquecarrier) %&gt;%\n  summarize(description = min(description), gain=mean(gain), \n            distance=mean(distance), depdelay=mean(depdelay)) %&gt;%\n  select(description, gain, distance, depdelay) %&gt;%\n  arrange(gain)\n\nSource:   query [?? x 4]\nDatabase: spark connection master=yarn-client app=sparklyr local=FALSE\n\n                    description       gain  distance depdelay\n                          &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1        ATA Airlines d/b/a ATA -5.5679651 1240.7219 61.84391\n2       Northwest Airlines Inc. -3.1134556  779.1926 48.84979\n3                     Envoy Air -2.2056576  437.0883 54.54923\n4             PSA Airlines Inc. -1.9267647  500.6955 55.60335\n5  ExpressJet Airlines Inc. (1) -1.5886314  537.3077 61.58386\n6               JetBlue Airways -1.3742524 1087.2337 59.80750\n7         SkyWest Airlines Inc. -1.1265678  419.6489 54.04198\n8          Delta Air Lines Inc. -0.9829374  956.9576 50.19338\n9        American Airlines Inc. -0.9631200 1066.8396 56.78222\n10  AirTran Airways Corporation -0.9411572  665.6574 53.38363\n# ... with more rows",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Cloudera cluster"
    ]
  },
  {
    "objectID": "deployment/cloudera-aws.html#train-a-linear-model",
    "href": "deployment/cloudera-aws.html#train-a-linear-model",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Train a linear model",
    "text": "Train a linear model\nPredict time gained or lost in flight as a function of distance, departure delay, and airline carrier.\n\n# Partition the data into training and validation sets\nmodel_partition &lt;- model_data %&gt;% \n  sdf_partition(train = 0.8, valid = 0.2, seed = 5555)\n\n# Fit a linear model\nml1 &lt;- model_partition$train %&gt;%\n  ml_linear_regression(gain ~ distance + depdelay + uniquecarrier)\n\n# Summarize the linear model\nsummary(ml1)\n\nCall: ml_linear_regression(., gain ~ distance + depdelay + uniquecarrier)\n\nDeviance Residuals: (approximate):\n     Min       1Q   Median       3Q      Max \n-302.343   -5.669    2.714    9.832  104.130 \n\nCoefficients:\n                    Estimate  Std. Error  t value  Pr(&gt;|t|)    \n(Intercept)      -1.26566581  0.10385870 -12.1864 &lt; 2.2e-16 ***\ndistance          0.00308711  0.00002404 128.4155 &lt; 2.2e-16 ***\ndepdelay         -0.01397013  0.00028816 -48.4812 &lt; 2.2e-16 ***\nuniquecarrier_AA -2.18483090  0.10985406 -19.8885 &lt; 2.2e-16 ***\nuniquecarrier_AQ  3.14330242  0.29114487  10.7964 &lt; 2.2e-16 ***\nuniquecarrier_AS  0.09210380  0.12825003   0.7182 0.4726598    \nuniquecarrier_B6 -2.66988794  0.12682192 -21.0523 &lt; 2.2e-16 ***\nuniquecarrier_CO -1.11611186  0.11795564  -9.4621 &lt; 2.2e-16 ***\nuniquecarrier_DL -1.95206198  0.11431110 -17.0767 &lt; 2.2e-16 ***\nuniquecarrier_EV  1.70420830  0.11337215  15.0320 &lt; 2.2e-16 ***\nuniquecarrier_F9 -1.03178176  0.15384863  -6.7065 1.994e-11 ***\nuniquecarrier_FL -0.99574060  0.12034738  -8.2739 2.220e-16 ***\nuniquecarrier_HA -1.16970713  0.34894788  -3.3521 0.0008020 ***\nuniquecarrier_MQ -1.55569040  0.10975613 -14.1741 &lt; 2.2e-16 ***\nuniquecarrier_NW -3.58502418  0.11534938 -31.0797 &lt; 2.2e-16 ***\nuniquecarrier_OH -1.40654797  0.12034858 -11.6873 &lt; 2.2e-16 ***\nuniquecarrier_OO -0.39069404  0.11132164  -3.5096 0.0004488 ***\nuniquecarrier_TZ -7.26285217  0.34428509 -21.0955 &lt; 2.2e-16 ***\nuniquecarrier_UA -0.56995737  0.11186757  -5.0949 3.489e-07 ***\nuniquecarrier_US -0.52000028  0.11218498  -4.6352 3.566e-06 ***\nuniquecarrier_WN  4.22838982  0.10629405  39.7801 &lt; 2.2e-16 ***\nuniquecarrier_XE -1.13836940  0.11332176 -10.0455 &lt; 2.2e-16 ***\nuniquecarrier_YV  3.17149538  0.11709253  27.0854 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nR-Squared: 0.02301\nRoot Mean Squared Error: 17.83",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Cloudera cluster"
    ]
  },
  {
    "objectID": "deployment/cloudera-aws.html#assess-model-performance",
    "href": "deployment/cloudera-aws.html#assess-model-performance",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Assess model performance",
    "text": "Assess model performance\nCompare the model performance using the validation data.\n\n# Calculate average gains by predicted decile\nmodel_deciles &lt;- lapply(model_partition, function(x) {\n  sdf_predict(ml1, x) %&gt;%\n    mutate(decile = ntile(desc(prediction), 10)) %&gt;%\n    group_by(decile) %&gt;%\n    summarize(gain = mean(gain)) %&gt;%\n    select(decile, gain) %&gt;%\n    collect()\n})\n\n# Create a summary dataset for plotting\ndeciles &lt;- rbind(\n  data.frame(data = 'train', model_deciles$train),\n  data.frame(data = 'valid', model_deciles$valid),\n  make.row.names = FALSE\n)\n\n# Plot average gains by predicted decile\ndeciles %&gt;%\n  ggplot(aes(factor(decile), gain, fill = data)) +\n  geom_bar(stat = 'identity', position = 'dodge') +\n  labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes')",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Cloudera cluster"
    ]
  },
  {
    "objectID": "deployment/cloudera-aws.html#visualize-predictions",
    "href": "deployment/cloudera-aws.html#visualize-predictions",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Visualize predictions",
    "text": "Visualize predictions\nCompare actual gains to predicted gains for an out of time sample.\n\n# Select data from an out of time sample\ndata_2008 &lt;- flights_tbl %&gt;%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %&gt;%\n  filter(depdelay &gt; 15 & depdelay &lt; 240) %&gt;%\n  filter(arrdelay &gt; -60 & arrdelay &lt; 360) %&gt;%\n  filter(year == 2008) %&gt;%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %&gt;%\n  mutate(gain = depdelay - arrdelay) %&gt;%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest)\n\n# Summarize data by carrier\ncarrier &lt;- sdf_predict(ml1, data_2008) %&gt;%\n  group_by(description) %&gt;%\n  summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %&gt;%\n  filter(freq &gt; 10000) %&gt;%\n  collect\n\n# Plot actual gains and predicted gains by airline carrier\nggplot(carrier, aes(gain, prediction)) + \n  geom_point(alpha = 0.75, color = 'red', shape = 3) +\n  geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') +\n  geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) +\n  labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted')\n\n\nSome carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights.",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Cloudera cluster"
    ]
  },
  {
    "objectID": "deployment/cloudera-aws.html#build-dashboard",
    "href": "deployment/cloudera-aws.html#build-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Build dashboard",
    "text": "Build dashboard\nAggregate the scored data by origin, destination, and airline. Save the aggregated data.\n\n# Summarize by origin, destination, and carrier\nsummary_2008 &lt;- sdf_predict(ml1, data_2008) %&gt;%\n  rename(carrier = uniquecarrier, airline = description) %&gt;%\n  group_by(origin, dest, carrier, airline) %&gt;%\n  summarize(\n    flights = n(),\n    distance = mean(distance),\n    avg_dep_delay = mean(depdelay),\n    avg_arr_delay = mean(arrdelay),\n    avg_gain = mean(gain),\n    pred_gain = mean(prediction)\n    )\n\n# Collect and save objects\npred_data &lt;- collect(summary_2008)\nairports &lt;- collect(select(airports_tbl, name, faa, lat, lon))\nml1_summary &lt;- capture.output(summary(ml1))\nsave(pred_data, airports, ml1_summary, file = 'flights_pred_2008.RData')",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Cloudera cluster"
    ]
  },
  {
    "objectID": "deployment/cloudera-aws.html#publish-dashboard",
    "href": "deployment/cloudera-aws.html#publish-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Publish dashboard",
    "text": "Publish dashboard\nUse the saved data to build an R Markdown flexdashboard. Publish the flexdashboard\n\n#Appendix\n\nAppendix A - Data files\nRun the following script to download data from the web onto your master node. Download the yearly flight data and the airlines lookup table.\n\n# Make download directory\nmkdir /tmp/flights\n\n# Download flight data by year\nfor i in {2006..2008}\n  do\n    echo \"$(date) $i Download\"\n    fnam=$i.csv.bz2\n    wget -O /tmp/flights/$fnam http://stat-computing.org/dataexpo/2009/$fnam\n    echo \"$(date) $i Unzip\"\n    bunzip2 /tmp/flights/$fnam\n  done\n\n# Download airline carrier data\nwget -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS\n\n# Download airports data\nwget -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\n\n\n\nHive tables\nWe used the Hue interface, logged in as ‘admin’ to load the data into HDFS and then into Hive.\nCREATE EXTERNAL TABLE IF NOT EXISTS flights\n(\nyear int,\nmonth int,\ndayofmonth int,\ndayofweek int,\ndeptime int,\ncrsdeptime int,\narrtime int, \ncrsarrtime int,\nuniquecarrier string,\nflightnum int,\ntailnum string, \nactualelapsedtime int,\ncrselapsedtime int,\nairtime string,\narrdelay int,\ndepdelay int, \norigin string,\ndest string,\ndistance int,\ntaxiin string,\ntaxiout string,\ncancelled int,\ncancellationcode string,\ndiverted int,\ncarrierdelay string,\nweatherdelay string,\nnasdelay string,\nsecuritydelay string,\nlateaircraftdelay string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLINES TERMINATED BY '\\n'\nTBLPROPERTIES(\"skip.header.line.count\"=\"1\");\nLOAD DATA INPATH '/user/admin/flights/2006.csv/' INTO TABLE flights;\nLOAD DATA INPATH '/user/admin/flights/2007.csv/' INTO TABLE flights;\nLOAD DATA INPATH '/user/admin/flights/2008.csv/' INTO TABLE flights;\n# Create metadata for airlines\nCREATE EXTERNAL TABLE IF NOT EXISTS airlines\n(\nCode string,\nDescription string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE\ntblproperties(\"skip.header.line.count\"=\"1\");\nLOAD DATA INPATH '/user/admin/L_UNIQUE_CARRIERS.csv' INTO TABLE airlines;\nCREATE EXTERNAL TABLE IF NOT EXISTS airports\n(\nid string,\nname string,\ncity string,\ncountry string,\nfaa string,\nicao string,\nlat double,\nlon double,\nalt int,\ntz_offset double,\ndst string,\ntz_name string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE;\nLOAD DATA INPATH '/user/admin/airports.dat' INTO TABLE airports;",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "Cloudera cluster"
    ]
  },
  {
    "objectID": "deployment/qubole-overview.html",
    "href": "deployment/qubole-overview.html",
    "title": "Using sparklyr with Qubole",
    "section": "",
    "text": "This documentation demonstrates how to use sparklyr with Apache Spark in Qubole along with RStudio Server Pro and RStudio Connect.",
    "crumbs": [
      "Deployment",
      "Stand Alone Clusters",
      "Qubole cluster"
    ]
  },
  {
    "objectID": "deployment/qubole-overview.html#overview",
    "href": "deployment/qubole-overview.html#overview",
    "title": "Using sparklyr with Qubole",
    "section": "",
    "text": "This documentation demonstrates how to use sparklyr with Apache Spark in Qubole along with RStudio Server Pro and RStudio Connect.",
    "crumbs": [
      "Deployment",
      "Stand Alone Clusters",
      "Qubole cluster"
    ]
  },
  {
    "objectID": "deployment/qubole-overview.html#best-practices-for-working-with-qubole",
    "href": "deployment/qubole-overview.html#best-practices-for-working-with-qubole",
    "title": "Using sparklyr with Qubole",
    "section": "Best practices for working with Qubole",
    "text": "Best practices for working with Qubole\n\nManage packages via Qubole Environments - Packages installed via install.packages() are not available on cluster restart. Packages managed through Qubole Environments are persistent.\nRestrict workloads to interactive analysis - Only perform workloads related to exploratory or interactive analysis with Spark, then write the results to a database, file system, or cloud storage for more efficient retrieval in apps, reports, and APIs.\nLoad and query results efficiently - Because of the nature of Spark computations and the associated overhead, Shiny apps that use Spark on the backend tend to have performance and runtime issues; consider reading the results from a database, file system, or cloud storage instead.",
    "crumbs": [
      "Deployment",
      "Stand Alone Clusters",
      "Qubole cluster"
    ]
  },
  {
    "objectID": "deployment/qubole-overview.html#using-rstudio-workbench-with-qubole",
    "href": "deployment/qubole-overview.html#using-rstudio-workbench-with-qubole",
    "title": "Using sparklyr with Qubole",
    "section": "Using RStudio Workbench with Qubole",
    "text": "Using RStudio Workbench with Qubole\nThe Qubole platform includes RStudio Workbench. More details about how to request RStudio Workbench and access it from within a Qubole cluster are available from Qubole.\nTo steps for running RStudio Workbench inside Qubole go to the follow article: Qubole cluster",
    "crumbs": [
      "Deployment",
      "Stand Alone Clusters",
      "Qubole cluster"
    ]
  },
  {
    "objectID": "deployment/qubole-overview.html#using-rstudio-connect-with-qubole",
    "href": "deployment/qubole-overview.html#using-rstudio-connect-with-qubole",
    "title": "Using sparklyr with Qubole",
    "section": "Using RStudio Connect with Qubole",
    "text": "Using RStudio Connect with Qubole\nThe best configuration for working with Qubole and RStudio Connect is to install RStudio Connect outside of the Qubole cluster and connect to Qubole remotely. This is accomplished using the Qubole ODBC Driver.",
    "crumbs": [
      "Deployment",
      "Stand Alone Clusters",
      "Qubole cluster"
    ]
  },
  {
    "objectID": "deployment/databricks-posit-connect.html",
    "href": "deployment/databricks-posit-connect.html",
    "title": "Deploying to Posit Connect",
    "section": "",
    "text": "We recommend that for simply accessing data from the Unity Catalog, such as in a Shiny app, use an ODBC connection instead of a Spark session. The advantage of this is that a connection to the Databricks Warehouse does not require a running cluster. For more information about creating dashboards with databases visit Database with R site.\nHowever, there are cases when it is necessary to deploy a solution that requires a Spark session. For example, when there is a long running job that needs to run on a schedule. Those kinds of jobs could be put inside a Quarto document and published to Posit Connect, where they can run on specific date/time intervals. Posit Connect supports Python environments, so it is an ideal platform to deploy these kinds of solutions.",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Deploying to Posit Connect"
    ]
  },
  {
    "objectID": "deployment/databricks-posit-connect.html#preparing-for-deployment",
    "href": "deployment/databricks-posit-connect.html#preparing-for-deployment",
    "title": "Deploying to Posit Connect",
    "section": "Preparing for deployment",
    "text": "Preparing for deployment\nWhen deploying to Posit Connect, there are specific pieces of information that we need to make sure are sent over:\n\nYour cluster’s ID\nYour workspace URL\nYour token\nYour Python environment\nTo use the pysparklyr extension\n\nBased on the recommendations in this article, the cluster’s ID should be in the code of your document, the workspace URL and token should already be inside the DATABRICKS_HOST, and DATABRICKS_TOKEN environment variables. If you are using something other than these environment variables to authenticate, then see the section Alternatives to Environment Variables.\nMake sure that your document has a library(pysparklyr) call. This will let Posit Connect deployment process know that it needs to install this package and its dependencies, such as reticulate.\nThe next section will introduce a function that will ease finding the Python environment.\nlibrary(sparklyr)\nlibrary(pysparklyr)\n\nsc &lt;- spark_connect(\n    cluster_id = \"1026-175310-7cpsh3g8\",\n    method = \"databricks_connect\"\n)",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Deploying to Posit Connect"
    ]
  },
  {
    "objectID": "deployment/databricks-posit-connect.html#using-deploy_databricks",
    "href": "deployment/databricks-posit-connect.html#using-deploy_databricks",
    "title": "Deploying to Posit Connect",
    "section": "Using deploy_databricks()",
    "text": "Using deploy_databricks()\nThe deploy_databricks() function makes it easier to deploy your content to Posit Connect. It does its best to gather all of the pieces of information mentioned above and builds the correct command to publish.\nThe path to your content is the last piece of information we need. Ideally your content is either located in its own folder inside your project or it is at the root level of your project. There are three ways that you can let deploy_databricks() know the path to use:\n\nIf you are in RStudio and the document you wish to publish is open, deploy_databricks() will use the RStudio API to get the path of that document and then use its containing folder as the content’s location. This is the preferred method for deployment.\nUse the appDir argument to pass the path to be used. Something such as here::here(\"my-cool-document\") would work.\nIf no document is opened in RStudio and appDir is left empty, then getwd() will be used\n\nThe aim of the new function is to be both flexible and helpful. It will gather your document location, credentials, and URLs. The Python location needs to defined when calling deploy_databricks(). If you are in RStudio, and your document is opened, here are several ways to do this:\n\nIf you know the cluster’s DBR version:\npysparklyr::deploy_databricks(version = \"14.1\")\nThe cluster’s ID can also be used and pysparklyr will automatically determine the required DBR version:\npysparklyr::deploy_databricks(cluster_id = \"1026-175310-7cpsh3g8\")\nIf you just ran the code of the content you plan to deploy, then the Python environment will already be loaded in the R session. deploy_databricks() will validate that the path of that Python environment conforms to one that pysparklyr creates and use that. This will happen if no version, cluster_id, or python argument is provided. At that point simply run:\npysparklyr::deploy_databricks()\nYou can also pass the path to the Python environment to use by setting the python argument:\npysparklyr::deploy_databricks(python = \"/Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1\")\nYou can use DATABRICKS_CLUSTER_ID environment variable. If you have it set, simply run:\npysparklyr::deploy_databricks()\n\nHere is an example of the output returned when using deploy_databricks(). Before submitting your content, you will be prompted to confirm that the information gathered is correct. Also notice that if you have more than one Posit Connect server setup in your RStudio IDE, it will choose the top one as the default but allow you to easily change it if necessary:\n&gt; pysparklyr::deploy_databricks(version = \"14.1\")\n\n── Starting deployment ──────────────────────────────────────────────────────────────────────────────────────\nℹ Source directory: /Users/edgar/r_projects/practice/test-deploy\nℹ Python: /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python\nℹ Posit server: colorado.posit.co\n  Account name: edgar\nℹ Host URL: rstudio-partner-posit-default.cloud.databricks.com\n  Token: '&lt;REDACTED&gt;'\n\nDoes everything look correct?\n\n1: Yes\n2: No\n3: Change 'Posit server'\n\nSelection: 1\nThe first time that you publish, the function will check to see if you have a requirements.txt file. If you do not have the file, it will ask you if you wish to create it. The requirements.txt file contains the list of the your current Python environment, as well as its versions. This will help when you re-publish your content, because you will not need to pass the version.\nWould you like to create the 'requirements.txt' file?\nWhy consider? This will allow you to skip using `version` or `cluster_id`\n\n1: Yes\n2: No\nIf you select No, deploy_databricks() will not ask you again when you re-deploy.",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Deploying to Posit Connect"
    ]
  },
  {
    "objectID": "deployment/databricks-posit-connect.html#alternatives-to-environment-variables",
    "href": "deployment/databricks-posit-connect.html#alternatives-to-environment-variables",
    "title": "Deploying to Posit Connect",
    "section": "Alternatives to Environment Variables",
    "text": "Alternatives to Environment Variables\nThe deploy_databricks() function has a host, and token arguments. These arguments take precedent over the environment variables, if set.\nThere are a variety of reasons for you to set these arguments when publishing. For example, locally you authenticate with a Databricks configuration file, but when deploying, you will need to let deploy_databricks() what values to use for the PAT and Host URL. Another example could be that your deployed content may need to use a service account, that differs from the credentials you use when developing.\nAs usual, we recommend that you avoid using open text values with your credentials in your code. An effective way of managing local-vs-remote credentials is with the config package. Here is an example:\nconfig.yml\ndefault:\n  host_url: \"[Your Host URL]\"\n  token: \"[Your Token]\"\n\nrsconnect:\n  host_url: \"[Your Host URL]\"\n  token: \"[Service Token]\"\nR script\nconfig &lt;- config::get()\n\npysparklyr::deploy_databricks(\n  version = \"14.1\", \n  host = config$host_url,\n  token = config$token\n  )\nThe integration with Posit Connect and config, allows your deployed content to automatically use the values under the rsconnect section in the YAML file, instead of the values from the default section.",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Deploying to Posit Connect"
    ]
  },
  {
    "objectID": "deployment/databricks-connect.html",
    "href": "deployment/databricks-connect.html",
    "title": "Databricks Connect v2",
    "section": "",
    "text": "Last updated: Mon Feb 26 14:01:47 2024",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Getting Started"
    ]
  },
  {
    "objectID": "deployment/databricks-connect.html#intro",
    "href": "deployment/databricks-connect.html#intro",
    "title": "Databricks Connect v2",
    "section": "Intro",
    "text": "Intro\nDatabricks Connect enables the interaction with Spark clusters remotely. It is based on Spark Connect, which enables remote connectivity thanks to its new decoupled client-server architecture. This allows users to interact with the Spark cluster without having to run the jobs from a node. Additionally, it removes the requirement of having Java components installed in the user’s machine.\nThe API is very different than the “legacy” Spark and using the Spark shell is no longer an option. We have decided to use Python as the new interface. In turn, Python uses gRPC to interact with Spark.\n\n\n\n\n\n\n\n\nflowchart LR\n  subgraph lp[test]\n    subgraph r[R]\n      sr[sparklyr]\n      rt[reticulate]\n    end\n    subgraph ps[Python]\n      dc[Databricks Connect]\n      g1[gRPC]\n    end\n  end   \n  subgraph db[Databricks]\n    sp[Spark]   \n  end\n  sr &lt;--&gt; rt\n  rt &lt;--&gt; dc\n  g1 &lt;-- Internet&lt;br&gt;Connection --&gt; sp\n  dc &lt;--&gt; g1\n  \n  style r   fill:#fff,stroke:#666,color:#000\n  style sr  fill:#fff,stroke:#666,color:#000\n  style rt  fill:#fff,stroke:#666,color:#000\n  style ps  fill:#fff,stroke:#666,color:#000\n  style lp  fill:#fff,stroke:#666,color:#fff\n  style db  fill:#fff,stroke:#666,color:#000\n  style sp  fill:#fff,stroke:#666,color:#000\n  style g1  fill:#fff,stroke:#666,color:#000\n  style dc  fill:#fff,stroke:#666,color:#000\n\n\n\n\n\n\n\n\nFigure 1: How sparklyr communicates with Databricks Connect\n\n\n\nWe are using reticulate to interact with the Python API. sparklyr extends the functionality, and user experience, by providing the dplyrback-end, DBI back-end, and integration with RStudio’s Connection pane.\nIn order to quickly iterate on enhancements and bug fixes, we have decided to isolate the Python integration into its own package. The new package, called pysparklyr, is an extension of sparklyr.",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Getting Started"
    ]
  },
  {
    "objectID": "deployment/databricks-connect.html#package-installation",
    "href": "deployment/databricks-connect.html#package-installation",
    "title": "Databricks Connect v2",
    "section": "Package Installation",
    "text": "Package Installation\nTo access Databricks Connect, you will need the following two packages:\n\nsparklyr - 1.8.4\npysparklyr - 0.1.3\n\ninstall.packages(\"sparklyr\")\ninstall.packages(\"pysparklyr\")",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Getting Started"
    ]
  },
  {
    "objectID": "deployment/databricks-connect.html#setup-credentials",
    "href": "deployment/databricks-connect.html#setup-credentials",
    "title": "Databricks Connect v2",
    "section": "Setup credentials",
    "text": "Setup credentials\nTo use with Databricks Connect, in run-time 13 or above, you will need three configuration items:\n\nYour Workspace Instance URL\nYour Personal Authentication Token (PAT), or a Posit Workbench instance configured to manage with Databricks services (see next section)\nYour Cluster ID\n\n\nPosit Workbench\nPosit Workbench can manage Databricks credentials on behalf of the user. For users of Posit Workbench, this is the recommended approach to setting up credentials as it provides an additional layer of security. If you are not currently using Posit Workbench, feel free to skip this section.\nDetails for how to setup and configure this feature can be found here.\nFor users who have signed into a Databricks Workspace via Posit Workbench, the credentials will be automatically configured and no additional setup is required. The only thing that still needs to be supplied when making a connection to Databricks is the cluster ID.\n\n\n\nEnvironment Variables\nWe have developed this solution to align with other applications that integrate with Databricks. All applications need, at minimum, a work space (1), and an authentication token (2). For default values, those applications initially look for these environment variables:\n\nDATABRICKS_HOST - Your Workspace Instance URL\nDATABRICKS_TOKEN - Your Personal Authentication Token (Not needed if using Posit Workbench)\n\nEnvironment variables work well, because they rarely vary between projects. The thing that will change more often is the cluster you are connecting to. Using environment variables also makes connection safer, because token contents will not be in your code in plain text. We recommend that you set these two variables at your user level. To do this run:\nusethis::edit_r_environ()\nThat command will open a text file that controls the environment variables at the user level. If missing, insert the entries for the two variables:\nDATABRICKS_HOST=\"Enter here your Workspace URL\"\nDATABRICKS_TOKEN=\"Enter here your personal token\" # Not needed if using Posit Workbench\nThis is a one time operation. After saving and closing the file, restart your R session.",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Getting Started"
    ]
  },
  {
    "objectID": "deployment/databricks-connect.html#first-time-connecting",
    "href": "deployment/databricks-connect.html#first-time-connecting",
    "title": "Databricks Connect v2",
    "section": "First time connecting",
    "text": "First time connecting\nAfter setting up your Host and Token environment variables, you can now connect to your cluster by simply providing the cluster’s ID, and the method to spark_connect():\nlibrary(sparklyr)\n\nsc &lt;- spark_connect(\n  cluster_id = \"Enter here your cluster ID\",\n  method = \"databricks_connect\"\n)\nIn order to connect and interact with Databricks, you will need a specific set of Python libraries installed and available. To make it easier to get started, we provide functionality that will automatically do the following:\n\nCreate or re-create the necessary Python environment. Based on your OS, it will choose to create a Virtual Environment or use Conda.\nInstall the needed Python libraries into the new environment.\n\nspark_connect() will check to see if you have the expected Python environment and prompt you to accept its installation if missing. Here is an example of the code and output you would expect to see:\nsc &lt;- spark_connect(\n    cluster_id = \"1026-175310-7cpsh3g8\",\n    method = \"databricks_connect\"\n)\n\n#&gt; ! Retrieving version from cluster '1026-175310-7cpsh3g8' \n#&gt; Cluster version: '14.1' \n#&gt; ! No viable Python Environment was identified for Databricks Connect version 14.1 \n#&gt; Do you wish to install Databricks Connect version 14.1? \n#&gt;  \n#&gt; 1: Yes \n#&gt; 2: No \n#&gt; 3: Cancel \n#&gt;  \n#&gt; Selection: 1 \nAfter accepting, the Python environment will be created with a specific name, and all of the needed Python libraries will be installed within. After it is done, it will attempt to connect to your cluster. Here is an abbreviated example of the output that occurs when selecting “Yes”:\n#&gt; ✔ Automatically naming the environment:'r-sparklyr-databricks-14.1' \n#&gt; Using Python: /Users/edgar/.pyenv/versions/3.10.13/bin/python3.10 \n#&gt; Creating virtual environment 'r-sparklyr-databricks-14.1' ... \n#&gt; + /Users/edgar/.pyenv/versions/3.10.13/bin/python3.10 -m venv /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1 \n#&gt; Done! \n#&gt;   Installing packages: pip, wheel, setuptools \n#&gt; + /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python -m pip install --upgrade pip wheel setuptools \n#&gt; Requirement already satisfied: pip in /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/lib/python3.10/site-packages (23.0.1) \n#&gt; Collecting pip \n#&gt; Using cached pip-23.3.1-py3-none-any.whl (2.1 MB) \n#&gt; Collecting wheel \n#&gt; Using cached wheel-0.42.0-py3-none-any.whl (65 kB) \n#&gt; Requirement already satisfied: setuptools in /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/lib/python3.10/site-packages (65.5.0) \n...\n...\n...\n#&gt; Successfully installed PyArrow-14.0.1 cachetools-5.3.2 certifi-2023.11.17 charset-normalizer-3.3.2 databricks-connect-14.1.0 databricks-sdk-0.14.0 google-api-core-2.14.0 google-api-python-client-2.109.0 google-auth-2.25.0 google-auth-httplib2-0.1.1 googleapis-common-protos-1.61.0 grpcio-1.59.3 grpcio_status-1.59.3 httplib2-0.22.0 idna-3.6 numpy-1.26.2 pandas-2.1.3 protobuf-4.25.1 py4j-0.10.9.7 pyasn1-0.5.1 pyasn1-modules-0.3.0 pyparsing-3.1.1 python-dateutil-2.8.2 pytz-2023.3.post1 requests-2.31.0 rsa-4.9 six-1.16.0 tzdata-2023.3 uritemplate-4.1.1 urllib3-2.1.0 \n#&gt; ✔ Using the 'r-sparklyr-databricks-14.1' Python environment \n#&gt; Path: /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Getting Started"
    ]
  },
  {
    "objectID": "deployment/databricks-connect.html#interacting-with-the-cluster",
    "href": "deployment/databricks-connect.html#interacting-with-the-cluster",
    "title": "Databricks Connect v2",
    "section": "Interacting with the cluster",
    "text": "Interacting with the cluster\n\nRStudio’s Connection pane\nThanks to the new way we are integrating with Spark, it is now possible to display the same structure displayed in the Databricks Data Explorer. In Databricks, the current data structure levels are:\n\nCatalog\n\nDatabase\n\nTable\n\n\n\nIn the RStudio Connections Pane, you can navigate the data structure by expanding from the top level, all the way down to the table you wish to explore. Once expanded, the table’s fields and their types are displayed.\n\nYou can also click on the table icon, situated to the right of the table name, to preview the first 1,000 rows:\n\n\n\nUsing the Connection to Access Data\n\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(sparklyr)\n\nsc &lt;- spark_connect(\n    cluster_id = \"1026-175310-7cpsh3g8\",\n    method = \"databricks_connect\"\n)\n#&gt; ! Changing host URL to: https://rstudio-partner-posit-default.cloud.databricks.com\n#&gt; ℹ Retrieving info for cluster:'1026-175310-7cpsh3g8'\n#&gt; ✔ Cluster: '1026-175310-7cpsh3g8' | DBR: '14.1' [470ms]\n#&gt; \n#&gt; ℹ Attempting to load 'r-sparklyr-databricks-14.1'\n#&gt; ✔ Python environment: 'r-sparklyr-databricks-14.1' [1.8s]\n#&gt; \n#&gt; ℹ Connecting to '14.1 cluster'\n#&gt; ✔ Connected to: '14.1 cluster' [7ms]\n#&gt; \n\nAfter connecting, you can use dbplyr’s in_catalog() function to access any table in your data catalog. You will only need to pass the respective names of the three levels as comma separated character entries to in_catalog() in this order: Catalog, Database, and Table.\nHere is an example of using tbl() and in_catalog() to point to the trips table, which is inside nyctaxi database, which is inside the *samples catalog:\n\ntrips &lt;- tbl(sc, in_catalog(\"samples\", \"nyctaxi\", \"trips\"))\n\ntrips\n#&gt; # Source:   table&lt;trips&gt; [?? x 6]\n#&gt; # Database: spark_connection\n#&gt;    tpep_pickup_datetime tpep_dropoff_datetime trip_distance fare_amount\n#&gt;    &lt;dttm&gt;               &lt;dttm&gt;                        &lt;dbl&gt;       &lt;dbl&gt;\n#&gt;  1 2016-02-14 10:52:13  2016-02-14 11:16:04            4.94        19  \n#&gt;  2 2016-02-04 12:44:19  2016-02-04 12:46:00            0.28         3.5\n#&gt;  3 2016-02-17 11:13:57  2016-02-17 11:17:55            0.7          5  \n#&gt;  4 2016-02-18 04:36:07  2016-02-18 04:41:45            0.8          6  \n#&gt;  5 2016-02-22 08:14:41  2016-02-22 08:31:52            4.51        17  \n#&gt;  6 2016-02-05 00:45:02  2016-02-05 00:50:26            1.8          7  \n#&gt;  7 2016-02-15 09:03:28  2016-02-15 09:18:45            2.58        12  \n#&gt;  8 2016-02-25 13:09:26  2016-02-25 13:24:50            1.4         11  \n#&gt;  9 2016-02-13 10:28:18  2016-02-13 10:36:36            1.21         7.5\n#&gt; 10 2016-02-13 18:03:48  2016-02-13 18:10:24            0.6          6  \n#&gt; # ℹ more rows\n#&gt; # ℹ 2 more variables: pickup_zip &lt;int&gt;, dropoff_zip &lt;int&gt;\n\nAfter pointing tbl() to that specific table, you can then use dplyr to execute queries against the data.\n\ntrips %&gt;%\n  group_by(pickup_zip) %&gt;%\n  summarise(\n    count = n(),\n    avg_distance = mean(trip_distance, na.rm = TRUE)\n  )\n#&gt; # Source:   SQL [?? x 3]\n#&gt; # Database: spark_connection\n#&gt;    pickup_zip count avg_distance\n#&gt;         &lt;int&gt; &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1      10032    15         4.49\n#&gt;  2      10013   273         2.98\n#&gt;  3      10022   519         2.00\n#&gt;  4      10162   414         2.19\n#&gt;  5      10018  1012         2.60\n#&gt;  6      11106    39         2.03\n#&gt;  7      10011  1129         2.29\n#&gt;  8      11103    16         2.75\n#&gt;  9      11237    15         3.31\n#&gt; 10      11422   429        15.5 \n#&gt; # ℹ more rows\n\n\n\nPosit Workench’s ‘Databricks Pane’\nPosit Workbench provides users with a Databricks pane for direct access to available Databricks clusters. From this pane, users can view details about Databricks clusters and connect directly to them. More details about this feature can be found here.",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Getting Started"
    ]
  },
  {
    "objectID": "deployment/databricks-connect.html#machine-learning",
    "href": "deployment/databricks-connect.html#machine-learning",
    "title": "Databricks Connect v2",
    "section": "Machine Learning",
    "text": "Machine Learning\nMachine Learning capabilities are currently available starting with Databricks Runtime version 14.1. Compared to “legacy” Spark, Spark Connect’s ML capabilities are limited. At this time, there is only one supported model, Logistic Regression, and two scaler transformers, namely Standard Scaler and Max Abs Scaler. sparklyr makes that functionality available.\n\nUsing for the first time\nBy default, the Python environment that sparklyr creates does not include libraries that relate to Machine Learning. These include Torch and “scikit-learn”. Some of the libraries are large in size and they may have Python requirements that are challenging to new users. Additionally, we have noticed there are not many users that need to utilize ML capabilities at this time.\nThe first time an ML function is accessed through sparklyr, you will be prompted to install the additional Python libraries which are needed to access such ML capabilities.\nml_logistic_regression(tbl_mtcars, am ~ .)\n#&gt; ! Required Python libraries to run ML functions are missing\n#&gt;   Could not find: torch, torcheval, and scikit-learn\n#&gt;   Do you wish to install? (This will be a one time operation)\n#&gt; \n#&gt; 1: Yes\n#&gt; 2: Cancel\n#&gt; \n#&gt; Selection: 1\n#&gt; Using virtual environment '/Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1' ...\n#&gt; + /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python -m pip install --upgrade --no-user torch torcheval scikit-learn\n#&gt; Collecting torch\n...\n\n\n\n\n\n\nNote\n\n\n\nIt is possible to install the ML libraries along with the required libraries. There may be several reasons to do this, including trying to recreate the environment after upgrading Python in your machine. Just pass install_ml=TRUE to the installation function:\ninstall_databricks(cluster_id = \"Enter your cluster's ID\", install_ml = TRUE)\nor\ninstall_databricks(version = \"14.1\", install_ml = TRUE)\n\n\n\n\nEasily fit and use\nAt this time, Logistic Regression is the only model supported. As usual, there are specific data preparation steps in order to run. sparklyr automates those steps, so all you have to do is pass the Spark data frame and the formula to use:\n\ntbl_mtcars &lt;- copy_to(sc, mtcars)\n\nmodel1 &lt;- ml_logistic_regression(tbl_mtcars, am ~ .)\n\nThe output for Spark Connect based models has been upgraded. It will display the model parameters.\n\nmodel1\n#&gt; \n#&gt; ── ML Connect model:\n#&gt; Logistic Regression\n#&gt; \n#&gt; ── Parameters:\n#&gt; ◼ batchSize:       32            ◼ momentum:        0.9        \n#&gt; ◼ featuresCol:     features      ◼ numTrainWorkers: 1          \n#&gt; ◼ fitIntercept:    TRUE          ◼ predictionCol:   prediction \n#&gt; ◼ labelCol:        label         ◼ probabilityCol:  probability\n#&gt; ◼ learningRate:    0.001         ◼ seed:            0          \n#&gt; ◼ maxIter:         100           ◼ tol:             1e-06\n\nAs shown in the following screenshot, the new output features a first-of-its-kind tooltip, it will popup the description of the parameter when hovered over. This functionality works when used in RStudio, and any console that supports this kind of enhanced user experience.\n\nTo use the model, you can run ml_predict():\n\nml_predict(model1, tbl_mtcars)\n#&gt; # Source:   table&lt;sparklyr_tmp_table_094111f1_5410_4fc4_a728_e2b10f13eaa0&gt; [?? x 13]\n#&gt; # Database: spark_connection\n#&gt;      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb prediction\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4          0\n#&gt;  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4          0\n#&gt;  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1          1\n#&gt;  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1          0\n#&gt;  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2          0\n#&gt;  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1          0\n#&gt;  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4          0\n#&gt;  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2          0\n#&gt;  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2          0\n#&gt; 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4          1\n#&gt; # ℹ more rows\n#&gt; # ℹ 1 more variable: probability &lt;chr&gt;\n\n\n\nUsing feature transformers\nThese are the two feature transformers currently supported:\n\nStandard Scaler - ft_standard_scaler()\nMax Abs Scaler - ft_max_abs_scaler()\n\nTo access simply call the function by passing a vector of column names. Please note that it will create a single column with an array field that contains all of the newly scaled values.\n\ntbl_mtcars %&gt;% \n  ft_standard_scaler(c(\"wt\", \"mpg\"), \"features\") %&gt;% \n  select(wt, mpg, features)\n#&gt; # Source:   SQL [?? x 3]\n#&gt; # Database: spark_connection\n#&gt;       wt   mpg features                                  \n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                                     \n#&gt;  1  2.62  21   c(-0.610399567481535, 0.150884824647656)  \n#&gt;  2  2.88  21   c(-0.349785269100972, 0.150884824647656)  \n#&gt;  3  2.32  22.8 c(-0.917004624399845, 0.449543446630647)  \n#&gt;  4  3.22  21.4 c(-0.00229953792688741, 0.217253407310543)\n#&gt;  5  3.44  18.7 c(0.227654254761845, -0.230734525663943)  \n#&gt;  6  3.46  18.1 c(0.248094591889732, -0.330287399658273)  \n#&gt;  7  3.57  14.3 c(0.360516446093113, -0.960788934955698)  \n#&gt;  8  3.19  24.4 c(-0.0278499593367465, 0.715017777282194) \n#&gt;  9  3.15  22.8 c(-0.0687306335925211, 0.449543446630647) \n#&gt; 10  3.44  19.2 c(0.227654254761845, -0.147773797335335)  \n#&gt; # ℹ more rows\n\nWhen you are done with you queries and computations, you should disconnect from the cluster.\n\nspark_disconnect(sc)",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Getting Started"
    ]
  },
  {
    "objectID": "deployment/databricks-connect.html#environments",
    "href": "deployment/databricks-connect.html#environments",
    "title": "Databricks Connect v2",
    "section": "Environments",
    "text": "Environments\n\nInstall different version of databricks.connect\nHere are three different options to create a custom Python environment, that will contain the needed Python libraries to interact with Databricks Connect:\n\nTo install the latest versions of all the needed libraries, use:\npysparklyr::install_databricks()\nsparklyr will query PyPi.org to get the latest version of databricks.connect and installs that version.\nIt is recommended that the version of the databricks.connect library matches the DBR version of your cluster. To do this, pass the DBR version in the version argument:\npysparklyr::install_databricks(\"14.0\")\nThis will create a Python environment and install databricks.connect version 14.0, and it will automatically name it r-sparklyr-databricks-14.0. By using this name, sparklyr is able to know what version of databricks.connect is available inside this particular Python environment.\nIf you are not sure about the version of the cluster you want to interact with, then use the cluster_id argument. We have added a way to pull the cluster’s information without starting Spark Connect. This allows us to query the cluster and get the DBR version:\npysparklyr::install_databricks(cluster_id = \"[Your cluster's ID]\")\n\n\n\nRestricted Python environments\nIf your organization restricts Python environment creation, you can point sparklyr to the designated Python installation. To do this, pass the path to the environment in the envname argument of spark_connect():\nlibrary(sparklyr)\n\nsc &lt;- spark_connect(\n  method = \"databricks_connect\",\n  cluster_id = \"Enter here your cluster ID\",\n  envname = \"Enter here the path to your Python environment\"\n)\nTo successfully connect to a Databricks cluster, you will need to match the proper version of the databricks.connect Python library to the Databricks Runtime (DBR) version in the cluster. For example, if you are trying to use a Databricks cluster with a DBR version 14.0 then databricks.connect will also need to be version 14.0. Failure to do so can result in instability or even the inability to connect.\nBesides datbricks.connect, the Python environment will also need to have other Python libraries installed. The full list is in the Python Libraries section.\n\n\n\n\n\n\nImportant\n\n\n\nIf your server, or machine, has only one Python installation and no ability to create Conda or Virtual environments, then you will encounter issues when connecting to a Databricks cluster with a mismatched version of databricks.connect to DBR.\n\n\nImportant - This step needs only to be done one time. If you need to connect to a different cluster that has the same DBR version, sparklyr will use the same Python environment. If the new cluster has a different DBR version, then it is recommended that you run the installation function using the new DBR version or cluster ID.",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Getting Started"
    ]
  },
  {
    "objectID": "deployment/databricks-connect.html#python-libraries",
    "href": "deployment/databricks-connect.html#python-libraries",
    "title": "Databricks Connect v2",
    "section": "Python Libraries",
    "text": "Python Libraries\nHere is the list of the Python libraries needed in order to work with the cluster:\nRequired libraries:\n\ndatabricks-connect\ndelta-spark\npandas\nPyArrow\ngrpcio\ngoogle-api-python-client\ngrpcio_status\n\nML libraries (Optional):\n\ntorch\ntorcheval\nscikit-learn\n\nTo enable R User Defined Functions (UDFs):\n\nrpy2 (see Run R code in Databricks)",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Getting Started"
    ]
  },
  {
    "objectID": "deployment/databricks-connect.html#what-is-supported",
    "href": "deployment/databricks-connect.html#what-is-supported",
    "title": "Databricks Connect v2",
    "section": "What is supported",
    "text": "What is supported\nHere is a list of what we currently support, and do not support via sparklyr and Databricks Connect:\nSupported:\n\nIntegration with most of the dplyr, and DBI, APIs\nIntegration with the invoke() command\nRStudio Connections Pane navigation\nSupport for Personal Access Token security authentication for Databricks Connect\nSupport for most read and write commands. These have only been tested in Spark Connect.\n\nNot supported:\n\nSDF functions - Most of these functions require SparkSession, which is currently not supported in Spark 3.4.\ntidyr - This is ongoing work that we are focusing on in sparklyr. We are implementing these functions using PySpark DataFrame commands instead of depending on the Scala implementation.",
    "crumbs": [
      "Deployment",
      "Databricks Connect (v2)",
      "Getting Started"
    ]
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html",
    "href": "deployment/yarn-cluster-emr.html",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "",
    "text": "This document demonstrates how to use sparklyr with an Apache Spark cluster. Data are downloaded from the web and stored in Hive tables on HDFS across multiple worker nodes. RStudio Server is installed on the master node and orchestrates the analysis in spark. Here is the basic workflow.",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "EMR cluster"
    ]
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#set-up-the-cluster",
    "href": "deployment/yarn-cluster-emr.html#set-up-the-cluster",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Set up the cluster",
    "text": "Set up the cluster\nThis demonstration uses Amazon Web Services (AWS), but it could just as easily use Microsoft, Google, or any other provider. We will use Elastic Map Reduce (EMR) to easily set up a cluster with two core nodes and one master node. Nodes use virtual servers from the Elastic Compute Cloud (EC2). Note: There is no free tier for EMR, charges will apply.\nBefore beginning this setup we assume you have:\n\nFamiliarity with and access to an AWS account\nFamiliarity with basic linux commands\nSudo privileges in order to install software from the command line\n\n\n\nBuild an EMR cluster\nBefore beginning the EMR wizard setup, make sure you create the following in AWS:\n\nAn AWS key pair (.pem key) so you can SSH into the EC2 master node\nA security group that gives you access to port 22 on your IP and port 8787 from anywhere\n\n\n\n\nStep 1: Select software\nMake sure to select Hive and Spark as part of the install. Note that by choosing Spark, R will also be installed on the master node as part of the distribution.\n\n\n\n\nStep 2: Select hardware\nInstall 2 core nodes and one master node with m3.xlarge 80 GiB storage per node. You can easily increase the number of nodes later.\n\n\n\n\nStep 3: Select general cluster settings\nClick next on the general cluster settings.\n\n\n\n\nStep 4: Select security\nEnter your EC2 key pair and security group. Make sure the security group has ports 22 and 8787 open.\n\n\n\n\n\nConnect to EMR\nThe cluster page will give you details about your EMR cluster and instructions on connecting.\n\nConnect to the master node via SSH using your key pair. Once you connect you will see the EMR welcome.\n\n# Log in to master node\nssh -i ~/spark-demo.pem hadoop@ec2-52-10-102-11.us-west-2.compute.amazonaws.com\n\n\n\n\nInstall RStudio Server\nEMR uses Amazon Linux which is based on Centos. Update your master node and install dependencies that will be used by R packages.\n\n# Update\nsudo yum update\nsudo yum install libcurl-devel openssl-devel # used for devtools\n\nThe installation of RStudio Server is easy. Download the preview version of RStudio and install on the master node.\n\n# Install RStudio Server\nwget -P /tmp https://s3.amazonaws.com/rstudio-dailybuilds/rstudio-server-rhel-0.99.1266-x86_64.rpm\nsudo yum install --nogpgcheck /tmp/rstudio-server-rhel-0.99.1266-x86_64.rpm\n\n\n\nCreate a User\nCreate a user called rstudio-user that will perform the data analysis. Create a user directory for rstudio-user on HDFS with the hadoop fs command.\n\n# Make User\nsudo useradd -m rstudio-user\nsudo passwd rstudio-user\n\n# Create new directory in hdfs\nhadoop fs -mkdir /user/rstudio-user\nhadoop fs -chmod 777 /user/rstudio-user",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "EMR cluster"
    ]
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#download-flights-data",
    "href": "deployment/yarn-cluster-emr.html#download-flights-data",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Download flights data",
    "text": "Download flights data\nThe flights data is a well known data source representing 123 million flights over 22 years. It consumes roughly 12 GiB of storage in uncompressed CSV format in yearly files.\n\nSwitch User\nFor data loading and analysis, make sure you are logged in as regular user.\n\n# create directories on hdfs for new user\nhadoop fs -mkdir /user/rstudio-user\nhadoop fs -chmod 777 /user/rstudio-user\n\n# switch user\nsu rstudio-user\n\n\n\nDownload data\nRun the following script to download data from the web onto your master node. Download the yearly flight data and the airlines lookup table.\n\n# Make download directory\nmkdir /tmp/flights\n\n# Download flight data by year\nfor i in {1987..2008}\n  do\n    echo \"$(date) $i Download\"\n    fnam=$i.csv.bz2\n    wget -O /tmp/flights/$fnam http://stat-computing.org/dataexpo/2009/$fnam\n    echo \"$(date) $i Unzip\"\n    bunzip2 /tmp/flights/$fnam\n  done\n\n# Download airline carrier data\nwget -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS\n\n# Download airports data\nwget -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\n\n\n\nDistribute into HDFS\nCopy data into HDFS using the hadoop fs command.\n\n# Copy flight data to HDFS\nhadoop fs -mkdir /user/rstudio-user/flights/\nhadoop fs -put /tmp/flights /user/rstudio-user/\n\n# Copy airline data to HDFS\nhadoop fs -mkdir /user/rstudio-user/airlines/\nhadoop fs -put /tmp/airlines.csv /user/rstudio-user/airlines\n\n# Copy airport data to HDFS\nhadoop fs -mkdir /user/rstudio-user/airports/\nhadoop fs -put /tmp/airports.csv /user/rstudio-user/airports",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "EMR cluster"
    ]
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#create-hive-tables",
    "href": "deployment/yarn-cluster-emr.html#create-hive-tables",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Create Hive tables",
    "text": "Create Hive tables\nLaunch Hive from the command line.\n\n# Open Hive prompt\nhive\n\nCreate the metadata that will structure the flights table. Load data into the Hive table.\n# Create metadata for flights\nCREATE EXTERNAL TABLE IF NOT EXISTS flights\n(\nyear int,\nmonth int,\ndayofmonth int,\ndayofweek int,\ndeptime int,\ncrsdeptime int,\narrtime int, \ncrsarrtime int,\nuniquecarrier string,\nflightnum int,\ntailnum string, \nactualelapsedtime int,\ncrselapsedtime int,\nairtime string,\narrdelay int,\ndepdelay int, \norigin string,\ndest string,\ndistance int,\ntaxiin string,\ntaxiout string,\ncancelled int,\ncancellationcode string,\ndiverted int,\ncarrierdelay string,\nweatherdelay string,\nnasdelay string,\nsecuritydelay string,\nlateaircraftdelay string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLINES TERMINATED BY '\\n'\nTBLPROPERTIES(\"skip.header.line.count\"=\"1\");\n\n# Load data into table\nLOAD DATA INPATH '/user/rstudio-user/flights' INTO TABLE flights;\nCreate the metadata that will structure the airlines table. Load data into the Hive table.\n# Create metadata for airlines\nCREATE EXTERNAL TABLE IF NOT EXISTS airlines\n(\nCode string,\nDescription string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE\ntblproperties(\"skip.header.line.count\"=\"1\");\n\n# Load data into table\nLOAD DATA INPATH '/user/rstudio-user/airlines' INTO TABLE airlines;\nCreate the metadata that will structure the airports table. Load data into the Hive table.\n# Create metadata for airports\nCREATE EXTERNAL TABLE IF NOT EXISTS airports\n(\nid string,\nname string,\ncity string,\ncountry string,\nfaa string,\nicao string,\nlat double,\nlon double,\nalt int,\ntz_offset double,\ndst string,\ntz_name string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE;\n\n# Load data into table\nLOAD DATA INPATH '/user/rstudio-user/airports' INTO TABLE airports;",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "EMR cluster"
    ]
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#connect-to-spark",
    "href": "deployment/yarn-cluster-emr.html#connect-to-spark",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Connect to Spark",
    "text": "Connect to Spark\nLog in to RStudio Server by pointing a browser at your master node IP:8787.\n\nSet the environment variable SPARK_HOME and then run spark_connect. After connecting you will be able to browse the Hive metadata in the RStudio Server Spark pane.\n\n# Connect to Spark\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nSys.setenv(SPARK_HOME=\"/usr/lib/spark\")\nconfig &lt;- spark_config()\nsc &lt;- spark_connect(master = \"yarn-client\", config = config, version = '1.6.2')\n\nOnce you are connected, you will see the Spark pane appear along with your hive tables.\n\nYou can inspect your tables by clicking on the data icon.",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "EMR cluster"
    ]
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#cache-the-tables-into-memory",
    "href": "deployment/yarn-cluster-emr.html#cache-the-tables-into-memory",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Cache the tables into memory",
    "text": "Cache the tables into memory\nUse tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame.\n\n# Cache flights Hive table into Spark\ntbl_cache(sc, 'flights')\nflights_tbl &lt;- tbl(sc, 'flights')\n\n# Cache airlines Hive table into Spark\ntbl_cache(sc, 'airlines')\nairlines_tbl &lt;- tbl(sc, 'airlines')\n\n# Cache airports Hive table into Spark\ntbl_cache(sc, 'airports')\nairports_tbl &lt;- tbl(sc, 'airports')",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "EMR cluster"
    ]
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#create-a-model-data-set",
    "href": "deployment/yarn-cluster-emr.html#create-a-model-data-set",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Create a model data set",
    "text": "Create a model data set\nFilter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight.\n\n# Filter records and create target variable 'gain'\nmodel_data &lt;- flights_tbl %&gt;%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %&gt;%\n  filter(depdelay &gt; 15 & depdelay &lt; 240) %&gt;%\n  filter(arrdelay &gt; -60 & arrdelay &lt; 360) %&gt;%\n  filter(year &gt;= 2003 & year &lt;= 2007) %&gt;%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %&gt;%\n  mutate(gain = depdelay - arrdelay) %&gt;%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain)\n\n# Summarize data by carrier\nmodel_data %&gt;%\n  group_by(uniquecarrier) %&gt;%\n  summarize(description = min(description), gain=mean(gain), \n            distance=mean(distance), depdelay=mean(depdelay)) %&gt;%\n  select(description, gain, distance, depdelay) %&gt;%\n  arrange(gain)\n\nSource:   query [?? x 4]\nDatabase: spark connection master=yarn-client app=sparklyr local=FALSE\n\n                    description       gain  distance depdelay\n                          &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1        ATA Airlines d/b/a ATA -3.3480120 1134.7084 56.06583\n2  ExpressJet Airlines Inc. (1) -3.0326180  519.7125 59.41659\n3                     Envoy Air -2.5434415  416.3716 53.12529\n4       Northwest Airlines Inc. -2.2030586  779.2342 48.52828\n5          Delta Air Lines Inc. -1.8248026  868.3997 50.77174\n6   AirTran Airways Corporation -1.4331555  641.8318 54.96702\n7    Continental Air Lines Inc. -0.9617003 1116.6668 57.00553\n8        American Airlines Inc. -0.8860262 1074.4388 55.45045\n9             Endeavor Air Inc. -0.6392733  467.1951 58.47395\n10              JetBlue Airways -0.3262134 1139.0443 54.06156\n# ... with more rows",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "EMR cluster"
    ]
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#train-a-linear-model",
    "href": "deployment/yarn-cluster-emr.html#train-a-linear-model",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Train a linear model",
    "text": "Train a linear model\nPredict time gained or lost in flight as a function of distance, departure delay, and airline carrier.\n\n# Partition the data into training and validation sets\nmodel_partition &lt;- model_data %&gt;% \n  sdf_partition(train = 0.8, valid = 0.2, seed = 5555)\n\n# Fit a linear model\nml1 &lt;- model_partition$train %&gt;%\n  ml_linear_regression(gain ~ distance + depdelay + uniquecarrier)\n\n# Summarize the linear model\nsummary(ml1)\n\nDeviance Residuals: (approximate):\n     Min       1Q   Median       3Q      Max \n-305.422   -5.593    2.699    9.750  147.871 \n\nCoefficients:\n                    Estimate  Std. Error  t value  Pr(&gt;|t|)    \n(Intercept)      -1.24342576  0.10248281 -12.1330 &lt; 2.2e-16 ***\ndistance          0.00326600  0.00001670 195.5709 &lt; 2.2e-16 ***\ndepdelay         -0.01466233  0.00020337 -72.0977 &lt; 2.2e-16 ***\nuniquecarrier_AA -2.32650517  0.10522524 -22.1098 &lt; 2.2e-16 ***\nuniquecarrier_AQ  2.98773637  0.28798507  10.3746 &lt; 2.2e-16 ***\nuniquecarrier_AS  0.92054894  0.11298561   8.1475 4.441e-16 ***\nuniquecarrier_B6 -1.95784698  0.11728289 -16.6934 &lt; 2.2e-16 ***\nuniquecarrier_CO -2.52618081  0.11006631 -22.9514 &lt; 2.2e-16 ***\nuniquecarrier_DH  2.23287189  0.11608798  19.2343 &lt; 2.2e-16 ***\nuniquecarrier_DL -2.68848119  0.10621977 -25.3106 &lt; 2.2e-16 ***\nuniquecarrier_EV  1.93484736  0.10724290  18.0417 &lt; 2.2e-16 ***\nuniquecarrier_F9 -0.89788137  0.14422281  -6.2257 4.796e-10 ***\nuniquecarrier_FL -1.46706706  0.11085354 -13.2343 &lt; 2.2e-16 ***\nuniquecarrier_HA -0.14506644  0.25031456  -0.5795    0.5622    \nuniquecarrier_HP  2.09354855  0.12337515  16.9690 &lt; 2.2e-16 ***\nuniquecarrier_MQ -1.88297535  0.10550507 -17.8473 &lt; 2.2e-16 ***\nuniquecarrier_NW -2.79538927  0.10752182 -25.9983 &lt; 2.2e-16 ***\nuniquecarrier_OH  0.83520117  0.11032997   7.5700 3.730e-14 ***\nuniquecarrier_OO  0.61993842  0.10679884   5.8047 6.447e-09 ***\nuniquecarrier_TZ -4.99830389  0.15912629 -31.4109 &lt; 2.2e-16 ***\nuniquecarrier_UA -0.68294396  0.10638099  -6.4198 1.365e-10 ***\nuniquecarrier_US -0.61589284  0.10669583  -5.7724 7.815e-09 ***\nuniquecarrier_WN  3.86386059  0.10362275  37.2878 &lt; 2.2e-16 ***\nuniquecarrier_XE -2.59658123  0.10775736 -24.0966 &lt; 2.2e-16 ***\nuniquecarrier_YV  3.11113140  0.11659679  26.6828 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nR-Squared: 0.02385\nRoot Mean Squared Error: 17.74",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "EMR cluster"
    ]
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#assess-model-performance",
    "href": "deployment/yarn-cluster-emr.html#assess-model-performance",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Assess model performance",
    "text": "Assess model performance\nCompare the model performance using the validation data.\n\n# Calculate average gains by predicted decile\nmodel_deciles &lt;- lapply(model_partition, function(x) {\n  sdf_predict(ml1, x) %&gt;%\n    mutate(decile = ntile(desc(prediction), 10)) %&gt;%\n    group_by(decile) %&gt;%\n    summarize(gain = mean(gain)) %&gt;%\n    select(decile, gain) %&gt;%\n    collect()\n})\n\n# Create a summary dataset for plotting\ndeciles &lt;- rbind(\n  data.frame(data = 'train', model_deciles$train),\n  data.frame(data = 'valid', model_deciles$valid),\n  make.row.names = FALSE\n)\n\n# Plot average gains by predicted decile\ndeciles %&gt;%\n  ggplot(aes(factor(decile), gain, fill = data)) +\n  geom_bar(stat = 'identity', position = 'dodge') +\n  labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes')",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "EMR cluster"
    ]
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#visualize-predictions",
    "href": "deployment/yarn-cluster-emr.html#visualize-predictions",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Visualize predictions",
    "text": "Visualize predictions\nCompare actual gains to predicted gains for an out of time sample.\n\n# Select data from an out of time sample\ndata_2008 &lt;- flights_tbl %&gt;%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %&gt;%\n  filter(depdelay &gt; 15 & depdelay &lt; 240) %&gt;%\n  filter(arrdelay &gt; -60 & arrdelay &lt; 360) %&gt;%\n  filter(year == 2008) %&gt;%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %&gt;%\n  mutate(gain = depdelay - arrdelay) %&gt;%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest)\n\n# Summarize data by carrier\ncarrier &lt;- sdf_predict(ml1, data_2008) %&gt;%\n  group_by(description) %&gt;%\n  summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %&gt;%\n  filter(freq &gt; 10000) %&gt;%\n  collect\n\n# Plot actual gains and predicted gains by airline carrier\nggplot(carrier, aes(gain, prediction)) + \n  geom_point(alpha = 0.75, color = 'red', shape = 3) +\n  geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') +\n  geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) +\n  labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted')\n\n\nSome carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights.",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "EMR cluster"
    ]
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#build-dashboard",
    "href": "deployment/yarn-cluster-emr.html#build-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Build dashboard",
    "text": "Build dashboard\nAggregate the scored data by origin, destination, and airline. Save the aggregated data.\n\n# Summarize by origin, destination, and carrier\nsummary_2008 &lt;- sdf_predict(ml1, data_2008) %&gt;%\n  rename(carrier = uniquecarrier, airline = description) %&gt;%\n  group_by(origin, dest, carrier, airline) %&gt;%\n  summarize(\n    flights = n(),\n    distance = mean(distance),\n    avg_dep_delay = mean(depdelay),\n    avg_arr_delay = mean(arrdelay),\n    avg_gain = mean(gain),\n    pred_gain = mean(prediction)\n    )\n\n# Collect and save objects\npred_data &lt;- collect(summary_2008)\nairports &lt;- collect(select(airports_tbl, name, faa, lat, lon))\nml1_summary &lt;- capture.output(summary(ml1))\nsave(pred_data, airports, ml1_summary, file = 'flights_pred_2008.RData')",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "EMR cluster"
    ]
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#publish-dashboard",
    "href": "deployment/yarn-cluster-emr.html#publish-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Publish dashboard",
    "text": "Publish dashboard\nUse the saved data to build an R Markdown flexdashboard. Publish the flexdashboard to Shiny Server, Shinyapps.io or RStudio Connect.",
    "crumbs": [
      "Deployment",
      "YARN (Hadoop)",
      "EMR cluster"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_is_streaming.html",
    "href": "packages/sparklyr/latest/reference/sdf_is_streaming.html",
    "title": "Spark DataFrame is Streaming",
    "section": "",
    "text": "R/sdf_streaming.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_is_streaming.html#sdf_is_streaming",
    "href": "packages/sparklyr/latest/reference/sdf_is_streaming.html#sdf_is_streaming",
    "title": "Spark DataFrame is Streaming",
    "section": "sdf_is_streaming",
    "text": "sdf_is_streaming"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_is_streaming.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_is_streaming.html#description",
    "title": "Spark DataFrame is Streaming",
    "section": "Description",
    "text": "Description\nIs the given Spark DataFrame a streaming data?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_is_streaming.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_is_streaming.html#usage",
    "title": "Spark DataFrame is Streaming",
    "section": "Usage",
    "text": "Usage\nsdf_is_streaming(x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_is_streaming.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_is_streaming.html#arguments",
    "title": "Spark DataFrame is Streaming",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_num_partitions.html",
    "href": "packages/sparklyr/latest/reference/sdf_num_partitions.html",
    "title": "Gets number of partitions of a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_num_partitions.html#sdf_num_partitions",
    "href": "packages/sparklyr/latest/reference/sdf_num_partitions.html#sdf_num_partitions",
    "title": "Gets number of partitions of a Spark DataFrame",
    "section": "sdf_num_partitions",
    "text": "sdf_num_partitions"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_num_partitions.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_num_partitions.html#description",
    "title": "Gets number of partitions of a Spark DataFrame",
    "section": "Description",
    "text": "Description\nGets number of partitions of a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_num_partitions.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_num_partitions.html#usage",
    "title": "Gets number of partitions of a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_num_partitions(x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_num_partitions.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_num_partitions.html#arguments",
    "title": "Gets number of partitions of a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_chisquare_test.html",
    "href": "packages/sparklyr/latest/reference/ml_chisquare_test.html",
    "title": "Chi-square hypothesis testing for categorical data.",
    "section": "",
    "text": "R/ml_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_chisquare_test.html#ml_chisquare_test",
    "href": "packages/sparklyr/latest/reference/ml_chisquare_test.html#ml_chisquare_test",
    "title": "Chi-square hypothesis testing for categorical data.",
    "section": "ml_chisquare_test",
    "text": "ml_chisquare_test"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_chisquare_test.html#description",
    "href": "packages/sparklyr/latest/reference/ml_chisquare_test.html#description",
    "title": "Chi-square hypothesis testing for categorical data.",
    "section": "Description",
    "text": "Description\nConduct Pearson’s independence test for every feature against the label. For each feature, the (feature, label) pairs are converted into a contingency matrix for which the Chi-squared statistic is computed. All label and feature values must be categorical."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_chisquare_test.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_chisquare_test.html#usage",
    "title": "Chi-square hypothesis testing for categorical data.",
    "section": "Usage",
    "text": "Usage\n \nml_chisquare_test(x, features, label)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_chisquare_test.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_chisquare_test.html#arguments",
    "title": "Chi-square hypothesis testing for categorical data.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA tbl_spark.\n\n\nfeatures\nThe name(s) of the feature columns. This can also be the name of a single vector column created using ft_vector_assembler().\n\n\nlabel\nThe name of the label column."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_chisquare_test.html#value",
    "href": "packages/sparklyr/latest/reference/ml_chisquare_test.html#value",
    "title": "Chi-square hypothesis testing for categorical data.",
    "section": "Value",
    "text": "Value\nA data frame with one row for each (feature, label) pair with p-values, degrees of freedom, and test statistics."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_chisquare_test.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_chisquare_test.html#examples",
    "title": "Chi-square hypothesis testing for categorical data.",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \nfeatures &lt;- c(\"Petal_Width\", \"Petal_Length\", \"Sepal_Length\", \"Sepal_Width\") \n \nml_chisquare_test(iris_tbl, features = features, label = \"Species\") \n#&gt;        feature   label      p_value degrees_of_freedom statistic\n#&gt; 1  Petal_Width Species 0.000000e+00                 42 271.75000\n#&gt; 2 Petal_Length Species 0.000000e+00                 84 271.80000\n#&gt; 3 Sepal_Length Species 6.665987e-09                 68 156.26667\n#&gt; 4  Sepal_Width Species 6.016031e-05                 44  89.54629"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_project.html",
    "href": "packages/sparklyr/latest/reference/sdf_project.html",
    "title": "Project features onto principal components",
    "section": "",
    "text": "R/sdf_ml.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_project.html#sdf_project",
    "href": "packages/sparklyr/latest/reference/sdf_project.html#sdf_project",
    "title": "Project features onto principal components",
    "section": "sdf_project",
    "text": "sdf_project"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_project.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_project.html#description",
    "title": "Project features onto principal components",
    "section": "Description",
    "text": "Description\nProject features onto principal components"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_project.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_project.html#usage",
    "title": "Project features onto principal components",
    "section": "Usage",
    "text": "Usage\nsdf_project( \n  object, \n  newdata, \n  features = dimnames(object$pc)[[1]], \n  feature_prefix = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_project.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_project.html#arguments",
    "title": "Project features onto principal components",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nobject\nA Spark PCA model object\n\n\nnewdata\nAn object coercible to a Spark DataFrame\n\n\nfeatures\nA vector of names of columns to be projected\n\n\nfeature_prefix\nThe prefix used in naming the output features\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_project.html#section",
    "href": "packages/sparklyr/latest/reference/sdf_project.html#section",
    "title": "Project features onto principal components",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_project.html#transforming-spark-dataframes",
    "href": "packages/sparklyr/latest/reference/sdf_project.html#transforming-spark-dataframes",
    "title": "Project features onto principal components",
    "section": "Transforming Spark DataFrames",
    "text": "Transforming Spark DataFrames\nThe family of functions prefixed with sdf_ generally access the Scala Spark DataFrame API directly, as opposed to the dplyr interface which uses Spark SQL. These functions will ‘force’ any pending SQL in a dplyr pipeline, such that the resulting tbl_spark object returned will no longer have the attached ‘lazy’ SQL operations. Note that the underlying Spark DataFrame does execute its operations lazily, so that even though the pending set of operations (currently) are not exposed at the R level, these operations will only be executed when you explicitly collect() the table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_add_stage.html",
    "href": "packages/sparklyr/latest/reference/ml_add_stage.html",
    "title": "Add a Stage to a Pipeline",
    "section": "",
    "text": "R/ml_pipeline_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_add_stage.html#ml_add_stage",
    "href": "packages/sparklyr/latest/reference/ml_add_stage.html#ml_add_stage",
    "title": "Add a Stage to a Pipeline",
    "section": "ml_add_stage",
    "text": "ml_add_stage"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_add_stage.html#description",
    "href": "packages/sparklyr/latest/reference/ml_add_stage.html#description",
    "title": "Add a Stage to a Pipeline",
    "section": "Description",
    "text": "Description\nAdds a stage to a pipeline."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_add_stage.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_add_stage.html#usage",
    "title": "Add a Stage to a Pipeline",
    "section": "Usage",
    "text": "Usage\nml_add_stage(x, stage)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_add_stage.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_add_stage.html#arguments",
    "title": "Add a Stage to a Pipeline",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA pipeline or a pipeline stage.\n\n\nstage\nA pipeline stage."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_orc.html",
    "href": "packages/sparklyr/latest/reference/spark_write_orc.html",
    "title": "Write a Spark DataFrame to a ORC file",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_orc.html#spark_write_orc",
    "href": "packages/sparklyr/latest/reference/spark_write_orc.html#spark_write_orc",
    "title": "Write a Spark DataFrame to a ORC file",
    "section": "spark_write_orc",
    "text": "spark_write_orc"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_orc.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_orc.html#description",
    "title": "Write a Spark DataFrame to a ORC file",
    "section": "Description",
    "text": "Description\nSerialize a Spark DataFrame to the ORC format."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_orc.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_orc.html#usage",
    "title": "Write a Spark DataFrame to a ORC file",
    "section": "Usage",
    "text": "Usage\nspark_write_orc( \n  x, \n  path, \n  mode = NULL, \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_orc.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_orc.html#arguments",
    "title": "Write a Spark DataFrame to a ORC file",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_orc.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_orc.html#see-also",
    "title": "Write a Spark DataFrame to a ORC file",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rpois.html",
    "href": "packages/sparklyr/latest/reference/sdf_rpois.html",
    "title": "Generate random samples from a Poisson distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rpois.html#sdf_rpois",
    "href": "packages/sparklyr/latest/reference/sdf_rpois.html#sdf_rpois",
    "title": "Generate random samples from a Poisson distribution",
    "section": "sdf_rpois",
    "text": "sdf_rpois"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rpois.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rpois.html#description",
    "title": "Generate random samples from a Poisson distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Poisson distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rpois.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rpois.html#usage",
    "title": "Generate random samples from a Poisson distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rpois(sc, n, lambda, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rpois.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rpois.html#arguments",
    "title": "Generate random samples from a Poisson distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nlambda\nMean, or lambda, of the Poisson distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rpois.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rpois.html#see-also",
    "title": "Generate random samples from a Poisson distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/dbisparkresult-class.html",
    "href": "packages/sparklyr/latest/reference/dbisparkresult-class.html",
    "title": "DBI Spark Result.",
    "section": "",
    "text": "R/dbi_spark_result.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/dbisparkresult-class.html#dbisparkresult-class",
    "href": "packages/sparklyr/latest/reference/dbisparkresult-class.html#dbisparkresult-class",
    "title": "DBI Spark Result.",
    "section": "DBISparkResult-class",
    "text": "DBISparkResult-class"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/dbisparkresult-class.html#description",
    "href": "packages/sparklyr/latest/reference/dbisparkresult-class.html#description",
    "title": "DBI Spark Result.",
    "section": "Description",
    "text": "Description\nDBI Spark Result."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/dbisparkresult-class.html#section",
    "href": "packages/sparklyr/latest/reference/dbisparkresult-class.html#section",
    "title": "DBI Spark Result.",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/dbisparkresult-class.html#slots",
    "href": "packages/sparklyr/latest/reference/dbisparkresult-class.html#slots",
    "title": "DBI Spark Result.",
    "section": "Slots",
    "text": "Slots\nsql\ncharacter.\nsdf\nspark_jobj.\nconn\nspark_connection.\nstate\nenvironment."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_log.html",
    "href": "packages/sparklyr/latest/reference/spark_log.html",
    "title": "View Entries in the Spark Log",
    "section": "",
    "text": "R/spark_connection.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_log.html#spark_log",
    "href": "packages/sparklyr/latest/reference/spark_log.html#spark_log",
    "title": "View Entries in the Spark Log",
    "section": "spark_log",
    "text": "spark_log"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_log.html#description",
    "href": "packages/sparklyr/latest/reference/spark_log.html#description",
    "title": "View Entries in the Spark Log",
    "section": "Description",
    "text": "Description\nView the most recent entries in the Spark log. This can be useful when inspecting output / errors produced by Spark during the invocation of various commands."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_log.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_log.html#usage",
    "title": "View Entries in the Spark Log",
    "section": "Usage",
    "text": "Usage\nspark_log(sc, n = 100, filter = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_log.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_log.html#arguments",
    "title": "View Entries in the Spark Log",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nn\nThe max number of log entries to retrieve. Use NULL to retrieve all entries within the log.\n\n\nfilter\nCharacter string to filter log entries.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluate.html",
    "href": "packages/sparklyr/latest/reference/ml_evaluate.html",
    "title": "Evaluate the Model on a Validation Set",
    "section": "",
    "text": "R/ml_evaluate.R, R/ml_evaluator.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluate.html#ml_evaluate",
    "href": "packages/sparklyr/latest/reference/ml_evaluate.html#ml_evaluate",
    "title": "Evaluate the Model on a Validation Set",
    "section": "ml_evaluate",
    "text": "ml_evaluate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluate.html#description",
    "href": "packages/sparklyr/latest/reference/ml_evaluate.html#description",
    "title": "Evaluate the Model on a Validation Set",
    "section": "Description",
    "text": "Description\nCompute performance metrics."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluate.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_evaluate.html#usage",
    "title": "Evaluate the Model on a Validation Set",
    "section": "Usage",
    "text": "Usage\n \nml_evaluate(x, dataset) \n \n## S3 method for class 'ml_model_logistic_regression'\nml_evaluate(x, dataset) \n \n## S3 method for class 'ml_logistic_regression_model'\nml_evaluate(x, dataset) \n \n## S3 method for class 'ml_model_linear_regression'\nml_evaluate(x, dataset) \n \n## S3 method for class 'ml_linear_regression_model'\nml_evaluate(x, dataset) \n \n## S3 method for class 'ml_model_generalized_linear_regression'\nml_evaluate(x, dataset) \n \n## S3 method for class 'ml_generalized_linear_regression_model'\nml_evaluate(x, dataset) \n \n## S3 method for class 'ml_model_clustering'\nml_evaluate(x, dataset) \n \n## S3 method for class 'ml_model_classification'\nml_evaluate(x, dataset) \n \n## S3 method for class 'ml_evaluator'\nml_evaluate(x, dataset)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluate.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_evaluate.html#arguments",
    "title": "Evaluate the Model on a Validation Set",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn ML model object or an evaluator object.\n\n\ndataset\nThe dataset to be validate the model on."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluate.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_evaluate.html#examples",
    "title": "Evaluate the Model on a Validation Set",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \nml_gaussian_mixture(iris_tbl, Species ~ .) %&gt;% \n  ml_evaluate(iris_tbl) \n#&gt; # A tibble: 1 × 1\n#&gt;   Silhouette\n#&gt;        &lt;dbl&gt;\n#&gt; 1      0.477\n \nml_kmeans(iris_tbl, Species ~ .) %&gt;% \n  ml_evaluate(iris_tbl) \n#&gt; # A tibble: 1 × 1\n#&gt;   Silhouette\n#&gt;        &lt;dbl&gt;\n#&gt; 1      0.850\n \nml_bisecting_kmeans(iris_tbl, Species ~ .) %&gt;% \n  ml_evaluate(iris_tbl) \n#&gt; # A tibble: 1 × 1\n#&gt;   Silhouette\n#&gt;        &lt;dbl&gt;\n#&gt; 1      0.517"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_json.html",
    "href": "packages/sparklyr/latest/reference/stream_write_json.html",
    "title": "Write JSON Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_json.html#stream_write_json",
    "href": "packages/sparklyr/latest/reference/stream_write_json.html#stream_write_json",
    "title": "Write JSON Stream",
    "section": "stream_write_json",
    "text": "stream_write_json"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_json.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_json.html#description",
    "title": "Write JSON Stream",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into a JSON stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_json.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_json.html#usage",
    "title": "Write JSON Stream",
    "section": "Usage",
    "text": "Usage\nstream_write_json( \n  x, \n  path, \n  mode = c(\"append\", \"complete\", \"update\"), \n  trigger = stream_trigger_interval(), \n  checkpoint = file.path(path, \"checkpoints\", random_string(\"\")), \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_json.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_json.html#arguments",
    "title": "Write JSON Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe destination path. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\".\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous.\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_json.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_json.html#examples",
    "title": "Write JSON Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\") \ndir.create(\"json-in\") \njsonlite::write_json(list(a = c(1, 2), b = c(10, 20)), \"json-in/data.json\") \njson_path &lt;- file.path(\"file://\", getwd(), \"json-in\") \nstream &lt;- stream_read_json(sc, json_path) %&gt;% stream_write_json(\"json-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_json.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_json.html#see-also",
    "title": "Write JSON Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_count_vectorizer.html",
    "href": "packages/sparklyr/latest/reference/ft_count_vectorizer.html",
    "title": "Feature Transformation – CountVectorizer (Estimator)",
    "section": "",
    "text": "R/ml_feature_count_vectorizer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#ft_count_vectorizer",
    "href": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#ft_count_vectorizer",
    "title": "Feature Transformation – CountVectorizer (Estimator)",
    "section": "ft_count_vectorizer",
    "text": "ft_count_vectorizer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#description",
    "title": "Feature Transformation – CountVectorizer (Estimator)",
    "section": "Description",
    "text": "Description\nExtracts a vocabulary from document collections."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#usage",
    "title": "Feature Transformation – CountVectorizer (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_count_vectorizer( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  binary = FALSE, \n  min_df = 1, \n  min_tf = 1, \n  vocab_size = 2^18, \n  uid = random_string(\"count_vectorizer_\"), \n  ... \n) \n\nml_vocabulary(model)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#arguments",
    "title": "Feature Transformation – CountVectorizer (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nbinary\nBinary toggle to control the output vector values. If TRUE, all nonzero counts (after min_tf filter applied) are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. Default: FALSE\n\n\nmin_df\nSpecifies the minimum number of different documents a term must appear in to be included in the vocabulary. If this is an integer greater than or equal to 1, this specifies the number of documents the term must appear in; if this is a double in [0,1), then this specifies the fraction of documents. Default: 1.\n\n\nmin_tf\nFilter to ignore rare words in a document. For each document, terms with frequency/count less than the given threshold are ignored. If this is an integer greater than or equal to 1, then this specifies a count (of times the term must appear in the document); if this is a double in [0,1), then this specifies a fraction (out of the document’s token count). Default: 1.\n\n\nvocab_size\nBuild a vocabulary that only considers the top vocab_size terms ordered by term frequency across the corpus. Default: 2^18.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused.\n\n\nmodel\nA ml_count_vectorizer_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#details",
    "href": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#details",
    "title": "Feature Transformation – CountVectorizer (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#value",
    "title": "Feature Transformation – CountVectorizer (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark\n\nml_vocabulary() returns a vector of vocabulary built."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#see-also",
    "title": "Feature Transformation – CountVectorizer (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_keys.html",
    "href": "packages/sparklyr/latest/reference/hof_transform_keys.html",
    "title": "Transforms keys of a map",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_keys.html#hof_transform_keys",
    "href": "packages/sparklyr/latest/reference/hof_transform_keys.html#hof_transform_keys",
    "title": "Transforms keys of a map",
    "section": "hof_transform_keys",
    "text": "hof_transform_keys"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_keys.html#description",
    "href": "packages/sparklyr/latest/reference/hof_transform_keys.html#description",
    "title": "Transforms keys of a map",
    "section": "Description",
    "text": "Description\nApplies the transformation function specified to all keys of a map (this is essentially a dplyr wrapper to the transform_keys(expr, func) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_keys.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_transform_keys.html#usage",
    "title": "Transforms keys of a map",
    "section": "Usage",
    "text": "Usage\n \nhof_transform_keys(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_keys.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_transform_keys.html#arguments",
    "title": "Transforms keys of a map",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\nfunc\nThe transformation function to apply (it should take (key, value) as arguments and return a transformed key)\n\n\nexpr\nThe map being transformed, could be any SQL expression evaluating to a map (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the transformed result (default: expr)\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_keys.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_transform_keys.html#examples",
    "title": "Transforms keys of a map",
    "section": "Examples",
    "text": "Examples\n\n \n \nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\", version = \"3.0.0\") \nsdf &lt;- sdf_len(sc, 1) %&gt;% dplyr::mutate(m = map(\"a\", 0L, \"b\", 2L, \"c\", -1L)) \ntransformed_sdf &lt;- sdf %&gt;% hof_transform_keys(~ CONCAT(.x, \" == \", .y))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/worker_spark_apply_unbundle.html",
    "href": "packages/sparklyr/latest/reference/worker_spark_apply_unbundle.html",
    "title": "Extracts a bundle of dependencies required by spark_apply()",
    "section": "",
    "text": "R/worker_apply.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/worker_spark_apply_unbundle.html#worker_spark_apply_unbundle",
    "href": "packages/sparklyr/latest/reference/worker_spark_apply_unbundle.html#worker_spark_apply_unbundle",
    "title": "Extracts a bundle of dependencies required by spark_apply()",
    "section": "worker_spark_apply_unbundle",
    "text": "worker_spark_apply_unbundle"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/worker_spark_apply_unbundle.html#description",
    "href": "packages/sparklyr/latest/reference/worker_spark_apply_unbundle.html#description",
    "title": "Extracts a bundle of dependencies required by spark_apply()",
    "section": "Description",
    "text": "Description\nExtracts a bundle of dependencies required by spark_apply()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/worker_spark_apply_unbundle.html#usage",
    "href": "packages/sparklyr/latest/reference/worker_spark_apply_unbundle.html#usage",
    "title": "Extracts a bundle of dependencies required by spark_apply()",
    "section": "Usage",
    "text": "Usage\nworker_spark_apply_unbundle(bundle_path, base_path, bundle_name)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/worker_spark_apply_unbundle.html#arguments",
    "href": "packages/sparklyr/latest/reference/worker_spark_apply_unbundle.html#arguments",
    "title": "Extracts a bundle of dependencies required by spark_apply()",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nbundle_path\nPath to the bundle created using spark_apply_bundle()\n\n\nbase_path\nBase path to use while extracting bundles"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html",
    "href": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html",
    "title": "Spark ML – Survival Regression",
    "section": "",
    "text": "R/ml_regression_aft_survival_regression.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#ml_aft_survival_regression",
    "href": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#ml_aft_survival_regression",
    "title": "Spark ML – Survival Regression",
    "section": "ml_aft_survival_regression",
    "text": "ml_aft_survival_regression"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#description",
    "href": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#description",
    "title": "Spark ML – Survival Regression",
    "section": "Description",
    "text": "Description\nFit a parametric survival regression model named accelerated failure time (AFT) model (see Accelerated failure time model (Wikipedia)) based on the Weibull distribution of the survival time."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#usage",
    "title": "Spark ML – Survival Regression",
    "section": "Usage",
    "text": "Usage\n \nml_aft_survival_regression( \n  x, \n  formula = NULL, \n  censor_col = \"censor\", \n  quantile_probabilities = c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99), \n  fit_intercept = TRUE, \n  max_iter = 100L, \n  tol = 1e-06, \n  aggregation_depth = 2, \n  quantiles_col = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"aft_survival_regression_\"), \n  ... \n) \n \nml_survival_regression( \n  x, \n  formula = NULL, \n  censor_col = \"censor\", \n  quantile_probabilities = c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99), \n  fit_intercept = TRUE, \n  max_iter = 100L, \n  tol = 1e-06, \n  aggregation_depth = 2, \n  quantiles_col = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"aft_survival_regression_\"), \n  response = NULL, \n  features = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#arguments",
    "title": "Spark ML – Survival Regression",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\ncensor_col\nCensor column name. The value of this column could be 0 or 1. If the value is 1, it means the event has occurred i.e. uncensored; otherwise censored.\n\n\nquantile_probabilities\nQuantile probabilities array. Values of the quantile probabilities array should be in the range (0, 1) and the array should be non-empty.\n\n\nfit_intercept\nBoolean; should the model be fit with an intercept term?\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\naggregation_depth\n(Spark 2.1.0+) Suggested depth for treeAggregate (&gt;= 2).\n\n\nquantiles_col\nQuantiles column name. This column will output quantiles of corresponding quantileProbabilities if it is set.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details.\n\n\nresponse\n(Deprecated) The name of the response column (as a length-one character vector.)\n\n\nfeatures\n(Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#details",
    "href": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#details",
    "title": "Spark ML – Survival Regression",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows. ml_survival_regression() is an alias for ml_aft_survival_regression() for backwards compatibility."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#value",
    "href": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#value",
    "title": "Spark ML – Survival Regression",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#examples",
    "title": "Spark ML – Survival Regression",
    "section": "Examples",
    "text": "Examples\n\n \n \nlibrary(survival) \nlibrary(sparklyr) \n \nsc &lt;- spark_connect(master = \"local\") \novarian_tbl &lt;- sdf_copy_to(sc, ovarian, name = \"ovarian_tbl\", overwrite = TRUE) \n \npartitions &lt;- ovarian_tbl %&gt;% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \n \novarian_training &lt;- partitions$training \novarian_test &lt;- partitions$test \n \nsur_reg &lt;- ovarian_training %&gt;% \n  ml_aft_survival_regression(futime ~ ecog_ps + rx + age + resid_ds, censor_col = \"fustat\") \n \npred &lt;- ml_predict(sur_reg, ovarian_test) \npred \n#&gt; # Source: spark&lt;?&gt; [?? x 7]\n#&gt;   futime fustat   age resid_ds    rx ecog_ps prediction\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1    115      1  74.5        2     1       1       191.\n#&gt; 2    353      1  63.2        1     2       2      1369.\n#&gt; 3    377      0  58.3        1     2       1      1751.\n#&gt; 4    475      1  59.9        2     2       2       818.\n#&gt; 5    744      0  50.1        1     2       1      2792.\n#&gt; 6    770      0  57.1        2     2       1       928."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#see-also",
    "title": "Spark ML – Survival Regression",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms. Other ml algorithms: ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans_cluster_eval.html",
    "href": "packages/sparklyr/latest/reference/ml_kmeans_cluster_eval.html",
    "title": "Evaluate a K-mean clustering",
    "section": "",
    "text": "R/ml_model_kmeans.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans_cluster_eval.html#ml_kmeans_cluster_eval",
    "href": "packages/sparklyr/latest/reference/ml_kmeans_cluster_eval.html#ml_kmeans_cluster_eval",
    "title": "Evaluate a K-mean clustering",
    "section": "ml_kmeans_cluster_eval",
    "text": "ml_kmeans_cluster_eval"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans_cluster_eval.html#description",
    "href": "packages/sparklyr/latest/reference/ml_kmeans_cluster_eval.html#description",
    "title": "Evaluate a K-mean clustering",
    "section": "Description",
    "text": "Description\nEvaluate a K-mean clustering"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans_cluster_eval.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_kmeans_cluster_eval.html#arguments",
    "title": "Evaluate a K-mean clustering",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nmodel\nA fitted K-means model returned by ml_kmeans()\n\n\ndataset\nDataset on which to calculate K-means cost"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_filter.html",
    "href": "packages/sparklyr/latest/reference/hof_filter.html",
    "title": "Filter Array Column",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_filter.html#hof_filter",
    "href": "packages/sparklyr/latest/reference/hof_filter.html#hof_filter",
    "title": "Filter Array Column",
    "section": "hof_filter",
    "text": "hof_filter"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_filter.html#description",
    "href": "packages/sparklyr/latest/reference/hof_filter.html#description",
    "title": "Filter Array Column",
    "section": "Description",
    "text": "Description\nApply an element-wise filtering function to an array column (this is essentially a dplyr wrapper for the filter(array&lt;T&gt;, function&lt;T, Boolean&gt;): array&lt;T&gt; built-in Spark SQL functions)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_filter.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_filter.html#usage",
    "title": "Filter Array Column",
    "section": "Usage",
    "text": "Usage\n \nhof_filter(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_filter.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_filter.html#arguments",
    "title": "Filter Array Column",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark data frame to filter\n\n\nfunc\nThe filtering function\n\n\nexpr\nThe array being filtered, could be any SQL expression evaluating to an array (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the filtered result (default: expr)\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_filter.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_filter.html#examples",
    "title": "Filter Array Column",
    "section": "Examples",
    "text": "Examples\n\n \n \nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\") \n# only keep odd elements in each array in `array_column` \ncopy_to(sc, tibble::tibble(array_column = list(1:5, 21:25))) %&gt;% \n  hof_filter(~ .x %% 2 == 1) \n#&gt; # Source: spark&lt;?&gt; [?? x 1]\n#&gt;   array_column\n#&gt;   &lt;list&gt;      \n#&gt; 1 &lt;dbl [3]&gt;   \n#&gt; 2 &lt;dbl [3]&gt;"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_elementwise_product.html",
    "href": "packages/sparklyr/latest/reference/ft_elementwise_product.html",
    "title": "Feature Transformation – ElementwiseProduct (Transformer)",
    "section": "",
    "text": "R/ml_feature_elementwise_product.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_elementwise_product.html#ft_elementwise_product",
    "href": "packages/sparklyr/latest/reference/ft_elementwise_product.html#ft_elementwise_product",
    "title": "Feature Transformation – ElementwiseProduct (Transformer)",
    "section": "ft_elementwise_product",
    "text": "ft_elementwise_product"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_elementwise_product.html#description",
    "href": "packages/sparklyr/latest/reference/ft_elementwise_product.html#description",
    "title": "Feature Transformation – ElementwiseProduct (Transformer)",
    "section": "Description",
    "text": "Description\nOutputs the Hadamard product (i.e., the element-wise product) of each input vector with a provided “weight” vector. In other words, it scales each column of the dataset by a scalar multiplier."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_elementwise_product.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_elementwise_product.html#usage",
    "title": "Feature Transformation – ElementwiseProduct (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_elementwise_product( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  scaling_vec = NULL, \n  uid = random_string(\"elementwise_product_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_elementwise_product.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_elementwise_product.html#arguments",
    "title": "Feature Transformation – ElementwiseProduct (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nscaling_vec\nthe vector to multiply with input vectors\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_elementwise_product.html#value",
    "href": "packages/sparklyr/latest/reference/ft_elementwise_product.html#value",
    "title": "Feature Transformation – ElementwiseProduct (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_elementwise_product.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_elementwise_product.html#see-also",
    "title": "Feature Transformation – ElementwiseProduct (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression.html",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression.html",
    "title": "Spark ML – Isotonic Regression",
    "section": "",
    "text": "R/ml_regression_isotonic_regression.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#ml_isotonic_regression",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#ml_isotonic_regression",
    "title": "Spark ML – Isotonic Regression",
    "section": "ml_isotonic_regression",
    "text": "ml_isotonic_regression"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#description",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#description",
    "title": "Spark ML – Isotonic Regression",
    "section": "Description",
    "text": "Description\nCurrently implemented using parallelized pool adjacent violators algorithm. Only univariate (single feature) algorithm supported."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#usage",
    "title": "Spark ML – Isotonic Regression",
    "section": "Usage",
    "text": "Usage\n \nml_isotonic_regression( \n  x, \n  formula = NULL, \n  feature_index = 0, \n  isotonic = TRUE, \n  weight_col = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"isotonic_regression_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#arguments",
    "title": "Spark ML – Isotonic Regression",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nfeature_index\nIndex of the feature if features_col is a vector column (default: 0), no effect otherwise.\n\n\nisotonic\nWhether the output sequence should be isotonic/increasing (true) or antitonic/decreasing (false). Default: true\n\n\nweight_col\nThe name of the column to use as weights for the model fit.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#details",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#details",
    "title": "Spark ML – Isotonic Regression",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#value",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#value",
    "title": "Spark ML – Isotonic Regression",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#examples",
    "title": "Spark ML – Isotonic Regression",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \npartitions &lt;- iris_tbl %&gt;% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \n \niris_training &lt;- partitions$training \niris_test &lt;- partitions$test \n \niso_res &lt;- iris_tbl %&gt;% \n  ml_isotonic_regression(Petal_Length ~ Petal_Width) \n \npred &lt;- ml_predict(iso_res, iris_test) \n \npred \n#&gt; # Source: spark&lt;?&gt; [?? x 6]\n#&gt;    Sepal_Length Sepal_Width Petal_…¹ Petal…² Species predi…³\n#&gt;           &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n#&gt;  1          4.4         2.9      1.4     0.2 setosa     1.4 \n#&gt;  2          4.6         3.1      1.5     0.2 setosa     1.4 \n#&gt;  3          4.6         3.4      1.4     0.3 setosa     1.52\n#&gt;  4          4.8         3        1.4     0.3 setosa     1.52\n#&gt;  5          4.9         2.4      3.3     1   versic…    3.3 \n#&gt;  6          4.9         3        1.4     0.2 setosa     1.4 \n#&gt;  7          5           3.4      1.5     0.2 setosa     1.4 \n#&gt;  8          5           3.5      1.6     0.6 setosa     1.73\n#&gt;  9          5.2         3.5      1.5     0.2 setosa     1.4 \n#&gt; 10          5.2         4.1      1.5     0.1 setosa     1.31\n#&gt; # … with more rows, and abbreviated variable names\n#&gt; #   ¹​Petal_Length, ²​Petal_Width, ³​prediction"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#see-also",
    "title": "Spark ML – Isotonic Regression",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms. Other ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_standardize_formula.html",
    "href": "packages/sparklyr/latest/reference/ml_standardize_formula.html",
    "title": "Standardize Formula Input for ml_model",
    "section": "",
    "text": "R/ml_validator_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_standardize_formula.html#ml_standardize_formula",
    "href": "packages/sparklyr/latest/reference/ml_standardize_formula.html#ml_standardize_formula",
    "title": "Standardize Formula Input for ml_model",
    "section": "ml_standardize_formula",
    "text": "ml_standardize_formula"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_standardize_formula.html#description",
    "href": "packages/sparklyr/latest/reference/ml_standardize_formula.html#description",
    "title": "Standardize Formula Input for ml_model",
    "section": "Description",
    "text": "Description\nGenerates a formula string from user inputs, to be used in ml_model constructor."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_standardize_formula.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_standardize_formula.html#usage",
    "title": "Standardize Formula Input for ml_model",
    "section": "Usage",
    "text": "Usage\nml_standardize_formula(formula = NULL, response = NULL, features = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_standardize_formula.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_standardize_formula.html#arguments",
    "title": "Standardize Formula Input for ml_model",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nformula\nThe formula argument.\n\n\nresponse\nThe response argument.\n\n\nfeatures\nThe features argument."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_zip_with.html",
    "href": "packages/sparklyr/latest/reference/hof_map_zip_with.html",
    "title": "Merges two maps into one",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_zip_with.html#hof_map_zip_with",
    "href": "packages/sparklyr/latest/reference/hof_map_zip_with.html#hof_map_zip_with",
    "title": "Merges two maps into one",
    "section": "hof_map_zip_with",
    "text": "hof_map_zip_with"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_zip_with.html#description",
    "href": "packages/sparklyr/latest/reference/hof_map_zip_with.html#description",
    "title": "Merges two maps into one",
    "section": "Description",
    "text": "Description\nMerges two maps into a single map by applying the function specified to pairs of values with the same key (this is essentially a dplyr wrapper to the map_zip_with(map1, map2, func) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_zip_with.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_map_zip_with.html#usage",
    "title": "Merges two maps into one",
    "section": "Usage",
    "text": "Usage\n \nhof_map_zip_with(x, func, dest_col = NULL, map1 = NULL, map2 = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_zip_with.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_map_zip_with.html#arguments",
    "title": "Merges two maps into one",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\nfunc\nThe function to apply (it should take (key, value1, value2) as arguments, where (key, value1) is a key-value pair present in map1, (key, value2) is a key-value pair present in map2, and return a transformed value associated with key in the resulting map\n\n\ndest_col\nColumn to store the query result (default: the last column of the Spark data frame)\n\n\nmap1\nThe first map being merged, could be any SQL expression evaluating to a map (default: the first column of the Spark data frame)\n\n\nmap2\nThe second map being merged, could be any SQL expression evaluating to a map (default: the second column of the Spark data frame)\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_zip_with.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_map_zip_with.html#examples",
    "title": "Merges two maps into one",
    "section": "Examples",
    "text": "Examples\n\n \n \nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\", version = \"3.0.0\") \n \n# create a Spark dataframe with 2 columns of type MAP&lt;STRING, INT&gt; \ntwo_maps_tbl &lt;- sdf_copy_to( \n  sc, \n  tibble::tibble( \n    m1 = c(\"{\\\"1\\\":2,\\\"3\\\":4,\\\"5\\\":6}\", \"{\\\"2\\\":1,\\\"4\\\":3,\\\"6\\\":5}\"), \n    m2 = c(\"{\\\"1\\\":1,\\\"3\\\":3,\\\"5\\\":5}\", \"{\\\"2\\\":2,\\\"4\\\":4,\\\"6\\\":6}\") \n  ), \n  overwrite = TRUE \n) %&gt;% \n  dplyr::mutate(m1 = from_json(m1, \"MAP&lt;STRING, INT&gt;\"), \n                m2 = from_json(m2, \"MAP&lt;STRING, INT&gt;\")) \n \n# create a 3rd column containing MAP&lt;STRING, INT&gt; values derived from the \n# first 2 columns \n \ntransformed_two_maps_tbl &lt;- two_maps_tbl %&gt;% \n  hof_map_zip_with( \n    func = .(k, v1, v2) %-&gt;% (CONCAT(k, \"_\", v1, \"_\", v2)), \n    dest_col = m3 \n  )"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans.html",
    "href": "packages/sparklyr/latest/reference/ml_kmeans.html",
    "title": "Spark ML – K-Means Clustering",
    "section": "",
    "text": "R/ml_clustering_kmeans.R, R/ml_model_kmeans.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans.html#ml_kmeans",
    "href": "packages/sparklyr/latest/reference/ml_kmeans.html#ml_kmeans",
    "title": "Spark ML – K-Means Clustering",
    "section": "ml_kmeans",
    "text": "ml_kmeans"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans.html#description",
    "href": "packages/sparklyr/latest/reference/ml_kmeans.html#description",
    "title": "Spark ML – K-Means Clustering",
    "section": "Description",
    "text": "Description\nK-means clustering with support for k-means|| initialization proposed by Bahmani et al. Using ml_kmeans() with the formula interface requires Spark 2.0+."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_kmeans.html#usage",
    "title": "Spark ML – K-Means Clustering",
    "section": "Usage",
    "text": "Usage\n \nml_kmeans( \n  x, \n  formula = NULL, \n  k = 2, \n  max_iter = 20, \n  tol = 1e-04, \n  init_steps = 2, \n  init_mode = \"k-means||\", \n  seed = NULL, \n  features_col = \"features\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"kmeans_\"), \n  ... \n) \n \nml_compute_cost(model, dataset) \n \nml_compute_silhouette_measure( \n  model, \n  dataset, \n  distance_measure = c(\"squaredEuclidean\", \"cosine\") \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_kmeans.html#arguments",
    "title": "Spark ML – K-Means Clustering",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nk\nThe number of clusters to create\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\ninit_steps\nNumber of steps for the k-means\n\n\ninit_mode\nInitialization algorithm. This can be either “random” to choose random points as initial cluster centers, or “k-means\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments, see Details.\n\n\nmodel\nA fitted K-means model returned by ml_kmeans()\n\n\ndataset\nDataset on which to calculate K-means cost\n\n\ndistance_measure\nDistance measure to apply when computing the Silhouette measure."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans.html#value",
    "href": "packages/sparklyr/latest/reference/ml_kmeans.html#value",
    "title": "Spark ML – K-Means Clustering",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the clustering estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, an estimator is constructed then immediately fit with the input tbl_spark, returning a clustering model.\ntbl_spark, with formula or features specified: When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the estimator. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model. This signature does not apply to ml_lda().\nml_compute_cost() returns the K-means cost (sum of squared distances of points to their nearest center) for the model on the given data. ml_compute_silhouette_measure() returns the Silhouette measure of the clustering on the given data."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_kmeans.html#examples",
    "title": "Spark ML – K-Means Clustering",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \nml_kmeans(iris_tbl, Species ~ .) \n#&gt; K-means clustering with 2 clusters\n#&gt; \n#&gt; Cluster centers:\n#&gt;   Sepal_Length Sepal_Width Petal_Length Petal_Width\n#&gt; 1     6.301031    2.886598     4.958763    1.695876\n#&gt; 2     5.005660    3.369811     1.560377    0.290566\n#&gt; \n#&gt; Within Set Sum of Squared Errors =  not computed."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_kmeans.html#see-also",
    "title": "Spark ML – K-Means Clustering",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-clustering.html for more information on the set of clustering algorithms. Other ml clustering algorithms: ml_bisecting_kmeans(), ml_gaussian_mixture(), ml_lda()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_quantile.html",
    "href": "packages/sparklyr/latest/reference/sdf_quantile.html",
    "title": "Compute (Approximate) Quantiles with a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_quantile.html#sdf_quantile",
    "href": "packages/sparklyr/latest/reference/sdf_quantile.html#sdf_quantile",
    "title": "Compute (Approximate) Quantiles with a Spark DataFrame",
    "section": "sdf_quantile",
    "text": "sdf_quantile"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_quantile.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_quantile.html#description",
    "title": "Compute (Approximate) Quantiles with a Spark DataFrame",
    "section": "Description",
    "text": "Description\nGiven a numeric column within a Spark DataFrame, compute approximate quantiles."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_quantile.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_quantile.html#usage",
    "title": "Compute (Approximate) Quantiles with a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_quantile( \n  x, \n  column, \n  probabilities = c(0, 0.25, 0.5, 0.75, 1), \n  relative.error = 1e-05, \n  weight.column = NULL \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_quantile.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_quantile.html#arguments",
    "title": "Compute (Approximate) Quantiles with a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ncolumn\nThe column(s) for which quantiles should be computed. Multiple columns are only supported in Spark 2.0+.\n\n\nprobabilities\nA numeric vector of probabilities, for which quantiles should be computed.\n\n\nrelative.error\nThe maximal possible difference between the actual percentile of a result and its expected percentile (e.g., if relative.error is 0.01 and probabilities is 0.95, then any value between the 94th and 96th percentile will be considered an acceptable approximation).\n\n\nweight.column\nIf not NULL, then a generalized version of the Greenwald- Khanna algorithm will be run to compute weighted percentiles, with each sample from column having a relative weight specified by the corresponding value in weight.column. The weights can be considered as relative frequencies of sample data points."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh.html",
    "href": "packages/sparklyr/latest/reference/ft_lsh.html",
    "title": "Feature Transformation – LSH (Estimator)",
    "section": "",
    "text": "R/ml_feature_bucketed_random_projection_lsh.R,"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh.html#ft_lsh",
    "href": "packages/sparklyr/latest/reference/ft_lsh.html#ft_lsh",
    "title": "Feature Transformation – LSH (Estimator)",
    "section": "ft_lsh",
    "text": "ft_lsh"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh.html#description",
    "href": "packages/sparklyr/latest/reference/ft_lsh.html#description",
    "title": "Feature Transformation – LSH (Estimator)",
    "section": "Description",
    "text": "Description\nLocality Sensitive Hashing functions for Euclidean distance (Bucketed Random Projection) and Jaccard distance (MinHash)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_lsh.html#usage",
    "title": "Feature Transformation – LSH (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_bucketed_random_projection_lsh( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  bucket_length = NULL, \n  num_hash_tables = 1, \n  seed = NULL, \n  uid = random_string(\"bucketed_random_projection_lsh_\"), \n  ... \n) \n\nft_minhash_lsh( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  num_hash_tables = 1L, \n  seed = NULL, \n  uid = random_string(\"minhash_lsh_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_lsh.html#arguments",
    "title": "Feature Transformation – LSH (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nbucket_length\nThe length of each hash bucket, a larger bucket lowers the false negative rate. The number of buckets will be (max L2 norm of input vectors) / bucketLength.\n\n\nnum_hash_tables\nNumber of hash tables used in LSH OR-amplification. LSH OR-amplification can be used to reduce the false negative rate. Higher values for this param lead to a reduced false negative rate, at the expense of added computational complexity.\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh.html#details",
    "href": "packages/sparklyr/latest/reference/ft_lsh.html#details",
    "title": "Feature Transformation – LSH (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh.html#value",
    "href": "packages/sparklyr/latest/reference/ft_lsh.html#value",
    "title": "Feature Transformation – LSH (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_lsh.html#see-also",
    "title": "Feature Transformation – LSH (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nft_lsh_utils\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/right_join.html",
    "href": "packages/sparklyr/latest/reference/right_join.html",
    "title": "Right join",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/right_join.html#right_join",
    "href": "packages/sparklyr/latest/reference/right_join.html#right_join",
    "title": "Right join",
    "section": "right_join",
    "text": "right_join"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/right_join.html#description",
    "href": "packages/sparklyr/latest/reference/right_join.html#description",
    "title": "Right join",
    "section": "Description",
    "text": "Description\nSee right_join for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_random_forest.html",
    "href": "packages/sparklyr/latest/reference/ml_random_forest.html",
    "title": "Spark ML – Random Forest",
    "section": "",
    "text": "R/ml_classification_random_forest_classifier.R,"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_random_forest.html#ml_random_forest_classifier",
    "href": "packages/sparklyr/latest/reference/ml_random_forest.html#ml_random_forest_classifier",
    "title": "Spark ML – Random Forest",
    "section": "ml_random_forest_classifier",
    "text": "ml_random_forest_classifier"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_random_forest.html#description",
    "href": "packages/sparklyr/latest/reference/ml_random_forest.html#description",
    "title": "Spark ML – Random Forest",
    "section": "Description",
    "text": "Description\nPerform classification and regression using random forests."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_random_forest.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_random_forest.html#usage",
    "title": "Spark ML – Random Forest",
    "section": "Usage",
    "text": "Usage\n \nml_random_forest_classifier( \n  x, \n  formula = NULL, \n  num_trees = 20, \n  subsampling_rate = 1, \n  max_depth = 5, \n  min_instances_per_node = 1, \n  feature_subset_strategy = \"auto\", \n  impurity = \"gini\", \n  min_info_gain = 0, \n  max_bins = 32, \n  seed = NULL, \n  thresholds = NULL, \n  checkpoint_interval = 10, \n  cache_node_ids = FALSE, \n  max_memory_in_mb = 256, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  uid = random_string(\"random_forest_classifier_\"), \n  ... \n) \n \nml_random_forest( \n  x, \n  formula = NULL, \n  type = c(\"auto\", \"regression\", \"classification\"), \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  feature_subset_strategy = \"auto\", \n  impurity = \"auto\", \n  checkpoint_interval = 10, \n  max_bins = 32, \n  max_depth = 5, \n  num_trees = 20, \n  min_info_gain = 0, \n  min_instances_per_node = 1, \n  subsampling_rate = 1, \n  seed = NULL, \n  thresholds = NULL, \n  cache_node_ids = FALSE, \n  max_memory_in_mb = 256, \n  uid = random_string(\"random_forest_\"), \n  response = NULL, \n  features = NULL, \n  ... \n) \n \nml_random_forest_regressor( \n  x, \n  formula = NULL, \n  num_trees = 20, \n  subsampling_rate = 1, \n  max_depth = 5, \n  min_instances_per_node = 1, \n  feature_subset_strategy = \"auto\", \n  impurity = \"variance\", \n  min_info_gain = 0, \n  max_bins = 32, \n  seed = NULL, \n  checkpoint_interval = 10, \n  cache_node_ids = FALSE, \n  max_memory_in_mb = 256, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"random_forest_regressor_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_random_forest.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_random_forest.html#arguments",
    "title": "Spark ML – Random Forest",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nnum_trees\nNumber of trees to train (&gt;= 1). If 1, then no bootstrapping is used. If &gt; 1, then bootstrapping is done.\n\n\nsubsampling_rate\nFraction of the training data used for learning each decision tree, in range (0, 1]. (default = 1.0)\n\n\nmax_depth\nMaximum depth of the tree (&gt;= 0); that is, the maximum number of nodes separating any leaves from the root of the tree.\n\n\nmin_instances_per_node\nMinimum number of instances each child must have after split.\n\n\nfeature_subset_strategy\nThe number of features to consider for splits at each tree node. See details for options.\n\n\nimpurity\nCriterion used for information gain calculation. Supported: “entropy” and “gini” (default) for classification and “variance” (default) for regression. For ml_decision_tree, setting \"auto\" will default to the appropriate criterion based on model type.\n\n\nmin_info_gain\nMinimum information gain for a split to be considered at a tree node. Should be &gt;= 0, defaults to 0.\n\n\nmax_bins\nThe maximum number of bins used for discretizing continuous features and for choosing how to split on features at each node. More bins give higher granularity.\n\n\nseed\nSeed for random numbers.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\ncheckpoint_interval\nSet checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.\n\n\ncache_node_ids\nIf FALSE, the algorithm will pass trees to executors to match instances with nodes. If TRUE, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Defaults to FALSE.\n\n\nmax_memory_in_mb\nMaximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. Defaults to 256.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details.\n\n\ntype\nThe type of model to fit. \"regression\" treats the response as a continuous variable, while \"classification\" treats the response as a categorical variable. When \"auto\" is used, the model type is inferred based on the response variable type – if it is a numeric type, then regression is used; classification otherwise.\n\n\nresponse\n(Deprecated) The name of the response column (as a length-one character vector.)\n\n\nfeatures\n(Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_random_forest.html#details",
    "href": "packages/sparklyr/latest/reference/ml_random_forest.html#details",
    "title": "Spark ML – Random Forest",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows. The supported options for feature_subset_strategy are\n\n\"auto\": Choose automatically for task: If num_trees == 1, set to \"all\". If num_trees &gt; 1 (forest), set to \"sqrt\" for classification and to \"onethird\" for regression.\n\"all\": use all features\n\"onethird\": use 1/3 of the features\n\"sqrt\": use use sqrt(number of features)\n\"log2\": use log2(number of features)\n\"n\": when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features. (default = \"auto\")\nml_random_forest is a wrapper around ml_random_forest_regressor.tbl_spark and ml_random_forest_classifier.tbl_spark and calls the appropriate method based on model type."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_random_forest.html#value",
    "href": "packages/sparklyr/latest/reference/ml_random_forest.html#value",
    "title": "Spark ML – Random Forest",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_random_forest.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_random_forest.html#examples",
    "title": "Spark ML – Random Forest",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \npartitions &lt;- iris_tbl %&gt;% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \n \niris_training &lt;- partitions$training \niris_test &lt;- partitions$test \n \nrf_model &lt;- iris_training %&gt;% \n  ml_random_forest(Species ~ ., type = \"classification\") \n \npred &lt;- ml_predict(rf_model, iris_test) \n \nml_multiclass_classification_evaluator(pred) \n#&gt; [1] 0.9695056"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_random_forest.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_random_forest.html#see-also",
    "title": "Spark ML – Random Forest",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms. Other ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_orc.html",
    "href": "packages/sparklyr/latest/reference/stream_read_orc.html",
    "title": "Read ORC Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_orc.html#stream_read_orc",
    "href": "packages/sparklyr/latest/reference/stream_read_orc.html#stream_read_orc",
    "title": "Read ORC Stream",
    "section": "stream_read_orc",
    "text": "stream_read_orc"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_orc.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_orc.html#description",
    "title": "Read ORC Stream",
    "section": "Description",
    "text": "Description\nReads an ORC stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_orc.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_orc.html#usage",
    "title": "Read ORC Stream",
    "section": "Usage",
    "text": "Usage\nstream_read_orc(sc, path, name = NULL, columns = NULL, options = list(), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_orc.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_orc.html#arguments",
    "title": "Read ORC Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_orc.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_orc.html#examples",
    "title": "Read ORC Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\") \nsdf_len(sc, 10) %&gt;% spark_write_orc(\"orc-in\") \nstream &lt;- stream_read_orc(sc, \"orc-in\") %&gt;% stream_write_orc(\"orc-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_orc.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_orc.html#see-also",
    "title": "Read ORC Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_config.html",
    "href": "packages/sparklyr/latest/reference/livy_config.html",
    "title": "Create a Spark Configuration for Livy",
    "section": "",
    "text": "R/livy_connection.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_config.html#livy_config",
    "href": "packages/sparklyr/latest/reference/livy_config.html#livy_config",
    "title": "Create a Spark Configuration for Livy",
    "section": "livy_config",
    "text": "livy_config"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_config.html#description",
    "href": "packages/sparklyr/latest/reference/livy_config.html#description",
    "title": "Create a Spark Configuration for Livy",
    "section": "Description",
    "text": "Description\nCreate a Spark Configuration for Livy"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_config.html#usage",
    "href": "packages/sparklyr/latest/reference/livy_config.html#usage",
    "title": "Create a Spark Configuration for Livy",
    "section": "Usage",
    "text": "Usage\nlivy_config( \n  config = spark_config(), \n  username = NULL, \n  password = NULL, \n  negotiate = FALSE, \n  custom_headers = list(`X-Requested-By` = \"sparklyr\"), \n  proxy = NULL, \n  curl_opts = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_config.html#arguments",
    "href": "packages/sparklyr/latest/reference/livy_config.html#arguments",
    "title": "Create a Spark Configuration for Livy",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nconfig\nOptional base configuration\n\n\nusername\nThe username to use in the Authorization header\n\n\npassword\nThe password to use in the Authorization header\n\n\nnegotiate\nWhether to use gssnegotiate method or not\n\n\ncustom_headers\nList of custom headers to append to http requests. Defaults to list(\"X-Requested-By\" = \"sparklyr\").\n\n\nproxy\nEither NULL or a proxy specified by httr::use_proxy(). Defaults to NULL.\n\n\ncurl_opts\nList of CURL options (e.g., verbose, connecttimeout, dns_cache_timeout, etc, see httr::httr_options() for a list of valid options) – NOTE: these configurations are for libcurl only and separate from HTTP headers or Livy session parameters.\n\n\n…\nadditional Livy session parameters"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_config.html#details",
    "href": "packages/sparklyr/latest/reference/livy_config.html#details",
    "title": "Create a Spark Configuration for Livy",
    "section": "Details",
    "text": "Details\nExtends a Spark spark_config() configuration with settings for Livy. For instance, username and password\ndefine the basic authentication settings for a Livy session.\nThe default value of \"custom_headers\" is set to list(\"X-Requested-By\" = \"sparklyr\")\nin order to facilitate connection to Livy servers with CSRF protection enabled.\nAdditional parameters for Livy sessions are:\nproxy_user\nUser to impersonate when starting the session\njars\njars to be used in this session\npy_files\nPython files to be used in this session\nfiles\nfiles to be used in this session\ndriver_memory\nAmount of memory to use for the driver process\ndriver_cores\nNumber of cores to use for the driver process\nexecutor_memory\nAmount of memory to use per executor process\nexecutor_cores\nNumber of cores to use for each executor\nnum_executors\nNumber of executors to launch for this session\narchives\nArchives to be used in this session\nqueue\nThe name of the YARN queue to which submitted\nname\nThe name of this session\nheartbeat_timeout\nTimeout in seconds to which session be orphaned\nconf\nSpark configuration properties (Map of key=value)\nNote that queue is supported only by version 0.4.0 of Livy or newer. If you are using the older one, specify queue via config (e.g. config = spark_config(spark.yarn.queue = \"my_queue\"))."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_config.html#value",
    "href": "packages/sparklyr/latest/reference/livy_config.html#value",
    "title": "Create a Spark Configuration for Livy",
    "section": "Value",
    "text": "Value\nNamed list with configuration data"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html",
    "href": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html",
    "title": "Retrieves or sets status of Spark AQE",
    "section": "",
    "text": "R/spark_context_config.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#spark_adaptive_query_execution",
    "href": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#spark_adaptive_query_execution",
    "title": "Retrieves or sets status of Spark AQE",
    "section": "spark_adaptive_query_execution",
    "text": "spark_adaptive_query_execution"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#description",
    "href": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#description",
    "title": "Retrieves or sets status of Spark AQE",
    "section": "Description",
    "text": "Description\nRetrieves or sets whether Spark adaptive query execution is enabled"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#usage",
    "title": "Retrieves or sets status of Spark AQE",
    "section": "Usage",
    "text": "Usage\nspark_adaptive_query_execution(sc, enable = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#arguments",
    "title": "Retrieves or sets status of Spark AQE",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nenable\nWhether to enable Spark adaptive query execution. Defaults to NULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#see-also",
    "title": "Retrieves or sets status of Spark AQE",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_advisory_shuffle_partition_size(), spark_auto_broadcast_join_threshold(), spark_coalesce_initial_num_partitions(), spark_coalesce_min_num_partitions(), spark_coalesce_shuffle_partitions(), spark_session_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/left_join.html",
    "href": "packages/sparklyr/latest/reference/left_join.html",
    "title": "Left join",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/left_join.html#left_join",
    "href": "packages/sparklyr/latest/reference/left_join.html#left_join",
    "title": "Left join",
    "section": "left_join",
    "text": "left_join"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/left_join.html#description",
    "href": "packages/sparklyr/latest/reference/left_join.html#description",
    "title": "Left join",
    "section": "Description",
    "text": "Description\nSee left_join for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_statistical_routines.html",
    "href": "packages/sparklyr/latest/reference/spark_statistical_routines.html",
    "title": "Generate random samples from some distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_statistical_routines.html#spark_statistical_routines",
    "href": "packages/sparklyr/latest/reference/spark_statistical_routines.html#spark_statistical_routines",
    "title": "Generate random samples from some distribution",
    "section": "spark_statistical_routines",
    "text": "spark_statistical_routines"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_statistical_routines.html#description",
    "href": "packages/sparklyr/latest/reference/spark_statistical_routines.html#description",
    "title": "Generate random samples from some distribution",
    "section": "Description",
    "text": "Description\nGenerator methods for creating single-column Spark dataframes comprised of i.i.d. samples from some distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_statistical_routines.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_statistical_routines.html#arguments",
    "title": "Generate random samples from some distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_interaction.html",
    "href": "packages/sparklyr/latest/reference/ft_interaction.html",
    "title": "Feature Transformation – Interaction (Transformer)",
    "section": "",
    "text": "R/ml_feature_interaction.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_interaction.html#ft_interaction",
    "href": "packages/sparklyr/latest/reference/ft_interaction.html#ft_interaction",
    "title": "Feature Transformation – Interaction (Transformer)",
    "section": "ft_interaction",
    "text": "ft_interaction"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_interaction.html#description",
    "href": "packages/sparklyr/latest/reference/ft_interaction.html#description",
    "title": "Feature Transformation – Interaction (Transformer)",
    "section": "Description",
    "text": "Description\nImplements the feature interaction transform. This transformer takes in Double and Vector type columns and outputs a flattened vector of their feature interactions. To handle interaction, we first one-hot encode any nominal features. Then, a vector of the feature cross-products is produced."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_interaction.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_interaction.html#usage",
    "title": "Feature Transformation – Interaction (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_interaction( \n  x, \n  input_cols = NULL, \n  output_col = NULL, \n  uid = random_string(\"interaction_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_interaction.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_interaction.html#arguments",
    "title": "Feature Transformation – Interaction (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_cols\nThe names of the input columns\n\n\noutput_col\nThe name of the output column.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_interaction.html#value",
    "href": "packages/sparklyr/latest/reference/ft_interaction.html#value",
    "title": "Feature Transformation – Interaction (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_interaction.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_interaction.html#see-also",
    "title": "Feature Transformation – Interaction (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_pca.html",
    "href": "packages/sparklyr/latest/reference/ft_pca.html",
    "title": "Feature Transformation – PCA (Estimator)",
    "section": "",
    "text": "R/ml_feature_pca.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_pca.html#ft_pca",
    "href": "packages/sparklyr/latest/reference/ft_pca.html#ft_pca",
    "title": "Feature Transformation – PCA (Estimator)",
    "section": "ft_pca",
    "text": "ft_pca"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_pca.html#description",
    "href": "packages/sparklyr/latest/reference/ft_pca.html#description",
    "title": "Feature Transformation – PCA (Estimator)",
    "section": "Description",
    "text": "Description\nPCA trains a model to project vectors to a lower dimensional space of the top k principal components."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_pca.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_pca.html#usage",
    "title": "Feature Transformation – PCA (Estimator)",
    "section": "Usage",
    "text": "Usage\n \nft_pca( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  k = NULL, \n  uid = random_string(\"pca_\"), \n  ... \n) \n \nml_pca(x, features = tbl_vars(x), k = length(features), pc_prefix = \"PC\", ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_pca.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_pca.html#arguments",
    "title": "Feature Transformation – PCA (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nk\nThe number of principal components\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused.\n\n\nfeatures\nThe columns to use in the principal components analysis. Defaults to all columns in x.\n\n\npc_prefix\nLength-one character vector used to prepend names of components."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_pca.html#details",
    "href": "packages/sparklyr/latest/reference/ft_pca.html#details",
    "title": "Feature Transformation – PCA (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark. ml_pca() is a wrapper around ft_pca() that returns a ml_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_pca.html#value",
    "href": "packages/sparklyr/latest/reference/ft_pca.html#value",
    "title": "Feature Transformation – PCA (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_pca.html#examples",
    "href": "packages/sparklyr/latest/reference/ft_pca.html#examples",
    "title": "Feature Transformation – PCA (Estimator)",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nlibrary(dplyr) \n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \niris_tbl %&gt;% \n  select(-Species) %&gt;% \n  ml_pca(k = 2) \n#&gt; Explained variance:\n#&gt; \n#&gt;        PC1        PC2 \n#&gt; 0.92461872 0.05306648 \n#&gt; \n#&gt; Rotation:\n#&gt;                      PC1         PC2\n#&gt; Sepal_Length -0.36138659 -0.65658877\n#&gt; Sepal_Width   0.08452251 -0.73016143\n#&gt; Petal_Length -0.85667061  0.17337266\n#&gt; Petal_Width  -0.35828920  0.07548102"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_pca.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_pca.html#see-also",
    "title": "Feature Transformation – PCA (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark. Other feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/register_extension.html",
    "href": "packages/sparklyr/latest/reference/register_extension.html",
    "title": "Register a Package that Implements a Spark Extension",
    "section": "",
    "text": "R/spark_extensions.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/register_extension.html#register_extension",
    "href": "packages/sparklyr/latest/reference/register_extension.html#register_extension",
    "title": "Register a Package that Implements a Spark Extension",
    "section": "register_extension",
    "text": "register_extension"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/register_extension.html#description",
    "href": "packages/sparklyr/latest/reference/register_extension.html#description",
    "title": "Register a Package that Implements a Spark Extension",
    "section": "Description",
    "text": "Description\nRegistering an extension package will result in the package being automatically scanned for spark dependencies when a connection to Spark is created."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/register_extension.html#usage",
    "href": "packages/sparklyr/latest/reference/register_extension.html#usage",
    "title": "Register a Package that Implements a Spark Extension",
    "section": "Usage",
    "text": "Usage\nregister_extension(package) \n\nregistered_extensions()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/register_extension.html#arguments",
    "href": "packages/sparklyr/latest/reference/register_extension.html#arguments",
    "title": "Register a Package that Implements a Spark Extension",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\npackage\nThe package(s) to register."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/register_extension.html#note",
    "href": "packages/sparklyr/latest/reference/register_extension.html#note",
    "title": "Register a Package that Implements a Spark Extension",
    "section": "Note",
    "text": "Note\nPackages should typically register their extensions in their .onLoad hook – this ensures that their extensions are registered when their namespaces are loaded."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_context_config.html",
    "href": "packages/sparklyr/latest/reference/spark_context_config.html",
    "title": "Runtime configuration interface for the Spark Context.",
    "section": "",
    "text": "R/spark_context_config.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_context_config.html#spark_context_config",
    "href": "packages/sparklyr/latest/reference/spark_context_config.html#spark_context_config",
    "title": "Runtime configuration interface for the Spark Context.",
    "section": "spark_context_config",
    "text": "spark_context_config"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_context_config.html#description",
    "href": "packages/sparklyr/latest/reference/spark_context_config.html#description",
    "title": "Runtime configuration interface for the Spark Context.",
    "section": "Description",
    "text": "Description\nRetrieves the runtime configuration interface for the Spark Context."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_context_config.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_context_config.html#usage",
    "title": "Runtime configuration interface for the Spark Context.",
    "section": "Usage",
    "text": "Usage\nspark_context_config(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_context_config.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_context_config.html#arguments",
    "title": "Runtime configuration interface for the Spark Context.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html",
    "title": "Feature Transformation – OneHotEncoderEstimator (Estimator)",
    "section": "",
    "text": "R/ml_feature_one_hot_encoder_estimator.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#ft_one_hot_encoder_estimator",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#ft_one_hot_encoder_estimator",
    "title": "Feature Transformation – OneHotEncoderEstimator (Estimator)",
    "section": "ft_one_hot_encoder_estimator",
    "text": "ft_one_hot_encoder_estimator"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#description",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#description",
    "title": "Feature Transformation – OneHotEncoderEstimator (Estimator)",
    "section": "Description",
    "text": "Description\nA one-hot encoder that maps a column of category indices to a column of binary vectors, with at most a single one-value per row that indicates the input category index. For example with 5 categories, an input value of 2.0 would map to an output vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included by default (configurable via dropLast), because it makes the vector entries sum up to one, and hence linearly dependent. So an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0]."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#usage",
    "title": "Feature Transformation – OneHotEncoderEstimator (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_one_hot_encoder_estimator( \n  x, \n  input_cols = NULL, \n  output_cols = NULL, \n  handle_invalid = \"error\", \n  drop_last = TRUE, \n  uid = random_string(\"one_hot_encoder_estimator_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#arguments",
    "title": "Feature Transformation – OneHotEncoderEstimator (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_cols\nNames of input columns.\n\n\noutput_cols\nNames of output columns.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error”\n\n\ndrop_last\nWhether to drop the last category. Defaults to TRUE.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#details",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#details",
    "title": "Feature Transformation – OneHotEncoderEstimator (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#value",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#value",
    "title": "Feature Transformation – OneHotEncoderEstimator (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#see-also",
    "title": "Feature Transformation – OneHotEncoderEstimator (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_avro.html",
    "href": "packages/sparklyr/latest/reference/spark_read_avro.html",
    "title": "Read Apache Avro data into a Spark DataFrame.",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_avro.html#spark_read_avro",
    "href": "packages/sparklyr/latest/reference/spark_read_avro.html#spark_read_avro",
    "title": "Read Apache Avro data into a Spark DataFrame.",
    "section": "spark_read_avro",
    "text": "spark_read_avro"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_avro.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_avro.html#description",
    "title": "Read Apache Avro data into a Spark DataFrame.",
    "section": "Description",
    "text": "Description\nRead Apache Avro data into a Spark DataFrame. Notice this functionality requires the Spark connection sc to be instantiated with either an explicitly specified Spark version (i.e., spark_connect(..., version = &lt;version&gt;, packages = c(\"avro\", &lt;other package(s)&gt;), ...)) or a specific version of Spark avro package to use (e.g., spark_connect(..., packages = c(\"org.apache.spark:spark-avro_2.12:3.0.0\", &lt;other package(s)&gt;), ...))."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_avro.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_avro.html#usage",
    "title": "Read Apache Avro data into a Spark DataFrame.",
    "section": "Usage",
    "text": "Usage\nspark_read_avro( \n  sc, \n  name = NULL, \n  path = name, \n  avro_schema = NULL, \n  ignore_extension = TRUE, \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_avro.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_avro.html#arguments",
    "title": "Read Apache Avro data into a Spark DataFrame.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\navro_schema\nOptional Avro schema in JSON format\n\n\nignore_extension\nIf enabled, all files with and without .avro extension are loaded (default: TRUE)\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_avro.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_avro.html#see-also",
    "title": "Read Apache Avro data into a Spark DataFrame.",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_expand_grid.html",
    "href": "packages/sparklyr/latest/reference/sdf_expand_grid.html",
    "title": "Create a Spark dataframe containing all combinations of inputs",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_expand_grid.html#sdf_expand_grid",
    "href": "packages/sparklyr/latest/reference/sdf_expand_grid.html#sdf_expand_grid",
    "title": "Create a Spark dataframe containing all combinations of inputs",
    "section": "sdf_expand_grid",
    "text": "sdf_expand_grid"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_expand_grid.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_expand_grid.html#description",
    "title": "Create a Spark dataframe containing all combinations of inputs",
    "section": "Description",
    "text": "Description\nGiven one or more R vectors/factors or single-column Spark dataframes, perform an expand.grid operation on all of them and store the result in a Spark dataframe"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_expand_grid.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_expand_grid.html#usage",
    "title": "Create a Spark dataframe containing all combinations of inputs",
    "section": "Usage",
    "text": "Usage\n \nsdf_expand_grid( \n  sc, \n  ..., \n  broadcast_vars = NULL, \n  memory = TRUE, \n  repartition = NULL, \n  partition_by = NULL \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_expand_grid.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_expand_grid.html#arguments",
    "title": "Create a Spark dataframe containing all combinations of inputs",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\n…\nEach input variable can be either a R vector/factor or a Spark dataframe. Unnamed inputs will assume the default names of ‘Var1’, ‘Var2’, etc in the result, similar to what expand.grid does for unnamed inputs.\n\n\nbroadcast_vars\nIndicates which input(s) should be broadcasted to all nodes of the Spark cluster during the join process (default: none).\n\n\nmemory\nBoolean; whether the resulting Spark dataframe should be cached into memory (default: TRUE)\n\n\nrepartition\nNumber of partitions the resulting Spark dataframe should have\n\n\npartition_by\nVector of column names used for partitioning the resulting Spark dataframe, only supported for Spark 2.0+"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_expand_grid.html#examples",
    "href": "packages/sparklyr/latest/reference/sdf_expand_grid.html#examples",
    "title": "Create a Spark dataframe containing all combinations of inputs",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \ngrid_sdf &lt;- sdf_expand_grid(sc, seq(5), rnorm(10), letters)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html",
    "href": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html",
    "title": "Infix operator for composing a lambda expression",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#section",
    "href": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#section",
    "title": "Infix operator for composing a lambda expression",
    "section": "%->%",
    "text": "%-&gt;%"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#description",
    "href": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#description",
    "title": "Infix operator for composing a lambda expression",
    "section": "Description",
    "text": "Description\nInfix operator that allows a lambda expression to be composed in R and be translated to Spark SQL equivalent using ’ dbplyr::translate_sql functionalities"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#usage",
    "href": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#usage",
    "title": "Infix operator for composing a lambda expression",
    "section": "Usage",
    "text": "Usage\n \nparams %-&gt;% ..."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#arguments",
    "href": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#arguments",
    "title": "Infix operator for composing a lambda expression",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nparams\nParameter(s) of the lambda expression, can be either a single parameter or a comma separated listed of parameters in the form of .(param1, param2, ... ) (see examples)\n\n\n…\nBody of the lambda expression, must be within parentheses"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#details",
    "href": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#details",
    "title": "Infix operator for composing a lambda expression",
    "section": "Details",
    "text": "Details\nNotice when composing a lambda expression in R, the body of the lambda expression must always be surrounded with parentheses, otherwise a parsing error will occur."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#examples",
    "href": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#examples",
    "title": "Infix operator for composing a lambda expression",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \n \na %-&gt;% (mean(a) + 1) # translates to &lt;SQL&gt; `a` -&gt; (AVG(`a`) OVER () + 1.0) \n#&gt; Warning: Missing values are always removed in SQL aggregation functions.\n#&gt; Use `na.rm = TRUE` to silence this warning\n#&gt; This warning is displayed once every 8 hours.\n#&gt; &lt;SQL&gt; a -&gt; (AVG(`a`) OVER () + 1.0)\n \n.(a, b) %-&gt;% (a &lt; 1 && b &gt; 1) # translates to &lt;SQL&gt; `a`,`b` -&gt; (`a` &lt; 1.0 AND `b` &gt; 1.0) \n#&gt; &lt;SQL&gt; (a, b) -&gt; (`a` &lt; 1.0 AND `b` &gt; 1.0)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_longer.html",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_longer.html",
    "title": "Unnest longer",
    "section": "",
    "text": "R/sdf_unnest_longer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#sdf_unnest_longer",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#sdf_unnest_longer",
    "title": "Unnest longer",
    "section": "sdf_unnest_longer",
    "text": "sdf_unnest_longer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#description",
    "title": "Unnest longer",
    "section": "Description",
    "text": "Description\nExpand a struct column or an array column within a Spark dataframe into one or more rows, similar what to tidyr::unnest_longer does to an R dataframe. An index column, if included, will be 1-based if col is an array column."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#usage",
    "title": "Unnest longer",
    "section": "Usage",
    "text": "Usage\n \nsdf_unnest_longer( \n  data, \n  col, \n  values_to = NULL, \n  indices_to = NULL, \n  include_indices = NULL, \n  names_repair = \"check_unique\", \n  ptype = list(), \n  transform = list() \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#arguments",
    "title": "Unnest longer",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\ndata\nThe Spark dataframe to be unnested\n\n\ncol\nThe struct column to extract components from\n\n\nvalues_to\nName of column to store vector values. Defaults to col.\n\n\nindices_to\nA string giving the name of column which will contain the inner names or position (if not named) of the values. Defaults to col with _id suffix\n\n\ninclude_indices\nWhether to include an index column. An index column will be included by default if col is a struct column. It will also be included if indices_to is not NULL.\n\n\nnames_repair\nStrategy for fixing duplicate column names (the semantic will be exactly identical to that of .name_repair option in tibble)\n\n\nptype\nOptionally, supply an R data frame prototype for the output. Each column of the unnested result will be casted based on the Spark equivalent of the type of the column with the same name within ptype, e.g., if ptype has a column x of type character, then column x of the unnested result will be casted from its original SQL type to StringType.\n\n\ntransform\nOptionally, a named list of transformation functions applied"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#examples",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#examples",
    "title": "Unnest longer",
    "section": "Examples",
    "text": "Examples\n\n \nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\", version = \"2.4.0\") \n \n# unnesting a struct column \nsdf &lt;- copy_to( \n  sc, \n  tibble::tibble( \n    x = 1:3, \n    y = list(list(a = 1, b = 2), list(a = 3, b = 4), list(a = 5, b = 6)) \n  ) \n) \n \nunnested &lt;- sdf %&gt;% sdf_unnest_longer(y, indices_to = \"attr\") \n \n# unnesting an array column \nsdf &lt;- copy_to( \n  sc, \n  tibble::tibble( \n    x = 1:3, \n    y = list(1:10, 1:5, 1:2) \n  ) \n) \n \nunnested &lt;- sdf %&gt;% sdf_unnest_longer(y, indices_to = \"array_idx\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_install.html",
    "href": "packages/sparklyr/latest/reference/livy_install.html",
    "title": "Install Livy",
    "section": "",
    "text": "R/livy_install.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_install.html#livy_install",
    "href": "packages/sparklyr/latest/reference/livy_install.html#livy_install",
    "title": "Install Livy",
    "section": "livy_install",
    "text": "livy_install"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_install.html#description",
    "href": "packages/sparklyr/latest/reference/livy_install.html#description",
    "title": "Install Livy",
    "section": "Description",
    "text": "Description\nAutomatically download and install livy. livy provides a REST API to Spark.\nFind the LIVY_HOME directory for a given version of Livy that was previously installed using livy_install."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_install.html#usage",
    "href": "packages/sparklyr/latest/reference/livy_install.html#usage",
    "title": "Install Livy",
    "section": "Usage",
    "text": "Usage\nlivy_install(version = \"0.6.0\", spark_home = NULL, spark_version = NULL) \n\nlivy_available_versions() \n\nlivy_install_dir() \n\nlivy_installed_versions() \n\nlivy_home_dir(version = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_install.html#arguments",
    "href": "packages/sparklyr/latest/reference/livy_install.html#arguments",
    "title": "Install Livy",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nversion\nVersion of Livy\n\n\nspark_home\nThe path to a Spark installation. The downloaded and installed version of livy will then be associated with this Spark installation. When unset (NULL), the value is inferred based on the value of spark_version supplied.\n\n\nspark_version\nThe version of Spark to use. When unset (NULL), the value is inferred based on the value of livy_version supplied. A version of Spark known to be compatible with the requested version of livy is chosen when possible."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_install.html#value",
    "href": "packages/sparklyr/latest/reference/livy_install.html#value",
    "title": "Install Livy",
    "section": "Value",
    "text": "Value\nPath to LIVY_HOME (or NULL if the specified version was not found)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html",
    "href": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html",
    "title": "Spark ML – Bisecting K-Means Clustering",
    "section": "",
    "text": "R/ml_clustering_bisecting_kmeans.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#ml_bisecting_kmeans",
    "href": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#ml_bisecting_kmeans",
    "title": "Spark ML – Bisecting K-Means Clustering",
    "section": "ml_bisecting_kmeans",
    "text": "ml_bisecting_kmeans"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#description",
    "href": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#description",
    "title": "Spark ML – Bisecting K-Means Clustering",
    "section": "Description",
    "text": "Description\nA bisecting k-means algorithm based on the paper “A comparison of document clustering techniques” by Steinbach, Karypis, and Kumar, with modification to fit Spark. The algorithm starts from a single cluster that contains all points. Iteratively it finds divisible clusters on the bottom level and bisects each of them using k-means, until there are k leaf clusters in total or no leaf clusters are divisible. The bisecting steps of clusters on the same level are grouped together to increase parallelism. If bisecting all divisible clusters on the bottom level would result more than k leaf clusters, larger clusters get higher priority."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#usage",
    "title": "Spark ML – Bisecting K-Means Clustering",
    "section": "Usage",
    "text": "Usage\n \nml_bisecting_kmeans( \n  x, \n  formula = NULL, \n  k = 4, \n  max_iter = 20, \n  seed = NULL, \n  min_divisible_cluster_size = 1, \n  features_col = \"features\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"bisecting_bisecting_kmeans_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#arguments",
    "title": "Spark ML – Bisecting K-Means Clustering",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nk\nThe number of clusters to create\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\nmin_divisible_cluster_size\nThe minimum number of points (if greater than or equal to 1.0) or the minimum proportion of points (if less than 1.0) of a divisible cluster (default: 1.0).\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments, see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#value",
    "href": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#value",
    "title": "Spark ML – Bisecting K-Means Clustering",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the clustering estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, an estimator is constructed then immediately fit with the input tbl_spark, returning a clustering model.\ntbl_spark, with formula or features specified: When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the estimator. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model. This signature does not apply to ml_lda()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#examples",
    "title": "Spark ML – Bisecting K-Means Clustering",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nlibrary(dplyr) \n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \niris_tbl %&gt;% \n  select(-Species) %&gt;% \n  ml_bisecting_kmeans(k = 4, Species ~ .) \n#&gt; K-means clustering with 4 clusters\n#&gt; \n#&gt; Cluster centers:\n#&gt;   Sepal_Length Sepal_Width Petal_Length Petal_Width\n#&gt; 1     4.733333    3.158333     1.391667   0.2000000\n#&gt; 2     5.231034    3.544828     1.700000   0.3655172\n#&gt; 3     5.947458    2.766102     4.454237   1.4542373\n#&gt; 4     6.850000    3.073684     5.742105   2.0710526\n#&gt; \n#&gt; Within Set Sum of Squared Errors =  77.38099"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#see-also",
    "title": "Spark ML – Bisecting K-Means Clustering",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-clustering.html for more information on the set of clustering algorithms. Other ml clustering algorithms: ml_gaussian_mixture(), ml_kmeans(), ml_lda()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sql-transformer.html",
    "href": "packages/sparklyr/latest/reference/sql-transformer.html",
    "title": "Feature Transformation – SQLTransformer",
    "section": "",
    "text": "R/ml_feature_sql_transformer.R,"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sql-transformer.html#ft_sql_transformer",
    "href": "packages/sparklyr/latest/reference/sql-transformer.html#ft_sql_transformer",
    "title": "Feature Transformation – SQLTransformer",
    "section": "ft_sql_transformer",
    "text": "ft_sql_transformer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sql-transformer.html#description",
    "href": "packages/sparklyr/latest/reference/sql-transformer.html#description",
    "title": "Feature Transformation – SQLTransformer",
    "section": "Description",
    "text": "Description\nImplements the transformations which are defined by SQL statement. Currently we only support SQL syntax like ‘SELECT … FROM THIS …’ where ‘THIS’ represents the underlying table of the input dataset. The select clause specifies the fields, constants, and expressions to display in the output, it can be any select clause that Spark SQL supports. Users can also use Spark SQL built-in function and UDFs to operate on these selected columns."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sql-transformer.html#usage",
    "href": "packages/sparklyr/latest/reference/sql-transformer.html#usage",
    "title": "Feature Transformation – SQLTransformer",
    "section": "Usage",
    "text": "Usage\nft_sql_transformer( \n  x, \n  statement = NULL, \n  uid = random_string(\"sql_transformer_\"), \n  ... \n) \n\nft_dplyr_transformer(x, tbl, uid = random_string(\"dplyr_transformer_\"), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sql-transformer.html#arguments",
    "href": "packages/sparklyr/latest/reference/sql-transformer.html#arguments",
    "title": "Feature Transformation – SQLTransformer",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nstatement\nA SQL statement.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused.\n\n\ntbl\nA tbl_spark generated using dplyr transformations."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sql-transformer.html#details",
    "href": "packages/sparklyr/latest/reference/sql-transformer.html#details",
    "title": "Feature Transformation – SQLTransformer",
    "section": "Details",
    "text": "Details\nft_dplyr_transformer() is mostly a wrapper around ft_sql_transformer() that takes a tbl_spark instead of a SQL statement. Internally, the ft_dplyr_transformer()\nextracts the dplyr transformations used to generate tbl as a SQL statement or a sampling operation. Note that only single-table dplyr verbs are supported and that the sdf_ family of functions are not."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sql-transformer.html#value",
    "href": "packages/sparklyr/latest/reference/sql-transformer.html#value",
    "title": "Feature Transformation – SQLTransformer",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sql-transformer.html#see-also",
    "href": "packages/sparklyr/latest/reference/sql-transformer.html#see-also",
    "title": "Feature Transformation – SQLTransformer",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh_utils.html",
    "href": "packages/sparklyr/latest/reference/ft_lsh_utils.html",
    "title": "Utility functions for LSH models",
    "section": "",
    "text": "R/ml_feature_lsh_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh_utils.html#ft_lsh_utils",
    "href": "packages/sparklyr/latest/reference/ft_lsh_utils.html#ft_lsh_utils",
    "title": "Utility functions for LSH models",
    "section": "ft_lsh_utils",
    "text": "ft_lsh_utils"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh_utils.html#description",
    "href": "packages/sparklyr/latest/reference/ft_lsh_utils.html#description",
    "title": "Utility functions for LSH models",
    "section": "Description",
    "text": "Description\nUtility functions for LSH models"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh_utils.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_lsh_utils.html#usage",
    "title": "Utility functions for LSH models",
    "section": "Usage",
    "text": "Usage\nml_approx_nearest_neighbors( \n  model, \n  dataset, \n  key, \n  num_nearest_neighbors, \n  dist_col = \"distCol\" \n) \n\nml_approx_similarity_join( \n  model, \n  dataset_a, \n  dataset_b, \n  threshold, \n  dist_col = \"distCol\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh_utils.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_lsh_utils.html#arguments",
    "title": "Utility functions for LSH models",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nmodel\nA fitted LSH model, returned by either ft_minhash_lsh()or ft_bucketed_random_projection_lsh().\n\n\ndataset\nThe dataset to search for nearest neighbors of the key.\n\n\nkey\nFeature vector representing the item to search for.\n\n\nnum_nearest_neighbors\nThe maximum number of nearest neighbors.\n\n\ndist_col\nOutput column for storing the distance between each result row and the key.\n\n\ndataset_a\nOne of the datasets to join.\n\n\ndataset_b\nAnother dataset to join.\n\n\nthreshold\nThe threshold for the distance of row pairs."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rnorm.html",
    "href": "packages/sparklyr/latest/reference/sdf_rnorm.html",
    "title": "Generate random samples from the standard normal distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rnorm.html#sdf_rnorm",
    "href": "packages/sparklyr/latest/reference/sdf_rnorm.html#sdf_rnorm",
    "title": "Generate random samples from the standard normal distribution",
    "section": "sdf_rnorm",
    "text": "sdf_rnorm"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rnorm.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rnorm.html#description",
    "title": "Generate random samples from the standard normal distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from the standard normal distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rnorm.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rnorm.html#usage",
    "title": "Generate random samples from the standard normal distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rnorm( \n  sc, \n  n, \n  mean = 0, \n  sd = 1, \n  num_partitions = NULL, \n  seed = NULL, \n  output_col = \"x\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rnorm.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rnorm.html#arguments",
    "title": "Generate random samples from the standard normal distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nmean\nThe mean value of the normal distribution.\n\n\nsd\nThe standard deviation of the normal distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rnorm.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rnorm.html#see-also",
    "title": "Generate random samples from the standard normal distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-persistence.html",
    "href": "packages/sparklyr/latest/reference/ml-persistence.html",
    "title": "Spark ML – Model Persistence",
    "section": "",
    "text": "R/ml_persistence.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-persistence.html#ml-persistence",
    "href": "packages/sparklyr/latest/reference/ml-persistence.html#ml-persistence",
    "title": "Spark ML – Model Persistence",
    "section": "ml-persistence",
    "text": "ml-persistence"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-persistence.html#description",
    "href": "packages/sparklyr/latest/reference/ml-persistence.html#description",
    "title": "Spark ML – Model Persistence",
    "section": "Description",
    "text": "Description\nSave/load Spark ML objects"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-persistence.html#usage",
    "href": "packages/sparklyr/latest/reference/ml-persistence.html#usage",
    "title": "Spark ML – Model Persistence",
    "section": "Usage",
    "text": "Usage\nml_save(x, path, overwrite = FALSE, ...) \n\n## S3 method for class 'ml_model'\nml_save( \n  x, \n  path, \n  overwrite = FALSE, \n  type = c(\"pipeline_model\", \"pipeline\"), \n  ... \n) \n\nml_load(sc, path)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-persistence.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml-persistence.html#arguments",
    "title": "Spark ML – Model Persistence",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA ML object, which could be a ml_pipeline_stage or a ml_model\n\n\npath\nThe path where the object is to be serialized/deserialized.\n\n\noverwrite\nWhether to overwrite the existing path, defaults to FALSE.\n\n\n…\nOptional arguments; currently unused.\n\n\ntype\nWhether to save the pipeline model or the pipeline.\n\n\nsc\nA Spark connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-persistence.html#value",
    "href": "packages/sparklyr/latest/reference/ml-persistence.html#value",
    "title": "Spark ML – Model Persistence",
    "section": "Value",
    "text": "Value\nml_save() serializes a Spark object into a format that can be read back into sparklyr or by the Scala or PySpark APIs. When called on ml_model objects, i.e. those that were created via the tbl_spark - formula signature, the associated pipeline model is serialized. In other words, the saved model contains both the data processing (RFormulaModel) stage and the machine learning stage.\nml_load() reads a saved Spark object into sparklyr. It calls the correct Scala load method based on parsing the saved metadata. Note that a PipelineModel object saved from a sparklyr ml_model via ml_save() will be read back in as an ml_pipeline_model, rather than the ml_model object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_avro.html",
    "href": "packages/sparklyr/latest/reference/spark_write_avro.html",
    "title": "Serialize a Spark DataFrame into Apache Avro format",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_avro.html#spark_write_avro",
    "href": "packages/sparklyr/latest/reference/spark_write_avro.html#spark_write_avro",
    "title": "Serialize a Spark DataFrame into Apache Avro format",
    "section": "spark_write_avro",
    "text": "spark_write_avro"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_avro.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_avro.html#description",
    "title": "Serialize a Spark DataFrame into Apache Avro format",
    "section": "Description",
    "text": "Description\nSerialize a Spark DataFrame into Apache Avro format. Notice this functionality requires the Spark connection sc to be instantiated with either an explicitly specified Spark version (i.e., spark_connect(..., version = &lt;version&gt;, packages = c(\"avro\", &lt;other package(s)&gt;), ...)) or a specific version of Spark avro package to use (e.g., spark_connect(..., packages = c(\"org.apache.spark:spark-avro_2.12:3.0.0\", &lt;other package(s)&gt;), ...))."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_avro.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_avro.html#usage",
    "title": "Serialize a Spark DataFrame into Apache Avro format",
    "section": "Usage",
    "text": "Usage\nspark_write_avro( \n  x, \n  path, \n  avro_schema = NULL, \n  record_name = \"topLevelRecord\", \n  record_namespace = \"\", \n  compression = \"snappy\", \n  partition_by = NULL \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_avro.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_avro.html#arguments",
    "title": "Serialize a Spark DataFrame into Apache Avro format",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\navro_schema\nOptional Avro schema in JSON format\n\n\nrecord_name\nOptional top level record name in write result (default: “topLevelRecord”)\n\n\nrecord_namespace\nRecord namespace in write result (default: ““)\n\n\ncompression\nCompression codec to use (default: “snappy”)\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_avro.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_avro.html#see-also",
    "title": "Serialize a Spark DataFrame into Apache Avro format",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_stage.html",
    "href": "packages/sparklyr/latest/reference/ml_stage.html",
    "title": "Spark ML – Pipeline stage extraction",
    "section": "",
    "text": "R/ml_pipeline_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_stage.html#ml_stage",
    "href": "packages/sparklyr/latest/reference/ml_stage.html#ml_stage",
    "title": "Spark ML – Pipeline stage extraction",
    "section": "ml_stage",
    "text": "ml_stage"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_stage.html#description",
    "href": "packages/sparklyr/latest/reference/ml_stage.html#description",
    "title": "Spark ML – Pipeline stage extraction",
    "section": "Description",
    "text": "Description\nExtraction of stages from a Pipeline or PipelineModel object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_stage.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_stage.html#usage",
    "title": "Spark ML – Pipeline stage extraction",
    "section": "Usage",
    "text": "Usage\nml_stage(x, stage) \n\nml_stages(x, stages = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_stage.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_stage.html#arguments",
    "title": "Spark ML – Pipeline stage extraction",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA ml_pipeline or a ml_pipeline_model object\n\n\nstage\nThe UID of a stage in the pipeline.\n\n\nstages\nThe UIDs of stages in the pipeline as a character vector."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_stage.html#value",
    "href": "packages/sparklyr/latest/reference/ml_stage.html#value",
    "title": "Spark ML – Pipeline stage extraction",
    "section": "Value",
    "text": "Value\nFor ml_stage(): The stage specified.\nFor ml_stages(): A list of stages. If stages is not set, the function returns all stages of the pipeline in a list."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke_method.html",
    "href": "packages/sparklyr/latest/reference/invoke_method.html",
    "title": "Generic Call Interface",
    "section": "",
    "text": "R/spark_invoke.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke_method.html#invoke_method",
    "href": "packages/sparklyr/latest/reference/invoke_method.html#invoke_method",
    "title": "Generic Call Interface",
    "section": "invoke_method",
    "text": "invoke_method"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke_method.html#description",
    "href": "packages/sparklyr/latest/reference/invoke_method.html#description",
    "title": "Generic Call Interface",
    "section": "Description",
    "text": "Description\nGeneric Call Interface"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke_method.html#usage",
    "href": "packages/sparklyr/latest/reference/invoke_method.html#usage",
    "title": "Generic Call Interface",
    "section": "Usage",
    "text": "Usage\ninvoke_method(sc, static, object, method, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke_method.html#arguments",
    "href": "packages/sparklyr/latest/reference/invoke_method.html#arguments",
    "title": "Generic Call Interface",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nspark_connection\n\n\nstatic\nIs this a static method call (including a constructor). If so then the object parameter should be the name of a class (otherwise it should be a spark_jobj instance).\n\n\nobject\nObject instance or name of class (for static)\n\n\nmethod\nName of method\n\n\n…\nCall parameters"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression_tidiers.html",
    "title": "Tidying methods for Spark ML Isotonic Regression",
    "section": "",
    "text": "R/tidiers_ml_isotonic_regression.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression_tidiers.html#ml_isotonic_regression_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression_tidiers.html#ml_isotonic_regression_tidiers",
    "title": "Tidying methods for Spark ML Isotonic Regression",
    "section": "ml_isotonic_regression_tidiers",
    "text": "ml_isotonic_regression_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression_tidiers.html#description",
    "title": "Tidying methods for Spark ML Isotonic Regression",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression_tidiers.html#usage",
    "title": "Tidying methods for Spark ML Isotonic Regression",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_isotonic_regression'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_isotonic_regression'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_isotonic_regression'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML Isotonic Regression",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html",
    "href": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html",
    "title": "Retrieves or sets the auto broadcast join threshold",
    "section": "",
    "text": "R/spark_context_config.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#spark_auto_broadcast_join_threshold",
    "href": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#spark_auto_broadcast_join_threshold",
    "title": "Retrieves or sets the auto broadcast join threshold",
    "section": "spark_auto_broadcast_join_threshold",
    "text": "spark_auto_broadcast_join_threshold"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#description",
    "href": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#description",
    "title": "Retrieves or sets the auto broadcast join threshold",
    "section": "Description",
    "text": "Description\nConfigures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command ANALYZE TABLE &lt;tableName&gt; COMPUTE STATISTICS noscan has been run, and file-based data source tables where the statistics are computed directly on the files of data."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#usage",
    "title": "Retrieves or sets the auto broadcast join threshold",
    "section": "Usage",
    "text": "Usage\nspark_auto_broadcast_join_threshold(sc, threshold = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#arguments",
    "title": "Retrieves or sets the auto broadcast join threshold",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nthreshold\nMaximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. Defaults to NULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#see-also",
    "title": "Retrieves or sets the auto broadcast join threshold",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_adaptive_query_execution(), spark_advisory_shuffle_partition_size(), spark_coalesce_initial_num_partitions(), spark_coalesce_min_num_partitions(), spark_coalesce_shuffle_partitions(), spark_session_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/replace_na.html",
    "href": "packages/sparklyr/latest/reference/replace_na.html",
    "title": "Replace NA",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/replace_na.html#replace_na",
    "href": "packages/sparklyr/latest/reference/replace_na.html#replace_na",
    "title": "Replace NA",
    "section": "replace_na",
    "text": "replace_na"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/replace_na.html#description",
    "href": "packages/sparklyr/latest/reference/replace_na.html#description",
    "title": "Replace NA",
    "section": "Description",
    "text": "Description\nSee replace_na for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_json.html",
    "href": "packages/sparklyr/latest/reference/stream_read_json.html",
    "title": "Read JSON Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_json.html#stream_read_json",
    "href": "packages/sparklyr/latest/reference/stream_read_json.html#stream_read_json",
    "title": "Read JSON Stream",
    "section": "stream_read_json",
    "text": "stream_read_json"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_json.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_json.html#description",
    "title": "Read JSON Stream",
    "section": "Description",
    "text": "Description\nReads a JSON stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_json.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_json.html#usage",
    "title": "Read JSON Stream",
    "section": "Usage",
    "text": "Usage\nstream_read_json(sc, path, name = NULL, columns = NULL, options = list(), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_json.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_json.html#arguments",
    "title": "Read JSON Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_json.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_json.html#examples",
    "title": "Read JSON Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\") \ndir.create(\"json-in\") \njsonlite::write_json(list(a = c(1, 2), b = c(10, 20)), \"json-in/data.json\") \njson_path &lt;- file.path(\"file://\", getwd(), \"json-in\") \nstream &lt;- stream_read_json(sc, json_path) %&gt;% stream_write_json(\"json-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_json.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_json.html#see-also",
    "title": "Read JSON Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sql.html",
    "href": "packages/sparklyr/latest/reference/sdf_sql.html",
    "title": "Spark DataFrame from SQL",
    "section": "",
    "text": "R/sdf_sql.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sql.html#sdf_sql",
    "href": "packages/sparklyr/latest/reference/sdf_sql.html#sdf_sql",
    "title": "Spark DataFrame from SQL",
    "section": "sdf_sql",
    "text": "sdf_sql"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sql.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_sql.html#description",
    "title": "Spark DataFrame from SQL",
    "section": "Description",
    "text": "Description\nDefines a Spark DataFrame from a SQL query, useful to create Spark DataFrames without collecting the results immediately."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sql.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_sql.html#usage",
    "title": "Spark DataFrame from SQL",
    "section": "Usage",
    "text": "Usage\nsdf_sql(sc, sql)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sql.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_sql.html#arguments",
    "title": "Spark DataFrame from SQL",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nsql\na ‘SQL’ query used to generate a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_change_db.html",
    "href": "packages/sparklyr/latest/reference/tbl_change_db.html",
    "title": "Use specific database",
    "section": "",
    "text": "R/tables_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_change_db.html#tbl_change_db",
    "href": "packages/sparklyr/latest/reference/tbl_change_db.html#tbl_change_db",
    "title": "Use specific database",
    "section": "tbl_change_db",
    "text": "tbl_change_db"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_change_db.html#description",
    "href": "packages/sparklyr/latest/reference/tbl_change_db.html#description",
    "title": "Use specific database",
    "section": "Description",
    "text": "Description\nUse specific database"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_change_db.html#usage",
    "href": "packages/sparklyr/latest/reference/tbl_change_db.html#usage",
    "title": "Use specific database",
    "section": "Usage",
    "text": "Usage\ntbl_change_db(sc, name)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_change_db.html#arguments",
    "href": "packages/sparklyr/latest/reference/tbl_change_db.html#arguments",
    "title": "Use specific database",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe database name."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_copy_to.html",
    "href": "packages/sparklyr/latest/reference/sdf_copy_to.html",
    "title": "Copy an Object into Spark",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_copy_to.html#sdf_copy_to",
    "href": "packages/sparklyr/latest/reference/sdf_copy_to.html#sdf_copy_to",
    "title": "Copy an Object into Spark",
    "section": "sdf_copy_to",
    "text": "sdf_copy_to"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_copy_to.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_copy_to.html#description",
    "title": "Copy an Object into Spark",
    "section": "Description",
    "text": "Description\nCopy an object into Spark, and return an R object wrapping the copied object (typically, a Spark DataFrame)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_copy_to.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_copy_to.html#usage",
    "title": "Copy an Object into Spark",
    "section": "Usage",
    "text": "Usage\n \nsdf_copy_to(sc, x, name, memory, repartition, overwrite, struct_columns, ...) \n \nsdf_import(x, sc, name, memory, repartition, overwrite, struct_columns, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_copy_to.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_copy_to.html#arguments",
    "title": "Copy an Object into Spark",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nx\nAn R object from which a Spark DataFrame can be generated.\n\n\nname\nThe name to assign to the copied table in Spark.\n\n\nmemory\nBoolean; should the table be cached into memory?\n\n\nrepartition\nThe number of partitions to use when distributing the table across the Spark cluster. The default (0) can be used to avoid partitioning.\n\n\noverwrite\nBoolean; overwrite a pre-existing table with the name name if one already exists?\n\n\nstruct_columns\n(only supported with Spark 2.4.0 or higher) A list of columns from the source data frame that should be converted to Spark SQL StructType columns. The source columns can contain either json strings or nested lists. All rows within each source column should have identical schemas (because otherwise the conversion result will contain unexpected null values or missing values as Spark currently does not support schema discovery on individual rows within a struct column).\n\n\n…\nOptional arguments, passed to implementing methods."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_copy_to.html#section",
    "href": "packages/sparklyr/latest/reference/sdf_copy_to.html#section",
    "title": "Copy an Object into Spark",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_copy_to.html#advanced-usage",
    "href": "packages/sparklyr/latest/reference/sdf_copy_to.html#advanced-usage",
    "title": "Copy an Object into Spark",
    "section": "Advanced Usage",
    "text": "Advanced Usage\nsdf_copy_to is an S3 generic that, by default, dispatches to sdf_import. Package authors that would like to implement sdf_copy_to for a custom object type can accomplish this by implementing the associated method on sdf_import."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_copy_to.html#examples",
    "href": "packages/sparklyr/latest/reference/sdf_copy_to.html#examples",
    "title": "Copy an Object into Spark",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"spark://HOST:PORT\") \nsdf_copy_to(sc, iris) \n#&gt;     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n#&gt; 1            5.1         3.5          1.4         0.2     setosa\n#&gt; 2            4.9         3.0          1.4         0.2     setosa\n#&gt; 3            4.7         3.2          1.3         0.2     setosa\n#&gt; 4            4.6         3.1          1.5         0.2     setosa\n#&gt; 5            5.0         3.6          1.4         0.2     setosa\n#&gt; 6            5.4         3.9          1.7         0.4     setosa\n#&gt; 7            4.6         3.4          1.4         0.3     setosa\n#&gt; 8            5.0         3.4          1.5         0.2     setosa\n#&gt; 9            4.4         2.9          1.4         0.2     setosa\n#&gt; 10           4.9         3.1          1.5         0.1     setosa\n#&gt; 11           5.4         3.7          1.5         0.2     setosa\n#&gt; 12           4.8         3.4          1.6         0.2     setosa\n#&gt; 13           4.8         3.0          1.4         0.1     setosa\n#&gt; 14           4.3         3.0          1.1         0.1     setosa\n#&gt; 15           5.8         4.0          1.2         0.2     setosa\n#&gt; 16           5.7         4.4          1.5         0.4     setosa\n#&gt; 17           5.4         3.9          1.3         0.4     setosa\n#&gt; 18           5.1         3.5          1.4         0.3     setosa\n#&gt; 19           5.7         3.8          1.7         0.3     setosa\n#&gt; 20           5.1         3.8          1.5         0.3     setosa\n#&gt; 21           5.4         3.4          1.7         0.2     setosa\n#&gt; 22           5.1         3.7          1.5         0.4     setosa\n#&gt; 23           4.6         3.6          1.0         0.2     setosa\n#&gt; 24           5.1         3.3          1.7         0.5     setosa\n#&gt; 25           4.8         3.4          1.9         0.2     setosa\n#&gt; 26           5.0         3.0          1.6         0.2     setosa\n#&gt; 27           5.0         3.4          1.6         0.4     setosa\n#&gt; 28           5.2         3.5          1.5         0.2     setosa\n#&gt; 29           5.2         3.4          1.4         0.2     setosa\n#&gt; 30           4.7         3.2          1.6         0.2     setosa\n#&gt; 31           4.8         3.1          1.6         0.2     setosa\n#&gt; 32           5.4         3.4          1.5         0.4     setosa\n#&gt; 33           5.2         4.1          1.5         0.1     setosa\n#&gt; 34           5.5         4.2          1.4         0.2     setosa\n#&gt; 35           4.9         3.1          1.5         0.2     setosa\n#&gt; 36           5.0         3.2          1.2         0.2     setosa\n#&gt; 37           5.5         3.5          1.3         0.2     setosa\n#&gt; 38           4.9         3.6          1.4         0.1     setosa\n#&gt; 39           4.4         3.0          1.3         0.2     setosa\n#&gt; 40           5.1         3.4          1.5         0.2     setosa\n#&gt; 41           5.0         3.5          1.3         0.3     setosa\n#&gt; 42           4.5         2.3          1.3         0.3     setosa\n#&gt; 43           4.4         3.2          1.3         0.2     setosa\n#&gt; 44           5.0         3.5          1.6         0.6     setosa\n#&gt; 45           5.1         3.8          1.9         0.4     setosa\n#&gt; 46           4.8         3.0          1.4         0.3     setosa\n#&gt; 47           5.1         3.8          1.6         0.2     setosa\n#&gt; 48           4.6         3.2          1.4         0.2     setosa\n#&gt; 49           5.3         3.7          1.5         0.2     setosa\n#&gt; 50           5.0         3.3          1.4         0.2     setosa\n#&gt; 51           7.0         3.2          4.7         1.4 versicolor\n#&gt; 52           6.4         3.2          4.5         1.5 versicolor\n#&gt; 53           6.9         3.1          4.9         1.5 versicolor\n#&gt; 54           5.5         2.3          4.0         1.3 versicolor\n#&gt; 55           6.5         2.8          4.6         1.5 versicolor\n#&gt; 56           5.7         2.8          4.5         1.3 versicolor\n#&gt; 57           6.3         3.3          4.7         1.6 versicolor\n#&gt; 58           4.9         2.4          3.3         1.0 versicolor\n#&gt; 59           6.6         2.9          4.6         1.3 versicolor\n#&gt; 60           5.2         2.7          3.9         1.4 versicolor\n#&gt; 61           5.0         2.0          3.5         1.0 versicolor\n#&gt; 62           5.9         3.0          4.2         1.5 versicolor\n#&gt; 63           6.0         2.2          4.0         1.0 versicolor\n#&gt; 64           6.1         2.9          4.7         1.4 versicolor\n#&gt; 65           5.6         2.9          3.6         1.3 versicolor\n#&gt; 66           6.7         3.1          4.4         1.4 versicolor\n#&gt; 67           5.6         3.0          4.5         1.5 versicolor\n#&gt; 68           5.8         2.7          4.1         1.0 versicolor\n#&gt; 69           6.2         2.2          4.5         1.5 versicolor\n#&gt; 70           5.6         2.5          3.9         1.1 versicolor\n#&gt; 71           5.9         3.2          4.8         1.8 versicolor\n#&gt; 72           6.1         2.8          4.0         1.3 versicolor\n#&gt; 73           6.3         2.5          4.9         1.5 versicolor\n#&gt; 74           6.1         2.8          4.7         1.2 versicolor\n#&gt; 75           6.4         2.9          4.3         1.3 versicolor\n#&gt; 76           6.6         3.0          4.4         1.4 versicolor\n#&gt; 77           6.8         2.8          4.8         1.4 versicolor\n#&gt; 78           6.7         3.0          5.0         1.7 versicolor\n#&gt; 79           6.0         2.9          4.5         1.5 versicolor\n#&gt; 80           5.7         2.6          3.5         1.0 versicolor\n#&gt; 81           5.5         2.4          3.8         1.1 versicolor\n#&gt; 82           5.5         2.4          3.7         1.0 versicolor\n#&gt; 83           5.8         2.7          3.9         1.2 versicolor\n#&gt; 84           6.0         2.7          5.1         1.6 versicolor\n#&gt; 85           5.4         3.0          4.5         1.5 versicolor\n#&gt; 86           6.0         3.4          4.5         1.6 versicolor\n#&gt; 87           6.7         3.1          4.7         1.5 versicolor\n#&gt; 88           6.3         2.3          4.4         1.3 versicolor\n#&gt; 89           5.6         3.0          4.1         1.3 versicolor\n#&gt; 90           5.5         2.5          4.0         1.3 versicolor\n#&gt; 91           5.5         2.6          4.4         1.2 versicolor\n#&gt; 92           6.1         3.0          4.6         1.4 versicolor\n#&gt; 93           5.8         2.6          4.0         1.2 versicolor\n#&gt; 94           5.0         2.3          3.3         1.0 versicolor\n#&gt; 95           5.6         2.7          4.2         1.3 versicolor\n#&gt; 96           5.7         3.0          4.2         1.2 versicolor\n#&gt; 97           5.7         2.9          4.2         1.3 versicolor\n#&gt; 98           6.2         2.9          4.3         1.3 versicolor\n#&gt; 99           5.1         2.5          3.0         1.1 versicolor\n#&gt; 100          5.7         2.8          4.1         1.3 versicolor\n#&gt; 101          6.3         3.3          6.0         2.5  virginica\n#&gt; 102          5.8         2.7          5.1         1.9  virginica\n#&gt; 103          7.1         3.0          5.9         2.1  virginica\n#&gt; 104          6.3         2.9          5.6         1.8  virginica\n#&gt; 105          6.5         3.0          5.8         2.2  virginica\n#&gt; 106          7.6         3.0          6.6         2.1  virginica\n#&gt; 107          4.9         2.5          4.5         1.7  virginica\n#&gt; 108          7.3         2.9          6.3         1.8  virginica\n#&gt; 109          6.7         2.5          5.8         1.8  virginica\n#&gt; 110          7.2         3.6          6.1         2.5  virginica\n#&gt; 111          6.5         3.2          5.1         2.0  virginica\n#&gt; 112          6.4         2.7          5.3         1.9  virginica\n#&gt; 113          6.8         3.0          5.5         2.1  virginica\n#&gt; 114          5.7         2.5          5.0         2.0  virginica\n#&gt; 115          5.8         2.8          5.1         2.4  virginica\n#&gt; 116          6.4         3.2          5.3         2.3  virginica\n#&gt; 117          6.5         3.0          5.5         1.8  virginica\n#&gt; 118          7.7         3.8          6.7         2.2  virginica\n#&gt; 119          7.7         2.6          6.9         2.3  virginica\n#&gt; 120          6.0         2.2          5.0         1.5  virginica\n#&gt; 121          6.9         3.2          5.7         2.3  virginica\n#&gt; 122          5.6         2.8          4.9         2.0  virginica\n#&gt; 123          7.7         2.8          6.7         2.0  virginica\n#&gt; 124          6.3         2.7          4.9         1.8  virginica\n#&gt; 125          6.7         3.3          5.7         2.1  virginica\n#&gt; 126          7.2         3.2          6.0         1.8  virginica\n#&gt; 127          6.2         2.8          4.8         1.8  virginica\n#&gt; 128          6.1         3.0          4.9         1.8  virginica\n#&gt; 129          6.4         2.8          5.6         2.1  virginica\n#&gt; 130          7.2         3.0          5.8         1.6  virginica\n#&gt; 131          7.4         2.8          6.1         1.9  virginica\n#&gt; 132          7.9         3.8          6.4         2.0  virginica\n#&gt; 133          6.4         2.8          5.6         2.2  virginica\n#&gt; 134          6.3         2.8          5.1         1.5  virginica\n#&gt; 135          6.1         2.6          5.6         1.4  virginica\n#&gt; 136          7.7         3.0          6.1         2.3  virginica\n#&gt; 137          6.3         3.4          5.6         2.4  virginica\n#&gt; 138          6.4         3.1          5.5         1.8  virginica\n#&gt; 139          6.0         3.0          4.8         1.8  virginica\n#&gt; 140          6.9         3.1          5.4         2.1  virginica\n#&gt; 141          6.7         3.1          5.6         2.4  virginica\n#&gt; 142          6.9         3.1          5.1         2.3  virginica\n#&gt; 143          5.8         2.7          5.1         1.9  virginica\n#&gt; 144          6.8         3.2          5.9         2.3  virginica\n#&gt; 145          6.7         3.3          5.7         2.5  virginica\n#&gt; 146          6.7         3.0          5.2         2.3  virginica\n#&gt; 147          6.3         2.5          5.0         1.9  virginica\n#&gt; 148          6.5         3.0          5.2         2.0  virginica\n#&gt; 149          6.2         3.4          5.4         2.3  virginica\n#&gt; 150          5.9         3.0          5.1         1.8  virginica"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_copy_to.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_copy_to.html#see-also",
    "title": "Copy an Object into Spark",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_distinct(), sdf_random_split(), sdf_register(), sdf_sample(), sdf_sort(), sdf_weighted_sample()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply.html",
    "href": "packages/sparklyr/latest/reference/spark_apply.html",
    "title": "Apply an R Function in Spark",
    "section": "",
    "text": "R/spark_apply.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply.html#spark_apply",
    "href": "packages/sparklyr/latest/reference/spark_apply.html#spark_apply",
    "title": "Apply an R Function in Spark",
    "section": "spark_apply",
    "text": "spark_apply"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply.html#description",
    "href": "packages/sparklyr/latest/reference/spark_apply.html#description",
    "title": "Apply an R Function in Spark",
    "section": "Description",
    "text": "Description\nApplies an R function to a Spark object (typically, a Spark DataFrame)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_apply.html#usage",
    "title": "Apply an R Function in Spark",
    "section": "Usage",
    "text": "Usage\n \nspark_apply( \n  x, \n  f, \n  columns = NULL, \n  memory = TRUE, \n  group_by = NULL, \n  packages = NULL, \n  context = NULL, \n  name = NULL, \n  barrier = NULL, \n  fetch_result_as_sdf = TRUE, \n  partition_index_param = \"\", \n  arrow_max_records_per_batch = NULL, \n  auto_deps = FALSE, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_apply.html#arguments",
    "title": "Apply an R Function in Spark",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object (usually a spark_tbl) coercable to a Spark DataFrame.\n\n\nf\nA function that transforms a data frame partition into a data frame. The function f has signature f(df, context, group1, group2, ...) where df is a data frame with the data to be processed, context is an optional object passed as the context parameter and group1 to groupN contain the values of the group_by values. When group_by is not specified, f takes only one argument. Can also be an rlang anonymous function. For example, as ~ .x + 1 to define an expression that adds one to the given .x data frame.\n\n\ncolumns\nA vector of column names or a named vector of column types for the transformed object. When not specified, a sample of 10 rows is taken to infer out the output columns automatically, to avoid this performance penalty, specify the column types. The sample size is configurable using the sparklyr.apply.schema.infer configuration option.\n\n\nmemory\nBoolean; should the table be cached into memory?\n\n\ngroup_by\nColumn name used to group by data frame partitions.\n\n\npackages\nBoolean to distribute .libPaths() packages to each node, a list of packages to distribute, or a package bundle created with spark_apply_bundle(). Defaults to TRUE or the sparklyr.apply.packages value set in spark_config(). For clusters using Yarn cluster mode, packages can point to a package bundle created using spark_apply_bundle() and made available as a Spark file using config$sparklyr.shell.files. For clusters using Livy, packages can be manually installed on the driver node. For offline clusters where available.packages() is not available, manually download the packages database from https://cran.r-project.org/web/packages/packages.rds and set Sys.setenv(sparklyr.apply.packagesdb = \"&lt;pathl-to-rds&gt;\"). Otherwise, all packages will be used by default. For clusters where R packages already installed in every worker node, the spark.r.libpaths config entry can be set in spark_config() to the local packages library. To specify multiple paths collapse them (without spaces) with a comma delimiter (e.g., \"/lib/path/one,/lib/path/two\").\n\n\ncontext\nOptional object to be serialized and passed back to f().\n\n\nname\nOptional table name while registering the resulting data frame.\n\n\nbarrier\nOptional to support Barrier Execution Mode in the scheduler.\n\n\nfetch_result_as_sdf\nWhether to return the transformed results in a Spark Dataframe (defaults to TRUE). When set to FALSE, results will be returned as a list of R objects instead. NOTE: fetch_result_as_sdf must be set to FALSE when the transformation function being applied is returning R objects that cannot be stored in a Spark Dataframe (e.g., complex numbers or any other R data type that does not have an equivalent representation among Spark SQL data types).\n\n\npartition_index_param\nOptional if non-empty, then f also receives the index of the partition being processed as a named argument with this name, in addition to all positional argument(s) it will receive NOTE: when fetch_result_as_sdf is set to FALSE, object returned from the transformation function also must be serializable by the base::serialize function in R.\n\n\narrow_max_records_per_batch\nMaximum size of each Arrow record batch, ignored if Arrow serialization is not enabled.\n\n\nauto_deps\n[Experimental] Whether to infer all required R packages by examining the closure f() and only distribute required R and their transitive dependencies to Spark worker nodes (default: FALSE). NOTE: this option will only take effect if packages is set to TRUE or is a character vector of R package names. If packages is a character vector of R package names, then both the set of packages specified by packages and the set of inferred packages will be distributed to Spark workers.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply.html#section",
    "href": "packages/sparklyr/latest/reference/spark_apply.html#section",
    "title": "Apply an R Function in Spark",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply.html#configuration",
    "href": "packages/sparklyr/latest/reference/spark_apply.html#configuration",
    "title": "Apply an R Function in Spark",
    "section": "Configuration",
    "text": "Configuration\nspark_config() settings can be specified to change the workers environment. For instance, to set additional environment variables to each worker node use the sparklyr.apply.env.* config, to launch workers without --vanilla use sparklyr.apply.options.vanilla set to FALSE, to run a custom script before launching Rscript use sparklyr.apply.options.rscript.before."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply.html#examples",
    "href": "packages/sparklyr/latest/reference/spark_apply.html#examples",
    "title": "Apply an R Function in Spark",
    "section": "Examples",
    "text": "Examples\n\n \n \nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local[3]\") \n \n# creates an Spark data frame with 10 elements then multiply times 10 in R \nsdf_len(sc, 10) %&gt;% spark_apply(function(df) df * 10) \n#&gt; # Source: spark&lt;?&gt; [?? x 1]\n#&gt;       id\n#&gt;    &lt;dbl&gt;\n#&gt;  1    10\n#&gt;  2    20\n#&gt;  3    30\n#&gt;  4    40\n#&gt;  5    50\n#&gt;  6    60\n#&gt;  7    70\n#&gt;  8    80\n#&gt;  9    90\n#&gt; 10   100\n \n# using barrier mode \nsdf_len(sc, 3, repartition = 3) %&gt;% \n  spark_apply(nrow, barrier = TRUE, columns = c(id = \"integer\")) %&gt;% \n  collect() \n#&gt; # A tibble: 3 × 1\n#&gt;      id\n#&gt;   &lt;int&gt;\n#&gt; 1     1\n#&gt; 2     1\n#&gt; 3     1"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbinom.html",
    "href": "packages/sparklyr/latest/reference/sdf_rbinom.html",
    "title": "Generate random samples from a binomial distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbinom.html#sdf_rbinom",
    "href": "packages/sparklyr/latest/reference/sdf_rbinom.html#sdf_rbinom",
    "title": "Generate random samples from a binomial distribution",
    "section": "sdf_rbinom",
    "text": "sdf_rbinom"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbinom.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rbinom.html#description",
    "title": "Generate random samples from a binomial distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a binomial distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbinom.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rbinom.html#usage",
    "title": "Generate random samples from a binomial distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rbinom( \n  sc, \n  n, \n  size, \n  prob, \n  num_partitions = NULL, \n  seed = NULL, \n  output_col = \"x\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbinom.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rbinom.html#arguments",
    "title": "Generate random samples from a binomial distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nsize\nNumber of trials (zero or more).\n\n\nprob\nProbability of success on each trial.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbinom.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rbinom.html#see-also",
    "title": "Generate random samples from a binomial distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_chisq_selector.html",
    "href": "packages/sparklyr/latest/reference/ft_chisq_selector.html",
    "title": "Feature Transformation – ChiSqSelector (Estimator)",
    "section": "",
    "text": "R/ml_feature_chisq_selector.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_chisq_selector.html#ft_chisq_selector",
    "href": "packages/sparklyr/latest/reference/ft_chisq_selector.html#ft_chisq_selector",
    "title": "Feature Transformation – ChiSqSelector (Estimator)",
    "section": "ft_chisq_selector",
    "text": "ft_chisq_selector"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_chisq_selector.html#description",
    "href": "packages/sparklyr/latest/reference/ft_chisq_selector.html#description",
    "title": "Feature Transformation – ChiSqSelector (Estimator)",
    "section": "Description",
    "text": "Description\nChi-Squared feature selection, which selects categorical features to use for predicting a categorical label"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_chisq_selector.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_chisq_selector.html#usage",
    "title": "Feature Transformation – ChiSqSelector (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_chisq_selector( \n  x, \n  features_col = \"features\", \n  output_col = NULL, \n  label_col = \"label\", \n  selector_type = \"numTopFeatures\", \n  fdr = 0.05, \n  fpr = 0.05, \n  fwe = 0.05, \n  num_top_features = 50, \n  percentile = 0.1, \n  uid = random_string(\"chisq_selector_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_chisq_selector.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_chisq_selector.html#arguments",
    "title": "Feature Transformation – ChiSqSelector (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\noutput_col\nThe name of the output column.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nselector_type\n(Spark 2.1.0+) The selector type of the ChisqSelector. Supported options: “numTopFeatures” (default), “percentile”, “fpr”, “fdr”, “fwe”.\n\n\nfdr\n(Spark 2.2.0+) The upper bound of the expected false discovery rate. Only applicable when selector_type = “fdr”. Default value is 0.05.\n\n\nfpr\n(Spark 2.1.0+) The highest p-value for features to be kept. Only applicable when selector_type= “fpr”. Default value is 0.05.\n\n\nfwe\n(Spark 2.2.0+) The upper bound of the expected family-wise error rate. Only applicable when selector_type = “fwe”. Default value is 0.05.\n\n\nnum_top_features\nNumber of features that selector will select, ordered by ascending p-value. If the number of features is less than num_top_features, then this will select all features. Only applicable when selector_type = “numTopFeatures”. The default value of num_top_features is 50.\n\n\npercentile\n(Spark 2.1.0+) Percentile of features that selector will select, ordered by statistics value descending. Only applicable when selector_type = “percentile”. Default value is 0.1.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_chisq_selector.html#details",
    "href": "packages/sparklyr/latest/reference/ft_chisq_selector.html#details",
    "title": "Feature Transformation – ChiSqSelector (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_chisq_selector.html#value",
    "href": "packages/sparklyr/latest/reference/ft_chisq_selector.html#value",
    "title": "Feature Transformation – ChiSqSelector (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_chisq_selector.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_chisq_selector.html#see-also",
    "title": "Feature Transformation – ChiSqSelector (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj-class.html",
    "href": "packages/sparklyr/latest/reference/spark_jobj-class.html",
    "title": "spark_jobj class",
    "section": "",
    "text": "R/connection_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj-class.html#spark_jobj-class",
    "href": "packages/sparklyr/latest/reference/spark_jobj-class.html#spark_jobj-class",
    "title": "spark_jobj class",
    "section": "spark_jobj-class",
    "text": "spark_jobj-class"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj-class.html#description",
    "href": "packages/sparklyr/latest/reference/spark_jobj-class.html#description",
    "title": "spark_jobj class",
    "section": "Description",
    "text": "Description\nspark_jobj class"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_array_sort.html",
    "href": "packages/sparklyr/latest/reference/hof_array_sort.html",
    "title": "Sorts array using a custom comparator",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_array_sort.html#hof_array_sort",
    "href": "packages/sparklyr/latest/reference/hof_array_sort.html#hof_array_sort",
    "title": "Sorts array using a custom comparator",
    "section": "hof_array_sort",
    "text": "hof_array_sort"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_array_sort.html#description",
    "href": "packages/sparklyr/latest/reference/hof_array_sort.html#description",
    "title": "Sorts array using a custom comparator",
    "section": "Description",
    "text": "Description\nApplies a custom comparator function to sort an array (this is essentially a dplyr wrapper to the array_sort(expr, func) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_array_sort.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_array_sort.html#usage",
    "title": "Sorts array using a custom comparator",
    "section": "Usage",
    "text": "Usage\n \nhof_array_sort(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_array_sort.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_array_sort.html#arguments",
    "title": "Sorts array using a custom comparator",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\nfunc\nThe comparator function to apply (it should take 2 array elements as arguments and return an integer, with a return value of -1 indicating the first element is less than the second, 0 indicating equality, or 1 indicating the first element is greater than the second)\n\n\nexpr\nThe array being sorted, could be any SQL expression evaluating to an array (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the sorted result (default: expr)\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_array_sort.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_array_sort.html#examples",
    "title": "Sorts array using a custom comparator",
    "section": "Examples",
    "text": "Examples\n\n \n \nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\", version = \"3.0.0\") \ncopy_to( \n  sc, \n  tibble::tibble( \n    # x contains 2 arrays each having elements in ascending order \n    x = list(1:5, 6:10) \n  ) \n) %&gt;% \n  # now each array from x gets sorted in descending order \n  hof_array_sort(~ as.integer(sign(.y - .x))) \n#&gt; # Source: spark&lt;?&gt; [?? x 1]\n#&gt;   x        \n#&gt;   &lt;list&gt;   \n#&gt; 1 &lt;dbl [5]&gt;\n#&gt; 2 &lt;dbl [5]&gt;"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html",
    "title": "Retrieves or sets whether coalescing contiguous shuffle partitions is enabled",
    "section": "",
    "text": "R/spark_context_config.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#spark_coalesce_shuffle_partitions",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#spark_coalesce_shuffle_partitions",
    "title": "Retrieves or sets whether coalescing contiguous shuffle partitions is enabled",
    "section": "spark_coalesce_shuffle_partitions",
    "text": "spark_coalesce_shuffle_partitions"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#description",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#description",
    "title": "Retrieves or sets whether coalescing contiguous shuffle partitions is enabled",
    "section": "Description",
    "text": "Description\nRetrieves or sets whether coalescing contiguous shuffle partitions is enabled"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#usage",
    "title": "Retrieves or sets whether coalescing contiguous shuffle partitions is enabled",
    "section": "Usage",
    "text": "Usage\nspark_coalesce_shuffle_partitions(sc, enable = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#arguments",
    "title": "Retrieves or sets whether coalescing contiguous shuffle partitions is enabled",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nenable\nWhether to enable coalescing of contiguous shuffle partitions. Defaults to NULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#see-also",
    "title": "Retrieves or sets whether coalescing contiguous shuffle partitions is enabled",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_adaptive_query_execution(), spark_advisory_shuffle_partition_size(), spark_auto_broadcast_join_threshold(), spark_coalesce_initial_num_partitions(), spark_coalesce_min_num_partitions(), spark_session_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html",
    "title": "Read a CSV file into a Spark DataFrame",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html#spark_read_csv",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html#spark_read_csv",
    "title": "Read a CSV file into a Spark DataFrame",
    "section": "spark_read_csv",
    "text": "spark_read_csv"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html#description",
    "title": "Read a CSV file into a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRead a tabular data file into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html#usage",
    "title": "Read a CSV file into a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nspark_read_csv( \n  sc, \n  name = NULL, \n  path = name, \n  header = TRUE, \n  columns = NULL, \n  infer_schema = is.null(columns), \n  delimiter = \",\", \n  quote = \"\\\"\", \n  escape = \"\\\\\", \n  charset = \"UTF-8\", \n  null_value = NULL, \n  options = list(), \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html#arguments",
    "title": "Read a CSV file into a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nheader\nBoolean; should the first row of data be used as a header? Defaults to TRUE.\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\ninfer_schema\nBoolean; should column types be automatically inferred? Requires one extra pass over the data. Defaults to is.null(columns).\n\n\ndelimiter\nThe character used to delimit each column. Defaults to ','.\n\n\nquote\nThe character used as a quote. Defaults to '\"'.\n\n\nescape\nThe character used to escape other characters. Defaults to '\\'.\n\n\ncharset\nThe character set. Defaults to \"UTF-8\".\n\n\nnull_value\nThe character to use for null, or missing, values. Defaults to NULL.\n\n\noptions\nA list of strings with additional options.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html#details",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html#details",
    "title": "Read a CSV file into a Spark DataFrame",
    "section": "Details",
    "text": "Details\nYou can read data from HDFS (hdfs://), S3 (s3a://), as well as the local file system (file://).\nIf you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf spark.hadoop.fs.s3a.access.key, spark.hadoop.fs.s3a.secret.key or any of the methods outlined in the aws-sdk documentation Working with AWS credentials\nIn order to work with the newer s3a:// protocol also set the values for spark.hadoop.fs.s3a.impl and spark.hadoop.fs.s3a.endpoint. In addition, to support v4 of the S3 api be sure to pass the -Dcom.amazonaws.services.s3.enableV4 driver options for the config key spark.driver.extraJavaOptions\nFor instructions on how to configure s3n:// check the hadoop documentation: s3n authentication properties\nWhen header is FALSE, the column names are generated with a V prefix; e.g. V1, V2, ...."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html#see-also",
    "title": "Read a CSV file into a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html",
    "href": "packages/sparklyr/latest/reference/spark_config.html",
    "title": "Read Spark Configuration",
    "section": "",
    "text": "R/config_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html#spark_config",
    "href": "packages/sparklyr/latest/reference/spark_config.html#spark_config",
    "title": "Read Spark Configuration",
    "section": "spark_config",
    "text": "spark_config"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html#description",
    "href": "packages/sparklyr/latest/reference/spark_config.html#description",
    "title": "Read Spark Configuration",
    "section": "Description",
    "text": "Description\nRead Spark Configuration"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_config.html#usage",
    "title": "Read Spark Configuration",
    "section": "Usage",
    "text": "Usage\nspark_config(file = \"config.yml\", use_default = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_config.html#arguments",
    "title": "Read Spark Configuration",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nfile\nName of the configuration file\n\n\nuse_default\nTRUE to use the built-in defaults provided in this package"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html#details",
    "href": "packages/sparklyr/latest/reference/spark_config.html#details",
    "title": "Read Spark Configuration",
    "section": "Details",
    "text": "Details\nRead Spark configuration using the config package."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html#value",
    "href": "packages/sparklyr/latest/reference/spark_config.html#value",
    "title": "Read Spark Configuration",
    "section": "Value",
    "text": "Value\nNamed list with configuration data"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_dir.html",
    "href": "packages/sparklyr/latest/reference/spark_home_dir.html",
    "title": "Find the SPARK_HOME directory for a version of Spark",
    "section": "",
    "text": "R/spark_home.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_dir.html#spark_home_dir",
    "href": "packages/sparklyr/latest/reference/spark_home_dir.html#spark_home_dir",
    "title": "Find the SPARK_HOME directory for a version of Spark",
    "section": "spark_home_dir",
    "text": "spark_home_dir"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_dir.html#description",
    "href": "packages/sparklyr/latest/reference/spark_home_dir.html#description",
    "title": "Find the SPARK_HOME directory for a version of Spark",
    "section": "Description",
    "text": "Description\nFind the SPARK_HOME directory for a given version of Spark that was previously installed using spark_install."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_dir.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_home_dir.html#usage",
    "title": "Find the SPARK_HOME directory for a version of Spark",
    "section": "Usage",
    "text": "Usage\nspark_home_dir(version = NULL, hadoop_version = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_dir.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_home_dir.html#arguments",
    "title": "Find the SPARK_HOME directory for a version of Spark",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nversion\nVersion of Spark\n\n\nhadoop_version\nVersion of Hadoop"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_dir.html#value",
    "href": "packages/sparklyr/latest/reference/spark_home_dir.html#value",
    "title": "Find the SPARK_HOME directory for a version of Spark",
    "section": "Value",
    "text": "Value\nPath to SPARK_HOME (or NULL if the specified version was not found)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_config.html",
    "href": "packages/sparklyr/latest/reference/connection_config.html",
    "title": "Read configuration values for a connection",
    "section": "",
    "text": "R/spark_connection.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_config.html#connection_config",
    "href": "packages/sparklyr/latest/reference/connection_config.html#connection_config",
    "title": "Read configuration values for a connection",
    "section": "connection_config",
    "text": "connection_config"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_config.html#description",
    "href": "packages/sparklyr/latest/reference/connection_config.html#description",
    "title": "Read configuration values for a connection",
    "section": "Description",
    "text": "Description\nRead configuration values for a connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_config.html#usage",
    "href": "packages/sparklyr/latest/reference/connection_config.html#usage",
    "title": "Read configuration values for a connection",
    "section": "Usage",
    "text": "Usage\nconnection_config(sc, prefix, not_prefix = list())"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_config.html#arguments",
    "href": "packages/sparklyr/latest/reference/connection_config.html#arguments",
    "title": "Read configuration values for a connection",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nspark_connection\n\n\nprefix\nPrefix to read parameters for (e.g. spark.context., spark.sql., etc.)\n\n\nnot_prefix\nPrefix to not include."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_config.html#value",
    "href": "packages/sparklyr/latest/reference/connection_config.html#value",
    "title": "Read configuration values for a connection",
    "section": "Value",
    "text": "Value\nNamed list of config parameters (note that if a prefix was specified then the names will not include the prefix)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj.html",
    "href": "packages/sparklyr/latest/reference/spark_jobj.html",
    "title": "Retrieve a Spark JVM Object Reference",
    "section": "",
    "text": "R/core_jobj.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj.html#spark_jobj",
    "href": "packages/sparklyr/latest/reference/spark_jobj.html#spark_jobj",
    "title": "Retrieve a Spark JVM Object Reference",
    "section": "spark_jobj",
    "text": "spark_jobj"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj.html#description",
    "href": "packages/sparklyr/latest/reference/spark_jobj.html#description",
    "title": "Retrieve a Spark JVM Object Reference",
    "section": "Description",
    "text": "Description\nThis S3 generic is used for accessing the underlying Java Virtual Machine (JVM) Spark objects associated with R objects. These objects act as references to Spark objects living in the JVM. Methods on these objects can be called with the invoke family of functions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_jobj.html#usage",
    "title": "Retrieve a Spark JVM Object Reference",
    "section": "Usage",
    "text": "Usage\nspark_jobj(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_jobj.html#arguments",
    "title": "Retrieve a Spark JVM Object Reference",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn R object containing, or wrapping, a spark_jobj.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_jobj.html#see-also",
    "title": "Retrieve a Spark JVM Object Reference",
    "section": "See Also",
    "text": "See Also\ninvoke, for calling methods on Java object references."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect.html",
    "href": "packages/sparklyr/latest/reference/collect.html",
    "title": "Collect",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect.html#collect",
    "href": "packages/sparklyr/latest/reference/collect.html#collect",
    "title": "Collect",
    "section": "collect",
    "text": "collect"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect.html#description",
    "href": "packages/sparklyr/latest/reference/collect.html#description",
    "title": "Collect",
    "section": "Description",
    "text": "Description\nSee collect for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/dplyr_hof.html",
    "href": "packages/sparklyr/latest/reference/dplyr_hof.html",
    "title": "dplyr wrappers for Apache Spark higher order functions",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/dplyr_hof.html#dplyr_hof",
    "href": "packages/sparklyr/latest/reference/dplyr_hof.html#dplyr_hof",
    "title": "dplyr wrappers for Apache Spark higher order functions",
    "section": "dplyr_hof",
    "text": "dplyr_hof"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/dplyr_hof.html#description",
    "href": "packages/sparklyr/latest/reference/dplyr_hof.html#description",
    "title": "dplyr wrappers for Apache Spark higher order functions",
    "section": "Description",
    "text": "Description\nThese methods implement dplyr grammars for Apache Spark higher order functions"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html",
    "title": "Feature Transformation – Imputer (Estimator)",
    "section": "",
    "text": "R/ml_feature_imputer.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#ft_imputer",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#ft_imputer",
    "title": "Feature Transformation – Imputer (Estimator)",
    "section": "ft_imputer",
    "text": "ft_imputer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#description",
    "title": "Feature Transformation – Imputer (Estimator)",
    "section": "Description",
    "text": "Description\nImputation estimator for completing missing values, either using the mean or the median of the columns in which the missing values are located. The input columns should be of numeric type. This function requires Spark 2.2.0+."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#usage",
    "title": "Feature Transformation – Imputer (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_imputer( \n  x, \n  input_cols = NULL, \n  output_cols = NULL, \n  missing_value = NULL, \n  strategy = \"mean\", \n  uid = random_string(\"imputer_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#arguments",
    "title": "Feature Transformation – Imputer (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_cols\nThe names of the input columns\n\n\noutput_cols\nThe names of the output columns.\n\n\nmissing_value\nThe placeholder for the missing values. All occurrences of missing_value will be imputed. Note that null values are always treated as missing.\n\n\nstrategy\nThe imputation strategy. Currently only “mean” and “median” are supported. If “mean”, then replace missing values using the mean value of the feature. If “median”, then replace missing values using the approximate median value of the feature. Default: mean\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#details",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#details",
    "title": "Feature Transformation – Imputer (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#value",
    "title": "Feature Transformation – Imputer (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#see-also",
    "title": "Feature Transformation – Imputer (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html",
    "href": "packages/sparklyr/latest/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\nspark_connect() spark_connection_is_open() spark_disconnect() spark_disconnect_all() spark_submit()\nManage Spark Connections\n\n\nspark_connect() spark_connection_is_open() spark_disconnect() spark_disconnect_all() spark_submit()\nManage Spark Connections\n\n\nspark_config()\nRead Spark Configuration\n\n\nspark_install() spark_uninstall() spark_install_dir() spark_install_tar() spark_installed_versions() spark_available_versions()\nDownload and install various versions of Spark\n\n\nspark_log()\nView Entries in the Spark Log\n\n\nspark_web()\nOpen the Spark web interface",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-sessions",
    "href": "packages/sparklyr/latest/reference/index.html#spark-sessions",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\nspark_connect() spark_connection_is_open() spark_disconnect() spark_disconnect_all() spark_submit()\nManage Spark Connections\n\n\nspark_connect() spark_connection_is_open() spark_disconnect() spark_disconnect_all() spark_submit()\nManage Spark Connections\n\n\nspark_config()\nRead Spark Configuration\n\n\nspark_install() spark_uninstall() spark_install_dir() spark_install_tar() spark_installed_versions() spark_available_versions()\nDownload and install various versions of Spark\n\n\nspark_log()\nView Entries in the Spark Log\n\n\nspark_web()\nOpen the Spark web interface",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-data",
    "href": "packages/sparklyr/latest/reference/index.html#spark-data",
    "title": "sparklyr",
    "section": "Spark Data",
    "text": "Spark Data\n\n\n\nFunction(s)\nDescription\n\n\n\n\nspark_read()\nRead file(s) into a Spark DataFrame using a custom reader\n\n\nspark_read_avro()\nRead Apache Avro data into a Spark DataFrame.\n\n\nspark_read_binary()\nRead binary data into a Spark DataFrame.\n\n\nspark_read_csv()\nRead a CSV file into a Spark DataFrame\n\n\nspark_read_delta()\nRead from Delta Lake into a Spark DataFrame.\n\n\nspark_read_image()\nRead image data into a Spark DataFrame.\n\n\nspark_read_jdbc()\nRead from JDBC connection into a Spark DataFrame.\n\n\nspark_read_json()\nRead a JSON file into a Spark DataFrame\n\n\nspark_read_libsvm()\nRead libsvm file into a Spark DataFrame.\n\n\nspark_read_orc()\nRead a ORC file into a Spark DataFrame\n\n\nspark_read_parquet()\nRead a Parquet file into a Spark DataFrame\n\n\nspark_read_source()\nRead from a generic source into a Spark DataFrame.\n\n\nspark_read_table()\nReads from a Spark Table into a Spark DataFrame.\n\n\nspark_read_text()\nRead a Text file into a Spark DataFrame\n\n\nspark_write()\nWrite Spark DataFrame to file using a custom writer\n\n\nspark_write_avro()\nSerialize a Spark DataFrame into Apache Avro format\n\n\nspark_write_csv()\nWrite a Spark DataFrame to a CSV\n\n\nspark_write_delta()\nWrites a Spark DataFrame into Delta Lake\n\n\nspark_write_jdbc()\nWrites a Spark DataFrame into a JDBC table\n\n\nspark_write_json()\nWrite a Spark DataFrame to a JSON file\n\n\nspark_write_orc()\nWrite a Spark DataFrame to a ORC file\n\n\nspark_write_parquet()\nWrite a Spark DataFrame to a Parquet file\n\n\nspark_write_rds()\nWrite Spark DataFrame to RDS files\n\n\nspark_write_source()\nWrites a Spark DataFrame into a generic source\n\n\nspark_write_table()\nWrites a Spark DataFrame into a Spark table\n\n\nspark_write_text()\nWrite a Spark DataFrame to a Text file\n\n\nspark_insert_table()\nInserts a Spark DataFrame into a Spark table\n\n\nspark_save_table()\nSaves a Spark DataFrame as a Spark table\n\n\ncollect_from_rds()\nCollect Spark data serialized in RDS format into R",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-tables",
    "href": "packages/sparklyr/latest/reference/index.html#spark-tables",
    "title": "sparklyr",
    "section": "Spark Tables",
    "text": "Spark Tables\n\n\n\nFunction(s)\nDescription\n\n\n\n\nsrc_databases()\nShow database list\n\n\ntbl_cache()\nCache a Spark Table\n\n\ntbl_change_db()\nUse specific database\n\n\ntbl_uncache()\nUncache a Spark Table",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-dataframes",
    "href": "packages/sparklyr/latest/reference/index.html#spark-dataframes",
    "title": "sparklyr",
    "section": "Spark DataFrames",
    "text": "Spark DataFrames\n\n\n\nFunction(s)\nDescription\n\n\n\n\ndplyr_hof\ndplyr wrappers for Apache Spark higher order functions\n\n\nsdf_save_table() sdf_load_table() sdf_save_parquet() sdf_load_parquet()\nSave / Load a Spark DataFrame\n\n\nsdf_predict() sdf_transform() sdf_fit() sdf_fit_and_transform()\nSpark ML – Transform, fit, and predict methods (sdf_ interface)\n\n\nsdf_along()\nCreate DataFrame for along Object\n\n\nsdf_bind_rows() sdf_bind_cols()\nBind multiple Spark DataFrames by row and column\n\n\nsdf_broadcast()\nBroadcast hint\n\n\nsdf_checkpoint()\nCheckpoint a Spark DataFrame\n\n\nsdf_coalesce()\nCoalesces a Spark DataFrame\n\n\nsdf_collect()\nCollect a Spark DataFrame into R.\n\n\nsdf_copy_to() sdf_import()\nCopy an Object into Spark\n\n\nsdf_crosstab()\nCross Tabulation\n\n\nsdf_debug_string()\nDebug Info for Spark DataFrame\n\n\nsdf_describe()\nCompute summary statistics for columns of a data frame\n\n\nsdf_dim() sdf_nrow() sdf_ncol()\nSupport for Dimension Operations\n\n\nsdf_distinct()\nInvoke distinct on a Spark DataFrame\n\n\nsdf_drop_duplicates()\nRemove duplicates from a Spark DataFrame\n\n\nsdf_expand_grid()\nCreate a Spark dataframe containing all combinations of inputs\n\n\nsdf_from_avro()\nConvert column(s) from avro format\n\n\nsdf_is_streaming()\nSpark DataFrame is Streaming\n\n\nsdf_last_index()\nReturns the last index of a Spark DataFrame\n\n\nsdf_len()\nCreate DataFrame for Length\n\n\nsdf_num_partitions()\nGets number of partitions of a Spark DataFrame\n\n\nsdf_partition_sizes()\nCompute the number of records within each partition of a Spark DataFrame\n\n\nsdf_persist()\nPersist a Spark DataFrame\n\n\nsdf_pivot()\nPivot a Spark DataFrame\n\n\nsdf_project()\nProject features onto principal components\n\n\nsdf_quantile()\nCompute (Approximate) Quantiles with a Spark DataFrame\n\n\nsdf_random_split() sdf_partition()\nPartition a Spark Dataframe\n\n\nsdf_rbeta()\nGenerate random samples from a Beta distribution\n\n\nsdf_rbinom()\nGenerate random samples from a binomial distribution\n\n\nsdf_rcauchy()\nGenerate random samples from a Cauchy distribution\n\n\nsdf_rchisq()\nGenerate random samples from a chi-squared distribution\n\n\nsdf_read_column()\nRead a Column from a Spark DataFrame\n\n\nsdf_register()\nRegister a Spark DataFrame\n\n\nsdf_repartition()\nRepartition a Spark DataFrame\n\n\nsdf_residuals()\nModel Residuals\n\n\nsdf_rexp()\nGenerate random samples from an exponential distribution\n\n\nsdf_rgamma()\nGenerate random samples from a Gamma distribution\n\n\nsdf_rgeom()\nGenerate random samples from a geometric distribution\n\n\nsdf_rhyper()\nGenerate random samples from a hypergeometric distribution\n\n\nsdf_rlnorm()\nGenerate random samples from a log normal distribution\n\n\nsdf_rnorm()\nGenerate random samples from the standard normal distribution\n\n\nsdf_rpois()\nGenerate random samples from a Poisson distribution\n\n\nsdf_rt()\nGenerate random samples from a t-distribution\n\n\nsdf_runif()\nGenerate random samples from the uniform distribution U(0, 1).\n\n\nsdf_rweibull()\nGenerate random samples from a Weibull distribution.\n\n\nsdf_sample()\nRandomly Sample Rows from a Spark DataFrame\n\n\nsdf_schema()\nRead the Schema of a Spark DataFrame\n\n\nsdf_separate_column()\nSeparate a Vector Column into Scalar Columns\n\n\nsdf_seq()\nCreate DataFrame for Range\n\n\nsdf_sort()\nSort a Spark DataFrame\n\n\nsdf_sql()\nSpark DataFrame from SQL\n\n\nsdf_to_avro()\nConvert column(s) to avro format\n\n\nsdf_unnest_longer()\nUnnest longer\n\n\nsdf_unnest_wider()\nUnnest wider\n\n\nsdf_weighted_sample()\nPerform Weighted Random Sampling on a Spark DataFrame\n\n\nsdf_with_sequential_id()\nAdd a Sequential ID Column to a Spark DataFrame\n\n\nsdf_with_unique_id()\nAdd a Unique ID Column to a Spark DataFrame\n\n\nhof_aggregate()\nApply Aggregate Function to Array Column\n\n\nhof_array_sort()\nSorts array using a custom comparator\n\n\nhof_exists()\nDetermine Whether Some Element Exists in an Array Column\n\n\nhof_filter()\nFilter Array Column\n\n\nhof_forall()\nChecks whether all elements in an array satisfy a predicate\n\n\nhof_map_filter()\nFilters a map\n\n\nhof_map_zip_with()\nMerges two maps into one\n\n\nhof_transform()\nTransform Array Column\n\n\nhof_transform_keys()\nTransforms keys of a map\n\n\nhof_transform_values()\nTransforms values of a map\n\n\nhof_zip_with()\nCombines 2 Array Columns\n\n\ntransform_sdf()\ntransform a subset of column(s) in a Spark Dataframe",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---regression",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---regression",
    "title": "sparklyr",
    "section": "Spark ML - Regression",
    "text": "Spark ML - Regression\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_linear_regression()\nSpark ML – Linear Regression\n\n\nml_aft_survival_regression() ml_survival_regression()\nSpark ML – Survival Regression\n\n\nml_isotonic_regression()\nSpark ML – Isotonic Regression\n\n\nml_aft_survival_regression() ml_survival_regression()\nSpark ML – Survival Regression\n\n\nml_generalized_linear_regression()\nSpark ML – Generalized Linear Regression",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---classification",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---classification",
    "title": "sparklyr",
    "section": "Spark ML - Classification",
    "text": "Spark ML - Classification\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_naive_bayes()\nSpark ML – Naive-Bayes\n\n\nml_one_vs_rest()\nSpark ML – OneVsRest\n\n\nml_logistic_regression()\nSpark ML – Logistic Regression\n\n\nml_multilayer_perceptron_classifier() ml_multilayer_perceptron()\nSpark ML – Multilayer Perceptron\n\n\nml_linear_svc()\nSpark ML – LinearSVC",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---tree",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---tree",
    "title": "sparklyr",
    "section": "Spark ML - Tree",
    "text": "Spark ML - Tree\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_decision_tree_classifier() ml_decision_tree() ml_decision_tree_regressor()\nSpark ML – Decision Trees\n\n\nml_decision_tree_classifier() ml_decision_tree() ml_decision_tree_regressor()\nSpark ML – Decision Trees\n\n\nml_decision_tree_classifier() ml_decision_tree() ml_decision_tree_regressor()\nSpark ML – Decision Trees\n\n\nml_gbt_classifier() ml_gradient_boosted_trees() ml_gbt_regressor()\nSpark ML – Gradient Boosted Trees\n\n\nml_gbt_classifier() ml_gradient_boosted_trees() ml_gbt_regressor()\nSpark ML – Gradient Boosted Trees\n\n\nml_gbt_classifier() ml_gradient_boosted_trees() ml_gbt_regressor()\nSpark ML – Gradient Boosted Trees\n\n\nml_random_forest_classifier() ml_random_forest() ml_random_forest_regressor()\nSpark ML – Random Forest\n\n\nml_random_forest_classifier() ml_random_forest() ml_random_forest_regressor()\nSpark ML – Random Forest\n\n\nml_random_forest_classifier() ml_random_forest() ml_random_forest_regressor()\nSpark ML – Random Forest\n\n\nml_feature_importances() ml_tree_feature_importance()\nSpark ML - Feature Importance for Tree Models\n\n\nml_feature_importances() ml_tree_feature_importance()\nSpark ML - Feature Importance for Tree Models",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---clustering",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---clustering",
    "title": "sparklyr",
    "section": "Spark ML - Clustering",
    "text": "Spark ML - Clustering\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_kmeans() ml_compute_cost() ml_compute_silhouette_measure()\nSpark ML – K-Means Clustering\n\n\nml_kmeans_cluster_eval\nEvaluate a K-mean clustering\n\n\nml_bisecting_kmeans()\nSpark ML – Bisecting K-Means Clustering\n\n\nml_gaussian_mixture()\nSpark ML – Gaussian Mixture clustering.\n\n\nml_kmeans() ml_compute_cost() ml_compute_silhouette_measure()\nSpark ML – K-Means Clustering\n\n\nml_power_iteration()\nSpark ML – Power Iteration Clustering",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---text",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---text",
    "title": "sparklyr",
    "section": "Spark ML - Text",
    "text": "Spark ML - Text\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_lda() ml_describe_topics() ml_log_likelihood() ml_log_perplexity() ml_topics_matrix()\nSpark ML – Latent Dirichlet Allocation\n\n\nml_chisquare_test()\nChi-square hypothesis testing for categorical data.\n\n\nml_default_stop_words()\nDefault stop words\n\n\nml_fpgrowth() ml_association_rules() ml_freq_itemsets()\nFrequent Pattern Mining – FPGrowth\n\n\nml_prefixspan() ml_freq_seq_patterns()\nFrequent Pattern Mining – PrefixSpan\n\n\nft_count_vectorizer() ml_vocabulary()\nFeature Transformation – CountVectorizer (Estimator)",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---recommendations",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---recommendations",
    "title": "sparklyr",
    "section": "Spark ML - Recommendations",
    "text": "Spark ML - Recommendations\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_als() ml_recommend()\nSpark ML – ALS",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---hyper-parameter-tuning",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---hyper-parameter-tuning",
    "title": "sparklyr",
    "section": "Spark ML - Hyper-parameter tuning",
    "text": "Spark ML - Hyper-parameter tuning\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_sub_models() ml_validation_metrics() ml_cross_validator() ml_train_validation_split()\nSpark ML – Tuning",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---evaluation",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---evaluation",
    "title": "sparklyr",
    "section": "Spark ML - Evaluation",
    "text": "Spark ML - Evaluation\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_metrics_binary()\nExtracts metrics from a fitted table\n\n\nml_metrics_multiclass()\nExtracts metrics from a fitted table\n\n\nml_metrics_regression()\nExtracts metrics from a fitted table\n\n\nml_evaluate()\nEvaluate the Model on a Validation Set\n\n\nml_binary_classification_evaluator() ml_binary_classification_eval() ml_multiclass_classification_evaluator() ml_classification_eval() ml_regression_evaluator()\nSpark ML - Evaluators\n\n\nml_binary_classification_evaluator() ml_binary_classification_eval() ml_multiclass_classification_evaluator() ml_classification_eval() ml_regression_evaluator()\nSpark ML - Evaluators\n\n\nml_clustering_evaluator()\nSpark ML - Clustering Evaluator\n\n\nml_binary_classification_evaluator() ml_binary_classification_eval() ml_multiclass_classification_evaluator() ml_classification_eval() ml_regression_evaluator()\nSpark ML - Evaluators",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-ml---operations",
    "href": "packages/sparklyr/latest/reference/index.html#spark-ml---operations",
    "title": "sparklyr",
    "section": "Spark ML - Operations",
    "text": "Spark ML - Operations\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_model_data()\nExtracts data associated with a Spark ML model\n\n\nml_call_constructor()\nWrap a Spark ML JVM object\n\n\nml_corr()\nCompute correlation matrix\n\n\nis_ml_transformer() is_ml_estimator() ml_fit() ml_transform() ml_fit_and_transform() ml_predict()\nSpark ML – Transform, fit, and predict methods (ml_ interface)\n\n\nis_ml_transformer() is_ml_estimator() ml_fit() ml_transform() ml_fit_and_transform() ml_predict()\nSpark ML – Transform, fit, and predict methods (ml_ interface)\n\n\nft_string_indexer() ml_labels() ft_string_indexer_model()\nFeature Transformation – StringIndexer (Estimator)\n\n\nml_save() ml_load()\nSpark ML – Model Persistence\n\n\nml_is_set() ml_param_map() ml_param() ml_params()\nSpark ML – ML Params\n\n\nis_ml_transformer() is_ml_estimator() ml_fit() ml_transform() ml_fit_and_transform() ml_predict()\nSpark ML – Transform, fit, and predict methods (ml_ interface)\n\n\nml_save() ml_load()\nSpark ML – Model Persistence\n\n\nml_standardize_formula()\nStandardize Formula Input for ml_model\n\n\nml_summary()\nSpark ML – Extraction of summary metrics\n\n\nml_supervised_pipeline() ml_clustering_pipeline() ml_construct_model_supervised() ml_construct_model_clustering() new_ml_model_prediction() new_ml_model() new_ml_model_classification() new_ml_model_regression() new_ml_model_clustering()\nConstructors for ml_model Objects\n\n\nis_ml_transformer() is_ml_estimator() ml_fit() ml_transform() ml_fit_and_transform() ml_predict()\nSpark ML – Transform, fit, and predict methods (ml_ interface)\n\n\nml_uid()\nSpark ML – UID",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-pipelines",
    "href": "packages/sparklyr/latest/reference/index.html#spark-pipelines",
    "title": "sparklyr",
    "section": "Spark Pipelines",
    "text": "Spark Pipelines\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_pipeline()\nSpark ML – Pipelines\n\n\nml_stage() ml_stages()\nSpark ML – Pipeline stage extraction\n\n\nml_add_stage()\nAdd a Stage to a Pipeline",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-feature-transformers",
    "href": "packages/sparklyr/latest/reference/index.html#spark-feature-transformers",
    "title": "sparklyr",
    "section": "Spark Feature Transformers",
    "text": "Spark Feature Transformers\n\n\n\nFunction(s)\nDescription\n\n\n\n\nft_binarizer()\nFeature Transformation – Binarizer (Transformer)\n\n\nft_bucketizer()\nFeature Transformation – Bucketizer (Transformer)\n\n\nft_chisq_selector()\nFeature Transformation – ChiSqSelector (Estimator)\n\n\nft_count_vectorizer() ml_vocabulary()\nFeature Transformation – CountVectorizer (Estimator)\n\n\nft_dct() ft_discrete_cosine_transform()\nFeature Transformation – Discrete Cosine Transform (DCT) (Transformer)\n\n\nft_elementwise_product()\nFeature Transformation – ElementwiseProduct (Transformer)\n\n\nft_feature_hasher()\nFeature Transformation – FeatureHasher (Transformer)\n\n\nft_hashing_tf()\nFeature Transformation – HashingTF (Transformer)\n\n\nft_idf()\nFeature Transformation – IDF (Estimator)\n\n\nft_imputer()\nFeature Transformation – Imputer (Estimator)\n\n\nft_index_to_string()\nFeature Transformation – IndexToString (Transformer)\n\n\nft_interaction()\nFeature Transformation – Interaction (Transformer)\n\n\nft_bucketed_random_projection_lsh() ft_minhash_lsh()\nFeature Transformation – LSH (Estimator)\n\n\nml_approx_nearest_neighbors() ml_approx_similarity_join()\nUtility functions for LSH models\n\n\nft_max_abs_scaler()\nFeature Transformation – MaxAbsScaler (Estimator)\n\n\nft_min_max_scaler()\nFeature Transformation – MinMaxScaler (Estimator)\n\n\nft_ngram()\nFeature Transformation – NGram (Transformer)\n\n\nft_normalizer()\nFeature Transformation – Normalizer (Transformer)\n\n\nft_one_hot_encoder()\nFeature Transformation – OneHotEncoder (Transformer)\n\n\nft_one_hot_encoder_estimator()\nFeature Transformation – OneHotEncoderEstimator (Estimator)\n\n\nft_pca() ml_pca()\nFeature Transformation – PCA (Estimator)\n\n\nft_polynomial_expansion()\nFeature Transformation – PolynomialExpansion (Transformer)\n\n\nft_quantile_discretizer()\nFeature Transformation – QuantileDiscretizer (Estimator)\n\n\nft_r_formula()\nFeature Transformation – RFormula (Estimator)\n\n\nft_regex_tokenizer()\nFeature Transformation – RegexTokenizer (Transformer)\n\n\nft_robust_scaler()\nFeature Transformation – RobustScaler (Estimator)\n\n\nft_standard_scaler()\nFeature Transformation – StandardScaler (Estimator)\n\n\nft_stop_words_remover()\nFeature Transformation – StopWordsRemover (Transformer)\n\n\nft_string_indexer() ml_labels() ft_string_indexer_model()\nFeature Transformation – StringIndexer (Estimator)\n\n\nft_tokenizer()\nFeature Transformation – Tokenizer (Transformer)\n\n\nft_vector_assembler()\nFeature Transformation – VectorAssembler (Transformer)\n\n\nft_vector_indexer()\nFeature Transformation – VectorIndexer (Estimator)\n\n\nft_vector_slicer()\nFeature Transformation – VectorSlicer (Transformer)\n\n\nft_word2vec() ml_find_synonyms()\nFeature Transformation – Word2Vec (Estimator)\n\n\nft_sql_transformer() ft_dplyr_transformer()\nFeature Transformation – SQLTransformer\n\n\nft_pca() ml_pca()\nFeature Transformation – PCA (Estimator)",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#extensions",
    "href": "packages/sparklyr/latest/reference/index.html#extensions",
    "title": "sparklyr",
    "section": "Extensions",
    "text": "Extensions\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_supervised_pipeline() ml_clustering_pipeline() ml_construct_model_supervised() ml_construct_model_clustering() new_ml_model_prediction() new_ml_model() new_ml_model_classification() new_ml_model_regression() new_ml_model_clustering()\nConstructors for ml_model Objects\n\n\ncompile_package_jars()\nCompile Scala sources into a Java Archive (jar)\n\n\nconnection_config()\nRead configuration values for a connection\n\n\ndownload_scalac()\nDownloads default Scala Compilers\n\n\nfind_scalac()\nDiscover the Scala Compiler\n\n\nspark_context() java_context() hive_context() spark_session()\nAccess the Spark API\n\n\nhive_context_config()\nRuntime configuration interface for Hive\n\n\ninvoke() invoke_static() invoke_new()\nInvoke a Method on a JVM Object\n\n\nj_invoke() j_invoke_static() j_invoke_new()\nInvoke a Java function.\n\n\njarray()\nInstantiate a Java array with a specific element type.\n\n\njfloat()\nInstantiate a Java float type.\n\n\njfloat_array()\nInstantiate an Array[Float].\n\n\nspark_context() java_context() hive_context() spark_session()\nAccess the Spark API\n\n\nregister_extension() registered_extensions()\nRegister a Package that Implements a Spark Extension\n\n\nspark_compilation_spec()\nDefine a Spark Compilation Specification\n\n\nspark_default_compilation_spec()\nDefault Compilation Specification for Spark Extensions\n\n\nspark_context() java_context() hive_context() spark_session()\nAccess the Spark API\n\n\nspark_context_config()\nRuntime configuration interface for the Spark Context.\n\n\nspark_dataframe()\nRetrieve a Spark DataFrame\n\n\nspark_dependency()\nDefine a Spark dependency\n\n\nspark_home_set()\nSet the SPARK_HOME environment variable\n\n\nspark_jobj()\nRetrieve a Spark JVM Object Reference\n\n\nspark_context() java_context() hive_context() spark_session()\nAccess the Spark API\n\n\nspark_version()\nGet the Spark Version Associated with a Spark Connection",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#distributed-computing",
    "href": "packages/sparklyr/latest/reference/index.html#distributed-computing",
    "title": "sparklyr",
    "section": "Distributed Computing",
    "text": "Distributed Computing\n\n\n\nFunction(s)\nDescription\n\n\n\n\nspark_apply()\nApply an R Function in Spark\n\n\nspark_apply_bundle()\nCreate Bundle for Spark Apply\n\n\nspark_apply_log()\nLog Writer for Spark Apply\n\n\nregisterDoSpark()\nRegister a Parallel Backend",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#livy",
    "href": "packages/sparklyr/latest/reference/index.html#livy",
    "title": "sparklyr",
    "section": "Livy",
    "text": "Livy\n\n\n\nFunction(s)\nDescription\n\n\n\n\nlivy_config()\nCreate a Spark Configuration for Livy\n\n\nlivy_service_start() livy_service_stop()\nStart Livy",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#streaming",
    "href": "packages/sparklyr/latest/reference/index.html#streaming",
    "title": "sparklyr",
    "section": "Streaming",
    "text": "Streaming\n\n\n\nFunction(s)\nDescription\n\n\n\n\nstream_find()\nFind Stream\n\n\nstream_generate_test()\nGenerate Test Stream\n\n\nstream_id()\nSpark Stream’s Identifier\n\n\nstream_lag()\nApply lag function to columns of a Spark Streaming DataFrame\n\n\nstream_name()\nSpark Stream’s Name\n\n\nstream_read_csv()\nRead CSV Stream\n\n\nstream_read_delta()\nRead Delta Stream\n\n\nstream_read_json()\nRead JSON Stream\n\n\nstream_read_kafka()\nRead Kafka Stream\n\n\nstream_read_orc()\nRead ORC Stream\n\n\nstream_read_parquet()\nRead Parquet Stream\n\n\nstream_read_socket()\nRead Socket Stream\n\n\nstream_read_text()\nRead Text Stream\n\n\nstream_render()\nRender Stream\n\n\nstream_stats()\nStream Statistics\n\n\nstream_stop()\nStops a Spark Stream\n\n\nstream_trigger_continuous()\nSpark Stream Continuous Trigger\n\n\nstream_trigger_interval()\nSpark Stream Interval Trigger\n\n\nstream_view()\nView Stream\n\n\nstream_watermark()\nWatermark Stream\n\n\nstream_write_console()\nWrite Console Stream\n\n\nstream_write_csv()\nWrite CSV Stream\n\n\nstream_write_delta()\nWrite Delta Stream\n\n\nstream_write_json()\nWrite JSON Stream\n\n\nstream_write_kafka()\nWrite Kafka Stream\n\n\nstream_write_memory()\nWrite Memory Stream\n\n\nstream_write_orc()\nWrite a ORC Stream\n\n\nstream_write_parquet()\nWrite Parquet Stream\n\n\nstream_write_text()\nWrite Text Stream\n\n\nreactiveSpark()\nReactive spark reader",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#dplyr-integration",
    "href": "packages/sparklyr/latest/reference/index.html#dplyr-integration",
    "title": "sparklyr",
    "section": "dplyr integration",
    "text": "dplyr integration\n\n\n\nFunction(s)\nDescription\n\n\n\n\ncopy_to()\nCopy an R Data Frame to Spark\n\n\ndistinct\nDistinct\n\n\nfilter\nFilter\n\n\nfull_join\nFull join\n\n\ninner_join\nInner join\n\n\ninner_join() left_join() right_join() full_join()\nJoin Spark tbls.\n\n\nleft_join\nLeft join\n\n\nmutate\nMutate\n\n\nright_join\nRight join\n\n\nselect\nSelect",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#tidyr-integration",
    "href": "packages/sparklyr/latest/reference/index.html#tidyr-integration",
    "title": "sparklyr",
    "section": "tidyr integration",
    "text": "tidyr integration\n\n\n\nFunction(s)\nDescription\n\n\n\n\npivot_longer\nPivot longer\n\n\npivot_wider\nPivot wider\n\n\nfill\nFill\n\n\nna.replace()\nReplace Missing Values in Objects\n\n\nnest\nNest\n\n\nreplace_na\nReplace NA\n\n\nseparate\nSeparate\n\n\nunite\nUnite\n\n\nunnest\nUnnest",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#tidymodels-integration",
    "href": "packages/sparklyr/latest/reference/index.html#tidymodels-integration",
    "title": "sparklyr",
    "section": "tidymodels integration",
    "text": "tidymodels integration\n\n\n\nFunction(s)\nDescription\n\n\n\n\ntidy() augment() glance()\nTidying methods for Spark ML ALS\n\n\nml_glm_tidiers tidy.ml_model_generalized_linear_regression tidy.ml_model_linear_regression augment.ml_model_generalized_linear_regression augment._ml_model_linear_regression augment.ml_model_linear_regression glance.ml_model_generalized_linear_regression glance.ml_model_linear_regression\nTidying methods for Spark ML linear models\n\n\ntidy() augment() glance()\nTidying methods for Spark ML Isotonic Regression\n\n\ntidy() augment() glance()\nTidying methods for Spark ML LDA models\n\n\ntidy() augment() glance()\nTidying methods for Spark ML linear svc\n\n\nml_logistic_regression_tidiers tidy.ml_model_logistic_regression augment.ml_model_logistic_regression augment._ml_model_logistic_regression glance.ml_model_logistic_regression\nTidying methods for Spark ML Logistic Regression\n\n\ntidy() augment() glance()\nTidying methods for Spark ML MLP\n\n\ntidy() augment() glance()\nTidying methods for Spark ML Naive Bayes\n\n\ntidy() augment() glance()\nTidying methods for Spark ML Principal Component Analysis\n\n\ntidy() augment() glance()\nTidying methods for Spark ML Survival Regression\n\n\nml_tree_tidiers tidy.ml_model_decision_tree_classification tidy.ml_model_decision_tree_regression augment.ml_model_decision_tree_classification augment._ml_model_decision_tree_classification augment.ml_model_decision_tree_regression augment._ml_model_decision_tree_regression glance.ml_model_decision_tree_classification glance.ml_model_decision_tree_regression tidy.ml_model_random_forest_classification tidy.ml_model_random_forest_regression augment.ml_model_random_forest_classification augment._ml_model_random_forest_classification augment.ml_model_random_forest_regression augment._ml_model_random_forest_regression glance.ml_model_random_forest_classification glance.ml_model_random_forest_regression tidy.ml_model_gbt_classification tidy.ml_model_gbt_regression augment.ml_model_gbt_classification augment._ml_model_gbt_classification augment.ml_model_gbt_regression augment._ml_model_gbt_regression glance.ml_model_gbt_classification glance.ml_model_gbt_regression\nTidying methods for Spark ML tree models\n\n\ntidy() augment() glance() tidy() augment() glance() tidy() augment() glance()\nTidying methods for Spark ML unsupervised models",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-operations",
    "href": "packages/sparklyr/latest/reference/index.html#spark-operations",
    "title": "sparklyr",
    "section": "Spark Operations",
    "text": "Spark Operations\n\n\n\nFunction(s)\nDescription\n\n\n\n\nget_spark_sql_catalog_implementation()\nRetrieve the Spark connection’s SQL catalog implementation property\n\n\nconnection_is_open()\nCheck whether the connection is open\n\n\nconnection_spark_shinyapp()\nA Shiny app that can be used to construct a spark_connect statement\n\n\nspark_session_config()\nRuntime configuration interface for the Spark Session\n\n\nspark_set_checkpoint_dir() spark_get_checkpoint_dir()\nSet/Get Spark checkpoint directory\n\n\nspark_connect() spark_connection_is_open() spark_disconnect() spark_disconnect_all() spark_submit()\nManage Spark Connections\n\n\nspark_table_name()\nGenerate a Table Name from Expression\n\n\nspark_install() spark_uninstall() spark_install_dir() spark_install_tar() spark_installed_versions() spark_available_versions()\nDownload and install various versions of Spark\n\n\nspark_version_from_home()\nGet the Spark Version Associated with a Spark Installation\n\n\nspark_versions()\nRetrieves a dataframe available Spark versions that van be installed.\n\n\nspark_config_kubernetes()\nKubernetes Configuration\n\n\nspark_config_settings()\nRetrieve Available Settings\n\n\nspark_connection_find()\nFind Spark Connection\n\n\nspark_dependency_fallback()\nFallback to Spark Dependency\n\n\nspark_extension()\nCreate Spark Extension\n\n\nspark_load_table()\nReads from a Spark Table into a Spark DataFrame.\n\n\nlist_sparklyr_jars()\nlist all sparklyr-*.jar files that have been built\n\n\nspark_config_packages()\nCreates Spark Configuration\n\n\nspark_connection()\nRetrieve the Spark Connection Associated with an R Object\n\n\nspark_adaptive_query_execution()\nRetrieves or sets status of Spark AQE\n\n\nspark_advisory_shuffle_partition_size()\nRetrieves or sets advisory size of the shuffle partition\n\n\nspark_auto_broadcast_join_threshold()\nRetrieves or sets the auto broadcast join threshold\n\n\nspark_coalesce_initial_num_partitions()\nRetrieves or sets initial number of shuffle partitions before coalescing\n\n\nspark_coalesce_min_num_partitions()\nRetrieves or sets the minimum number of shuffle partitions after coalescing\n\n\nspark_coalesce_shuffle_partitions()\nRetrieves or sets whether coalescing contiguous shuffle partitions is enabled\n\n\nspark_connection-class\nspark_connection class\n\n\nspark_jobj-class\nspark_jobj class\n\n\nsparklyr_get_backend_port()\nReturn the port number of a sparklyr backend.",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#other",
    "href": "packages/sparklyr/latest/reference/index.html#other",
    "title": "sparklyr",
    "section": "Other",
    "text": "Other\n\n\n\nFunction(s)\nDescription\n\n\n\n\nspark_statistical_routines\nGenerate random samples from some distribution\n\n\nensure\nEnforce Specific Structure for R Objects\n\n\nrandom_string()\nRandom string generation\n\n\n%-&gt;%\nInfix operator for composing a lambda expression\n\n\n[()\nSubsetting operator for Spark dataframe\n\n\ngeneric_call_interface\nGeneric Call Interface",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparklyr/latest/reference/download_scalac.html",
    "href": "packages/sparklyr/latest/reference/download_scalac.html",
    "title": "Downloads default Scala Compilers",
    "section": "",
    "text": "R/spark_compile.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/download_scalac.html#download_scalac",
    "href": "packages/sparklyr/latest/reference/download_scalac.html#download_scalac",
    "title": "Downloads default Scala Compilers",
    "section": "download_scalac",
    "text": "download_scalac"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/download_scalac.html#description",
    "href": "packages/sparklyr/latest/reference/download_scalac.html#description",
    "title": "Downloads default Scala Compilers",
    "section": "Description",
    "text": "Description\ncompile_package_jars requires several versions of the scala compiler to work, this is to match Spark scala versions. To help setup your environment, this function will download the required compilers under the default search path."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/download_scalac.html#usage",
    "href": "packages/sparklyr/latest/reference/download_scalac.html#usage",
    "title": "Downloads default Scala Compilers",
    "section": "Usage",
    "text": "Usage\ndownload_scalac(dest_path = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/download_scalac.html#arguments",
    "href": "packages/sparklyr/latest/reference/download_scalac.html#arguments",
    "title": "Downloads default Scala Compilers",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\ndest_path\nThe destination path where scalac will be downloaded to."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/download_scalac.html#details",
    "href": "packages/sparklyr/latest/reference/download_scalac.html#details",
    "title": "Downloads default Scala Compilers",
    "section": "Details",
    "text": "Details\nSee find_scalac for a list of paths searched and used by this function to install the required compilers."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pivot_wider.html",
    "href": "packages/sparklyr/latest/reference/pivot_wider.html",
    "title": "Pivot wider",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pivot_wider.html#pivot_wider",
    "href": "packages/sparklyr/latest/reference/pivot_wider.html#pivot_wider",
    "title": "Pivot wider",
    "section": "pivot_wider",
    "text": "pivot_wider"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pivot_wider.html#description",
    "href": "packages/sparklyr/latest/reference/pivot_wider.html#description",
    "title": "Pivot wider",
    "section": "Description",
    "text": "Description\nSee pivot_wider for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html",
    "href": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html",
    "title": "Default Compilation Specification for Spark Extensions",
    "section": "",
    "text": "R/spark_compile.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#spark_default_compilation_spec",
    "href": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#spark_default_compilation_spec",
    "title": "Default Compilation Specification for Spark Extensions",
    "section": "spark_default_compilation_spec",
    "text": "spark_default_compilation_spec"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#description",
    "href": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#description",
    "title": "Default Compilation Specification for Spark Extensions",
    "section": "Description",
    "text": "Description\nThis is the default compilation specification used for Spark extensions, when used with compile_package_jars."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#usage",
    "title": "Default Compilation Specification for Spark Extensions",
    "section": "Usage",
    "text": "Usage\nspark_default_compilation_spec( \n  pkg = infer_active_package_name(), \n  locations = NULL \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#arguments",
    "title": "Default Compilation Specification for Spark Extensions",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\npkg\nThe package containing Spark extensions to be compiled.\n\n\nlocations\nAdditional locations to scan. By default, the directories /opt/scala and /usr/local/scala will be scanned."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_version.html",
    "href": "packages/sparklyr/latest/reference/spark_default_version.html",
    "title": "determine the version that will be used by default if version is NULL",
    "section": "",
    "text": "R/install_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_version.html#spark_default_version",
    "href": "packages/sparklyr/latest/reference/spark_default_version.html#spark_default_version",
    "title": "determine the version that will be used by default if version is NULL",
    "section": "spark_default_version",
    "text": "spark_default_version"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_version.html#description",
    "href": "packages/sparklyr/latest/reference/spark_default_version.html#description",
    "title": "determine the version that will be used by default if version is NULL",
    "section": "Description",
    "text": "Description\ndetermine the version that will be used by default if version is NULL"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_version.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_default_version.html#usage",
    "title": "determine the version that will be used by default if version is NULL",
    "section": "Usage",
    "text": "Usage\nspark_default_version()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-params.html",
    "href": "packages/sparklyr/latest/reference/ml-params.html",
    "title": "Spark ML – ML Params",
    "section": "",
    "text": "R/ml_param_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-params.html#ml-params",
    "href": "packages/sparklyr/latest/reference/ml-params.html#ml-params",
    "title": "Spark ML – ML Params",
    "section": "ml-params",
    "text": "ml-params"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-params.html#description",
    "href": "packages/sparklyr/latest/reference/ml-params.html#description",
    "title": "Spark ML – ML Params",
    "section": "Description",
    "text": "Description\nHelper methods for working with parameters for ML objects."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-params.html#usage",
    "href": "packages/sparklyr/latest/reference/ml-params.html#usage",
    "title": "Spark ML – ML Params",
    "section": "Usage",
    "text": "Usage\nml_is_set(x, param, ...) \n\nml_param_map(x, ...) \n\nml_param(x, param, allow_null = FALSE, ...) \n\nml_params(x, params = NULL, allow_null = FALSE, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-params.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml-params.html#arguments",
    "title": "Spark ML – ML Params",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark ML object, either a pipeline stage or an evaluator.\n\n\nparam\nThe parameter to extract or set.\n\n\n…\nOptional arguments; currently unused.\n\n\nallow_null\nWhether to allow NULL results when extracting parameters. If FALSE, an error will be thrown if the specified parameter is not found. Defaults to FALSE.\n\n\nparams\nA vector of parameters to extract."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_persist.html",
    "href": "packages/sparklyr/latest/reference/sdf_persist.html",
    "title": "Persist a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_persist.html#sdf_persist",
    "href": "packages/sparklyr/latest/reference/sdf_persist.html#sdf_persist",
    "title": "Persist a Spark DataFrame",
    "section": "sdf_persist",
    "text": "sdf_persist"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_persist.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_persist.html#description",
    "title": "Persist a Spark DataFrame",
    "section": "Description",
    "text": "Description\nPersist a Spark DataFrame, forcing any pending computations and (optionally) serializing the results to disk."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_persist.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_persist.html#usage",
    "title": "Persist a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_persist(x, storage.level = \"MEMORY_AND_DISK\", name = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_persist.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_persist.html#arguments",
    "title": "Persist a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nstorage.level\nThe storage level to be used. Please view the Spark Documentationfor information on what storage levels are accepted.\n\n\nname\nA name to assign this table. Passed to [sdf_register()]."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_persist.html#details",
    "href": "packages/sparklyr/latest/reference/sdf_persist.html#details",
    "title": "Persist a Spark DataFrame",
    "section": "Details",
    "text": "Details\nSpark DataFrames invoke their operations lazily – pending operations are deferred until their results are actually needed. Persisting a Spark DataFrame effectively ‘forces’ any pending computations, and then persists the generated Spark DataFrame as requested (to memory, to disk, or otherwise).\nUsers of Spark should be careful to persist the results of any computations which are non-deterministic – otherwise, one might see that the values within a column seem to ‘change’ as new operations are performed on that data set."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_web.html",
    "href": "packages/sparklyr/latest/reference/spark_web.html",
    "title": "Open the Spark web interface",
    "section": "",
    "text": "R/spark_connection.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_web.html#spark_web",
    "href": "packages/sparklyr/latest/reference/spark_web.html#spark_web",
    "title": "Open the Spark web interface",
    "section": "spark_web",
    "text": "spark_web"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_web.html#description",
    "href": "packages/sparklyr/latest/reference/spark_web.html#description",
    "title": "Open the Spark web interface",
    "section": "Description",
    "text": "Description\nOpen the Spark web interface"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_web.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_web.html#usage",
    "title": "Open the Spark web interface",
    "section": "Usage",
    "text": "Usage\nspark_web(sc, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_web.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_web.html#arguments",
    "title": "Open the Spark web interface",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html",
    "title": "Retrieves or sets the minimum number of shuffle partitions after coalescing",
    "section": "",
    "text": "R/spark_context_config.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#spark_coalesce_min_num_partitions",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#spark_coalesce_min_num_partitions",
    "title": "Retrieves or sets the minimum number of shuffle partitions after coalescing",
    "section": "spark_coalesce_min_num_partitions",
    "text": "spark_coalesce_min_num_partitions"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#description",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#description",
    "title": "Retrieves or sets the minimum number of shuffle partitions after coalescing",
    "section": "Description",
    "text": "Description\nRetrieves or sets the minimum number of shuffle partitions after coalescing"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#usage",
    "title": "Retrieves or sets the minimum number of shuffle partitions after coalescing",
    "section": "Usage",
    "text": "Usage\nspark_coalesce_min_num_partitions(sc, num_partitions = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#arguments",
    "title": "Retrieves or sets the minimum number of shuffle partitions after coalescing",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nnum_partitions\nMinimum number of shuffle partitions after coalescing. Defaults to NULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#see-also",
    "title": "Retrieves or sets the minimum number of shuffle partitions after coalescing",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_adaptive_query_execution(), spark_advisory_shuffle_partition_size(), spark_auto_broadcast_join_threshold(), spark_coalesce_initial_num_partitions(), spark_coalesce_shuffle_partitions(), spark_session_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rexp.html",
    "href": "packages/sparklyr/latest/reference/sdf_rexp.html",
    "title": "Generate random samples from an exponential distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rexp.html#sdf_rexp",
    "href": "packages/sparklyr/latest/reference/sdf_rexp.html#sdf_rexp",
    "title": "Generate random samples from an exponential distribution",
    "section": "sdf_rexp",
    "text": "sdf_rexp"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rexp.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rexp.html#description",
    "title": "Generate random samples from an exponential distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from an exponential distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rexp.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rexp.html#usage",
    "title": "Generate random samples from an exponential distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rexp(sc, n, rate = 1, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rexp.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rexp.html#arguments",
    "title": "Generate random samples from an exponential distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nrate\nRate of the exponential distribution (default: 1). The exponential distribution with rate lambda has mean 1 / lambda and density f(x) = lambda e ^- lambda x .\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rexp.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rexp.html#see-also",
    "title": "Generate random samples from an exponential distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect_from_rds.html",
    "href": "packages/sparklyr/latest/reference/collect_from_rds.html",
    "title": "Collect Spark data serialized in RDS format into R",
    "section": "",
    "text": "R/sdf_wrapper.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect_from_rds.html#collect_from_rds",
    "href": "packages/sparklyr/latest/reference/collect_from_rds.html#collect_from_rds",
    "title": "Collect Spark data serialized in RDS format into R",
    "section": "collect_from_rds",
    "text": "collect_from_rds"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect_from_rds.html#description",
    "href": "packages/sparklyr/latest/reference/collect_from_rds.html#description",
    "title": "Collect Spark data serialized in RDS format into R",
    "section": "Description",
    "text": "Description\nDeserialize Spark data that is serialized using spark_write_rds() into a R dataframe."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect_from_rds.html#usage",
    "href": "packages/sparklyr/latest/reference/collect_from_rds.html#usage",
    "title": "Collect Spark data serialized in RDS format into R",
    "section": "Usage",
    "text": "Usage\ncollect_from_rds(path)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect_from_rds.html#arguments",
    "href": "packages/sparklyr/latest/reference/collect_from_rds.html#arguments",
    "title": "Collect Spark data serialized in RDS format into R",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\npath\nPath to a local RDS file that is produced by spark_write_rds() (RDS files stored in HDFS will need to be downloaded to local filesystem first (e.g., by running hadoop fs -copyToLocal ... or similar)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect_from_rds.html#see-also",
    "href": "packages/sparklyr/latest/reference/collect_from_rds.html#see-also",
    "title": "Collect Spark data serialized in RDS format into R",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat.html",
    "href": "packages/sparklyr/latest/reference/jfloat.html",
    "title": "Instantiate a Java float type.",
    "section": "",
    "text": "R/utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat.html#jfloat",
    "href": "packages/sparklyr/latest/reference/jfloat.html#jfloat",
    "title": "Instantiate a Java float type.",
    "section": "jfloat",
    "text": "jfloat"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat.html#description",
    "href": "packages/sparklyr/latest/reference/jfloat.html#description",
    "title": "Instantiate a Java float type.",
    "section": "Description",
    "text": "Description\nInstantiate a java.lang.Float object with the value specified. NOTE: this method is useful when one has to invoke a Java/Scala method requiring a float (instead of double) type for at least one of its parameters."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat.html#usage",
    "href": "packages/sparklyr/latest/reference/jfloat.html#usage",
    "title": "Instantiate a Java float type.",
    "section": "Usage",
    "text": "Usage\njfloat(sc, x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat.html#arguments",
    "href": "packages/sparklyr/latest/reference/jfloat.html#arguments",
    "title": "Instantiate a Java float type.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nx\nA numeric value in R."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat.html#examples",
    "href": "packages/sparklyr/latest/reference/jfloat.html#examples",
    "title": "Instantiate a Java float type.",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"spark://HOST:PORT\") \njflt &lt;- jfloat(sc, 1.23e-8) \n# jflt is now a reference to a java.lang.Float object"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "",
    "text": "R/ml_feature_max_abs_scaler.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#ft_max_abs_scaler",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#ft_max_abs_scaler",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "ft_max_abs_scaler",
    "text": "ft_max_abs_scaler"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#description",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#description",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "Description",
    "text": "Description\nRescale each feature individually to range [-1, 1] by dividing through the largest maximum absolute value in each feature. It does not shift/center the data, and thus does not destroy any sparsity."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#usage",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "Usage",
    "text": "Usage\n \nft_max_abs_scaler( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  uid = random_string(\"max_abs_scaler_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#arguments",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#details",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#details",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#value",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#value",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#examples",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#examples",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \nfeatures &lt;- c(\"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\") \n \niris_tbl %&gt;% \n  ft_vector_assembler( \n    input_col = features, \n    output_col = \"features_temp\" \n  ) %&gt;% \n  ft_max_abs_scaler( \n    input_col = \"features_temp\", \n    output_col = \"features\" \n  ) \n#&gt; # Source: spark&lt;?&gt; [?? x 7]\n#&gt;    Sepal_L…¹ Sepal…² Petal…³ Petal…⁴ Species featu…⁵ featu…⁶\n#&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;list&gt;  &lt;list&gt; \n#&gt;  1       5.1     3.5     1.4     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  2       4.9     3       1.4     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  3       4.7     3.2     1.3     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  4       4.6     3.1     1.5     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  5       5       3.6     1.4     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  6       5.4     3.9     1.7     0.4 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  7       4.6     3.4     1.4     0.3 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  8       5       3.4     1.5     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  9       4.4     2.9     1.4     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt; 10       4.9     3.1     1.5     0.1 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt; # … with more rows, and abbreviated variable names\n#&gt; #   ¹​Sepal_Length, ²​Sepal_Width, ³​Petal_Length,\n#&gt; #   ⁴​Petal_Width, ⁵​features_temp, ⁶​features"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#see-also",
    "title": "Feature Transformation – MaxAbsScaler (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark. Other feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_feature_importances.html",
    "href": "packages/sparklyr/latest/reference/ml_feature_importances.html",
    "title": "Spark ML - Feature Importance for Tree Models",
    "section": "",
    "text": "R/ml_helpers.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_feature_importances.html#ml_feature_importances",
    "href": "packages/sparklyr/latest/reference/ml_feature_importances.html#ml_feature_importances",
    "title": "Spark ML - Feature Importance for Tree Models",
    "section": "ml_feature_importances",
    "text": "ml_feature_importances"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_feature_importances.html#description",
    "href": "packages/sparklyr/latest/reference/ml_feature_importances.html#description",
    "title": "Spark ML - Feature Importance for Tree Models",
    "section": "Description",
    "text": "Description\nSpark ML - Feature Importance for Tree Models"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_feature_importances.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_feature_importances.html#usage",
    "title": "Spark ML - Feature Importance for Tree Models",
    "section": "Usage",
    "text": "Usage\nml_feature_importances(model, ...) \n\nml_tree_feature_importance(model, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_feature_importances.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_feature_importances.html#arguments",
    "title": "Spark ML - Feature Importance for Tree Models",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nmodel\nA decision tree-based model.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_feature_importances.html#value",
    "href": "packages/sparklyr/latest/reference/ml_feature_importances.html#value",
    "title": "Spark ML - Feature Importance for Tree Models",
    "section": "Value",
    "text": "Value\nFor ml_model, a sorted data frame with feature labels and their relative importance. For ml_prediction_model, a vector of relative importances."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "",
    "text": "R/ml_regression_generalized_linear_regression.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#ml_generalized_linear_regression",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#ml_generalized_linear_regression",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "ml_generalized_linear_regression",
    "text": "ml_generalized_linear_regression"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#description",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#description",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "Description",
    "text": "Description\nPerform regression using Generalized Linear Model (GLM)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#usage",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "Usage",
    "text": "Usage\n \nml_generalized_linear_regression( \n  x, \n  formula = NULL, \n  family = \"gaussian\", \n  link = NULL, \n  fit_intercept = TRUE, \n  offset_col = NULL, \n  link_power = NULL, \n  link_prediction_col = NULL, \n  reg_param = 0, \n  max_iter = 25, \n  weight_col = NULL, \n  solver = \"irls\", \n  tol = 1e-06, \n  variance_power = 0, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"generalized_linear_regression_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#arguments",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nfamily\nName of family which is a description of the error distribution to be used in the model. Supported options: “gaussian”, “binomial”, “poisson”, “gamma” and “tweedie”. Default is “gaussian”.\n\n\nlink\nName of link function which provides the relationship between the linear predictor and the mean of the distribution function. See for supported link functions.\n\n\nfit_intercept\nBoolean; should the model be fit with an intercept term?\n\n\noffset_col\nOffset column name. If this is not set, we treat all instance offsets as 0.0. The feature specified as offset has a constant coefficient of 1.0.\n\n\nlink_power\nIndex in the power link function. Only applicable to the Tweedie family. Note that link power 0, 1, -1 or 0.5 corresponds to the Log, Identity, Inverse or Sqrt link, respectively. When not set, this value defaults to 1 - variancePower, which matches the R “statmod” package.\n\n\nlink_prediction_col\nLink prediction (linear predictor) column name. Default is not set, which means we do not output link prediction.\n\n\nreg_param\nRegularization parameter (aka lambda)\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nweight_col\nThe name of the column to use as weights for the model fit.\n\n\nsolver\nSolver algorithm for optimization.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nvariance_power\nPower in the variance function of the Tweedie distribution which provides the relationship between the variance and mean of the distribution. Only applicable to the Tweedie family. (see Tweedie Distribution (Wikipedia)) Supported values: 0 and [1, Inf). Note that variance power 0, 1, or 2 corresponds to the Gaussian, Poisson or Gamma family, respectively.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#details",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#details",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows. Valid link functions for each family is listed below. The first link function of each family is the default one.\n\ngaussian: “identity”, “log”, “inverse”\nbinomial: “logit”, “probit”, “loglog”\npoisson: “log”, “identity”, “sqrt”\ngamma: “inverse”, “identity”, “log”\ntweedie: power link function specified through link_power. The default link power in the tweedie family is 1 - variance_power."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#value",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#value",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#examples",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "Examples",
    "text": "Examples\n\n \nlibrary(sparklyr) \n \nsc &lt;- spark_connect(master = \"local\") \nmtcars_tbl &lt;- sdf_copy_to(sc, mtcars, name = \"mtcars_tbl\", overwrite = TRUE) \n \npartitions &lt;- mtcars_tbl %&gt;% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \n \nmtcars_training &lt;- partitions$training \nmtcars_test &lt;- partitions$test \n \n# Specify the grid \nfamily &lt;- c(\"gaussian\", \"gamma\", \"poisson\") \nlink &lt;- c(\"identity\", \"log\") \nfamily_link &lt;- expand.grid(family = family, link = link, stringsAsFactors = FALSE) \nfamily_link &lt;- data.frame(family_link, rmse = 0) \n \n# Train the models \nfor (i in seq_len(nrow(family_link))) { \n  glm_model &lt;- mtcars_training %&gt;% \n    ml_generalized_linear_regression(mpg ~ ., \n      family = family_link[i, 1], \n      link = family_link[i, 2] \n    ) \n \n  pred &lt;- ml_predict(glm_model, mtcars_test) \n  family_link[i, 3] &lt;- ml_regression_evaluator(pred, label_col = \"mpg\") \n} \n \nfamily_link \n#&gt;     family     link     rmse\n#&gt; 1 gaussian identity 2.881163\n#&gt; 2    gamma identity 2.954531\n#&gt; 3  poisson identity 2.942684\n#&gt; 4 gaussian      log 2.613220\n#&gt; 5    gamma      log 2.721447\n#&gt; 6  poisson      log 2.676343"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#see-also",
    "title": "Spark ML – Generalized Linear Regression",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms. Other ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/checkpoint_directory.html",
    "href": "packages/sparklyr/latest/reference/checkpoint_directory.html",
    "title": "Set/Get Spark checkpoint directory",
    "section": "",
    "text": "R/spark_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/checkpoint_directory.html#checkpoint_directory",
    "href": "packages/sparklyr/latest/reference/checkpoint_directory.html#checkpoint_directory",
    "title": "Set/Get Spark checkpoint directory",
    "section": "checkpoint_directory",
    "text": "checkpoint_directory"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/checkpoint_directory.html#description",
    "href": "packages/sparklyr/latest/reference/checkpoint_directory.html#description",
    "title": "Set/Get Spark checkpoint directory",
    "section": "Description",
    "text": "Description\nSet/Get Spark checkpoint directory"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/checkpoint_directory.html#usage",
    "href": "packages/sparklyr/latest/reference/checkpoint_directory.html#usage",
    "title": "Set/Get Spark checkpoint directory",
    "section": "Usage",
    "text": "Usage\nspark_set_checkpoint_dir(sc, dir) \n\nspark_get_checkpoint_dir(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/checkpoint_directory.html#arguments",
    "href": "packages/sparklyr/latest/reference/checkpoint_directory.html#arguments",
    "title": "Set/Get Spark checkpoint directory",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\ndir\ncheckpoint directory, must be HDFS path of running on cluster"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html",
    "title": "Spark ML - Evaluators",
    "section": "",
    "text": "R/ml_evaluation_prediction.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#ml_evaluator",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#ml_evaluator",
    "title": "Spark ML - Evaluators",
    "section": "ml_evaluator",
    "text": "ml_evaluator"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#description",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#description",
    "title": "Spark ML - Evaluators",
    "section": "Description",
    "text": "Description\nA set of functions to calculate performance metrics for prediction models. Also see the Spark ML Documentation https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.evaluation.package"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#usage",
    "title": "Spark ML - Evaluators",
    "section": "Usage",
    "text": "Usage\n \nml_binary_classification_evaluator( \n  x, \n  label_col = \"label\", \n  raw_prediction_col = \"rawPrediction\", \n  metric_name = \"areaUnderROC\", \n  uid = random_string(\"binary_classification_evaluator_\"), \n  ... \n) \n \nml_binary_classification_eval( \n  x, \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  metric_name = \"areaUnderROC\" \n) \n \nml_multiclass_classification_evaluator( \n  x, \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  metric_name = \"f1\", \n  uid = random_string(\"multiclass_classification_evaluator_\"), \n  ... \n) \n \nml_classification_eval( \n  x, \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  metric_name = \"f1\" \n) \n \nml_regression_evaluator( \n  x, \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  metric_name = \"rmse\", \n  uid = random_string(\"regression_evaluator_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#arguments",
    "title": "Spark ML - Evaluators",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection object or a tbl_spark containing label and prediction columns. The latter should be the output of sdf_predict.\n\n\nlabel_col\nName of column string specifying which column contains the true labels or values.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nmetric_name\nThe performance metric. See details.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; currently unused.\n\n\nprediction_col\nName of the column that contains the predicted label or value NOT the scored probability. Column should be of type Double."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#details",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#details",
    "title": "Spark ML - Evaluators",
    "section": "Details",
    "text": "Details\nThe following metrics are supported\n\nBinary Classification: areaUnderROC (default) or areaUnderPR (not available in Spark 2.X.)\nMulticlass Classification: f1 (default), precision, recall, weightedPrecision, weightedRecall or accuracy; for Spark 2.X: f1 (default), weightedPrecision, weightedRecall or accuracy.\nRegression: rmse (root mean squared error, default), mse (mean squared error), r2, or mae (mean absolute error.)\nml_binary_classification_eval() is an alias for ml_binary_classification_evaluator() for backwards compatibility. ml_classification_eval() is an alias for ml_multiclass_classification_evaluator() for backwards compatibility."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#value",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#value",
    "title": "Spark ML - Evaluators",
    "section": "Value",
    "text": "Value\nThe calculated performance metric"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#examples",
    "title": "Spark ML - Evaluators",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \nmtcars_tbl &lt;- sdf_copy_to(sc, mtcars, name = \"mtcars_tbl\", overwrite = TRUE) \n \npartitions &lt;- mtcars_tbl %&gt;% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \n \nmtcars_training &lt;- partitions$training \nmtcars_test &lt;- partitions$test \n \n# for multiclass classification \nrf_model &lt;- mtcars_training %&gt;% \n  ml_random_forest(cyl ~ ., type = \"classification\") \n \npred &lt;- ml_predict(rf_model, mtcars_test) \n \nml_multiclass_classification_evaluator(pred) \n#&gt; [1] 1\n \n# for regression \nrf_model &lt;- mtcars_training %&gt;% \n  ml_random_forest(cyl ~ ., type = \"regression\") \n \npred &lt;- ml_predict(rf_model, mtcars_test) \n \nml_regression_evaluator(pred, label_col = \"cyl\") \n#&gt; [1] 0.4444097\n \n# for binary classification \nrf_model &lt;- mtcars_training %&gt;% \n  ml_random_forest(am ~ gear + carb, type = \"classification\") \n \npred &lt;- ml_predict(rf_model, mtcars_test) \n \nml_binary_classification_evaluator(pred) \n#&gt; [1] 0.96875"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.html",
    "href": "packages/sparklyr/latest/reference/copy_to.html",
    "title": "Copy To",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.html#copy_to",
    "href": "packages/sparklyr/latest/reference/copy_to.html#copy_to",
    "title": "Copy To",
    "section": "copy_to",
    "text": "copy_to"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.html#description",
    "href": "packages/sparklyr/latest/reference/copy_to.html#description",
    "title": "Copy To",
    "section": "Description",
    "text": "Description\nSee copy_to for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html",
    "title": "Default stop words",
    "section": "",
    "text": "R/ml_feature_stop_words_remover.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#ml_default_stop_words",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#ml_default_stop_words",
    "title": "Default stop words",
    "section": "ml_default_stop_words",
    "text": "ml_default_stop_words"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#description",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#description",
    "title": "Default stop words",
    "section": "Description",
    "text": "Description\nLoads the default stop words for the given language."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#usage",
    "title": "Default stop words",
    "section": "Usage",
    "text": "Usage\nml_default_stop_words( \n  sc, \n  language = c(\"english\", \"danish\", \"dutch\", \"finnish\", \"french\", \"german\", \"hungarian\", \n    \"italian\", \"norwegian\", \"portuguese\", \"russian\", \"spanish\", \"swedish\", \"turkish\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#arguments",
    "title": "Default stop words",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection\n\n\nlanguage\nA character string.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#details",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#details",
    "title": "Default stop words",
    "section": "Details",
    "text": "Details\nSupported languages: danish, dutch, english, finnish, french, german, hungarian, italian, norwegian, portuguese, russian, spanish, swedish, turkish. Defaults to English. See https://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/\nfor more details"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#value",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#value",
    "title": "Default stop words",
    "section": "Value",
    "text": "Value\nA list of stop words."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#see-also",
    "title": "Default stop words",
    "section": "See Also",
    "text": "See Also\nft_stop_words_remover"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/full_join.html",
    "href": "packages/sparklyr/latest/reference/full_join.html",
    "title": "Full join",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/full_join.html#full_join",
    "href": "packages/sparklyr/latest/reference/full_join.html#full_join",
    "title": "Full join",
    "section": "full_join",
    "text": "full_join"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/full_join.html#description",
    "href": "packages/sparklyr/latest/reference/full_join.html#description",
    "title": "Full join",
    "section": "Description",
    "text": "Description\nSee full_join for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-transform-methods.html",
    "href": "packages/sparklyr/latest/reference/sdf-transform-methods.html",
    "title": "Spark ML – Transform, fit, and predict methods (sdf_ interface)",
    "section": "",
    "text": "R/ml_transformation_methods.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-transform-methods.html#sdf-transform-methods",
    "href": "packages/sparklyr/latest/reference/sdf-transform-methods.html#sdf-transform-methods",
    "title": "Spark ML – Transform, fit, and predict methods (sdf_ interface)",
    "section": "sdf-transform-methods",
    "text": "sdf-transform-methods"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-transform-methods.html#description",
    "href": "packages/sparklyr/latest/reference/sdf-transform-methods.html#description",
    "title": "Spark ML – Transform, fit, and predict methods (sdf_ interface)",
    "section": "Description",
    "text": "Description\nDeprecated methods for transformation, fit, and prediction. These are mirrors of the corresponding ml-transform-methods."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-transform-methods.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf-transform-methods.html#usage",
    "title": "Spark ML – Transform, fit, and predict methods (sdf_ interface)",
    "section": "Usage",
    "text": "Usage\nsdf_predict(x, model, ...) \n\nsdf_transform(x, transformer, ...) \n\nsdf_fit(x, estimator, ...) \n\nsdf_fit_and_transform(x, estimator, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-transform-methods.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf-transform-methods.html#arguments",
    "title": "Spark ML – Transform, fit, and predict methods (sdf_ interface)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA tbl_spark.\n\n\nmodel\nA ml_transformer or a ml_model object.\n\n\n…\nOptional arguments passed to the corresponding ml_ methods.\n\n\ntransformer\nA ml_transformer object.\n\n\nestimator\nA ml_estimator object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-transform-methods.html#value",
    "href": "packages/sparklyr/latest/reference/sdf-transform-methods.html#value",
    "title": "Spark ML – Transform, fit, and predict methods (sdf_ interface)",
    "section": "Value",
    "text": "Value\nsdf_predict(), sdf_transform(), and sdf_fit_and_transform() return a transformed dataframe whereas sdf_fit() returns a ml_transformer."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html",
    "href": "packages/sparklyr/latest/reference/ml_corr.html",
    "title": "Compute correlation matrix",
    "section": "",
    "text": "R/ml_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html#ml_corr",
    "href": "packages/sparklyr/latest/reference/ml_corr.html#ml_corr",
    "title": "Compute correlation matrix",
    "section": "ml_corr",
    "text": "ml_corr"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html#description",
    "href": "packages/sparklyr/latest/reference/ml_corr.html#description",
    "title": "Compute correlation matrix",
    "section": "Description",
    "text": "Description\nCompute correlation matrix"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_corr.html#usage",
    "title": "Compute correlation matrix",
    "section": "Usage",
    "text": "Usage\n \nml_corr(x, columns = NULL, method = c(\"pearson\", \"spearman\"))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_corr.html#arguments",
    "title": "Compute correlation matrix",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA tbl_spark.\n\n\ncolumns\nThe names of the columns to calculate correlations of. If only one column is specified, it must be a vector column (for example, assembled using ft_vector_assember()).\n\n\nmethod\nThe method to use, either \"pearson\" or \"spearman\"."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html#value",
    "href": "packages/sparklyr/latest/reference/ml_corr.html#value",
    "title": "Compute correlation matrix",
    "section": "Value",
    "text": "Value\nA correlation matrix organized as a data frame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_corr.html#examples",
    "title": "Compute correlation matrix",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \nfeatures &lt;- c(\"Petal_Width\", \"Petal_Length\", \"Sepal_Length\", \"Sepal_Width\") \n \nml_corr(iris_tbl, columns = features, method = \"pearson\") \n#&gt; New names:\n#&gt; • `` -&gt; `...1`\n#&gt; • `` -&gt; `...2`\n#&gt; • `` -&gt; `...3`\n#&gt; • `` -&gt; `...4`\n#&gt; # A tibble: 4 × 4\n#&gt;   Petal_Width Petal_Length Sepal_Length Sepal_Width\n#&gt;         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1       1            0.963        0.818      -0.366\n#&gt; 2       0.963        1            0.872      -0.428\n#&gt; 3       0.818        0.872        1          -0.118\n#&gt; 4      -0.366       -0.428       -0.118       1"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rcauchy.html",
    "href": "packages/sparklyr/latest/reference/sdf_rcauchy.html",
    "title": "Generate random samples from a Cauchy distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rcauchy.html#sdf_rcauchy",
    "href": "packages/sparklyr/latest/reference/sdf_rcauchy.html#sdf_rcauchy",
    "title": "Generate random samples from a Cauchy distribution",
    "section": "sdf_rcauchy",
    "text": "sdf_rcauchy"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rcauchy.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rcauchy.html#description",
    "title": "Generate random samples from a Cauchy distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Cauchy distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rcauchy.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rcauchy.html#usage",
    "title": "Generate random samples from a Cauchy distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rcauchy( \n  sc, \n  n, \n  location = 0, \n  scale = 1, \n  num_partitions = NULL, \n  seed = NULL, \n  output_col = \"x\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rcauchy.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rcauchy.html#arguments",
    "title": "Generate random samples from a Cauchy distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nlocation\nLocation parameter of the distribution.\n\n\nscale\nScale parameter of the distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rcauchy.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rcauchy.html#see-also",
    "title": "Generate random samples from a Cauchy distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/transform_sdf.html",
    "href": "packages/sparklyr/latest/reference/transform_sdf.html",
    "title": "transform a subset of column(s) in a Spark Dataframe",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/transform_sdf.html#transform_sdf",
    "href": "packages/sparklyr/latest/reference/transform_sdf.html#transform_sdf",
    "title": "transform a subset of column(s) in a Spark Dataframe",
    "section": "transform_sdf",
    "text": "transform_sdf"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/transform_sdf.html#description",
    "href": "packages/sparklyr/latest/reference/transform_sdf.html#description",
    "title": "transform a subset of column(s) in a Spark Dataframe",
    "section": "Description",
    "text": "Description\ntransform a subset of column(s) in a Spark Dataframe"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/transform_sdf.html#usage",
    "href": "packages/sparklyr/latest/reference/transform_sdf.html#usage",
    "title": "transform a subset of column(s) in a Spark Dataframe",
    "section": "Usage",
    "text": "Usage\ntransform_sdf(x, cols, fn)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/transform_sdf.html#arguments",
    "href": "packages/sparklyr/latest/reference/transform_sdf.html#arguments",
    "title": "transform a subset of column(s) in a Spark Dataframe",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercible to a Spark DataFrame\n\n\ncols\nSubset of columns to apply transformation to\n\n\nfn\nTransformation function taking column name as the 1st parameter, the corresponding org.apache.spark.sql.Column object as the 2nd parameter, and returning a transformed org.apache.spark.sql.Column object"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pipe.html",
    "href": "packages/sparklyr/latest/reference/pipe.html",
    "title": "Pipe operator",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pipe.html#section",
    "href": "packages/sparklyr/latest/reference/pipe.html#section",
    "title": "Pipe operator",
    "section": "%>%",
    "text": "%&gt;%"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pipe.html#description",
    "href": "packages/sparklyr/latest/reference/pipe.html#description",
    "title": "Pipe operator",
    "section": "Description",
    "text": "Description\nSee %&gt;% for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_delta.html",
    "href": "packages/sparklyr/latest/reference/spark_read_delta.html",
    "title": "Read from Delta Lake into a Spark DataFrame.",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_delta.html#spark_read_delta",
    "href": "packages/sparklyr/latest/reference/spark_read_delta.html#spark_read_delta",
    "title": "Read from Delta Lake into a Spark DataFrame.",
    "section": "spark_read_delta",
    "text": "spark_read_delta"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_delta.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_delta.html#description",
    "title": "Read from Delta Lake into a Spark DataFrame.",
    "section": "Description",
    "text": "Description\nRead from Delta Lake into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_delta.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_delta.html#usage",
    "title": "Read from Delta Lake into a Spark DataFrame.",
    "section": "Usage",
    "text": "Usage\nspark_read_delta( \n  sc, \n  path, \n  name = NULL, \n  version = NULL, \n  timestamp = NULL, \n  options = list(), \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_delta.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_delta.html#arguments",
    "title": "Read from Delta Lake into a Spark DataFrame.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nname\nThe name to assign to the newly generated table.\n\n\nversion\nThe version of the delta table to read.\n\n\ntimestamp\nThe timestamp of the delta table to read. For example, \"2019-01-01\" or \"2019-01-01'T'00:00:00.000Z\".\n\n\noptions\nA list of strings with additional options.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_delta.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_delta.html#see-also",
    "title": "Read from Delta Lake into a Spark DataFrame.",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_unique_id.html",
    "href": "packages/sparklyr/latest/reference/sdf_with_unique_id.html",
    "title": "Add a Unique ID Column to a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#sdf_with_unique_id",
    "href": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#sdf_with_unique_id",
    "title": "Add a Unique ID Column to a Spark DataFrame",
    "section": "sdf_with_unique_id",
    "text": "sdf_with_unique_id"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#description",
    "title": "Add a Unique ID Column to a Spark DataFrame",
    "section": "Description",
    "text": "Description\nAdd a unique ID column to a Spark DataFrame. The Spark monotonicallyIncreasingId function is used to produce these and is guaranteed to produce unique, monotonically increasing ids; however, there is no guarantee that these IDs will be sequential. The table is persisted immediately after the column is generated, to ensure that the column is stable – otherwise, it can differ across new computations."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#usage",
    "title": "Add a Unique ID Column to a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_with_unique_id(x, id = \"id\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#arguments",
    "title": "Add a Unique ID Column to a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nid\nThe name of the column to host the generated IDs."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html",
    "title": "Write CSV Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html#stream_write_csv",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html#stream_write_csv",
    "title": "Write CSV Stream",
    "section": "stream_write_csv",
    "text": "stream_write_csv"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html#description",
    "title": "Write CSV Stream",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into a tabular (typically, comma-separated) stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html#usage",
    "title": "Write CSV Stream",
    "section": "Usage",
    "text": "Usage\nstream_write_csv( \n  x, \n  path, \n  mode = c(\"append\", \"complete\", \"update\"), \n  trigger = stream_trigger_interval(), \n  checkpoint = file.path(path, \"checkpoint\"), \n  header = TRUE, \n  delimiter = \",\", \n  quote = \"\\\"\", \n  escape = \"\\\\\", \n  charset = \"UTF-8\", \n  null_value = NULL, \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html#arguments",
    "title": "Write CSV Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\".\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous.\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\nheader\nShould the first row of data be used as a header? Defaults to TRUE.\n\n\ndelimiter\nThe character used to delimit each column, defaults to ,.\n\n\nquote\nThe character used as a quote. Defaults to '\"'.\n\n\nescape\nThe character used to escape other characters, defaults to \\.\n\n\ncharset\nThe character set, defaults to \"UTF-8\".\n\n\nnull_value\nThe character to use for default values, defaults to NULL.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html#examples",
    "title": "Write CSV Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\") \ndir.create(\"csv-in\") \nwrite.csv(iris, \"csv-in/data.csv\", row.names = FALSE) \ncsv_path &lt;- file.path(\"file://\", getwd(), \"csv-in\") \nstream &lt;- stream_read_csv(sc, csv_path) %&gt;% stream_write_csv(\"csv-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html#see-also",
    "title": "Write CSV Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html",
    "title": "Spark ML – Logistic Regression",
    "section": "",
    "text": "R/ml_classification_logistic_regression.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#ml_logistic_regression",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#ml_logistic_regression",
    "title": "Spark ML – Logistic Regression",
    "section": "ml_logistic_regression",
    "text": "ml_logistic_regression"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#description",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#description",
    "title": "Spark ML – Logistic Regression",
    "section": "Description",
    "text": "Description\nPerform classification using logistic regression."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#usage",
    "title": "Spark ML – Logistic Regression",
    "section": "Usage",
    "text": "Usage\n \nml_logistic_regression( \n  x, \n  formula = NULL, \n  fit_intercept = TRUE, \n  elastic_net_param = 0, \n  reg_param = 0, \n  max_iter = 100, \n  threshold = 0.5, \n  thresholds = NULL, \n  tol = 1e-06, \n  weight_col = NULL, \n  aggregation_depth = 2, \n  lower_bounds_on_coefficients = NULL, \n  lower_bounds_on_intercepts = NULL, \n  upper_bounds_on_coefficients = NULL, \n  upper_bounds_on_intercepts = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  family = \"auto\", \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  uid = random_string(\"logistic_regression_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#arguments",
    "title": "Spark ML – Logistic Regression",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nfit_intercept\nBoolean; should the model be fit with an intercept term?\n\n\nelastic_net_param\nElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n\n\nreg_param\nRegularization parameter (aka lambda)\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nthreshold\nin binary classification prediction, in range [0, 1].\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nweight_col\nThe name of the column to use as weights for the model fit.\n\n\naggregation_depth\n(Spark 2.1.0+) Suggested depth for treeAggregate (&gt;= 2).\n\n\nlower_bounds_on_coefficients\n(Spark 2.2.0+) Lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression.\n\n\nlower_bounds_on_intercepts\n(Spark 2.2.0+) Lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression.\n\n\nupper_bounds_on_coefficients\n(Spark 2.2.0+) Upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression.\n\n\nupper_bounds_on_intercepts\n(Spark 2.2.0+) Upper bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nfamily\n(Spark 2.1.0+) Param for the name of family which is a description of the label distribution to be used in the model. Supported options: “auto”, “binomial”, and “multinomial.”\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#details",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#details",
    "title": "Spark ML – Logistic Regression",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#value",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#value",
    "title": "Spark ML – Logistic Regression",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#examples",
    "title": "Spark ML – Logistic Regression",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \nmtcars_tbl &lt;- sdf_copy_to(sc, mtcars, name = \"mtcars_tbl\", overwrite = TRUE) \n \npartitions &lt;- mtcars_tbl %&gt;% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \n \nmtcars_training &lt;- partitions$training \nmtcars_test &lt;- partitions$test \n \nlr_model &lt;- mtcars_training %&gt;% \n  ml_logistic_regression(am ~ gear + carb) \n \npred &lt;- ml_predict(lr_model, mtcars_test) \n \nml_binary_classification_evaluator(pred) \n#&gt; [1] 0.96875"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#see-also",
    "title": "Spark ML – Logistic Regression",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms. Other ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_zip_with.html",
    "href": "packages/sparklyr/latest/reference/hof_zip_with.html",
    "title": "Combines 2 Array Columns",
    "section": "",
    "text": "R/dplyr_hof.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_zip_with.html#hof_zip_with",
    "href": "packages/sparklyr/latest/reference/hof_zip_with.html#hof_zip_with",
    "title": "Combines 2 Array Columns",
    "section": "hof_zip_with",
    "text": "hof_zip_with"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_zip_with.html#description",
    "href": "packages/sparklyr/latest/reference/hof_zip_with.html#description",
    "title": "Combines 2 Array Columns",
    "section": "Description",
    "text": "Description\nApplies an element-wise function to combine elements from 2 array columns (this is essentially a dplyr wrapper for the zip_with(array&lt;T&gt;, array&lt;U&gt;, function&lt;T, U, R&gt;): array&lt;R&gt; built-in function in Spark SQL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_zip_with.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_zip_with.html#usage",
    "title": "Combines 2 Array Columns",
    "section": "Usage",
    "text": "Usage\n \nhof_zip_with(x, func, dest_col = NULL, left = NULL, right = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_zip_with.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_zip_with.html#arguments",
    "title": "Combines 2 Array Columns",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark data frame to process\n\n\nfunc\nElement-wise combining function to be applied\n\n\ndest_col\nColumn to store the query result (default: the last column of the Spark data frame)\n\n\nleft\nAny expression evaluating to an array (default: the first column of the Spark data frame)\n\n\nright\nAny expression evaluating to an array (default: the second column of the Spark data frame)\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_zip_with.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_zip_with.html#examples",
    "title": "Combines 2 Array Columns",
    "section": "Examples",
    "text": "Examples\n\n \n \nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\") \n# compute element-wise products of 2 arrays from each row of `left` and `right` \n# and store the resuling array in `res` \ncopy_to( \n  sc, \n  tibble::tibble( \n    left = list(1:5, 21:25), \n    right = list(6:10, 16:20), \n    res = c(0, 0) \n  ) \n) %&gt;% \n  hof_zip_with(~ .x * .y) \n#&gt; # Source: spark&lt;?&gt; [?? x 3]\n#&gt;   left      right     res      \n#&gt;   &lt;list&gt;    &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt;\n#&gt; 2 &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt;"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html",
    "href": "packages/sparklyr/latest/reference/ft_dct.html",
    "title": "Feature Transformation – Discrete Cosine Transform (DCT) (Transformer)",
    "section": "",
    "text": "R/ml_feature_dct.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#ft_dct",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#ft_dct",
    "title": "Feature Transformation – Discrete Cosine Transform (DCT) (Transformer)",
    "section": "ft_dct",
    "text": "ft_dct"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#description",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#description",
    "title": "Feature Transformation – Discrete Cosine Transform (DCT) (Transformer)",
    "section": "Description",
    "text": "Description\nA feature transformer that takes the 1D discrete cosine transform of a real vector. No zero padding is performed on the input vector. It returns a real vector of the same length representing the DCT. The return vector is scaled such that the transform matrix is unitary (aka scaled DCT-II)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#usage",
    "title": "Feature Transformation – Discrete Cosine Transform (DCT) (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_dct( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  inverse = FALSE, \n  uid = random_string(\"dct_\"), \n  ... \n) \n\nft_discrete_cosine_transform( \n  x, \n  input_col, \n  output_col, \n  inverse = FALSE, \n  uid = random_string(\"dct_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#arguments",
    "title": "Feature Transformation – Discrete Cosine Transform (DCT) (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\ninverse\nIndicates whether to perform the inverse DCT (TRUE) or forward DCT (FALSE).\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#details",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#details",
    "title": "Feature Transformation – Discrete Cosine Transform (DCT) (Transformer)",
    "section": "Details",
    "text": "Details\nft_discrete_cosine_transform() is an alias for ft_dct for backwards compatibility."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#value",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#value",
    "title": "Feature Transformation – Discrete Cosine Transform (DCT) (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#see-also",
    "title": "Feature Transformation – Discrete Cosine Transform (DCT) (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/select.html",
    "href": "packages/sparklyr/latest/reference/select.html",
    "title": "Select",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/select.html#select",
    "href": "packages/sparklyr/latest/reference/select.html#select",
    "title": "Select",
    "section": "select",
    "text": "select"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/select.html#description",
    "href": "packages/sparklyr/latest/reference/select.html#description",
    "title": "Select",
    "section": "Description",
    "text": "Description\nSee select for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html",
    "title": "Tidying methods for Spark ML linear svc",
    "section": "",
    "text": "R/tidiers_ml_svc_models.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#ml_linear_svc_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#ml_linear_svc_tidiers",
    "title": "Tidying methods for Spark ML linear svc",
    "section": "ml_linear_svc_tidiers",
    "text": "ml_linear_svc_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#description",
    "title": "Tidying methods for Spark ML linear svc",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#usage",
    "title": "Tidying methods for Spark ML linear svc",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_linear_svc'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_linear_svc'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_linear_svc'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML linear svc",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html",
    "title": "Set the SPARK_HOME environment variable",
    "section": "",
    "text": "R/spark_home.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html#spark_home_set",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html#spark_home_set",
    "title": "Set the SPARK_HOME environment variable",
    "section": "spark_home_set",
    "text": "spark_home_set"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html#description",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html#description",
    "title": "Set the SPARK_HOME environment variable",
    "section": "Description",
    "text": "Description\nSet the SPARK_HOME environment variable. This slightly speeds up some operations, including the connection time."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html#usage",
    "title": "Set the SPARK_HOME environment variable",
    "section": "Usage",
    "text": "Usage\n \nspark_home_set(path = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html#arguments",
    "title": "Set the SPARK_HOME environment variable",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\npath\nA string containing the path to the installation location of Spark. If NULL, the path to the most latest Spark/Hadoop versions is used.\n\n\n…\nAdditional parameters not currently used."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html#value",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html#value",
    "title": "Set the SPARK_HOME environment variable",
    "section": "Value",
    "text": "Value\nThe function is mostly invoked for the side-effect of setting the SPARK_HOME environment variable. It also returns TRUE if the environment was successfully set, and FALSE otherwise."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html#examples",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html#examples",
    "title": "Set the SPARK_HOME environment variable",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \n# Not run due to side-effects \nspark_home_set() \n#&gt; Setting SPARK_HOME environment variable to /Users/edgar/spark/spark-3.0.0-bin-hadoop3.2"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_collect.html",
    "href": "packages/sparklyr/latest/reference/sdf_collect.html",
    "title": "Collect a Spark DataFrame into R.",
    "section": "",
    "text": "R/sdf_wrapper.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_collect.html#sdf_collect",
    "href": "packages/sparklyr/latest/reference/sdf_collect.html#sdf_collect",
    "title": "Collect a Spark DataFrame into R.",
    "section": "sdf_collect",
    "text": "sdf_collect"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_collect.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_collect.html#description",
    "title": "Collect a Spark DataFrame into R.",
    "section": "Description",
    "text": "Description\nCollects a Spark dataframe into R."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_collect.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_collect.html#usage",
    "title": "Collect a Spark DataFrame into R.",
    "section": "Usage",
    "text": "Usage\nsdf_collect(object, impl = c(\"row-wise\", \"row-wise-iter\", \"column-wise\"), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_collect.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_collect.html#arguments",
    "title": "Collect a Spark DataFrame into R.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nobject\nSpark dataframe to collect\n\n\nimpl\nWhich implementation to use while collecting Spark dataframe - row-wise: fetch the entire dataframe into memory and then process it row-by-row - row-wise-iter: iterate through the dataframe using RDD local iterator, processing one row at a time (hence reducing memory footprint) - column-wise: fetch the entire dataframe into memory and then process it column-by-column NOTE: (1) this will not apply to streaming or arrow use cases (2) this parameter will only affect implementation detail, and will not affect result of sdf_collect, and should only be set if performance profiling indicates any particular choice will be significantly better than the default choice (“row-wise”)\n\n\n…\nAdditional options."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_libsvm.html",
    "href": "packages/sparklyr/latest/reference/spark_read_libsvm.html",
    "title": "Read libsvm file into a Spark DataFrame.",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_libsvm.html#spark_read_libsvm",
    "href": "packages/sparklyr/latest/reference/spark_read_libsvm.html#spark_read_libsvm",
    "title": "Read libsvm file into a Spark DataFrame.",
    "section": "spark_read_libsvm",
    "text": "spark_read_libsvm"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_libsvm.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_libsvm.html#description",
    "title": "Read libsvm file into a Spark DataFrame.",
    "section": "Description",
    "text": "Description\nRead libsvm file into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_libsvm.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_libsvm.html#usage",
    "title": "Read libsvm file into a Spark DataFrame.",
    "section": "Usage",
    "text": "Usage\nspark_read_libsvm( \n  sc, \n  name = NULL, \n  path = name, \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE, \n  options = list(), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_libsvm.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_libsvm.html#arguments",
    "title": "Read libsvm file into a Spark DataFrame.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_libsvm.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_libsvm.html#see-also",
    "title": "Read libsvm file into a Spark DataFrame.",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_extension.html",
    "href": "packages/sparklyr/latest/reference/spark_extension.html",
    "title": "Create Spark Extension",
    "section": "",
    "text": "R/project_template.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_extension.html#spark_extension",
    "href": "packages/sparklyr/latest/reference/spark_extension.html#spark_extension",
    "title": "Create Spark Extension",
    "section": "spark_extension",
    "text": "spark_extension"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_extension.html#description",
    "href": "packages/sparklyr/latest/reference/spark_extension.html#description",
    "title": "Create Spark Extension",
    "section": "Description",
    "text": "Description\nCreates an R package ready to be used as an Spark extension."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_extension.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_extension.html#usage",
    "title": "Create Spark Extension",
    "section": "Usage",
    "text": "Usage\nspark_extension(path)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_extension.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_extension.html#arguments",
    "title": "Create Spark Extension",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\npath\nLocation where the extension will be created."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html",
    "href": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html",
    "title": "Return the port number of a sparklyr backend.",
    "section": "",
    "text": "R/connection_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#sparklyr_get_backend_port",
    "href": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#sparklyr_get_backend_port",
    "title": "Return the port number of a sparklyr backend.",
    "section": "sparklyr_get_backend_port",
    "text": "sparklyr_get_backend_port"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#description",
    "href": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#description",
    "title": "Return the port number of a sparklyr backend.",
    "section": "Description",
    "text": "Description\nRetrieve the port number of the sparklyr backend associated with a Spark connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#usage",
    "href": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#usage",
    "title": "Return the port number of a sparklyr backend.",
    "section": "Usage",
    "text": "Usage\nsparklyr_get_backend_port(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#arguments",
    "href": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#arguments",
    "title": "Return the port number of a sparklyr backend.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#value",
    "href": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#value",
    "title": "Return the port number of a sparklyr backend.",
    "section": "Value",
    "text": "Value\nThe port number of the sparklyr backend associated with sc."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version_from_home.html",
    "href": "packages/sparklyr/latest/reference/spark_version_from_home.html",
    "title": "Get the Spark Version Associated with a Spark Installation",
    "section": "",
    "text": "R/spark_version.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version_from_home.html#spark_version_from_home",
    "href": "packages/sparklyr/latest/reference/spark_version_from_home.html#spark_version_from_home",
    "title": "Get the Spark Version Associated with a Spark Installation",
    "section": "spark_version_from_home",
    "text": "spark_version_from_home"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version_from_home.html#description",
    "href": "packages/sparklyr/latest/reference/spark_version_from_home.html#description",
    "title": "Get the Spark Version Associated with a Spark Installation",
    "section": "Description",
    "text": "Description\nRetrieve the version of Spark associated with a Spark installation."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version_from_home.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_version_from_home.html#usage",
    "title": "Get the Spark Version Associated with a Spark Installation",
    "section": "Usage",
    "text": "Usage\nspark_version_from_home(spark_home, default = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version_from_home.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_version_from_home.html#arguments",
    "title": "Get the Spark Version Associated with a Spark Installation",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nspark_home\nThe path to a Spark installation.\n\n\ndefault\nThe default version to be inferred, in case version lookup failed, e.g. no Spark installation was found at spark_home."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_load_table.html",
    "href": "packages/sparklyr/latest/reference/spark_load_table.html",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_load_table.html#spark_load_table",
    "href": "packages/sparklyr/latest/reference/spark_load_table.html#spark_load_table",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "spark_load_table",
    "text": "spark_load_table"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_load_table.html#description",
    "href": "packages/sparklyr/latest/reference/spark_load_table.html#description",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "Description",
    "text": "Description\nReads from a Spark Table into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_load_table.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_load_table.html#usage",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "Usage",
    "text": "Usage\nspark_load_table( \n  sc, \n  name, \n  path, \n  options = list(), \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_load_table.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_load_table.html#arguments",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\noptions\nA list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_load_table.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_load_table.html#see-also",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "",
    "text": "R/ml_classification_gbt_classifier.R,"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#ml_gbt_classifier",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#ml_gbt_classifier",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "ml_gbt_classifier",
    "text": "ml_gbt_classifier"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#description",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#description",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "Description",
    "text": "Description\nPerform binary classification and regression using gradient boosted trees. Multiclass classification is not supported yet."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#usage",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "Usage",
    "text": "Usage\n \nml_gbt_classifier( \n  x, \n  formula = NULL, \n  max_iter = 20, \n  max_depth = 5, \n  step_size = 0.1, \n  subsampling_rate = 1, \n  feature_subset_strategy = \"auto\", \n  min_instances_per_node = 1L, \n  max_bins = 32, \n  min_info_gain = 0, \n  loss_type = \"logistic\", \n  seed = NULL, \n  thresholds = NULL, \n  checkpoint_interval = 10, \n  cache_node_ids = FALSE, \n  max_memory_in_mb = 256, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  uid = random_string(\"gbt_classifier_\"), \n  ... \n) \n \nml_gradient_boosted_trees( \n  x, \n  formula = NULL, \n  type = c(\"auto\", \"regression\", \"classification\"), \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  checkpoint_interval = 10, \n  loss_type = c(\"auto\", \"logistic\", \"squared\", \"absolute\"), \n  max_bins = 32, \n  max_depth = 5, \n  max_iter = 20L, \n  min_info_gain = 0, \n  min_instances_per_node = 1, \n  step_size = 0.1, \n  subsampling_rate = 1, \n  feature_subset_strategy = \"auto\", \n  seed = NULL, \n  thresholds = NULL, \n  cache_node_ids = FALSE, \n  max_memory_in_mb = 256, \n  uid = random_string(\"gradient_boosted_trees_\"), \n  response = NULL, \n  features = NULL, \n  ... \n) \n \nml_gbt_regressor( \n  x, \n  formula = NULL, \n  max_iter = 20, \n  max_depth = 5, \n  step_size = 0.1, \n  subsampling_rate = 1, \n  feature_subset_strategy = \"auto\", \n  min_instances_per_node = 1, \n  max_bins = 32, \n  min_info_gain = 0, \n  loss_type = \"squared\", \n  seed = NULL, \n  checkpoint_interval = 10, \n  cache_node_ids = FALSE, \n  max_memory_in_mb = 256, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"gbt_regressor_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#arguments",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nmax_iter\nMaxmimum number of iterations.\n\n\nmax_depth\nMaximum depth of the tree (&gt;= 0); that is, the maximum number of nodes separating any leaves from the root of the tree.\n\n\nstep_size\nStep size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default = 0.1)\n\n\nsubsampling_rate\nFraction of the training data used for learning each decision tree, in range (0, 1]. (default = 1.0)\n\n\nfeature_subset_strategy\nThe number of features to consider for splits at each tree node. See details for options.\n\n\nmin_instances_per_node\nMinimum number of instances each child must have after split.\n\n\nmax_bins\nThe maximum number of bins used for discretizing continuous features and for choosing how to split on features at each node. More bins give higher granularity.\n\n\nmin_info_gain\nMinimum information gain for a split to be considered at a tree node. Should be &gt;= 0, defaults to 0.\n\n\nloss_type\nLoss function which GBT tries to minimize. Supported: \"squared\" (L2) and \"absolute\" (L1) (default = squared) for regression and \"logistic\" (default) for classification. For ml_gradient_boosted_trees, setting \"auto\" will default to the appropriate loss type based on model type.\n\n\nseed\nSeed for random numbers.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\ncheckpoint_interval\nSet checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.\n\n\ncache_node_ids\nIf FALSE, the algorithm will pass trees to executors to match instances with nodes. If TRUE, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Defaults to FALSE.\n\n\nmax_memory_in_mb\nMaximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. Defaults to 256.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details.\n\n\ntype\nThe type of model to fit. \"regression\" treats the response as a continuous variable, while \"classification\" treats the response as a categorical variable. When \"auto\" is used, the model type is inferred based on the response variable type – if it is a numeric type, then regression is used; classification otherwise.\n\n\nresponse\n(Deprecated) The name of the response column (as a length-one character vector.)\n\n\nfeatures\n(Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#details",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#details",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows. The supported options for feature_subset_strategy are\n\n\"auto\": Choose automatically for task: If num_trees == 1, set to \"all\". If num_trees &gt; 1 (forest), set to \"sqrt\" for classification and to \"onethird\" for regression.\n\"all\": use all features\n\"onethird\": use 1/3 of the features\n\"sqrt\": use use sqrt(number of features)\n\"log2\": use log2(number of features)\n\"n\": when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features. (default = \"auto\")\nml_gradient_boosted_trees is a wrapper around ml_gbt_regressor.tbl_spark and ml_gbt_classifier.tbl_spark and calls the appropriate method based on model type."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#value",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#value",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#examples",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \npartitions &lt;- iris_tbl %&gt;% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \n \niris_training &lt;- partitions$training \niris_test &lt;- partitions$test \n \ngbt_model &lt;- iris_training %&gt;% \n  ml_gradient_boosted_trees(Sepal_Length ~ Petal_Length + Petal_Width) \n \npred &lt;- ml_predict(gbt_model, iris_test) \n \nml_regression_evaluator(pred, label_col = \"Sepal_Length\") \n#&gt; [1] 0.4036941"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#see-also",
    "title": "Spark ML – Gradient Boosted Trees",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms. Other ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html",
    "title": "Read Kafka Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#stream_read_kafka",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#stream_read_kafka",
    "title": "Read Kafka Stream",
    "section": "stream_read_kafka",
    "text": "stream_read_kafka"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#description",
    "title": "Read Kafka Stream",
    "section": "Description",
    "text": "Description\nReads a Kafka stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#usage",
    "title": "Read Kafka Stream",
    "section": "Usage",
    "text": "Usage\nstream_read_kafka(sc, name = NULL, options = list(), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#arguments",
    "title": "Read Kafka Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#details",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#details",
    "title": "Read Kafka Stream",
    "section": "Details",
    "text": "Details\nPlease note that Kafka requires installing the appropriate package by setting the packages parameter to \"kafka\" in spark_connect()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#examples",
    "title": "Read Kafka Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\", version = \"2.3\", packages = \"kafka\") \nread_options &lt;- list(kafka.bootstrap.servers = \"localhost:9092\", subscribe = \"topic1\") \nwrite_options &lt;- list(kafka.bootstrap.servers = \"localhost:9092\", topic = \"topic2\") \nstream &lt;- stream_read_kafka(sc, options = read_options) %&gt;% \n  stream_write_kafka(options = write_options) \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#see-also",
    "title": "Read Kafka Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_glm_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_glm_tidiers.html",
    "title": "Tidying methods for Spark ML linear models",
    "section": "",
    "text": "R/tidiers_ml_linear_models.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#ml_glm_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#ml_glm_tidiers",
    "title": "Tidying methods for Spark ML linear models",
    "section": "ml_glm_tidiers",
    "text": "ml_glm_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#description",
    "title": "Tidying methods for Spark ML linear models",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#usage",
    "title": "Tidying methods for Spark ML linear models",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_generalized_linear_regression'\ntidy(x, exponentiate = FALSE, ...) \n\n## S3 method for class 'ml_model_linear_regression'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_generalized_linear_regression'\naugment( \n  x, \n  newdata = NULL, \n  type.residuals = c(\"working\", \"deviance\", \"pearson\", \"response\"), \n  ... \n) \n\n## S3 method for class '`_ml_model_linear_regression`'\naugment( \n  x, \n  new_data = NULL, \n  type.residuals = c(\"working\", \"deviance\", \"pearson\", \"response\"), \n  ... \n) \n\n## S3 method for class 'ml_model_linear_regression'\naugment( \n  x, \n  newdata = NULL, \n  type.residuals = c(\"working\", \"deviance\", \"pearson\", \"response\"), \n  ... \n) \n\n## S3 method for class 'ml_model_generalized_linear_regression'\nglance(x, ...) \n\n## S3 method for class 'ml_model_linear_regression'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML linear models",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\nexponentiate\nFor GLM, whether to exponentiate the coefficient estimates (typical for logistic regression.)\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction.\n\n\ntype.residuals\ntype of residuals, defaults to \"working\". Must be set to \"working\" when newdata is supplied.\n\n\nnew_data\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#details",
    "href": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#details",
    "title": "Tidying methods for Spark ML linear models",
    "section": "Details",
    "text": "Details\nThe residuals attached by augment are of type “working” by default, which is different from the default of “deviance” for residuals() or sdf_residuals()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_table.html",
    "href": "packages/sparklyr/latest/reference/spark_write_table.html",
    "title": "Writes a Spark DataFrame into a Spark table",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_table.html#spark_write_table",
    "href": "packages/sparklyr/latest/reference/spark_write_table.html#spark_write_table",
    "title": "Writes a Spark DataFrame into a Spark table",
    "section": "spark_write_table",
    "text": "spark_write_table"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_table.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_table.html#description",
    "title": "Writes a Spark DataFrame into a Spark table",
    "section": "Description",
    "text": "Description\nWrites a Spark DataFrame into a Spark table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_table.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_table.html#usage",
    "title": "Writes a Spark DataFrame into a Spark table",
    "section": "Usage",
    "text": "Usage\nspark_write_table( \n  x, \n  name, \n  mode = NULL, \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_table.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_table.html#arguments",
    "title": "Writes a Spark DataFrame into a Spark table",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nname\nThe name to assign to the newly generated table.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_table.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_table.html#see-also",
    "title": "Writes a Spark DataFrame into a Spark table",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html",
    "title": "Tidying methods for Spark ML Naive Bayes",
    "section": "",
    "text": "R/tidiers_ml_naive_bayes.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#ml_naive_bayes_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#ml_naive_bayes_tidiers",
    "title": "Tidying methods for Spark ML Naive Bayes",
    "section": "ml_naive_bayes_tidiers",
    "text": "ml_naive_bayes_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#description",
    "title": "Tidying methods for Spark ML Naive Bayes",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#usage",
    "title": "Tidying methods for Spark ML Naive Bayes",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_naive_bayes'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_naive_bayes'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class 'ml_model_naive_bayes'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML Naive Bayes",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html",
    "title": "Spark ML – Power Iteration Clustering",
    "section": "",
    "text": "R/ml_clustering_power_iteration.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html#ml_power_iteration",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html#ml_power_iteration",
    "title": "Spark ML – Power Iteration Clustering",
    "section": "ml_power_iteration",
    "text": "ml_power_iteration"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html#description",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html#description",
    "title": "Spark ML – Power Iteration Clustering",
    "section": "Description",
    "text": "Description\nPower iteration clustering (PIC) is a scalable and efficient algorithm for clustering vertices of a graph given pairwise similarities as edge properties, described in the paper “Power Iteration Clustering” by Frank Lin and William W. Cohen. It computes a pseudo-eigenvector of the normalized affinity matrix of the graph via power iteration and uses it to cluster vertices. spark.mllib includes an implementation of PIC using GraphX as its backend. It takes an RDD of (srcId, dstId, similarity) tuples and outputs a model with the clustering assignments. The similarities must be nonnegative. PIC assumes that the similarity measure is symmetric. A pair (srcId, dstId) regardless of the ordering should appear at most once in the input data. If a pair is missing from input, their similarity is treated as zero."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html#usage",
    "title": "Spark ML – Power Iteration Clustering",
    "section": "Usage",
    "text": "Usage\n \nml_power_iteration( \n  x, \n  k = 4, \n  max_iter = 20, \n  init_mode = \"random\", \n  src_col = \"src\", \n  dst_col = \"dst\", \n  weight_col = \"weight\", \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html#arguments",
    "title": "Spark ML – Power Iteration Clustering",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA ‘spark_connection’ or a ‘tbl_spark’.\n\n\nk\nThe number of clusters to create.\n\n\nmax_iter\nThe maximum number of iterations to run.\n\n\ninit_mode\nThis can be either “random”, which is the default, to use a random vector as vertex properties, or “degree” to use normalized sum similarities.\n\n\nsrc_col\nColumn in the input Spark dataframe containing 0-based indexes of all source vertices in the affinity matrix described in the PIC paper.\n\n\ndst_col\nColumn in the input Spark dataframe containing 0-based indexes of all destination vertices in the affinity matrix described in the PIC paper.\n\n\nweight_col\nColumn in the input Spark dataframe containing non-negative edge weights in the affinity matrix described in the PIC paper.\n\n\n…\nOptional arguments. Currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html#value",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html#value",
    "title": "Spark ML – Power Iteration Clustering",
    "section": "Value",
    "text": "Value\nA 2-column R dataframe with columns named “id” and “cluster” describing the resulting cluster assignments"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html#examples",
    "title": "Spark ML – Power Iteration Clustering",
    "section": "Examples",
    "text": "Examples\n\n \n \nlibrary(sparklyr) \n \nsc &lt;- spark_connect(master = \"local\") \n \nr1 &lt;- 1 \nn1 &lt;- 80L \nr2 &lt;- 4 \nn2 &lt;- 80L \n \ngen_circle &lt;- function(radius, num_pts) { \n  # generate evenly distributed points on a circle centered at the origin \n  seq(0, num_pts - 1) %&gt;% \n    lapply( \n      function(pt) { \n        theta &lt;- 2 * pi * pt / num_pts \n \n        radius * c(cos(theta), sin(theta)) \n      } \n    ) \n} \n \nguassian_similarity &lt;- function(pt1, pt2) { \n  dist2 &lt;- sum((pt2 - pt1)^2) \n \n  exp(-dist2 / 2) \n} \n \ngen_pic_data &lt;- function() { \n  # generate points on 2 concentric circle centered at the origin and then \n  # compute pairwise Gaussian similarity values of all unordered pair of \n  # points \n  n &lt;- n1 + n2 \n  pts &lt;- append(gen_circle(r1, n1), gen_circle(r2, n2)) \n  num_unordered_pairs &lt;- n * (n - 1) / 2 \n \n  src &lt;- rep(0L, num_unordered_pairs) \n  dst &lt;- rep(0L, num_unordered_pairs) \n  sim &lt;- rep(0, num_unordered_pairs) \n \n  idx &lt;- 1 \n  for (i in seq(2, n)) { \n    for (j in seq(i - 1)) { \n      src[[idx]] &lt;- i - 1L \n      dst[[idx]] &lt;- j - 1L \n      sim[[idx]] &lt;- guassian_similarity(pts[[i]], pts[[j]]) \n      idx &lt;- idx + 1 \n    } \n  } \n \n  tibble::tibble(src = src, dst = dst, sim = sim) \n} \n \npic_data &lt;- copy_to(sc, gen_pic_data()) \n \nclusters &lt;- ml_power_iteration( \n  pic_data, \n  src_col = \"src\", dst_col = \"dst\", weight_col = \"sim\", k = 2, max_iter = 40 \n) \nprint(clusters) \n#&gt; # A tibble: 160 × 2\n#&gt;       id cluster\n#&gt;    &lt;dbl&gt;   &lt;int&gt;\n#&gt;  1     0       1\n#&gt;  2     1       1\n#&gt;  3     2       1\n#&gt;  4     3       1\n#&gt;  5     4       1\n#&gt;  6     5       1\n#&gt;  7     6       1\n#&gt;  8     7       1\n#&gt;  9     8       1\n#&gt; 10     9       1\n#&gt; # … with 150 more rows"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_seq.html",
    "href": "packages/sparklyr/latest/reference/sdf_seq.html",
    "title": "Create DataFrame for Range",
    "section": "",
    "text": "R/sdf_sequence.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_seq.html#sdf_seq",
    "href": "packages/sparklyr/latest/reference/sdf_seq.html#sdf_seq",
    "title": "Create DataFrame for Range",
    "section": "sdf_seq",
    "text": "sdf_seq"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_seq.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_seq.html#description",
    "title": "Create DataFrame for Range",
    "section": "Description",
    "text": "Description\nCreates a DataFrame for the given range"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_seq.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_seq.html#usage",
    "title": "Create DataFrame for Range",
    "section": "Usage",
    "text": "Usage\nsdf_seq( \n  sc, \n  from = 1L, \n  to = 1L, \n  by = 1L, \n  repartition = NULL, \n  type = c(\"integer\", \"integer64\") \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_seq.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_seq.html#arguments",
    "title": "Create DataFrame for Range",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nfrom, to\nThe start and end to use as a range\n\n\nby\nThe increment of the sequence.\n\n\nrepartition\nThe number of partitions to use when distributing the data across the Spark cluster. Defaults to the minimum number of partitions.\n\n\ntype\nThe data type to use for the index, either \"integer\" or \"integer64\"."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_sync.html",
    "href": "packages/sparklyr/latest/reference/spark_install_sync.html",
    "title": "helper function to sync sparkinstall project to sparklyr",
    "section": "",
    "text": "R/install_tools.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_sync.html#spark_install_sync",
    "href": "packages/sparklyr/latest/reference/spark_install_sync.html#spark_install_sync",
    "title": "helper function to sync sparkinstall project to sparklyr",
    "section": "spark_install_sync",
    "text": "spark_install_sync"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_sync.html#description",
    "href": "packages/sparklyr/latest/reference/spark_install_sync.html#description",
    "title": "helper function to sync sparkinstall project to sparklyr",
    "section": "Description",
    "text": "Description\nSee: https://github.com/rstudio/spark-install"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_sync.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_install_sync.html#usage",
    "title": "helper function to sync sparkinstall project to sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_install_sync(project_path)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_sync.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_install_sync.html#arguments",
    "title": "helper function to sync sparkinstall project to sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nproject_path\nThe path to the sparkinstall project"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html",
    "href": "packages/sparklyr/latest/reference/spark_read.html",
    "title": "Read file(s) into a Spark DataFrame using a custom reader",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html#spark_read",
    "href": "packages/sparklyr/latest/reference/spark_read.html#spark_read",
    "title": "Read file(s) into a Spark DataFrame using a custom reader",
    "section": "spark_read",
    "text": "spark_read"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read.html#description",
    "title": "Read file(s) into a Spark DataFrame using a custom reader",
    "section": "Description",
    "text": "Description\nRun a custom R function on Spark workers to ingest data from one or more files into a Spark DataFrame, assuming all files follow the same schema."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read.html#usage",
    "title": "Read file(s) into a Spark DataFrame using a custom reader",
    "section": "Usage",
    "text": "Usage\nspark_read(sc, paths, reader, columns, packages = TRUE, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read.html#arguments",
    "title": "Read file(s) into a Spark DataFrame using a custom reader",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npaths\nA character vector of one or more file URIs (e.g., c(“hdfs://localhost:9000/file.txt”, “hdfs://localhost:9000/file2.txt”))\n\n\nreader\nA self-contained R function that takes a single file URI as argument and returns the data read from that file as a data frame.\n\n\ncolumns\na named list of column names and column types of the resulting data frame (e.g., list(column_1 = “integer”, column_2 = “character”)), or a list of column names only if column types should be inferred from the data (e.g., list(“column_1”, “column_2”), or NULL if column types should be inferred and resulting data frame can have arbitrary column names\n\n\npackages\nA list of R packages to distribute to Spark workers\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html#examples",
    "href": "packages/sparklyr/latest/reference/spark_read.html#examples",
    "title": "Read file(s) into a Spark DataFrame using a custom reader",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc &lt;- spark_connect( \n  master = \"yarn\", \n  spark_home = \"~/spark/spark-2.4.5-bin-hadoop2.7\" \n) \n# This is a contrived example to show reader tasks will be distributed across \n# all Spark worker nodes \nspark_read( \n  sc, \n  rep(\"/dev/null\", 10), \n  reader = function(path) system(\"hostname\", intern = TRUE), \n  columns = c(hostname = \"string\") \n) %&gt;% sdf_collect()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read.html#see-also",
    "title": "Read file(s) into a Spark DataFrame using a custom reader",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_broadcast.html",
    "href": "packages/sparklyr/latest/reference/sdf_broadcast.html",
    "title": "Broadcast hint",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_broadcast.html#sdf_broadcast",
    "href": "packages/sparklyr/latest/reference/sdf_broadcast.html#sdf_broadcast",
    "title": "Broadcast hint",
    "section": "sdf_broadcast",
    "text": "sdf_broadcast"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_broadcast.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_broadcast.html#description",
    "title": "Broadcast hint",
    "section": "Description",
    "text": "Description\nUsed to force broadcast hash joins."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_broadcast.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_broadcast.html#usage",
    "title": "Broadcast hint",
    "section": "Usage",
    "text": "Usage\nsdf_broadcast(x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_broadcast.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_broadcast.html#arguments",
    "title": "Broadcast hint",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html",
    "href": "packages/sparklyr/latest/reference/registerdospark.html",
    "title": "Register a Parallel Backend",
    "section": "",
    "text": "R/do_spark.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html#registerdospark",
    "href": "packages/sparklyr/latest/reference/registerdospark.html#registerdospark",
    "title": "Register a Parallel Backend",
    "section": "registerDoSpark",
    "text": "registerDoSpark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html#description",
    "href": "packages/sparklyr/latest/reference/registerdospark.html#description",
    "title": "Register a Parallel Backend",
    "section": "Description",
    "text": "Description\nRegisters a parallel backend using the foreach package."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html#usage",
    "href": "packages/sparklyr/latest/reference/registerdospark.html#usage",
    "title": "Register a Parallel Backend",
    "section": "Usage",
    "text": "Usage\n \nregisterDoSpark(spark_conn, parallelism = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html#arguments",
    "href": "packages/sparklyr/latest/reference/registerdospark.html#arguments",
    "title": "Register a Parallel Backend",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nspark_conn\nSpark connection to use\n\n\nparallelism\nLevel of parallelism to use for task execution (if unspecified, then it will take the value of SparkContext.defaultParallelism() which by default is the number of cores available to the sparklyr application)\n\n\n…\nadditional options for sparklyr parallel backend (currently only the only valid option is nocompile = T, F )"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html#value",
    "href": "packages/sparklyr/latest/reference/registerdospark.html#value",
    "title": "Register a Parallel Backend",
    "section": "Value",
    "text": "Value\nNone"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html#examples",
    "href": "packages/sparklyr/latest/reference/registerdospark.html#examples",
    "title": "Register a Parallel Backend",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \n \nsc &lt;- spark_connect(master = \"local\") \nregisterDoSpark(sc, nocompile = FALSE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_separate_column.html",
    "href": "packages/sparklyr/latest/reference/sdf_separate_column.html",
    "title": "Separate a Vector Column into Scalar Columns",
    "section": "",
    "text": "R/sdf_wrapper.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_separate_column.html#sdf_separate_column",
    "href": "packages/sparklyr/latest/reference/sdf_separate_column.html#sdf_separate_column",
    "title": "Separate a Vector Column into Scalar Columns",
    "section": "sdf_separate_column",
    "text": "sdf_separate_column"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_separate_column.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_separate_column.html#description",
    "title": "Separate a Vector Column into Scalar Columns",
    "section": "Description",
    "text": "Description\nGiven a vector column in a Spark DataFrame, split that into n separate columns, each column made up of the different elements in the column column."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_separate_column.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_separate_column.html#usage",
    "title": "Separate a Vector Column into Scalar Columns",
    "section": "Usage",
    "text": "Usage\nsdf_separate_column(x, column, into = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_separate_column.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_separate_column.html#arguments",
    "title": "Separate a Vector Column into Scalar Columns",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ncolumn\nThe name of a (vector-typed) column.\n\n\ninto\nA specification of the columns that should be generated from column. This can either be a vector of column names, or an R list mapping column names to the (1-based) index at which a particular vector element should be extracted."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_settings.html",
    "href": "packages/sparklyr/latest/reference/spark_config_settings.html",
    "title": "Retrieve Available Settings",
    "section": "",
    "text": "R/config_settings.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_settings.html#spark_config_settings",
    "href": "packages/sparklyr/latest/reference/spark_config_settings.html#spark_config_settings",
    "title": "Retrieve Available Settings",
    "section": "spark_config_settings",
    "text": "spark_config_settings"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_settings.html#description",
    "href": "packages/sparklyr/latest/reference/spark_config_settings.html#description",
    "title": "Retrieve Available Settings",
    "section": "Description",
    "text": "Description\nRetrieves available sparklyr settings that can be used in configuration files or spark_config()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_settings.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_config_settings.html#usage",
    "title": "Retrieve Available Settings",
    "section": "Usage",
    "text": "Usage\nspark_config_settings()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_delta.html",
    "href": "packages/sparklyr/latest/reference/spark_write_delta.html",
    "title": "Writes a Spark DataFrame into Delta Lake",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_delta.html#spark_write_delta",
    "href": "packages/sparklyr/latest/reference/spark_write_delta.html#spark_write_delta",
    "title": "Writes a Spark DataFrame into Delta Lake",
    "section": "spark_write_delta",
    "text": "spark_write_delta"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_delta.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_delta.html#description",
    "title": "Writes a Spark DataFrame into Delta Lake",
    "section": "Description",
    "text": "Description\nWrites a Spark DataFrame into Delta Lake."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_delta.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_delta.html#usage",
    "title": "Writes a Spark DataFrame into Delta Lake",
    "section": "Usage",
    "text": "Usage\nspark_write_delta( \n  x, \n  path, \n  mode = NULL, \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_delta.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_delta.html#arguments",
    "title": "Writes a Spark DataFrame into Delta Lake",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.  For more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_delta.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_delta.html#see-also",
    "title": "Writes a Spark DataFrame into Delta Lake",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html",
    "title": "Spark ML – Naive-Bayes",
    "section": "",
    "text": "R/ml_classification_naive_bayes.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#ml_naive_bayes",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#ml_naive_bayes",
    "title": "Spark ML – Naive-Bayes",
    "section": "ml_naive_bayes",
    "text": "ml_naive_bayes"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#description",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#description",
    "title": "Spark ML – Naive-Bayes",
    "section": "Description",
    "text": "Description\nNaive Bayes Classifiers. It supports Multinomial NB (see here) which can handle finitely supported discrete data. For example, by converting documents into TF-IDF vectors, it can be used for document classification. By making every vector a binary (0/1) data, it can also be used as Bernoulli NB (see here). The input feature values must be nonnegative."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#usage",
    "title": "Spark ML – Naive-Bayes",
    "section": "Usage",
    "text": "Usage\n \nml_naive_bayes( \n  x, \n  formula = NULL, \n  model_type = \"multinomial\", \n  smoothing = 1, \n  thresholds = NULL, \n  weight_col = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  uid = random_string(\"naive_bayes_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#arguments",
    "title": "Spark ML – Naive-Bayes",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nmodel_type\nThe model type. Supported options: \"multinomial\" and \"bernoulli\". (default = multinomial)\n\n\nsmoothing\nThe (Laplace) smoothing parameter. Defaults to 1.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\nweight_col\n(Spark 2.1.0+) Weight column name. If this is not set or empty, we treat all instance weights as 1.0.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#details",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#details",
    "title": "Spark ML – Naive-Bayes",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#value",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#value",
    "title": "Spark ML – Naive-Bayes",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#examples",
    "title": "Spark ML – Naive-Bayes",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \npartitions &lt;- iris_tbl %&gt;% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \n \niris_training &lt;- partitions$training \niris_test &lt;- partitions$test \n \nnb_model &lt;- iris_training %&gt;% \n  ml_naive_bayes(Species ~ .) \n \npred &lt;- ml_predict(nb_model, iris_test) \n \nml_multiclass_classification_evaluator(pred) \n#&gt; [1] 0.9393939"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#see-also",
    "title": "Spark ML – Naive-Bayes",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms. Other ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/arrow_enabled_object.html",
    "href": "packages/sparklyr/latest/reference/arrow_enabled_object.html",
    "title": "Determine whether arrow is able to serialize the given R object",
    "section": "",
    "text": "R/arrow_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/arrow_enabled_object.html#arrow_enabled_object",
    "href": "packages/sparklyr/latest/reference/arrow_enabled_object.html#arrow_enabled_object",
    "title": "Determine whether arrow is able to serialize the given R object",
    "section": "arrow_enabled_object",
    "text": "arrow_enabled_object"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/arrow_enabled_object.html#description",
    "href": "packages/sparklyr/latest/reference/arrow_enabled_object.html#description",
    "title": "Determine whether arrow is able to serialize the given R object",
    "section": "Description",
    "text": "Description\nIf the given R object is not serializable by arrow due to some known limitations of arrow, then return FALSE, otherwise return TRUE"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/arrow_enabled_object.html#usage",
    "href": "packages/sparklyr/latest/reference/arrow_enabled_object.html#usage",
    "title": "Determine whether arrow is able to serialize the given R object",
    "section": "Usage",
    "text": "Usage\n \narrow_enabled_object(object)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/arrow_enabled_object.html#arguments",
    "href": "packages/sparklyr/latest/reference/arrow_enabled_object.html#arguments",
    "title": "Determine whether arrow is able to serialize the given R object",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nobject\nThe object to be serialized"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/arrow_enabled_object.html#examples",
    "href": "packages/sparklyr/latest/reference/arrow_enabled_object.html#examples",
    "title": "Determine whether arrow is able to serialize the given R object",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \n \ndf &lt;- tibble::tibble(x = seq(5)) \narrow_enabled_object(df) \n#&gt; [1] TRUE"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "",
    "text": "R/ml_feature_min_max_scaler.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#ft_min_max_scaler",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#ft_min_max_scaler",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "ft_min_max_scaler",
    "text": "ft_min_max_scaler"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#description",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#description",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "Description",
    "text": "Description\nRescale each feature individually to a common range [min, max] linearly using column summary statistics, which is also known as min-max normalization or Rescaling"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#usage",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "Usage",
    "text": "Usage\n \nft_min_max_scaler( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  min = 0, \n  max = 1, \n  uid = random_string(\"min_max_scaler_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#arguments",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nmin\nLower bound after transformation, shared by all features Default: 0.0\n\n\nmax\nUpper bound after transformation, shared by all features Default: 1.0\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#details",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#details",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#value",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#value",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#examples",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#examples",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \nfeatures &lt;- c(\"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\") \n \niris_tbl %&gt;% \n  ft_vector_assembler( \n    input_col = features, \n    output_col = \"features_temp\" \n  ) %&gt;% \n  ft_min_max_scaler( \n    input_col = \"features_temp\", \n    output_col = \"features\" \n  ) \n#&gt; # Source: spark&lt;?&gt; [?? x 7]\n#&gt;    Sepal_L…¹ Sepal…² Petal…³ Petal…⁴ Species featu…⁵ featu…⁶\n#&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;list&gt;  &lt;list&gt; \n#&gt;  1       5.1     3.5     1.4     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  2       4.9     3       1.4     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  3       4.7     3.2     1.3     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  4       4.6     3.1     1.5     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  5       5       3.6     1.4     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  6       5.4     3.9     1.7     0.4 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  7       4.6     3.4     1.4     0.3 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  8       5       3.4     1.5     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt;  9       4.4     2.9     1.4     0.2 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt; 10       4.9     3.1     1.5     0.1 setosa  &lt;dbl&gt;   &lt;dbl&gt;  \n#&gt; # … with more rows, and abbreviated variable names\n#&gt; #   ¹​Sepal_Length, ²​Sepal_Width, ³​Petal_Length,\n#&gt; #   ⁴​Petal_Width, ⁵​features_temp, ⁶​features"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#see-also",
    "title": "Feature Transformation – MinMaxScaler (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark. Other feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_pipeline_stage.html",
    "href": "packages/sparklyr/latest/reference/spark_pipeline_stage.html",
    "title": "Create a Pipeline Stage Object",
    "section": "",
    "text": "R/ml_pipeline_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#spark_pipeline_stage",
    "href": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#spark_pipeline_stage",
    "title": "Create a Pipeline Stage Object",
    "section": "spark_pipeline_stage",
    "text": "spark_pipeline_stage"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#description",
    "href": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#description",
    "title": "Create a Pipeline Stage Object",
    "section": "Description",
    "text": "Description\nHelper function to create pipeline stage objects with common parameter setters."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#usage",
    "title": "Create a Pipeline Stage Object",
    "section": "Usage",
    "text": "Usage\nspark_pipeline_stage( \n  sc, \n  class, \n  uid, \n  features_col = NULL, \n  label_col = NULL, \n  prediction_col = NULL, \n  probability_col = NULL, \n  raw_prediction_col = NULL, \n  k = NULL, \n  max_iter = NULL, \n  seed = NULL, \n  input_col = NULL, \n  input_cols = NULL, \n  output_col = NULL, \n  output_cols = NULL \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#arguments",
    "title": "Create a Pipeline Stage Object",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection object.\n\n\nclass\nClass name for the pipeline stage.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nk\nThe number of clusters to create\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\ninput_col\nThe name of the input column.\n\n\ninput_cols\nNames of output columns.\n\n\noutput_col\nThe name of the output column.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_model_data.html",
    "href": "packages/sparklyr/latest/reference/ml_model_data.html",
    "title": "Extracts data associated with a Spark ML model",
    "section": "",
    "text": "R/ml_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_model_data.html#ml_model_data",
    "href": "packages/sparklyr/latest/reference/ml_model_data.html#ml_model_data",
    "title": "Extracts data associated with a Spark ML model",
    "section": "ml_model_data",
    "text": "ml_model_data"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_model_data.html#description",
    "href": "packages/sparklyr/latest/reference/ml_model_data.html#description",
    "title": "Extracts data associated with a Spark ML model",
    "section": "Description",
    "text": "Description\nExtracts data associated with a Spark ML model"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_model_data.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_model_data.html#usage",
    "title": "Extracts data associated with a Spark ML model",
    "section": "Usage",
    "text": "Usage\nml_model_data(object)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_model_data.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_model_data.html#arguments",
    "title": "Extracts data associated with a Spark ML model",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nobject\na Spark ML model"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_model_data.html#value",
    "href": "packages/sparklyr/latest/reference/ml_model_data.html#value",
    "title": "Extracts data associated with a Spark ML model",
    "section": "Value",
    "text": "Value\nA tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html",
    "href": "packages/sparklyr/latest/reference/spark_version.html",
    "title": "Get the Spark Version Associated with a Spark Connection",
    "section": "",
    "text": "R/spark_version.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html#spark_version",
    "href": "packages/sparklyr/latest/reference/spark_version.html#spark_version",
    "title": "Get the Spark Version Associated with a Spark Connection",
    "section": "spark_version",
    "text": "spark_version"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html#description",
    "href": "packages/sparklyr/latest/reference/spark_version.html#description",
    "title": "Get the Spark Version Associated with a Spark Connection",
    "section": "Description",
    "text": "Description\nRetrieve the version of Spark associated with a Spark connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_version.html#usage",
    "title": "Get the Spark Version Associated with a Spark Connection",
    "section": "Usage",
    "text": "Usage\nspark_version(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_version.html#arguments",
    "title": "Get the Spark Version Associated with a Spark Connection",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html#details",
    "href": "packages/sparklyr/latest/reference/spark_version.html#details",
    "title": "Get the Spark Version Associated with a Spark Connection",
    "section": "Details",
    "text": "Details\nSuffixes for e.g. preview versions, or snapshotted versions, are trimmed – if you require the full Spark version, you can retrieve it with invoke(spark_context(sc), \"version\")."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html#value",
    "href": "packages/sparklyr/latest/reference/spark_version.html#value",
    "title": "Get the Spark Version Associated with a Spark Connection",
    "section": "Value",
    "text": "Value\nThe Spark version as a numeric_version."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html",
    "title": "Write Parquet Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html#stream_write_parquet",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html#stream_write_parquet",
    "title": "Write Parquet Stream",
    "section": "stream_write_parquet",
    "text": "stream_write_parquet"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html#description",
    "title": "Write Parquet Stream",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into a parquet stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html#usage",
    "title": "Write Parquet Stream",
    "section": "Usage",
    "text": "Usage\nstream_write_parquet( \n  x, \n  path, \n  mode = c(\"append\", \"complete\", \"update\"), \n  trigger = stream_trigger_interval(), \n  checkpoint = file.path(path, \"checkpoints\", random_string(\"\")), \n  options = list(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html#arguments",
    "title": "Write Parquet Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe destination path. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\".\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous.\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html#examples",
    "title": "Write Parquet Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\") \nsdf_len(sc, 10) %&gt;% spark_write_parquet(\"parquet-in\") \nstream &lt;- stream_read_parquet(sc, \"parquet-in\") %&gt;% stream_write_parquet(\"parquet-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html#see-also",
    "title": "Write Parquet Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_repartition.html",
    "href": "packages/sparklyr/latest/reference/sdf_repartition.html",
    "title": "Repartition a Spark DataFrame",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_repartition.html#sdf_repartition",
    "href": "packages/sparklyr/latest/reference/sdf_repartition.html#sdf_repartition",
    "title": "Repartition a Spark DataFrame",
    "section": "sdf_repartition",
    "text": "sdf_repartition"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_repartition.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_repartition.html#description",
    "title": "Repartition a Spark DataFrame",
    "section": "Description",
    "text": "Description\nRepartition a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_repartition.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_repartition.html#usage",
    "title": "Repartition a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_repartition(x, partitions = NULL, partition_by = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_repartition.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_repartition.html#arguments",
    "title": "Repartition a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\npartitions\nnumber of partitions\n\n\npartition_by\nvector of column names used for partitioning, only supported for Spark 2.0+"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/fill.html",
    "href": "packages/sparklyr/latest/reference/fill.html",
    "title": "Fill",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/fill.html#fill",
    "href": "packages/sparklyr/latest/reference/fill.html#fill",
    "title": "Fill",
    "section": "fill",
    "text": "fill"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/fill.html#description",
    "href": "packages/sparklyr/latest/reference/fill.html#description",
    "title": "Fill",
    "section": "Description",
    "text": "Description\nSee fill for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dataframe.html",
    "href": "packages/sparklyr/latest/reference/spark_dataframe.html",
    "title": "Retrieve a Spark DataFrame",
    "section": "",
    "text": "R/spark_dataframe.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dataframe.html#spark_dataframe",
    "href": "packages/sparklyr/latest/reference/spark_dataframe.html#spark_dataframe",
    "title": "Retrieve a Spark DataFrame",
    "section": "spark_dataframe",
    "text": "spark_dataframe"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dataframe.html#description",
    "href": "packages/sparklyr/latest/reference/spark_dataframe.html#description",
    "title": "Retrieve a Spark DataFrame",
    "section": "Description",
    "text": "Description\nThis S3 generic is used to access a Spark DataFrame object (as a Java object reference) from an R object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dataframe.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_dataframe.html#usage",
    "title": "Retrieve a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nspark_dataframe(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dataframe.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_dataframe.html#arguments",
    "title": "Retrieve a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn R object wrapping, or containing, a Spark DataFrame.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dataframe.html#value",
    "href": "packages/sparklyr/latest/reference/spark_dataframe.html#value",
    "title": "Retrieve a Spark DataFrame",
    "section": "Value",
    "text": "Value\nA spark_jobj representing a Java object reference to a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html",
    "title": "Write Console Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html#stream_write_console",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html#stream_write_console",
    "title": "Write Console Stream",
    "section": "stream_write_console",
    "text": "stream_write_console"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html#description",
    "title": "Write Console Stream",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into console logs."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html#usage",
    "title": "Write Console Stream",
    "section": "Usage",
    "text": "Usage\nstream_write_console( \n  x, \n  mode = c(\"append\", \"complete\", \"update\"), \n  options = list(), \n  trigger = stream_trigger_interval(), \n  partition_by = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html#arguments",
    "title": "Write Console Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\".\n\n\noptions\nA list of strings with additional options.\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html#examples",
    "title": "Write Console Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\") \nsdf_len(sc, 10) %&gt;% \n  dplyr::transmute(text = as.character(id)) %&gt;% \n  spark_write_text(\"text-in\") \nstream &lt;- stream_read_text(sc, \"text-in\") %&gt;% stream_write_console() \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html#see-also",
    "title": "Write Console Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/print_jobj.html",
    "href": "packages/sparklyr/latest/reference/print_jobj.html",
    "title": "Generic method for print jobj for a connection type",
    "section": "",
    "text": "R/core_jobj.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/print_jobj.html#print_jobj",
    "href": "packages/sparklyr/latest/reference/print_jobj.html#print_jobj",
    "title": "Generic method for print jobj for a connection type",
    "section": "print_jobj",
    "text": "print_jobj"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/print_jobj.html#description",
    "href": "packages/sparklyr/latest/reference/print_jobj.html#description",
    "title": "Generic method for print jobj for a connection type",
    "section": "Description",
    "text": "Description\nGeneric method for print jobj for a connection type"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/print_jobj.html#usage",
    "href": "packages/sparklyr/latest/reference/print_jobj.html#usage",
    "title": "Generic method for print jobj for a connection type",
    "section": "Usage",
    "text": "Usage\nprint_jobj(sc, jobj, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/print_jobj.html#arguments",
    "href": "packages/sparklyr/latest/reference/print_jobj.html#arguments",
    "title": "Generic method for print jobj for a connection type",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nspark_connection (used for type dispatch)\n\n\njobj\nObject to print"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rchisq.html",
    "href": "packages/sparklyr/latest/reference/sdf_rchisq.html",
    "title": "Generate random samples from a chi-squared distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rchisq.html#sdf_rchisq",
    "href": "packages/sparklyr/latest/reference/sdf_rchisq.html#sdf_rchisq",
    "title": "Generate random samples from a chi-squared distribution",
    "section": "sdf_rchisq",
    "text": "sdf_rchisq"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rchisq.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rchisq.html#description",
    "title": "Generate random samples from a chi-squared distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a chi-squared distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rchisq.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rchisq.html#usage",
    "title": "Generate random samples from a chi-squared distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rchisq(sc, n, df, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rchisq.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rchisq.html#arguments",
    "title": "Generate random samples from a chi-squared distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\ndf\nDegrees of freedom (non-negative, but can be non-integer).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rchisq.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rchisq.html#see-also",
    "title": "Generate random samples from a chi-squared distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_view.html",
    "href": "packages/sparklyr/latest/reference/stream_view.html",
    "title": "View Stream",
    "section": "",
    "text": "R/stream_view.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_view.html#stream_view",
    "href": "packages/sparklyr/latest/reference/stream_view.html#stream_view",
    "title": "View Stream",
    "section": "stream_view",
    "text": "stream_view"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_view.html#description",
    "href": "packages/sparklyr/latest/reference/stream_view.html#description",
    "title": "View Stream",
    "section": "Description",
    "text": "Description\nOpens a Shiny gadget to visualize the given stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_view.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_view.html#usage",
    "title": "View Stream",
    "section": "Usage",
    "text": "Usage\nstream_view(stream, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_view.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_view.html#arguments",
    "title": "View Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nstream\nThe stream to visualize.\n\n\n…\nAdditional optional arguments."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_view.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_view.html#examples",
    "title": "View Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\") \ndir.create(\"iris-in\") \nwrite.csv(iris, \"iris-in/iris.csv\", row.names = FALSE) \nstream_read_csv(sc, \"iris-in/\") %&gt;% \n  stream_write_csv(\"iris-out/\") %&gt;% \n  stream_view() %&gt;% \n  stream_stop()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_debug_string.html",
    "href": "packages/sparklyr/latest/reference/sdf_debug_string.html",
    "title": "Debug Info for Spark DataFrame",
    "section": "",
    "text": "R/sdf_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_debug_string.html#sdf_debug_string",
    "href": "packages/sparklyr/latest/reference/sdf_debug_string.html#sdf_debug_string",
    "title": "Debug Info for Spark DataFrame",
    "section": "sdf_debug_string",
    "text": "sdf_debug_string"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_debug_string.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_debug_string.html#description",
    "title": "Debug Info for Spark DataFrame",
    "section": "Description",
    "text": "Description\nPrints plan of execution to generate x. This plan will, among other things, show the number of partitions in parenthesis at the far left and indicate stages using indentation."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_debug_string.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_debug_string.html#usage",
    "title": "Debug Info for Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_debug_string(x, print = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_debug_string.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_debug_string.html#arguments",
    "title": "Debug Info for Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn R object wrapping, or containing, a Spark DataFrame.\n\n\nprint\nPrint debug information?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html",
    "title": "Feature Transformation – Word2Vec (Estimator)",
    "section": "",
    "text": "R/ml_feature_word2vec.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#ft_word2vec",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#ft_word2vec",
    "title": "Feature Transformation – Word2Vec (Estimator)",
    "section": "ft_word2vec",
    "text": "ft_word2vec"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#description",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#description",
    "title": "Feature Transformation – Word2Vec (Estimator)",
    "section": "Description",
    "text": "Description\nWord2Vec transforms a word into a code for further natural language processing or machine learning process."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#usage",
    "title": "Feature Transformation – Word2Vec (Estimator)",
    "section": "Usage",
    "text": "Usage\nft_word2vec( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  vector_size = 100, \n  min_count = 5, \n  max_sentence_length = 1000, \n  num_partitions = 1, \n  step_size = 0.025, \n  max_iter = 1, \n  seed = NULL, \n  uid = random_string(\"word2vec_\"), \n  ... \n) \n\nml_find_synonyms(model, word, num)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#arguments",
    "title": "Feature Transformation – Word2Vec (Estimator)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nvector_size\nThe dimension of the code that you want to transform from words. Default: 100\n\n\nmin_count\nThe minimum number of times a token must appear to be included in the word2vec model’s vocabulary. Default: 5\n\n\nmax_sentence_length\n(Spark 2.0.0+) Sets the maximum length (in words) of each sentence in the input data. Any sentence longer than this threshold will be divided into chunks of up to max_sentence_length size. Default: 1000\n\n\nnum_partitions\nNumber of partitions for sentences of words. Default: 1\n\n\nstep_size\nParam for Step size to be used for each iteration of optimization (&gt; 0).\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused.\n\n\nmodel\nA fitted Word2Vec model, returned by ft_word2vec().\n\n\nword\nA word, as a length-one character vector.\n\n\nnum\nNumber of words closest in similarity to the given word to find."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#details",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#details",
    "title": "Feature Transformation – Word2Vec (Estimator)",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x\nto obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#value",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#value",
    "title": "Feature Transformation – Word2Vec (Estimator)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark\n\nml_find_synonyms() returns a DataFrame of synonyms and cosine similarities"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#see-also",
    "title": "Feature Transformation – Word2Vec (Estimator)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html",
    "title": "Tidying methods for Spark ML Logistic Regression",
    "section": "",
    "text": "R/tidiers_ml_logistic_regression.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#ml_logistic_regression_tidiers",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#ml_logistic_regression_tidiers",
    "title": "Tidying methods for Spark ML Logistic Regression",
    "section": "ml_logistic_regression_tidiers",
    "text": "ml_logistic_regression_tidiers"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#description",
    "title": "Tidying methods for Spark ML Logistic Regression",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#usage",
    "title": "Tidying methods for Spark ML Logistic Regression",
    "section": "Usage",
    "text": "Usage\n## S3 method for class 'ml_model_logistic_regression'\ntidy(x, ...) \n\n## S3 method for class 'ml_model_logistic_regression'\naugment(x, newdata = NULL, ...) \n\n## S3 method for class '`_ml_model_logistic_regression`'\naugment(x, new_data = NULL, ...) \n\n## S3 method for class 'ml_model_logistic_regression'\nglance(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#arguments",
    "title": "Tidying methods for Spark ML Logistic Regression",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction.\n\n\nnew_data\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/na.replace.html",
    "href": "packages/sparklyr/latest/reference/na.replace.html",
    "title": "Replace Missing Values in Objects",
    "section": "",
    "text": "R/na_actions.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/na.replace.html#na.replace",
    "href": "packages/sparklyr/latest/reference/na.replace.html#na.replace",
    "title": "Replace Missing Values in Objects",
    "section": "na.replace",
    "text": "na.replace"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/na.replace.html#description",
    "href": "packages/sparklyr/latest/reference/na.replace.html#description",
    "title": "Replace Missing Values in Objects",
    "section": "Description",
    "text": "Description\nThis S3 generic provides an interface for replacing NA values within an object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/na.replace.html#usage",
    "href": "packages/sparklyr/latest/reference/na.replace.html#usage",
    "title": "Replace Missing Values in Objects",
    "section": "Usage",
    "text": "Usage\nna.replace(object, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/na.replace.html#arguments",
    "href": "packages/sparklyr/latest/reference/na.replace.html#arguments",
    "title": "Replace Missing Values in Objects",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nobject\nAn R object.\n\n\n…\nArguments passed along to implementing methods."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html",
    "title": "Retrieves or sets initial number of shuffle partitions before coalescing",
    "section": "",
    "text": "R/spark_context_config.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#spark_coalesce_initial_num_partitions",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#spark_coalesce_initial_num_partitions",
    "title": "Retrieves or sets initial number of shuffle partitions before coalescing",
    "section": "spark_coalesce_initial_num_partitions",
    "text": "spark_coalesce_initial_num_partitions"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#description",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#description",
    "title": "Retrieves or sets initial number of shuffle partitions before coalescing",
    "section": "Description",
    "text": "Description\nRetrieves or sets initial number of shuffle partitions before coalescing"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#usage",
    "title": "Retrieves or sets initial number of shuffle partitions before coalescing",
    "section": "Usage",
    "text": "Usage\nspark_coalesce_initial_num_partitions(sc, num_partitions = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#arguments",
    "title": "Retrieves or sets initial number of shuffle partitions before coalescing",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nnum_partitions\nInitial number of shuffle partitions before coalescing. Defaults to NULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#see-also",
    "title": "Retrieves or sets initial number of shuffle partitions before coalescing",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_adaptive_query_execution(), spark_advisory_shuffle_partition_size(), spark_auto_broadcast_join_threshold(), spark_coalesce_min_num_partitions(), spark_coalesce_shuffle_partitions(), spark_session_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sub-.tbl_spark.html",
    "href": "packages/sparklyr/latest/reference/sub-.tbl_spark.html",
    "title": "Subsetting operator for Spark dataframe",
    "section": "",
    "text": "R/sdf_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#tbl_spark",
    "href": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#tbl_spark",
    "title": "Subsetting operator for Spark dataframe",
    "section": "[.tbl_spark",
    "text": "[.tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#description",
    "href": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#description",
    "title": "Subsetting operator for Spark dataframe",
    "section": "Description",
    "text": "Description\nSusetting operator for Spark dataframe allowing a subset of column(s) to be selected using syntaxes similar to those supported by R dataframes"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#usage",
    "href": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#usage",
    "title": "Subsetting operator for Spark dataframe",
    "section": "Usage",
    "text": "Usage\n \n## S3 method for class 'tbl_spark'\n[(x, i)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#arguments",
    "href": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#arguments",
    "title": "Subsetting operator for Spark dataframe",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nThe Spark dataframe\n\n\ni\nExpression specifying subset of column(s) to include or exclude from the result (e.g., [\"col1\"], [c(\"col1\", \"col2\")], [1:10], [-1], [NULL], or [])"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#examples",
    "href": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#examples",
    "title": "Subsetting operator for Spark dataframe",
    "section": "Examples",
    "text": "Examples\n\n \nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"spark://HOST:PORT\") \nexample_sdf &lt;- copy_to(sc, tibble::tibble(a = 1, b = 2)) \nexample_sdf[\"a\"] %&gt;% print() \n#&gt; # A tibble: 1 × 1\n#&gt;       a\n#&gt;   &lt;dbl&gt;\n#&gt; 1     1"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/separate.html",
    "href": "packages/sparklyr/latest/reference/separate.html",
    "title": "Separate",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/separate.html#separate",
    "href": "packages/sparklyr/latest/reference/separate.html#separate",
    "title": "Separate",
    "section": "separate",
    "text": "separate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/separate.html#description",
    "href": "packages/sparklyr/latest/reference/separate.html#description",
    "title": "Separate",
    "section": "Description",
    "text": "Description\nSee separate for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pivot_longer.html",
    "href": "packages/sparklyr/latest/reference/pivot_longer.html",
    "title": "Pivot longer",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pivot_longer.html#pivot_longer",
    "href": "packages/sparklyr/latest/reference/pivot_longer.html#pivot_longer",
    "title": "Pivot longer",
    "section": "pivot_longer",
    "text": "pivot_longer"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pivot_longer.html#description",
    "href": "packages/sparklyr/latest/reference/pivot_longer.html#description",
    "title": "Pivot longer",
    "section": "Description",
    "text": "Description\nSee pivot_longer for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html",
    "title": "Spark ML – OneVsRest",
    "section": "",
    "text": "R/ml_classification_one_vs_rest.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#ml_one_vs_rest",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#ml_one_vs_rest",
    "title": "Spark ML – OneVsRest",
    "section": "ml_one_vs_rest",
    "text": "ml_one_vs_rest"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#description",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#description",
    "title": "Spark ML – OneVsRest",
    "section": "Description",
    "text": "Description\nReduction of Multiclass Classification to Binary Classification. Performs reduction using one against all strategy. For a multiclass classification with k classes, train k models (one per class). Each example is scored against all k models and the model with highest score is picked to label the example."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#usage",
    "title": "Spark ML – OneVsRest",
    "section": "Usage",
    "text": "Usage\nml_one_vs_rest( \n  x, \n  formula = NULL, \n  classifier = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"one_vs_rest_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#arguments",
    "title": "Spark ML – OneVsRest",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nclassifier\nObject of class ml_estimator. Base binary classifier that we reduce multiclass classification into.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#details",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#details",
    "title": "Spark ML – OneVsRest",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#value",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#value",
    "title": "Spark ML – OneVsRest",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula\nis specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#see-also",
    "title": "Spark ML – OneVsRest",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_regression.html",
    "href": "packages/sparklyr/latest/reference/ml_metrics_regression.html",
    "title": "Extracts metrics from a fitted table",
    "section": "",
    "text": "R/ml_metrics.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_regression.html#ml_metrics_regression",
    "href": "packages/sparklyr/latest/reference/ml_metrics_regression.html#ml_metrics_regression",
    "title": "Extracts metrics from a fitted table",
    "section": "ml_metrics_regression",
    "text": "ml_metrics_regression"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_regression.html#description",
    "href": "packages/sparklyr/latest/reference/ml_metrics_regression.html#description",
    "title": "Extracts metrics from a fitted table",
    "section": "Description",
    "text": "Description\nThe function works best when passed a tbl_spark created by ml_predict(). The output tbl_spark will contain the correct variable types and format that the given Spark model “evaluator” expects."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_regression.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_metrics_regression.html#usage",
    "title": "Extracts metrics from a fitted table",
    "section": "Usage",
    "text": "Usage\n \nml_metrics_regression( \n  x, \n  truth, \n  estimate = prediction, \n  metrics = c(\"rmse\", \"rsq\", \"mae\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_regression.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_metrics_regression.html#arguments",
    "title": "Extracts metrics from a fitted table",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA tbl_spark containing the estimate (prediction) and the truth (value of what actually happened)\n\n\ntruth\nThe name of the column from x that contains the value of what actually happened\n\n\nestimate\nThe name of the column from x that contains the prediction. Defaults to prediction, since it is the default that ml_predict() uses.\n\n\nmetrics\nA character vector with the metrics to calculate. For regression models the possible values are: rmse (Root mean squared error), mse (Mean squared error),rsq (R squared), mae (Mean absolute error), and var (Explained variance). Defaults to: rmse, rsq, mae\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_regression.html#details",
    "href": "packages/sparklyr/latest/reference/ml_metrics_regression.html#details",
    "title": "Extracts metrics from a fitted table",
    "section": "Details",
    "text": "Details\nThe ml_metrics family of functions implement Spark’s evaluate closer to how the yardstick package works. The functions expect a table containing the truth and estimate, and return a tibble with the results. The tibble has the same format and variable names as the output of the yardstick functions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_regression.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_metrics_regression.html#examples",
    "title": "Extracts metrics from a fitted table",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(\"local\") \ntbl_iris &lt;- copy_to(sc, iris) \niris_split &lt;- sdf_random_split(tbl_iris, training = 0.5, test = 0.5) \ntraining &lt;- iris_split$training \nreg_formula &lt;- \"Sepal_Length ~ Sepal_Width + Petal_Length + Petal_Width\" \nmodel &lt;- ml_generalized_linear_regression(training, reg_formula) \ntbl_predictions &lt;- ml_predict(model, iris_split$test) \ntbl_predictions %&gt;% \n  ml_metrics_regression(Sepal_Length) \n#&gt; # A tibble: 3 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard       0.313\n#&gt; 2 rsq     standard       0.863\n#&gt; 3 mae     standard       0.249"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection.html",
    "href": "packages/sparklyr/latest/reference/spark_connection.html",
    "title": "Retrieve the Spark Connection Associated with an R Object",
    "section": "",
    "text": "R/spark_connection.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection.html#spark_connection",
    "href": "packages/sparklyr/latest/reference/spark_connection.html#spark_connection",
    "title": "Retrieve the Spark Connection Associated with an R Object",
    "section": "spark_connection",
    "text": "spark_connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection.html#description",
    "href": "packages/sparklyr/latest/reference/spark_connection.html#description",
    "title": "Retrieve the Spark Connection Associated with an R Object",
    "section": "Description",
    "text": "Description\nRetrieve the spark_connection associated with an R object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_connection.html#usage",
    "title": "Retrieve the Spark Connection Associated with an R Object",
    "section": "Usage",
    "text": "Usage\nspark_connection(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_connection.html#arguments",
    "title": "Retrieve the Spark Connection Associated with an R Object",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn R object from which a spark_connection can be obtained.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_binary.html",
    "href": "packages/sparklyr/latest/reference/ml_metrics_binary.html",
    "title": "Extracts metrics from a fitted table",
    "section": "",
    "text": "R/ml_metrics.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_binary.html#ml_metrics_binary",
    "href": "packages/sparklyr/latest/reference/ml_metrics_binary.html#ml_metrics_binary",
    "title": "Extracts metrics from a fitted table",
    "section": "ml_metrics_binary",
    "text": "ml_metrics_binary"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_binary.html#description",
    "href": "packages/sparklyr/latest/reference/ml_metrics_binary.html#description",
    "title": "Extracts metrics from a fitted table",
    "section": "Description",
    "text": "Description\nThe function works best when passed a tbl_spark created by ml_predict(). The output tbl_spark will contain the correct variable types and format that the given Spark model “evaluator” expects."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_binary.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_metrics_binary.html#usage",
    "title": "Extracts metrics from a fitted table",
    "section": "Usage",
    "text": "Usage\n \nml_metrics_binary( \n  x, \n  truth = label, \n  estimate = rawPrediction, \n  metrics = c(\"roc_auc\", \"pr_auc\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_binary.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_metrics_binary.html#arguments",
    "title": "Extracts metrics from a fitted table",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA tbl_spark containing the estimate (prediction) and the truth (value of what actually happened)\n\n\ntruth\nThe name of the column from x with an integer field containing the binary response (0 or 1). The ml_predict() function will create a new field named label which contains the expected type and values. truth defaults to label.\n\n\nestimate\nThe name of the column from x that contains the prediction. Defaults to rawPrediction, since its type and expected values will match truth.\n\n\nmetrics\nA character vector with the metrics to calculate. For binary models the possible values are: roc_auc (Area under the Receiver Operator curve), pr_auc (Area under the Precesion Recall curve). Defaults to: roc_auc, pr_auc\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_binary.html#details",
    "href": "packages/sparklyr/latest/reference/ml_metrics_binary.html#details",
    "title": "Extracts metrics from a fitted table",
    "section": "Details",
    "text": "Details\nThe ml_metrics family of functions implement Spark’s evaluate closer to how the yardstick package works. The functions expect a table containing the truth and estimate, and return a tibble with the results. The tibble has the same format and variable names as the output of the yardstick functions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_metrics_binary.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_metrics_binary.html#examples",
    "title": "Extracts metrics from a fitted table",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(\"local\") \ntbl_iris &lt;- copy_to(sc, iris) \nprep_iris &lt;- tbl_iris %&gt;% \n  mutate(is_setosa = ifelse(Species == \"setosa\", 1, 0)) \niris_split &lt;- sdf_random_split(prep_iris, training = 0.5, test = 0.5) \nmodel &lt;- ml_logistic_regression(iris_split$training, \"is_setosa ~ Sepal_Length\") \ntbl_predictions &lt;- ml_predict(model, iris_split$test) \nml_metrics_binary(tbl_predictions) \n#&gt; # A tibble: 2 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.988\n#&gt; 2 pr_auc  binary         0.978"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_image.html",
    "href": "packages/sparklyr/latest/reference/spark_read_image.html",
    "title": "Read image data into a Spark DataFrame.",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_image.html#spark_read_image",
    "href": "packages/sparklyr/latest/reference/spark_read_image.html#spark_read_image",
    "title": "Read image data into a Spark DataFrame.",
    "section": "spark_read_image",
    "text": "spark_read_image"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_image.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_image.html#description",
    "title": "Read image data into a Spark DataFrame.",
    "section": "Description",
    "text": "Description\nRead image files within a directory and convert each file into a record within the resulting Spark dataframe. The output will be a Spark dataframe consisting of struct types containing the following attributes:\n-origin: StringType\n-height: IntegerType\n-width: IntegerType\n-nChannels: IntegerType\n-mode: IntegerType\n-data: BinaryType"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_image.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_image.html#usage",
    "title": "Read image data into a Spark DataFrame.",
    "section": "Usage",
    "text": "Usage\nspark_read_image( \n  sc, \n  name = NULL, \n  dir = name, \n  drop_invalid = TRUE, \n  repartition = 0, \n  memory = TRUE, \n  overwrite = TRUE \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_image.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_image.html#arguments",
    "title": "Read image data into a Spark DataFrame.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\ndir\nDirectory to read binary files from.\n\n\ndrop_invalid\nWhether to drop files that are not valid images from the result (default: TRUE).\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_image.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_image.html#see-also",
    "title": "Read image data into a Spark DataFrame.",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html",
    "title": "Invoke distinct on a Spark DataFrame",
    "section": "",
    "text": "R/sdf_distinct.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#sdf_distinct",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#sdf_distinct",
    "title": "Invoke distinct on a Spark DataFrame",
    "section": "sdf_distinct",
    "text": "sdf_distinct"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#description",
    "title": "Invoke distinct on a Spark DataFrame",
    "section": "Description",
    "text": "Description\nInvoke distinct on a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#usage",
    "title": "Invoke distinct on a Spark DataFrame",
    "section": "Usage",
    "text": "Usage\nsdf_distinct(x, ..., name)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#arguments",
    "title": "Invoke distinct on a Spark DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame.\n\n\n…\nOptional variables to use when determining uniqueness. If there are multiple rows for a given combination of inputs, only the first row will be preserved. If omitted, will use all variables.\n\n\nname\nA name to assign this table. Passed to [sdf_register()]."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#section",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#section",
    "title": "Invoke distinct on a Spark DataFrame",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#transforming-spark-dataframes",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#transforming-spark-dataframes",
    "title": "Invoke distinct on a Spark DataFrame",
    "section": "Transforming Spark DataFrames",
    "text": "Transforming Spark DataFrames\nThe family of functions prefixed with sdf_ generally access the Scala Spark DataFrame API directly, as opposed to the dplyr interface which uses Spark SQL. These functions will ‘force’ any pending SQL in a dplyr pipeline, such that the resulting tbl_spark object returned will no longer have the attached ‘lazy’ SQL operations. Note that the underlying Spark DataFrame does execute its operations lazily, so that even though the pending set of operations (currently) are not exposed at the R level, these operations will only be executed when you explicitly collect() the table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#see-also",
    "title": "Invoke distinct on a Spark DataFrame",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_copy_to(), sdf_random_split(), sdf_register(), sdf_sample(), sdf_sort(), sdf_weighted_sample()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html",
    "title": "Read Delta Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#stream_read_delta",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#stream_read_delta",
    "title": "Read Delta Stream",
    "section": "stream_read_delta",
    "text": "stream_read_delta"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#description",
    "title": "Read Delta Stream",
    "section": "Description",
    "text": "Description\nReads a Delta Lake table as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#usage",
    "title": "Read Delta Stream",
    "section": "Usage",
    "text": "Usage\nstream_read_delta(sc, path, name = NULL, options = list(), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#arguments",
    "title": "Read Delta Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#details",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#details",
    "title": "Read Delta Stream",
    "section": "Details",
    "text": "Details\nPlease note that Delta Lake requires installing the appropriate package by setting the packages parameter to \"delta\" in spark_connect()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#examples",
    "title": "Read Delta Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\", version = \"2.4.0\", packages = \"delta\") \nsdf_len(sc, 5) %&gt;% spark_write_delta(path = \"delta-test\") \nstream &lt;- stream_read_delta(sc, \"delta-test\") %&gt;% \n  stream_write_json(\"json-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#see-also",
    "title": "Read Delta Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/nest.html",
    "href": "packages/sparklyr/latest/reference/nest.html",
    "title": "Nest",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/nest.html#nest",
    "href": "packages/sparklyr/latest/reference/nest.html#nest",
    "title": "Nest",
    "section": "nest",
    "text": "nest"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/nest.html#description",
    "href": "packages/sparklyr/latest/reference/nest.html#description",
    "title": "Nest",
    "section": "Description",
    "text": "Description\nSee nest for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html",
    "title": "Read CSV Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html#stream_read_csv",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html#stream_read_csv",
    "title": "Read CSV Stream",
    "section": "stream_read_csv",
    "text": "stream_read_csv"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html#description",
    "title": "Read CSV Stream",
    "section": "Description",
    "text": "Description\nReads a CSV stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html#usage",
    "title": "Read CSV Stream",
    "section": "Usage",
    "text": "Usage\nstream_read_csv( \n  sc, \n  path, \n  name = NULL, \n  header = TRUE, \n  columns = NULL, \n  delimiter = \",\", \n  quote = \"\\\"\", \n  escape = \"\\\\\", \n  charset = \"UTF-8\", \n  null_value = NULL, \n  options = list(), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html#arguments",
    "title": "Read CSV Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\nheader\nBoolean; should the first row of data be used as a header? Defaults to TRUE.\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\ndelimiter\nThe character used to delimit each column. Defaults to ','.\n\n\nquote\nThe character used as a quote. Defaults to '\"'.\n\n\nescape\nThe character used to escape other characters. Defaults to '\\'.\n\n\ncharset\nThe character set. Defaults to \"UTF-8\".\n\n\nnull_value\nThe character to use for null, or missing, values. Defaults to NULL.\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html#examples",
    "title": "Read CSV Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\") \ndir.create(\"csv-in\") \nwrite.csv(iris, \"csv-in/data.csv\", row.names = FALSE) \ncsv_path &lt;- file.path(\"file://\", getwd(), \"csv-in\") \nstream &lt;- stream_read_csv(sc, csv_path) %&gt;% stream_write_csv(\"csv-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html#see-also",
    "title": "Read CSV Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html",
    "href": "packages/sparklyr/latest/reference/ml_als.html",
    "title": "Spark ML – ALS",
    "section": "",
    "text": "R/ml_recommendation_als.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#ml_als",
    "href": "packages/sparklyr/latest/reference/ml_als.html#ml_als",
    "title": "Spark ML – ALS",
    "section": "ml_als",
    "text": "ml_als"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#description",
    "href": "packages/sparklyr/latest/reference/ml_als.html#description",
    "title": "Spark ML – ALS",
    "section": "Description",
    "text": "Description\nPerform recommendation using Alternating Least Squares (ALS) matrix factorization."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_als.html#usage",
    "title": "Spark ML – ALS",
    "section": "Usage",
    "text": "Usage\n \nml_als( \n  x, \n  formula = NULL, \n  rating_col = \"rating\", \n  user_col = \"user\", \n  item_col = \"item\", \n  rank = 10, \n  reg_param = 0.1, \n  implicit_prefs = FALSE, \n  alpha = 1, \n  nonnegative = FALSE, \n  max_iter = 10, \n  num_user_blocks = 10, \n  num_item_blocks = 10, \n  checkpoint_interval = 10, \n  cold_start_strategy = \"nan\", \n  intermediate_storage_level = \"MEMORY_AND_DISK\", \n  final_storage_level = \"MEMORY_AND_DISK\", \n  uid = random_string(\"als_\"), \n  ... \n) \n \nml_recommend(model, type = c(\"items\", \"users\"), n = 1)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_als.html#arguments",
    "title": "Spark ML – ALS",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details. The ALS model requires a specific formula format, please use rating_col ~ user_col + item_col.\n\n\nrating_col\nColumn name for ratings. Default: “rating”\n\n\nuser_col\nColumn name for user ids. Ids must be integers. Other numeric types are supported for this column, but will be cast to integers as long as they fall within the integer value range. Default: “user”\n\n\nitem_col\nColumn name for item ids. Ids must be integers. Other numeric types are supported for this column, but will be cast to integers as long as they fall within the integer value range. Default: “item”\n\n\nrank\nRank of the matrix factorization (positive). Default: 10\n\n\nreg_param\nRegularization parameter.\n\n\nimplicit_prefs\nWhether to use implicit preference. Default: FALSE.\n\n\nalpha\nAlpha parameter in the implicit preference formulation (nonnegative).\n\n\nnonnegative\nWhether to apply nonnegativity constraints. Default: FALSE.\n\n\nmax_iter\nMaximum number of iterations.\n\n\nnum_user_blocks\nNumber of user blocks (positive). Default: 10\n\n\nnum_item_blocks\nNumber of item blocks (positive). Default: 10\n\n\ncheckpoint_interval\nSet checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.\n\n\ncold_start_strategy\n(Spark 2.2.0+) Strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: - “nan”: predicted value for unknown ids will be NaN. - “drop”: rows in the input DataFrame containing unknown ids will be dropped from the output DataFrame containing predictions. Default: “nan”.\n\n\nintermediate_storage_level\n(Spark 2.0.0+) StorageLevel for intermediate datasets. Pass in a string representation of StorageLevel. Cannot be “NONE”. Default: “MEMORY_AND_DISK”.\n\n\nfinal_storage_level\n(Spark 2.0.0+) StorageLevel for ALS model factors. Pass in a string representation of StorageLevel. Default: “MEMORY_AND_DISK”.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; currently unused.\n\n\nmodel\nAn ALS model object\n\n\ntype\nWhat to recommend, one of items or users\n\n\nn\nMaximum number of recommendations to return"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#details",
    "href": "packages/sparklyr/latest/reference/ml_als.html#details",
    "title": "Spark ML – ALS",
    "section": "Details",
    "text": "Details\nml_recommend() returns the top n users/items recommended for each item/user, for all items/users. The output has been transformed (exploded and separated) from the default Spark outputs to be more user friendly."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#value",
    "href": "packages/sparklyr/latest/reference/ml_als.html#value",
    "title": "Spark ML – ALS",
    "section": "Value",
    "text": "Value\nALS attempts to estimate the ratings matrix R as the product of two lower-rank matrices, X and Y, i.e. X * Yt = R. Typically these approximations are called ‘factor’ matrices. The general approach is iterative. During each iteration, one of the factor matrices is held constant, while the other is solved for using least squares. The newly-solved factor matrix is then held constant while solving for the other factor matrix. This is a blocked implementation of the ALS factorization algorithm that groups the two sets of factors (referred to as “users” and “products”) into blocks and reduces communication by only sending one copy of each user vector to each product block on each iteration, and only for the product blocks that need that user’s feature vector. This is achieved by pre-computing some information about the ratings matrix to determine the “out-links” of each user (which blocks of products it will contribute to) and “in-link” information for each product (which of the feature vectors it receives from each user block it will depend on). This allows us to send only an array of feature vectors between each user block and product block, and have the product block find the users’ ratings and update the products based on these messages. For implicit preference data, the algorithm used is based on “Collaborative Filtering for Implicit Feedback Datasets”, available at 10.1109/ICDM.2008.22, adapted for the blocked approach used here. Essentially instead of finding the low-rank approximations to the rating matrix R, this finds the approximations for a preference matrix P where the elements of P are 1 if r is greater than 0 and 0 if r is less than or equal to 0. The ratings then act as ‘confidence’ values related to strength of indicated user preferences rather than explicit ratings given to items. The object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_als recommender object, which is an Estimator.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the recommender appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a recommender estimator is constructed then immediately fit with the input tbl_spark, returning a recommendation model, i.e. ml_als_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_als.html#examples",
    "title": "Spark ML – ALS",
    "section": "Examples",
    "text": "Examples\n\n \n \nlibrary(sparklyr) \nsc &lt;- spark_connect(master = \"local\") \n \nmovies &lt;- data.frame( \n  user   = c(1, 2, 0, 1, 2, 0), \n  item   = c(1, 1, 1, 2, 2, 0), \n  rating = c(3, 1, 2, 4, 5, 4) \n) \nmovies_tbl &lt;- sdf_copy_to(sc, movies) \n \nmodel &lt;- ml_als(movies_tbl, rating ~ user + item) \n \nml_predict(model, movies_tbl) \n#&gt; # Source: spark&lt;?&gt; [?? x 6]\n#&gt;    user  item rating features  label prediction\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;list&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1     1     1      3 &lt;dbl [2]&gt;     3       2.80\n#&gt; 2     2     1      1 &lt;dbl [2]&gt;     1       1.08\n#&gt; 3     0     1      2 &lt;dbl [2]&gt;     2       2.00\n#&gt; 4     1     2      4 &lt;dbl [2]&gt;     4       3.98\n#&gt; 5     2     2      5 &lt;dbl [2]&gt;     5       4.86\n#&gt; 6     0     0      4 &lt;dbl [2]&gt;     4       3.88\n \nml_recommend(model, type = \"item\", 1) \n#&gt; # Source: spark&lt;?&gt; [?? x 4]\n#&gt;    user recommendations   item rating\n#&gt;   &lt;int&gt; &lt;list&gt;           &lt;int&gt;  &lt;dbl&gt;\n#&gt; 1     1 &lt;named list [2]&gt;     2   3.98\n#&gt; 2     2 &lt;named list [2]&gt;     2   4.86\n#&gt; 3     0 &lt;named list [2]&gt;     0   3.88"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_value.html",
    "href": "packages/sparklyr/latest/reference/spark_config_value.html",
    "title": "A helper function to retrieve values from spark_config()",
    "section": "",
    "text": "R/core_config.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_value.html#spark_config_value",
    "href": "packages/sparklyr/latest/reference/spark_config_value.html#spark_config_value",
    "title": "A helper function to retrieve values from spark_config()",
    "section": "spark_config_value",
    "text": "spark_config_value"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_value.html#description",
    "href": "packages/sparklyr/latest/reference/spark_config_value.html#description",
    "title": "A helper function to retrieve values from spark_config()",
    "section": "Description",
    "text": "Description\nA helper function to retrieve values from spark_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_value.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_config_value.html#usage",
    "title": "A helper function to retrieve values from spark_config()",
    "section": "Usage",
    "text": "Usage\nspark_config_value(config, name, default = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_value.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_config_value.html#arguments",
    "title": "A helper function to retrieve values from spark_config()",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nconfig\nThe configuration list from spark_config()\n\n\nname\nThe name of the configuration entry\n\n\ndefault\nThe default value to use when entry is not present"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/filter.html",
    "href": "packages/sparklyr/latest/reference/filter.html",
    "title": "Filter",
    "section": "",
    "text": "R/reexports.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/filter.html#filter",
    "href": "packages/sparklyr/latest/reference/filter.html#filter",
    "title": "Filter",
    "section": "filter",
    "text": "filter"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/filter.html#description",
    "href": "packages/sparklyr/latest/reference/filter.html#description",
    "title": "Filter",
    "section": "Description",
    "text": "Description\nSee filter for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html",
    "title": "Bind multiple Spark DataFrames by row and column",
    "section": "",
    "text": "R/mutation.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html#sdf_bind",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html#sdf_bind",
    "title": "Bind multiple Spark DataFrames by row and column",
    "section": "sdf_bind",
    "text": "sdf_bind"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html#description",
    "title": "Bind multiple Spark DataFrames by row and column",
    "section": "Description",
    "text": "Description\nsdf_bind_rows() and sdf_bind_cols() are implementation of the common pattern of do.call(rbind, sdfs) or do.call(cbind, sdfs) for binding many Spark DataFrames into one."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html#usage",
    "title": "Bind multiple Spark DataFrames by row and column",
    "section": "Usage",
    "text": "Usage\nsdf_bind_rows(..., id = NULL) \n\nsdf_bind_cols(...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html#arguments",
    "title": "Bind multiple Spark DataFrames by row and column",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\n…\nSpark tbls to combine.  Each argument can either be a Spark DataFrame or a list of Spark DataFrames  When row-binding, columns are matched by name, and any missing columns with be filled with NA.  When column-binding, rows are matched by position, so all data frames must have the same number of rows.\n\n\nid\nData frame identifier.  When id is supplied, a new column of identifiers is created to link each row to its original Spark DataFrame. The labels are taken from the named arguments to sdf_bind_rows(). When a list of Spark DataFrames is supplied, the labels are taken from the names of the list. If no names are found a numeric sequence is used instead."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html#details",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html#details",
    "title": "Bind multiple Spark DataFrames by row and column",
    "section": "Details",
    "text": "Details\nThe output of sdf_bind_rows() will contain a column if that column appears in any of the inputs."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html#value",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html#value",
    "title": "Bind multiple Spark DataFrames by row and column",
    "section": "Value",
    "text": "Value\nsdf_bind_rows() and sdf_bind_cols() return tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_table.html",
    "href": "packages/sparklyr/latest/reference/spark_read_table.html",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "",
    "text": "R/data_interface.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_table.html#spark_read_table",
    "href": "packages/sparklyr/latest/reference/spark_read_table.html#spark_read_table",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "spark_read_table",
    "text": "spark_read_table"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_table.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_table.html#description",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "Description",
    "text": "Description\nReads from a Spark Table into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_table.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_table.html#usage",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "Usage",
    "text": "Usage\nspark_read_table( \n  sc, \n  name, \n  options = list(), \n  repartition = 0, \n  memory = TRUE, \n  columns = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_table.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_table.html#arguments",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\noptions\nA list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_table.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_table.html#see-also",
    "title": "Reads from a Spark Table into a Spark DataFrame.",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_insert_table(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke.html",
    "href": "packages/sparklyr/latest/reference/j_invoke.html",
    "title": "Invoke a Java function.",
    "section": "",
    "text": "R/spark_invoke.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke.html#j_invoke",
    "href": "packages/sparklyr/latest/reference/j_invoke.html#j_invoke",
    "title": "Invoke a Java function.",
    "section": "j_invoke",
    "text": "j_invoke"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke.html#description",
    "href": "packages/sparklyr/latest/reference/j_invoke.html#description",
    "title": "Invoke a Java function.",
    "section": "Description",
    "text": "Description\nInvoke a Java function and force return value of the call to be retrieved as a Java object reference."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke.html#usage",
    "href": "packages/sparklyr/latest/reference/j_invoke.html#usage",
    "title": "Invoke a Java function.",
    "section": "Usage",
    "text": "Usage\nj_invoke(jobj, method, ...) \n\nj_invoke_static(sc, class, method, ...) \n\nj_invoke_new(sc, class, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke.html#arguments",
    "href": "packages/sparklyr/latest/reference/j_invoke.html#arguments",
    "title": "Invoke a Java function.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\njobj\nAn R object acting as a Java object reference (typically, a spark_jobj).\n\n\nmethod\nThe name of the method to be invoked.\n\n\n…\nOptional arguments, currently unused.\n\n\nsc\nA spark_connection.\n\n\nclass\nThe name of the Java class whose methods should be invoked."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "",
    "text": "NULL"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#ml_multilayer_perceptron_classifier",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#ml_multilayer_perceptron_classifier",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "ml_multilayer_perceptron_classifier",
    "text": "ml_multilayer_perceptron_classifier"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#description",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#description",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "Description",
    "text": "Description\nClassification model based on the Multilayer Perceptron. Each layer has sigmoid activation function, output layer has softmax."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#usage",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "Usage",
    "text": "Usage\n \nml_multilayer_perceptron_classifier( \n  x, \n  formula = NULL, \n  layers = NULL, \n  max_iter = 100, \n  step_size = 0.03, \n  tol = 1e-06, \n  block_size = 128, \n  solver = \"l-bfgs\", \n  seed = NULL, \n  initial_weights = NULL, \n  thresholds = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  uid = random_string(\"multilayer_perceptron_classifier_\"), \n  ... \n) \n \nml_multilayer_perceptron( \n  x, \n  formula = NULL, \n  layers, \n  max_iter = 100, \n  step_size = 0.03, \n  tol = 1e-06, \n  block_size = 128, \n  solver = \"l-bfgs\", \n  seed = NULL, \n  initial_weights = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  thresholds = NULL, \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  uid = random_string(\"multilayer_perceptron_classifier_\"), \n  response = NULL, \n  features = NULL, \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#arguments",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nlayers\nA numeric vector describing the layers – each element in the vector gives the size of a layer. For example, c(4, 5, 2) would imply three layers, with an input (feature) layer of size 4, an intermediate layer of size 5, and an output (class) layer of size 2.\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nstep_size\nStep size to be used for each iteration of optimization (&gt; 0).\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nblock_size\nBlock size for stacking input data in matrices to speed up the computation. Data is stacked within partitions. If block size is more than remaining data in a partition then it is adjusted to the size of this data. Recommended size is between 10 and 1000. Default: 128\n\n\nsolver\nThe solver algorithm for optimization. Supported options: “gd” (minibatch gradient descent) or “l-bfgs”. Default: “l-bfgs”\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\ninitial_weights\nThe initial weights of the model.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details.\n\n\nresponse\n(Deprecated) The name of the response column (as a length-one character vector.)\n\n\nfeatures\n(Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#details",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#details",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows. ml_multilayer_perceptron() is an alias for ml_multilayer_perceptron_classifier() for backwards compatibility."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#value",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#value",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#examples",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n \nsc &lt;- spark_connect(master = \"local\") \n \niris_tbl &lt;- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \npartitions &lt;- iris_tbl %&gt;% \n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111) \n \niris_training &lt;- partitions$training \niris_test &lt;- partitions$test \n \nmlp_model &lt;- iris_training %&gt;% \n  ml_multilayer_perceptron_classifier(Species ~ ., layers = c(4, 3, 3)) \n \npred &lt;- ml_predict(mlp_model, iris_test) \n \nml_multiclass_classification_evaluator(pred) \n#&gt; [1] 0.5227273"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#see-also",
    "title": "Spark ML – Multilayer Perceptron",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms. Other ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_class.html",
    "href": "packages/sparklyr/latest/reference/jobj_class.html",
    "title": "Superclasses of object",
    "section": "",
    "text": "R/spark_utils.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_class.html#jobj_class",
    "href": "packages/sparklyr/latest/reference/jobj_class.html#jobj_class",
    "title": "Superclasses of object",
    "section": "jobj_class",
    "text": "jobj_class"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_class.html#description",
    "href": "packages/sparklyr/latest/reference/jobj_class.html#description",
    "title": "Superclasses of object",
    "section": "Description",
    "text": "Description\nExtract the classes that a Java object inherits from. This is the jobj equivalent of class()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_class.html#usage",
    "href": "packages/sparklyr/latest/reference/jobj_class.html#usage",
    "title": "Superclasses of object",
    "section": "Usage",
    "text": "Usage\njobj_class(jobj, simple_name = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_class.html#arguments",
    "href": "packages/sparklyr/latest/reference/jobj_class.html#arguments",
    "title": "Superclasses of object",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\njobj\nA spark_jobj\n\n\nsimple_name\nWhether to return simple names, defaults to TRUE"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html",
    "title": "Read Parquet Stream",
    "section": "",
    "text": "R/stream_data.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html#stream_read_parquet",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html#stream_read_parquet",
    "title": "Read Parquet Stream",
    "section": "stream_read_parquet",
    "text": "stream_read_parquet"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html#description",
    "title": "Read Parquet Stream",
    "section": "Description",
    "text": "Description\nReads a parquet stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html#usage",
    "title": "Read Parquet Stream",
    "section": "Usage",
    "text": "Usage\nstream_read_parquet( \n  sc, \n  path, \n  name = NULL, \n  columns = NULL, \n  options = list(), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html#arguments",
    "title": "Read Parquet Stream",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\" and \"file://\" protocols.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType.\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html#examples",
    "title": "Read Parquet Stream",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\") \nsdf_len(sc, 10) %&gt;% spark_write_parquet(\"parquet-in\") \nstream &lt;- stream_read_parquet(sc, \"parquet-in\") %&gt;% stream_write_parquet(\"parquet-out\") \nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html#see-also",
    "title": "Read Parquet Stream",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rlnorm.html",
    "href": "packages/sparklyr/latest/reference/sdf_rlnorm.html",
    "title": "Generate random samples from a log normal distribution",
    "section": "",
    "text": "R/sdf_stat.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rlnorm.html#sdf_rlnorm",
    "href": "packages/sparklyr/latest/reference/sdf_rlnorm.html#sdf_rlnorm",
    "title": "Generate random samples from a log normal distribution",
    "section": "sdf_rlnorm",
    "text": "sdf_rlnorm"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rlnorm.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rlnorm.html#description",
    "title": "Generate random samples from a log normal distribution",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a log normal distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rlnorm.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rlnorm.html#usage",
    "title": "Generate random samples from a log normal distribution",
    "section": "Usage",
    "text": "Usage\nsdf_rlnorm( \n  sc, \n  n, \n  meanlog = 0, \n  sdlog = 1, \n  num_partitions = NULL, \n  seed = NULL, \n  output_col = \"x\" \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rlnorm.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rlnorm.html#arguments",
    "title": "Generate random samples from a log normal distribution",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nmeanlog\nThe mean of the normally distributed natural logarithm of this distribution.\n\n\nsdlog\nThe Standard deviation of the normally distributed natural logarithm of this distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rlnorm.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rlnorm.html#see-also",
    "title": "Generate random samples from a log normal distribution",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html",
    "title": "Feature Transformation – OneHotEncoder (Transformer)",
    "section": "",
    "text": "R/ml_feature_one_hot_encoder.R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#ft_one_hot_encoder",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#ft_one_hot_encoder",
    "title": "Feature Transformation – OneHotEncoder (Transformer)",
    "section": "ft_one_hot_encoder",
    "text": "ft_one_hot_encoder"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#description",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#description",
    "title": "Feature Transformation – OneHotEncoder (Transformer)",
    "section": "Description",
    "text": "Description\nOne-hot encoding maps a column of label indices to a column of binary vectors, with at most a single one-value. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features. Typically, used with ft_string_indexer() to index a column first."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#usage",
    "title": "Feature Transformation – OneHotEncoder (Transformer)",
    "section": "Usage",
    "text": "Usage\nft_one_hot_encoder( \n  x, \n  input_cols = NULL, \n  output_cols = NULL, \n  handle_invalid = NULL, \n  drop_last = TRUE, \n  uid = random_string(\"one_hot_encoder_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#arguments",
    "title": "Feature Transformation – OneHotEncoder (Transformer)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_cols\nThe name of the input columns.\n\n\noutput_cols\nThe name of the output columns.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error”\n\n\ndrop_last\nWhether to drop the last category. Defaults to TRUE.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#value",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#value",
    "title": "Feature Transformation – OneHotEncoder (Transformer)",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#see-also",
    "title": "Feature Transformation – OneHotEncoder (Transformer)",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparktf/latest/news.html",
    "href": "packages/sparktf/latest/news.html",
    "title": "sparktf 0.1.0",
    "section": "",
    "text": "sparktf 0.1.0\nsparkdf is a sparklyr extension for reading and writing TensorFlow TFRecord files via Apache Spark.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_read_tfrecord.html",
    "href": "packages/sparktf/latest/reference/spark_read_tfrecord.html",
    "title": "Read a TFRecord File",
    "section": "",
    "text": "R/loader.R"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_read_tfrecord.html#spark_read_tfrecord",
    "href": "packages/sparktf/latest/reference/spark_read_tfrecord.html#spark_read_tfrecord",
    "title": "Read a TFRecord File",
    "section": "spark_read_tfrecord",
    "text": "spark_read_tfrecord"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_read_tfrecord.html#description",
    "href": "packages/sparktf/latest/reference/spark_read_tfrecord.html#description",
    "title": "Read a TFRecord File",
    "section": "Description",
    "text": "Description\nRead a TFRecord file as a Spark DataFrame."
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_read_tfrecord.html#usage",
    "href": "packages/sparktf/latest/reference/spark_read_tfrecord.html#usage",
    "title": "Read a TFRecord File",
    "section": "Usage",
    "text": "Usage\nspark_read_tfrecord(sc, name, path, schema = NULL, \n  record_type = c(\"Example\", \"SequenceExample\"), overwrite = TRUE)"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_read_tfrecord.html#arguments",
    "href": "packages/sparktf/latest/reference/spark_read_tfrecord.html#arguments",
    "title": "Read a TFRecord File",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA spark conneciton.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://”, “s3a://” and “file://” protocols.\n\n\nschema\n(Currently unsupported.) Schema of TensorFlow records. If not provided, the schema is inferred from TensorFlow records.\n\n\nrecord_type\nInput format of TensorFlow records. By default it is Example.\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_write_tfrecord.html",
    "href": "packages/sparktf/latest/reference/spark_write_tfrecord.html",
    "title": "Write a Spark DataFrame to a TFRecord file",
    "section": "",
    "text": "R/writer.R"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_write_tfrecord.html#spark_write_tfrecord",
    "href": "packages/sparktf/latest/reference/spark_write_tfrecord.html#spark_write_tfrecord",
    "title": "Write a Spark DataFrame to a TFRecord file",
    "section": "spark_write_tfrecord",
    "text": "spark_write_tfrecord"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_write_tfrecord.html#description",
    "href": "packages/sparktf/latest/reference/spark_write_tfrecord.html#description",
    "title": "Write a Spark DataFrame to a TFRecord file",
    "section": "Description",
    "text": "Description\nSerialize a Spark DataFrame to the TensorFlow TFRecord format for training or inference."
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_write_tfrecord.html#usage",
    "href": "packages/sparktf/latest/reference/spark_write_tfrecord.html#usage",
    "title": "Write a Spark DataFrame to a TFRecord file",
    "section": "Usage",
    "text": "Usage\nspark_write_tfrecord(x, path, record_type = c(\"Example\", \n  \"SequenceExample\"), write_locality = c(\"distributed\", \"local\"), \n  mode = NULL)"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_write_tfrecord.html#arguments",
    "href": "packages/sparktf/latest/reference/spark_write_tfrecord.html#arguments",
    "title": "Write a Spark DataFrame to a TFRecord file",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA Spark DataFrame\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://”, “s3a://”, and “file://” protocols.\n\n\nrecord_type\nOutput format of TensorFlow records. One of \"Example\" and \"SequenceExample\".\n\n\nwrite_locality\nDetermines whether the TensorFlow records are written locally on the workers or on a distributed file system. One of \"distributed\" and \"local\". See Details for more information.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ‘ignore’. Notice that ‘overwrite’ will also change the column structure.  For more details see also http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark."
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_write_tfrecord.html#details",
    "href": "packages/sparktf/latest/reference/spark_write_tfrecord.html#details",
    "title": "Write a Spark DataFrame to a TFRecord file",
    "section": "Details",
    "text": "Details\nFor write_locality = local, each of the workers stores on the local disk a subset of the data. The subset that is stored on each worker is determined by the partitioning of the DataFrame. Each of the partitions is coalesced into a single TFRecord file and written on the node where the partition lives. This is useful in the context of distributed training, in which each of the workers gets a subset of the data to work on. When this mode is activated, the path provided to the writer is interpreted as a base path that is created on each of the worker nodes, and that will be populated with data from the DataFrame."
  },
  {
    "objectID": "packages/graphframes/latest/news.html",
    "href": "packages/graphframes/latest/news.html",
    "title": "graphframes 0.1.2",
    "section": "",
    "text": "graphframes 0.1.2\n\nUpdated dependency to graphframes 0.6.0, with support for Spark 2.3.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertex_columns.html",
    "href": "packages/graphframes/latest/reference/gf_vertex_columns.html",
    "title": "Vertices column names",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertex_columns.html#gf_vertex_columns",
    "href": "packages/graphframes/latest/reference/gf_vertex_columns.html#gf_vertex_columns",
    "title": "Vertices column names",
    "section": "gf_vertex_columns",
    "text": "gf_vertex_columns"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertex_columns.html#description",
    "href": "packages/graphframes/latest/reference/gf_vertex_columns.html#description",
    "title": "Vertices column names",
    "section": "Description",
    "text": "Description\nVertices column names"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertex_columns.html#usage",
    "href": "packages/graphframes/latest/reference/gf_vertex_columns.html#usage",
    "title": "Vertices column names",
    "section": "Usage",
    "text": "Usage\ngf_vertex_columns(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertex_columns.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_vertex_columns.html#arguments",
    "title": "Vertices column names",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_register.html",
    "href": "packages/graphframes/latest/reference/gf_register.html",
    "title": "Register a GraphFrame object",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_register.html#gf_register",
    "href": "packages/graphframes/latest/reference/gf_register.html#gf_register",
    "title": "Register a GraphFrame object",
    "section": "gf_register",
    "text": "gf_register"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_register.html#description",
    "href": "packages/graphframes/latest/reference/gf_register.html#description",
    "title": "Register a GraphFrame object",
    "section": "Description",
    "text": "Description\nRegister a GraphFrame object"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_register.html#usage",
    "href": "packages/graphframes/latest/reference/gf_register.html#usage",
    "title": "Register a GraphFrame object",
    "section": "Usage",
    "text": "Usage\ngf_register(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_register.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_register.html#arguments",
    "title": "Register a GraphFrame object",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_in_degrees.html",
    "href": "packages/graphframes/latest/reference/gf_in_degrees.html",
    "title": "In-degrees of vertices",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_in_degrees.html#gf_in_degrees",
    "href": "packages/graphframes/latest/reference/gf_in_degrees.html#gf_in_degrees",
    "title": "In-degrees of vertices",
    "section": "gf_in_degrees",
    "text": "gf_in_degrees"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_in_degrees.html#description",
    "href": "packages/graphframes/latest/reference/gf_in_degrees.html#description",
    "title": "In-degrees of vertices",
    "section": "Description",
    "text": "Description\nIn-degrees of vertices"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_in_degrees.html#usage",
    "href": "packages/graphframes/latest/reference/gf_in_degrees.html#usage",
    "title": "In-degrees of vertices",
    "section": "Usage",
    "text": "Usage\ngf_in_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_in_degrees.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_in_degrees.html#arguments",
    "title": "In-degrees of vertices",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertices.html",
    "href": "packages/graphframes/latest/reference/gf_vertices.html",
    "title": "Extract vertices DataFrame",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertices.html#gf_vertices",
    "href": "packages/graphframes/latest/reference/gf_vertices.html#gf_vertices",
    "title": "Extract vertices DataFrame",
    "section": "gf_vertices",
    "text": "gf_vertices"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertices.html#description",
    "href": "packages/graphframes/latest/reference/gf_vertices.html#description",
    "title": "Extract vertices DataFrame",
    "section": "Description",
    "text": "Description\nExtract vertices DataFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertices.html#usage",
    "href": "packages/graphframes/latest/reference/gf_vertices.html#usage",
    "title": "Extract vertices DataFrame",
    "section": "Usage",
    "text": "Usage\ngf_vertices(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertices.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_vertices.html#arguments",
    "title": "Extract vertices DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_out_degrees.html",
    "href": "packages/graphframes/latest/reference/gf_out_degrees.html",
    "title": "Out-degrees of vertices",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_out_degrees.html#gf_out_degrees",
    "href": "packages/graphframes/latest/reference/gf_out_degrees.html#gf_out_degrees",
    "title": "Out-degrees of vertices",
    "section": "gf_out_degrees",
    "text": "gf_out_degrees"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_out_degrees.html#description",
    "href": "packages/graphframes/latest/reference/gf_out_degrees.html#description",
    "title": "Out-degrees of vertices",
    "section": "Description",
    "text": "Description\nOut-degrees of vertices"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_out_degrees.html#usage",
    "href": "packages/graphframes/latest/reference/gf_out_degrees.html#usage",
    "title": "Out-degrees of vertices",
    "section": "Usage",
    "text": "Usage\ngf_out_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_out_degrees.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_out_degrees.html#arguments",
    "title": "Out-degrees of vertices",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/index.html",
    "href": "packages/graphframes/latest/reference/index.html",
    "title": "graphframes",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ngf_bfs()\nBreadth-first search (BFS)\n\n\ngf_cache()\nCache the GraphFrame\n\n\ngf_chain()\nChain graph\n\n\ngf_connected_components()\nConnected components\n\n\ngf_degrees()\nDegrees of vertices\n\n\ngf_edge_columns()\nEdges column names\n\n\ngf_edges()\nExtract edges DataFrame\n\n\ngf_find()\nMotif finding: Searching the graph for structural patterns\n\n\ngf_friends()\nGraph of friends in a social network.\n\n\ngf_graphframe()\nCreate a new GraphFrame\n\n\ngf_grid_ising_model()\nGenerate a grid Ising model with random parameters\n\n\ngf_in_degrees()\nIn-degrees of vertices\n\n\ngf_lpa()\nLabel propagation algorithm (LPA)\n\n\ngf_out_degrees()\nOut-degrees of vertices\n\n\ngf_pagerank()\nPageRank\n\n\ngf_persist()\nPersist the GraphFrame\n\n\ngf_register()\nRegister a GraphFrame object\n\n\ngf_scc()\nStrongly connected components\n\n\ngf_shortest_paths()\nShortest paths\n\n\ngf_star()\nGenerate a star graph\n\n\ngf_triangle_count()\nComputes the number of triangles passing through each vertex.\n\n\ngf_triplets()\nTriplets of graph\n\n\ngf_two_blobs()\nGenerate two blobs\n\n\ngf_unpersist()\nUnpersist the GraphFrame\n\n\ngf_vertex_columns()\nVertices column names\n\n\ngf_vertices()\nExtract vertices DataFrame\n\n\nspark_graphframe() spark_graphframe()\nRetrieve a GraphFrame",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html",
    "title": "Generate a grid Ising model with random parameters",
    "section": "",
    "text": "R/gf_examples.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#gf_grid_ising_model",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#gf_grid_ising_model",
    "title": "Generate a grid Ising model with random parameters",
    "section": "gf_grid_ising_model",
    "text": "gf_grid_ising_model"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#description",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#description",
    "title": "Generate a grid Ising model with random parameters",
    "section": "Description",
    "text": "Description\nGenerate a grid Ising model with random parameters"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#usage",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#usage",
    "title": "Generate a grid Ising model with random parameters",
    "section": "Usage",
    "text": "Usage\ngf_grid_ising_model(sc, n, v_std = 1, e_std = 1)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#arguments",
    "title": "Generate a grid Ising model with random parameters",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nLength of one side of the grid. The grid will be of size n x n.\n\n\nv_std\nStandard deviation of normal distribution used to generate vertex factors “a”. Default of 1.0.\n\n\ne_std\nStandard deviation of normal distribution used to generate edge factors “b”. Default of 1.0."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#details",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#details",
    "title": "Generate a grid Ising model with random parameters",
    "section": "Details",
    "text": "Details\nThis method generates a grid Ising model with random parameters. Ising models are probabilistic graphical models over binary variables xi. Each binary variable xi corresponds to one vertex, and it may take values -1 or +1. The probability distribution P(X) (over all xi) is parameterized by vertex factors ai and edge factors bij:\n\\(P(X) = (1/Z) * exp[ \\sum_i a_i x_i + \\sum_{ij} b_{ij} x_i x_j ]\\)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#value",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#value",
    "title": "Generate a grid Ising model with random parameters",
    "section": "Value",
    "text": "Value\nGraphFrame. Vertices have columns “id” and “a”. Edges have columns “src”, “dst”, and “b”. Edges are directed, but they should be treated as undirected in any algorithms run on this model. Vertex IDs are of the form “i,j”. E.g., vertex “1,3” is in the second row and fourth column of the grid."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#examples",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#examples",
    "title": "Generate a grid Ising model with random parameters",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ngf_grid_ising_model(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html",
    "href": "packages/graphframes/latest/reference/gf_find.html",
    "title": "Motif finding: Searching the graph for structural patterns",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#gf_find",
    "href": "packages/graphframes/latest/reference/gf_find.html#gf_find",
    "title": "Motif finding: Searching the graph for structural patterns",
    "section": "gf_find",
    "text": "gf_find"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#description",
    "href": "packages/graphframes/latest/reference/gf_find.html#description",
    "title": "Motif finding: Searching the graph for structural patterns",
    "section": "Description",
    "text": "Description\nMotif finding uses a simple Domain-Specific Language (DSL) for expressing structural queries. For example, gf_find(g, “(a)-[e]-&gt;(b); (b)-[e2]-&gt;(a)”) will search for pairs of vertices a,b connected by edges in both directions. It will return a DataFrame of all such structures in the graph, with columns for each of the named elements (vertices or edges) in the motif. In this case, the returned columns will be in order of the pattern: “a, e, b, e2.”"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#usage",
    "href": "packages/graphframes/latest/reference/gf_find.html#usage",
    "title": "Motif finding: Searching the graph for structural patterns",
    "section": "Usage",
    "text": "Usage\ngf_find(x, pattern)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_find.html#arguments",
    "title": "Motif finding: Searching the graph for structural patterns",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\npattern\npattern specifying a motif to search for"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#examples",
    "href": "packages/graphframes/latest/reference/gf_find.html#examples",
    "title": "Motif finding: Searching the graph for structural patterns",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ngf_friends(sc) %&gt;% \n  gf_find(\"(a)-[e]-&gt;(b); (b)-[e2]-&gt;(a)\")"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edges.html",
    "href": "packages/graphframes/latest/reference/gf_edges.html",
    "title": "Extract edges DataFrame",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edges.html#gf_edges",
    "href": "packages/graphframes/latest/reference/gf_edges.html#gf_edges",
    "title": "Extract edges DataFrame",
    "section": "gf_edges",
    "text": "gf_edges"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edges.html#description",
    "href": "packages/graphframes/latest/reference/gf_edges.html#description",
    "title": "Extract edges DataFrame",
    "section": "Description",
    "text": "Description\nExtract edges DataFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edges.html#usage",
    "href": "packages/graphframes/latest/reference/gf_edges.html#usage",
    "title": "Extract edges DataFrame",
    "section": "Usage",
    "text": "Usage\ngf_edges(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edges.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_edges.html#arguments",
    "title": "Extract edges DataFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html",
    "href": "packages/graphframes/latest/reference/gf_bfs.html",
    "title": "Breadth-first search (BFS)",
    "section": "",
    "text": "R/gf_bfs.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#gf_bfs",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#gf_bfs",
    "title": "Breadth-first search (BFS)",
    "section": "gf_bfs",
    "text": "gf_bfs"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#description",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#description",
    "title": "Breadth-first search (BFS)",
    "section": "Description",
    "text": "Description\nBreadth-first search (BFS)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#usage",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#usage",
    "title": "Breadth-first search (BFS)",
    "section": "Usage",
    "text": "Usage\ngf_bfs(x, from_expr, to_expr, max_path_length = 10, edge_filter = NULL, \n  ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#arguments",
    "title": "Breadth-first search (BFS)",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\nfrom_expr\nSpark SQL expression specifying valid starting vertices for the BFS.\n\n\nto_expr\nSpark SQL expression specifying valid target vertices for the BFS.\n\n\nmax_path_length\nLimit on the length of paths.\n\n\nedge_filter\nSpark SQL expression specifying edges which may be used in the search.\n\n\n…\nOptional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#examples",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#examples",
    "title": "Breadth-first search (BFS)",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ng &lt;- gf_friends(sc) \ngf_bfs(g, from_expr = \"name = 'Esther'\", to_expr = \"age &lt; 32\")"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html",
    "title": "Computes the number of triangles passing through each vertex.",
    "section": "",
    "text": "R/gf_triangle_count.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#gf_triangle_count",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#gf_triangle_count",
    "title": "Computes the number of triangles passing through each vertex.",
    "section": "gf_triangle_count",
    "text": "gf_triangle_count"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#description",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#description",
    "title": "Computes the number of triangles passing through each vertex.",
    "section": "Description",
    "text": "Description\nThis algorithm ignores edge direction; i.e., all edges are treated as undirected. In a multigraph, duplicate edges will be counted only once."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#usage",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#usage",
    "title": "Computes the number of triangles passing through each vertex.",
    "section": "Usage",
    "text": "Usage\ngf_triangle_count(x, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#arguments",
    "title": "Computes the number of triangles passing through each vertex.",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\n…\nOptional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#examples",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#examples",
    "title": "Computes the number of triangles passing through each vertex.",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ng &lt;- gf_friends(sc) \ngf_triangle_count(g)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_cache.html",
    "href": "packages/graphframes/latest/reference/gf_cache.html",
    "title": "Cache the GraphFrame",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_cache.html#gf_cache",
    "href": "packages/graphframes/latest/reference/gf_cache.html#gf_cache",
    "title": "Cache the GraphFrame",
    "section": "gf_cache",
    "text": "gf_cache"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_cache.html#description",
    "href": "packages/graphframes/latest/reference/gf_cache.html#description",
    "title": "Cache the GraphFrame",
    "section": "Description",
    "text": "Description\nCache the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_cache.html#usage",
    "href": "packages/graphframes/latest/reference/gf_cache.html#usage",
    "title": "Cache the GraphFrame",
    "section": "Usage",
    "text": "Usage\ngf_cache(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_cache.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_cache.html#arguments",
    "title": "Cache the GraphFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html",
    "title": "PageRank",
    "section": "",
    "text": "R/gf_pagerank.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#gf_pagerank",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#gf_pagerank",
    "title": "PageRank",
    "section": "gf_pagerank",
    "text": "gf_pagerank"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#description",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#description",
    "title": "PageRank",
    "section": "Description",
    "text": "Description\nPageRank"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#usage",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#usage",
    "title": "PageRank",
    "section": "Usage",
    "text": "Usage\ngf_pagerank(x, tol = NULL, reset_probability = 0.15, max_iter = NULL, \n  source_id = NULL, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#arguments",
    "title": "PageRank",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\ntol\nTolerance.\n\n\nreset_probability\nReset probability.\n\n\nmax_iter\nMaximum number of iterations.\n\n\nsource_id\n(Optional) Source vertex for a personalized pagerank.\n\n\n…\nOptional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#examples",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#examples",
    "title": "PageRank",
    "section": "Examples",
    "text": "Examples\n\nlibrary(graphframes)\ng &lt;- gf_friends(sc) \ngf_pagerank(g, reset_probability = 0.15, tol = 0.01)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_persist.html",
    "href": "packages/graphframes/latest/reference/gf_persist.html",
    "title": "Persist the GraphFrame",
    "section": "",
    "text": "R/gf_interface.R"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_persist.html#gf_persist",
    "href": "packages/graphframes/latest/reference/gf_persist.html#gf_persist",
    "title": "Persist the GraphFrame",
    "section": "gf_persist",
    "text": "gf_persist"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_persist.html#description",
    "href": "packages/graphframes/latest/reference/gf_persist.html#description",
    "title": "Persist the GraphFrame",
    "section": "Description",
    "text": "Description\nPersist the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_persist.html#usage",
    "href": "packages/graphframes/latest/reference/gf_persist.html#usage",
    "title": "Persist the GraphFrame",
    "section": "Usage",
    "text": "Usage\ngf_persist(x, storage_level = \"MEMORY_AND_DISK\")"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_persist.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_persist.html#arguments",
    "title": "Persist the GraphFrame",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a gf_graphframe).\n\n\nstorage_level\nThe storage level to be used. Please view the Spark Documentationfor information on what storage levels are accepted."
  },
  {
    "objectID": "packages/sparkxgb/latest/news.html",
    "href": "packages/sparkxgb/latest/news.html",
    "title": "sparkxgb 0.1.2",
    "section": "",
    "text": "sparkxgb 0.1.2\n\nEdgar Ruiz (https://github.com/edgararuiz) will be the new maintainer of this package moving forward.\n\n\n\nsparkxgb 0.1.1\n\nsparkxgb is a sparklyr extension that provides an interface to XGBoost on Spark.",
    "crumbs": [
      "News"
    ]
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/index.html",
    "href": "packages/sparkxgb/latest/reference/index.html",
    "title": "sparkxgb",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\nxgboost_classifier()\nXGBoost Classifier\n\n\nxgboost_regressor()\nXGBoost Regressor",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_classifier.html",
    "href": "packages/sparkxgb/latest/reference/xgboost_classifier.html",
    "title": "XGBoost Classifier",
    "section": "",
    "text": "R/xgboost_classifier.R"
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_classifier.html#xgboost_classifier",
    "href": "packages/sparkxgb/latest/reference/xgboost_classifier.html#xgboost_classifier",
    "title": "XGBoost Classifier",
    "section": "xgboost_classifier",
    "text": "xgboost_classifier"
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_classifier.html#description",
    "href": "packages/sparkxgb/latest/reference/xgboost_classifier.html#description",
    "title": "XGBoost Classifier",
    "section": "Description",
    "text": "Description\nXGBoost classifier for Spark."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_classifier.html#usage",
    "href": "packages/sparkxgb/latest/reference/xgboost_classifier.html#usage",
    "title": "XGBoost Classifier",
    "section": "Usage",
    "text": "Usage\nxgboost_classifier( \n  x, \n  formula = NULL, \n  eta = 0.3, \n  gamma = 0, \n  max_depth = 6, \n  min_child_weight = 1, \n  max_delta_step = 0, \n  grow_policy = \"depthwise\", \n  max_bins = 16, \n  subsample = 1, \n  colsample_bytree = 1, \n  colsample_bylevel = 1, \n  lambda = 1, \n  alpha = 0, \n  tree_method = \"auto\", \n  sketch_eps = 0.03, \n  scale_pos_weight = 1, \n  sample_type = \"uniform\", \n  normalize_type = \"tree\", \n  rate_drop = 0, \n  skip_drop = 0, \n  lambda_bias = 0, \n  tree_limit = 0, \n  num_round = 1, \n  num_workers = 1, \n  nthread = 1, \n  use_external_memory = FALSE, \n  silent = 0, \n  custom_obj = NULL, \n  custom_eval = NULL, \n  missing = NaN, \n  seed = 0, \n  timeout_request_workers = 30 * 60 * 1000, \n  checkpoint_path = \"\", \n  checkpoint_interval = -1, \n  objective = \"multi:softprob\", \n  base_score = 0.5, \n  train_test_ratio = 1, \n  num_early_stopping_rounds = 0, \n  objective_type = \"classification\", \n  eval_metric = NULL, \n  maximize_evaluation_metrics = FALSE, \n  num_class = NULL, \n  base_margin_col = NULL, \n  thresholds = NULL, \n  weight_col = NULL, \n  features_col = \"features\", \n  label_col = \"label\", \n  prediction_col = \"prediction\", \n  probability_col = \"probability\", \n  raw_prediction_col = \"rawPrediction\", \n  uid = random_string(\"xgboost_classifier_\"), \n  ... \n)"
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_classifier.html#arguments",
    "href": "packages/sparkxgb/latest/reference/xgboost_classifier.html#arguments",
    "title": "XGBoost Classifier",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\neta\nStep size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features and eta actually shrinks the feature weights to make the boosting process more conservative. [default=0.3] range: [0,1]\n\n\ngamma\nMinimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be. [default=0]\n\n\nmax_depth\nMaximum depth of a tree, increase this value will make model more complex / likely to be overfitting. [default=6]\n\n\nmin_child_weight\nMinimum sum of instance weight(hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be. [default=1]\n\n\nmax_delta_step\nMaximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update. [default=0]\n\n\ngrow_policy\nGrowth policy for fast histogram algorithm.\n\n\nmax_bins\nMaximum number of bins in histogram.\n\n\nsubsample\nSubsample ratio of the training instance. Setting it to 0.5 means that XGBoost randomly collected half of the data instances to grow trees and this will prevent overfitting. [default=1] range:(0,1]\n\n\ncolsample_bytree\nSubsample ratio of columns when constructing each tree. [default=1] range: (0,1]\n\n\ncolsample_bylevel\nSubsample ratio of columns for each split, in each level. [default=1] range: (0,1]\n\n\nlambda\nL2 regularization term on weights, increase this value will make model more conservative. [default=1]\n\n\nalpha\nL1 regularization term on weights, increase this value will make model more conservative, defaults to 0.\n\n\ntree_method\nThe tree construction algorithm used in XGBoost. options: ‘auto’, ‘exact’, ‘approx’ [default=‘auto’]\n\n\nsketch_eps\nThis is only used for approximate greedy algorithm. This roughly translated into O(1 / sketch_eps) number of bins. Compared to directly select number of bins, this comes with theoretical guarantee with sketch accuracy. [default=0.03] range: (0, 1)\n\n\nscale_pos_weight\nControl the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative cases) / sum(positive cases). [default=1]\n\n\nsample_type\nParameter for Dart booster. Type of sampling algorithm. “uniform”: dropped trees are selected uniformly. “weighted”: dropped trees are selected in proportion to weight. [default=“uniform”]\n\n\nnormalize_type\nParameter of Dart booster. type of normalization algorithm, options: ‘tree’, ‘forest’ . [default=“tree”]\n\n\nrate_drop\nParameter of Dart booster. dropout rate. [default=0.0] range: [0.0, 1.0]\n\n\nskip_drop\nParameter of Dart booster. probability of skip dropout. If a dropout is skipped, new trees are added in the same manner as gbtree. [default=0.0] range: [0.0, 1.0]\n\n\nlambda_bias\nParameter of linear booster L2 regularization term on bias, default 0 (no L1 reg on bias because it is not important.)\n\n\ntree_limit\nLimit number of trees in the prediction; defaults to 0 (use all trees.)\n\n\nnum_round\nThe number of rounds for boosting.\n\n\nnum_workers\nnumber of workers used to train xgboost model. Defaults to 1.\n\n\nnthread\nNumber of threads used by per worker. Defaults to 1.\n\n\nuse_external_memory\nThe tree construction algorithm used in XGBoost. options: ‘auto’, ‘exact’, ‘approx’ [default=‘auto’]\n\n\nsilent\n0 means printing running messages, 1 means silent mode. default: 0\n\n\ncustom_obj\nCustomized objective function provided by user. Currently unsupported.\n\n\ncustom_eval\nCustomized evaluation function provided by user. Currently unsupported.\n\n\nmissing\nThe value treated as missing. default: Float.NaN\n\n\nseed\nRandom seed for the C++ part of XGBoost and train/test splitting.\n\n\ntimeout_request_workers\nthe maximum time to wait for the job requesting new workers. default: 30 minutes\n\n\ncheckpoint_path\nThe hdfs folder to load and save checkpoint boosters.\n\n\ncheckpoint_interval\nParam for set checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the trained model will get checkpointed every 10 iterations. Note: checkpoint_path must also be set if the checkpoint interval is greater than 0.\n\n\nobjective\nSpecify the learning task and the corresponding learning objective. options: reg:linear, reg:logistic, binary:logistic, binary:logitraw, count:poisson, multi:softmax, multi:softprob, rank:pairwise, reg:gamma. default: reg:linear.\n\n\nbase_score\nParam for initial prediction (aka base margin) column name. Defaults to 0.5.\n\n\ntrain_test_ratio\nFraction of training points to use for testing.\n\n\nnum_early_stopping_rounds\nIf non-zero, the training will be stopped after a specified number of consecutive increases in any evaluation metric.\n\n\nobjective_type\nThe learning objective type of the specified custom objective and eval. Corresponding type will be assigned if custom objective is defined options: regression, classification.\n\n\neval_metric\nEvaluation metrics for validation data, a default metric will be assigned according to objective(rmse for regression, and error for classification, mean average precision for ranking). options: rmse, mae, logloss, error, merror, mlogloss, auc, aucpr, ndcg, map, gamma-deviance\n\n\nmaximize_evaluation_metrics\nWhether to maximize evaluation metrics. Defaults to FALSE (for minization.)\n\n\nnum_class\nNumber of classes.\n\n\nbase_margin_col\nParam for initial prediction (aka base margin) column name.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\nweight_col\nWeight column.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/mleap/latest/index.html",
    "href": "packages/mleap/latest/index.html",
    "title": "R interface for MLeap",
    "section": "",
    "text": "mleap is a sparklyr extension that provides an interface to MLeap, which allows us to take Spark pipelines to production.\n\n\nmleap can be installed from CRAN via\ninstall.packages(\"mleap\")\nor, for the latest development version from GitHub, using\ndevtools::install_github(\"rstudio/mleap\")\nOnce mleap has been installed, we can install the external dependencies using\nlibrary(mleap)\ninstall_maven()\n# Alternatively, if you already have Maven installed, you can \n#  set options(maven.home = \"path/to/maven\")\ninstall_mleap()\nWe can now export Spark ML pipelines from sparklyr.\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\", version = \"2.4.0\")\nmtcars_tbl &lt;- sdf_copy_to(sc, mtcars, overwrite = TRUE)\n\n# Create a pipeline and fit it\npipeline &lt;- ml_pipeline(sc) %&gt;%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %&gt;%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %&gt;%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model &lt;- ml_fit(pipeline, mtcars_tbl)\n\n# Export model\nmodel_path &lt;- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, sample_input = mtcars_tbl, path = model_path)\n\n# Disconnect from Spark\nspark_disconnect(sc)\n## NULL\nAt this point, we can share mtcars_model.zip with our deployment/implementation engineers, and they would be able to embed the model in another application. See the MLeap docs for details.\nWe also provide R functions for testing that the saved models behave as expected. Here we load the previously saved model:\nmodel &lt;- mleap_load_bundle(model_path)\nmodel\n## MLeap Transformer\n## &lt;7e2f61ed-154b-4c9e-9926-85fa326d69ef&gt; \n##   Name: pipeline_c1754f374a53 \n##   Format: json \n##   MLeap Version: 0.14.0\nWe can retrieve the schema associated with the model:\nmleap_model_schema(model)\n## # A tibble: 6 x 5\n##   name       type   nullable dimension io    \n##   &lt;chr&gt;      &lt;chr&gt;  &lt;lgl&gt;    &lt;chr&gt;     &lt;chr&gt; \n## 1 qsec       double TRUE     &lt;NA&gt;      input \n## 2 hp         double FALSE    &lt;NA&gt;      input \n## 3 wt         double TRUE     &lt;NA&gt;      input \n## 4 big_hp     double FALSE    &lt;NA&gt;      output\n## 5 features   double TRUE     (3)       output\n## 6 prediction double FALSE    &lt;NA&gt;      output\nThen, we create a new data frame to be scored, and make predictions using our model:\nnewdata &lt;- tibble::tribble(\n  ~qsec, ~hp, ~wt,\n  16.2,  101, 2.68,\n  18.1,  99,  3.08\n)\n\n# Transform the data frame\ntransformed_df &lt;- mleap_transform(model, newdata)\ndplyr::glimpse(transformed_df)\n## Observations: 2\n## Variables: 6\n## $ qsec       &lt;dbl&gt; 16.2, 18.1\n## $ hp         &lt;dbl&gt; 101, 99\n## $ wt         &lt;dbl&gt; 2.68, 3.08\n## $ big_hp     &lt;dbl&gt; 1, 0\n## $ features   &lt;list&gt; [[[1, 2.68, 16.2], [3]], [[0, 3.08, 18.1], [3]]]\n## $ prediction &lt;dbl&gt; 21.00084, 20.56445",
    "crumbs": [
      "mleap"
    ]
  },
  {
    "objectID": "packages/mleap/latest/index.html#getting-started",
    "href": "packages/mleap/latest/index.html#getting-started",
    "title": "R interface for MLeap",
    "section": "",
    "text": "mleap can be installed from CRAN via\ninstall.packages(\"mleap\")\nor, for the latest development version from GitHub, using\ndevtools::install_github(\"rstudio/mleap\")\nOnce mleap has been installed, we can install the external dependencies using\nlibrary(mleap)\ninstall_maven()\n# Alternatively, if you already have Maven installed, you can \n#  set options(maven.home = \"path/to/maven\")\ninstall_mleap()\nWe can now export Spark ML pipelines from sparklyr.\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\", version = \"2.4.0\")\nmtcars_tbl &lt;- sdf_copy_to(sc, mtcars, overwrite = TRUE)\n\n# Create a pipeline and fit it\npipeline &lt;- ml_pipeline(sc) %&gt;%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %&gt;%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %&gt;%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model &lt;- ml_fit(pipeline, mtcars_tbl)\n\n# Export model\nmodel_path &lt;- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, sample_input = mtcars_tbl, path = model_path)\n\n# Disconnect from Spark\nspark_disconnect(sc)\n## NULL\nAt this point, we can share mtcars_model.zip with our deployment/implementation engineers, and they would be able to embed the model in another application. See the MLeap docs for details.\nWe also provide R functions for testing that the saved models behave as expected. Here we load the previously saved model:\nmodel &lt;- mleap_load_bundle(model_path)\nmodel\n## MLeap Transformer\n## &lt;7e2f61ed-154b-4c9e-9926-85fa326d69ef&gt; \n##   Name: pipeline_c1754f374a53 \n##   Format: json \n##   MLeap Version: 0.14.0\nWe can retrieve the schema associated with the model:\nmleap_model_schema(model)\n## # A tibble: 6 x 5\n##   name       type   nullable dimension io    \n##   &lt;chr&gt;      &lt;chr&gt;  &lt;lgl&gt;    &lt;chr&gt;     &lt;chr&gt; \n## 1 qsec       double TRUE     &lt;NA&gt;      input \n## 2 hp         double FALSE    &lt;NA&gt;      input \n## 3 wt         double TRUE     &lt;NA&gt;      input \n## 4 big_hp     double FALSE    &lt;NA&gt;      output\n## 5 features   double TRUE     (3)       output\n## 6 prediction double FALSE    &lt;NA&gt;      output\nThen, we create a new data frame to be scored, and make predictions using our model:\nnewdata &lt;- tibble::tribble(\n  ~qsec, ~hp, ~wt,\n  16.2,  101, 2.68,\n  18.1,  99,  3.08\n)\n\n# Transform the data frame\ntransformed_df &lt;- mleap_transform(model, newdata)\ndplyr::glimpse(transformed_df)\n## Observations: 2\n## Variables: 6\n## $ qsec       &lt;dbl&gt; 16.2, 18.1\n## $ hp         &lt;dbl&gt; 101, 99\n## $ wt         &lt;dbl&gt; 2.68, 3.08\n## $ big_hp     &lt;dbl&gt; 1, 0\n## $ features   &lt;list&gt; [[[1, 2.68, 16.2], [3]], [[0, 3.08, 18.1], [3]]]\n## $ prediction &lt;dbl&gt; 21.00084, 20.56445",
    "crumbs": [
      "mleap"
    ]
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html",
    "href": "packages/mleap/latest/reference/install_mleap.html",
    "title": "Install MLeap runtime",
    "section": "",
    "text": "R/installers.R"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#install_mleap",
    "href": "packages/mleap/latest/reference/install_mleap.html#install_mleap",
    "title": "Install MLeap runtime",
    "section": "install_mleap",
    "text": "install_mleap"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#description",
    "href": "packages/mleap/latest/reference/install_mleap.html#description",
    "title": "Install MLeap runtime",
    "section": "Description",
    "text": "Description\nInstall MLeap runtime"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#usage",
    "href": "packages/mleap/latest/reference/install_mleap.html#usage",
    "title": "Install MLeap runtime",
    "section": "Usage",
    "text": "Usage\ninstall_mleap(dir = NULL, version = NULL, use_temp_cache = TRUE)"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#arguments",
    "href": "packages/mleap/latest/reference/install_mleap.html#arguments",
    "title": "Install MLeap runtime",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\ndir\n(Optional) Directory to save the jars\n\n\nversion\nVersion of MLeap to install, defaults to the latest version tested with this package.\n\n\nuse_temp_cache\nWhether to use a temporary Maven cache directory for downloading. Setting this to TRUE prevents Maven from creating a persistent .m2/ directory. Defaults to TRUE."
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#examples",
    "href": "packages/mleap/latest/reference/install_mleap.html#examples",
    "title": "Install MLeap runtime",
    "section": "Examples",
    "text": "Examples\n\nlibrary(mleap)\ninstall_mleap()"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_installed_versions.html",
    "href": "packages/mleap/latest/reference/mleap_installed_versions.html",
    "title": "Find existing MLeap installations",
    "section": "",
    "text": "R/installers.R"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_installed_versions.html#mleap_installed_versions",
    "href": "packages/mleap/latest/reference/mleap_installed_versions.html#mleap_installed_versions",
    "title": "Find existing MLeap installations",
    "section": "mleap_installed_versions",
    "text": "mleap_installed_versions"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_installed_versions.html#description",
    "href": "packages/mleap/latest/reference/mleap_installed_versions.html#description",
    "title": "Find existing MLeap installations",
    "section": "Description",
    "text": "Description\nFind existing MLeap installations"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_installed_versions.html#usage",
    "href": "packages/mleap/latest/reference/mleap_installed_versions.html#usage",
    "title": "Find existing MLeap installations",
    "section": "Usage",
    "text": "Usage\nmleap_installed_versions()"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_installed_versions.html#value",
    "href": "packages/mleap/latest/reference/mleap_installed_versions.html#value",
    "title": "Find existing MLeap installations",
    "section": "Value",
    "text": "Value\nA data frame of MLeap Runtime installation versions and their locations."
  },
  {
    "objectID": "packages/mleap/latest/reference/index.html",
    "href": "packages/mleap/latest/reference/index.html",
    "title": "mleap",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ninstall_maven()\nInstall Maven\n\n\ninstall_mleap()\nInstall MLeap runtime\n\n\nml_write_bundle()\nExport a Spark pipeline for serving\n\n\nmleap_installed_versions()\nFind existing MLeap installations\n\n\nmleap_load_bundle()\nLoads an MLeap bundle\n\n\nmleap_model_schema()\nMLeap model schema\n\n\nmleap_transform()\nTransform data using an MLeap model",
    "crumbs": [
      "Reference",
      "Latest release"
    ]
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html",
    "href": "packages/mleap/latest/reference/mleap_transform.html",
    "title": "Transform data using an MLeap model",
    "section": "",
    "text": "R/prediction.R"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#mleap_transform",
    "href": "packages/mleap/latest/reference/mleap_transform.html#mleap_transform",
    "title": "Transform data using an MLeap model",
    "section": "mleap_transform",
    "text": "mleap_transform"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#description",
    "href": "packages/mleap/latest/reference/mleap_transform.html#description",
    "title": "Transform data using an MLeap model",
    "section": "Description",
    "text": "Description\nThis functions transforms an R data frame using an MLeap model."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#usage",
    "href": "packages/mleap/latest/reference/mleap_transform.html#usage",
    "title": "Transform data using an MLeap model",
    "section": "Usage",
    "text": "Usage\nmleap_transform(model, data)"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#arguments",
    "href": "packages/mleap/latest/reference/mleap_transform.html#arguments",
    "title": "Transform data using an MLeap model",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nmodel\nAn MLeap model object, obtained by mleap_load_bundle().\n\n\ndata\nAn R data frame."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#value",
    "href": "packages/mleap/latest/reference/mleap_transform.html#value",
    "title": "Transform data using an MLeap model",
    "section": "Value",
    "text": "Value\nA transformed data frame."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#see-also",
    "href": "packages/mleap/latest/reference/mleap_transform.html#see-also",
    "title": "Transform data using an MLeap model",
    "section": "See Also",
    "text": "See Also\n[mleap_load_bundle()]"
  },
  {
    "objectID": "packages/mleap/dev/index.html",
    "href": "packages/mleap/dev/index.html",
    "title": "R interface for MLeap",
    "section": "",
    "text": "mleap is a sparklyr extension that provides an interface to MLeap, which allows us to take Spark pipelines to production.\n\n\nmleap can be installed from CRAN via\ninstall.packages(\"mleap\")\nor, for the latest development version from GitHub, using\ndevtools::install_github(\"rstudio/mleap\")\nOnce mleap has been installed, we can install the external dependencies using\nlibrary(mleap)\ninstall_maven()\n# Alternatively, if you already have Maven installed, you can \n#  set options(maven.home = \"path/to/maven\")\ninstall_mleap()\nWe can now export Spark ML pipelines from sparklyr.\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\", version = \"2.4.0\")\nmtcars_tbl &lt;- sdf_copy_to(sc, mtcars, overwrite = TRUE)\n\n# Create a pipeline and fit it\npipeline &lt;- ml_pipeline(sc) %&gt;%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %&gt;%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %&gt;%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model &lt;- ml_fit(pipeline, mtcars_tbl)\n\n# Export model\nmodel_path &lt;- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, sample_input = mtcars_tbl, path = model_path)\n\n# Disconnect from Spark\nspark_disconnect(sc)\n## NULL\nAt this point, we can share mtcars_model.zip with our deployment/implementation engineers, and they would be able to embed the model in another application. See the MLeap docs for details.\nWe also provide R functions for testing that the saved models behave as expected. Here we load the previously saved model:\nmodel &lt;- mleap_load_bundle(model_path)\nmodel\n## MLeap Transformer\n## &lt;7e2f61ed-154b-4c9e-9926-85fa326d69ef&gt; \n##   Name: pipeline_c1754f374a53 \n##   Format: json \n##   MLeap Version: 0.14.0\nWe can retrieve the schema associated with the model:\nmleap_model_schema(model)\n## # A tibble: 6 x 5\n##   name       type   nullable dimension io    \n##   &lt;chr&gt;      &lt;chr&gt;  &lt;lgl&gt;    &lt;chr&gt;     &lt;chr&gt; \n## 1 qsec       double TRUE     &lt;NA&gt;      input \n## 2 hp         double FALSE    &lt;NA&gt;      input \n## 3 wt         double TRUE     &lt;NA&gt;      input \n## 4 big_hp     double FALSE    &lt;NA&gt;      output\n## 5 features   double TRUE     (3)       output\n## 6 prediction double FALSE    &lt;NA&gt;      output\nThen, we create a new data frame to be scored, and make predictions using our model:\nnewdata &lt;- tibble::tribble(\n  ~qsec, ~hp, ~wt,\n  16.2,  101, 2.68,\n  18.1,  99,  3.08\n)\n\n# Transform the data frame\ntransformed_df &lt;- mleap_transform(model, newdata)\ndplyr::glimpse(transformed_df)\n## Observations: 2\n## Variables: 6\n## $ qsec       &lt;dbl&gt; 16.2, 18.1\n## $ hp         &lt;dbl&gt; 101, 99\n## $ wt         &lt;dbl&gt; 2.68, 3.08\n## $ big_hp     &lt;dbl&gt; 1, 0\n## $ features   &lt;list&gt; [[[1, 2.68, 16.2], [3]], [[0, 3.08, 18.1], [3]]]\n## $ prediction &lt;dbl&gt; 21.00084, 20.56445"
  },
  {
    "objectID": "packages/mleap/dev/index.html#getting-started",
    "href": "packages/mleap/dev/index.html#getting-started",
    "title": "R interface for MLeap",
    "section": "",
    "text": "mleap can be installed from CRAN via\ninstall.packages(\"mleap\")\nor, for the latest development version from GitHub, using\ndevtools::install_github(\"rstudio/mleap\")\nOnce mleap has been installed, we can install the external dependencies using\nlibrary(mleap)\ninstall_maven()\n# Alternatively, if you already have Maven installed, you can \n#  set options(maven.home = \"path/to/maven\")\ninstall_mleap()\nWe can now export Spark ML pipelines from sparklyr.\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\", version = \"2.4.0\")\nmtcars_tbl &lt;- sdf_copy_to(sc, mtcars, overwrite = TRUE)\n\n# Create a pipeline and fit it\npipeline &lt;- ml_pipeline(sc) %&gt;%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %&gt;%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %&gt;%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model &lt;- ml_fit(pipeline, mtcars_tbl)\n\n# Export model\nmodel_path &lt;- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, sample_input = mtcars_tbl, path = model_path)\n\n# Disconnect from Spark\nspark_disconnect(sc)\n## NULL\nAt this point, we can share mtcars_model.zip with our deployment/implementation engineers, and they would be able to embed the model in another application. See the MLeap docs for details.\nWe also provide R functions for testing that the saved models behave as expected. Here we load the previously saved model:\nmodel &lt;- mleap_load_bundle(model_path)\nmodel\n## MLeap Transformer\n## &lt;7e2f61ed-154b-4c9e-9926-85fa326d69ef&gt; \n##   Name: pipeline_c1754f374a53 \n##   Format: json \n##   MLeap Version: 0.14.0\nWe can retrieve the schema associated with the model:\nmleap_model_schema(model)\n## # A tibble: 6 x 5\n##   name       type   nullable dimension io    \n##   &lt;chr&gt;      &lt;chr&gt;  &lt;lgl&gt;    &lt;chr&gt;     &lt;chr&gt; \n## 1 qsec       double TRUE     &lt;NA&gt;      input \n## 2 hp         double FALSE    &lt;NA&gt;      input \n## 3 wt         double TRUE     &lt;NA&gt;      input \n## 4 big_hp     double FALSE    &lt;NA&gt;      output\n## 5 features   double TRUE     (3)       output\n## 6 prediction double FALSE    &lt;NA&gt;      output\nThen, we create a new data frame to be scored, and make predictions using our model:\nnewdata &lt;- tibble::tribble(\n  ~qsec, ~hp, ~wt,\n  16.2,  101, 2.68,\n  18.1,  99,  3.08\n)\n\n# Transform the data frame\ntransformed_df &lt;- mleap_transform(model, newdata)\ndplyr::glimpse(transformed_df)\n## Observations: 2\n## Variables: 6\n## $ qsec       &lt;dbl&gt; 16.2, 18.1\n## $ hp         &lt;dbl&gt; 101, 99\n## $ wt         &lt;dbl&gt; 2.68, 3.08\n## $ big_hp     &lt;dbl&gt; 1, 0\n## $ features   &lt;list&gt; [[[1, 2.68, 16.2], [3]], [[0, 3.08, 18.1], [3]]]\n## $ prediction &lt;dbl&gt; 21.00084, 20.56445"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html",
    "href": "packages/mleap/dev/reference/install_mleap.html",
    "title": "Install MLeap runtime",
    "section": "",
    "text": "R/installers.R"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#install_mleap",
    "href": "packages/mleap/dev/reference/install_mleap.html#install_mleap",
    "title": "Install MLeap runtime",
    "section": "install_mleap",
    "text": "install_mleap"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#description",
    "href": "packages/mleap/dev/reference/install_mleap.html#description",
    "title": "Install MLeap runtime",
    "section": "Description",
    "text": "Description\nInstall MLeap runtime"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#usage",
    "href": "packages/mleap/dev/reference/install_mleap.html#usage",
    "title": "Install MLeap runtime",
    "section": "Usage",
    "text": "Usage\ninstall_mleap(dir = NULL, version = NULL, use_temp_cache = TRUE)"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#arguments",
    "href": "packages/mleap/dev/reference/install_mleap.html#arguments",
    "title": "Install MLeap runtime",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArguments\nDescription\n\n\n\n\ndir\n(Optional) Directory to save the jars\n\n\nversion\nVersion of MLeap to install, defaults to the latest version tested with this package.\n\n\nuse_temp_cache\nWhether to use a temporary Maven cache directory for downloading. Setting this to TRUE prevents Maven from creating a persistent .m2/ directory. Defaults to TRUE."
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#examples",
    "href": "packages/mleap/dev/reference/install_mleap.html#examples",
    "title": "Install MLeap runtime",
    "section": "Examples",
    "text": "Examples\n\nlibrary(mleap)\ninstall_mleap()"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_installed_versions.html",
    "href": "packages/mleap/dev/reference/mleap_installed_versions.html",
    "title": "Find existing MLeap installations",
    "section": "",
    "text": "R/installers.R"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_installed_versions.html#mleap_installed_versions",
    "href": "packages/mleap/dev/reference/mleap_installed_versions.html#mleap_installed_versions",
    "title": "Find existing MLeap installations",
    "section": "mleap_installed_versions",
    "text": "mleap_installed_versions"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_installed_versions.html#description",
    "href": "packages/mleap/dev/reference/mleap_installed_versions.html#description",
    "title": "Find existing MLeap installations",
    "section": "Description",
    "text": "Description\nFind existing MLeap installations"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_installed_versions.html#usage",
    "href": "packages/mleap/dev/reference/mleap_installed_versions.html#usage",
    "title": "Find existing MLeap installations",
    "section": "Usage",
    "text": "Usage\nmleap_installed_versions()"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_installed_versions.html#value",
    "href": "packages/mleap/dev/reference/mleap_installed_versions.html#value",
    "title": "Find existing MLeap installations",
    "section": "Value",
    "text": "Value\nA data frame of MLeap Runtime installation versions and their locations."
  },
  {
    "objectID": "packages/mleap/dev/reference/index.html",
    "href": "packages/mleap/dev/reference/index.html",
    "title": "mleap",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ninstall_maven()\nInstall Maven\n\n\ninstall_mleap()\nInstall MLeap runtime\n\n\nml_write_bundle()\nExport a Spark pipeline for serving\n\n\nmleap_installed_versions()\nFind existing MLeap installations\n\n\nmleap_load_bundle()\nLoads an MLeap bundle\n\n\nmleap_model_schema()\nMLeap model schema\n\n\nmleap_transform()\nTransform data using an MLeap model",
    "crumbs": [
      "Reference",
      "Dev release"
    ]
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html",
    "href": "packages/mleap/dev/reference/mleap_transform.html",
    "title": "Transform data using an MLeap model",
    "section": "",
    "text": "R/prediction.R"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#mleap_transform",
    "href": "packages/mleap/dev/reference/mleap_transform.html#mleap_transform",
    "title": "Transform data using an MLeap model",
    "section": "mleap_transform",
    "text": "mleap_transform"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#description",
    "href": "packages/mleap/dev/reference/mleap_transform.html#description",
    "title": "Transform data using an MLeap model",
    "section": "Description",
    "text": "Description\nThis functions transforms an R data frame using an MLeap model."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#usage",
    "href": "packages/mleap/dev/reference/mleap_transform.html#usage",
    "title": "Transform data using an MLeap model",
    "section": "Usage",
    "text": "Usage\nmleap_transform(model, data)"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#arguments",
    "href": "packages/mleap/dev/reference/mleap_transform.html#arguments",
    "title": "Transform data using an MLeap model",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArguments\nDescription\n\n\n\n\nmodel\nAn MLeap model object, obtained by mleap_load_bundle().\n\n\ndata\nAn R data frame."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#value",
    "href": "packages/mleap/dev/reference/mleap_transform.html#value",
    "title": "Transform data using an MLeap model",
    "section": "Value",
    "text": "Value\nA transformed data frame."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#see-also",
    "href": "packages/mleap/dev/reference/mleap_transform.html#see-also",
    "title": "Transform data using an MLeap model",
    "section": "See Also",
    "text": "See Also\n[mleap_load_bundle()]"
  },
  {
    "objectID": "get-started/prepare-data.html",
    "href": "get-started/prepare-data.html",
    "title": "Prepare Data",
    "section": "",
    "text": "sparklyr provide multiple methods to prepare data inside Spark:\nThis article will introduce each method and provide a simple example.",
    "crumbs": [
      "Get Started",
      "Prepare Data"
    ]
  },
  {
    "objectID": "get-started/prepare-data.html#exercise",
    "href": "get-started/prepare-data.html#exercise",
    "title": "Prepare Data",
    "section": "Exercise",
    "text": "Exercise\nFor the exercise start a local session of Spark. We’ll start by copying a data set from R into the Spark cluster (note that you may need to install the nycflights13)\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"local\")\nflights_tbl &lt;- copy_to(sc, nycflights13::flights, \"spark_flights\")",
    "crumbs": [
      "Get Started",
      "Prepare Data"
    ]
  },
  {
    "objectID": "get-started/prepare-data.html#using-dplyr",
    "href": "get-started/prepare-data.html#using-dplyr",
    "title": "Prepare Data",
    "section": "Using dplyr",
    "text": "Using dplyr\nWe can use familiar dplyr commands to prepare data inside Spark. The commands run inside Spark, so there are no unnecessary data transfers between R and Spark.\nIn this example, we can see how easy it is to summarize the flights data without having to know how to write Spark SQL:\n\ndelay &lt;- flights_tbl %&gt;%\n  group_by(tailnum) %&gt;%\n  summarise(\n    count = n(), \n    dist = mean(distance, na.rm = TRUE), \n    delay = mean(arr_delay, na.rm = TRUE)\n    ) %&gt;%\n  filter(count &gt; 20, dist &lt; 2000, !is.na(delay)) \n\ndelay\n#&gt; # Source: spark&lt;?&gt; [?? x 4]\n#&gt;    tailnum count  dist  delay\n#&gt;    &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 N24211    130 1330.  7.7  \n#&gt;  2 N793JB    283 1529.  4.72 \n#&gt;  3 N657JB    285 1286.  5.03 \n#&gt;  4 N633AA     24 1587. -0.625\n#&gt;  5 N9EAMQ    248  675.  9.24 \n#&gt;  6 N3GKAA     77 1247.  4.97 \n#&gt;  7 N997DL     63  868.  4.90 \n#&gt;  8 N318NB    202  814. -1.12 \n#&gt;  9 N651JB    261 1408.  7.58 \n#&gt; 10 N841UA     96 1208.  2.10 \n#&gt; # … with more rows\n\nsparklyr and dplyr translate the R commands into Spark SQL for us. To see the resulting query use show_query():\n\ndplyr::show_query(delay)\n#&gt; &lt;SQL&gt;\n#&gt; SELECT *\n#&gt; FROM (\n#&gt;   SELECT\n#&gt;     `tailnum`,\n#&gt;     COUNT(*) AS `count`,\n#&gt;     AVG(`distance`) AS `dist`,\n#&gt;     AVG(`arr_delay`) AS `delay`\n#&gt;   FROM `spark_flights`\n#&gt;   GROUP BY `tailnum`\n#&gt; ) `q01`\n#&gt; WHERE (`count` &gt; 20.0) AND (`dist` &lt; 2000.0) AND (NOT((`delay` IS NULL)))\n\nNotice that the delay variable does not contain data. It only contains the dplyr commands that are to run against the Spark connection.\nFor additional documentation on using dplyr with Spark see the Manipulating Data with dplyr article in this site",
    "crumbs": [
      "Get Started",
      "Prepare Data"
    ]
  },
  {
    "objectID": "get-started/prepare-data.html#using-sql",
    "href": "get-started/prepare-data.html#using-sql",
    "title": "Prepare Data",
    "section": "Using SQL",
    "text": "Using SQL\nIt’s also possible to execute SQL queries directly against tables within a Spark cluster. The spark_connection() object implements a DBI interface for Spark, so you can use dbGetQuery() to execute SQL and return the result as an R data frame:\n\nlibrary(DBI)\n\ndbGetQuery(sc, \"SELECT carrier, sched_dep_time, dep_time, dep_delay FROM spark_flights LIMIT 5\")\n#&gt;   carrier sched_dep_time dep_time dep_delay\n#&gt; 1      UA            515      517         2\n#&gt; 2      UA            529      533         4\n#&gt; 3      AA            540      542         2\n#&gt; 4      B6            545      544        -1\n#&gt; 5      DL            600      554        -6",
    "crumbs": [
      "Get Started",
      "Prepare Data"
    ]
  },
  {
    "objectID": "get-started/prepare-data.html#using-feature-transformers",
    "href": "get-started/prepare-data.html#using-feature-transformers",
    "title": "Prepare Data",
    "section": "Using Feature Transformers",
    "text": "Using Feature Transformers\nBoth of the previous methods rely on SQL statements. Spark provides commands that make some data transformation more convenient, and without the use of SQL.\nFor example, the ft_binarizer() command simplifies the creation of a new column that indicates if the value of another column is above a certain threshold.\n\nflights_tbl %&gt;% \n  ft_binarizer(\"dep_delay\", \"over_one\", threshold = 1) %&gt;% \n  select(dep_delay, over_one) %&gt;% \n  head(5)\n#&gt; # Source: spark&lt;?&gt; [?? x 2]\n#&gt;   dep_delay over_one\n#&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1         2        1\n#&gt; 2         4        1\n#&gt; 3         2        1\n#&gt; 4        -1        0\n#&gt; 5        -6        0\n\nFind a full list of the Spark Feature Transformers available through sparklyr here: Reference - FT.",
    "crumbs": [
      "Get Started",
      "Prepare Data"
    ]
  },
  {
    "objectID": "get-started/prepare-data.html#disconnect-from-spark",
    "href": "get-started/prepare-data.html#disconnect-from-spark",
    "title": "Prepare Data",
    "section": "Disconnect from Spark",
    "text": "Disconnect from Spark\nLastly, cleanup your session by disconnecting Spark:\n\nspark_disconnect(sc)",
    "crumbs": [
      "Get Started",
      "Prepare Data"
    ]
  },
  {
    "objectID": "get-started/index.html",
    "href": "get-started/index.html",
    "title": "Install",
    "section": "",
    "text": "You can install the sparklyr package from CRAN as follows:\n\ninstall.packages(\"sparklyr\")",
    "crumbs": [
      "Get Started",
      "Install"
    ]
  },
  {
    "objectID": "get-started/index.html#install-the-package",
    "href": "get-started/index.html#install-the-package",
    "title": "Install",
    "section": "",
    "text": "You can install the sparklyr package from CRAN as follows:\n\ninstall.packages(\"sparklyr\")",
    "crumbs": [
      "Get Started",
      "Install"
    ]
  },
  {
    "objectID": "get-started/index.html#install-spark-locally",
    "href": "get-started/index.html#install-spark-locally",
    "title": "Install",
    "section": "Install Spark locally",
    "text": "Install Spark locally\n\n\n\n\n\n\nCaution\n\n\n\nThe steps in this section are only needed if you need to run Spark in your computer. If you already have a running Spark cluster that you will use to learn sparklyr, then skip this section.\n\n\nThis section is meant for developers new to sparklyr. You will need a running Spark environment to connect to. sparklyr can install Spark in your computer. The installed Spark environment is meant for learning and prototyping purposes. The installation will work on all the major Operating Systems that R works on, including Linux, MacOS, and Windows.\n\nlibrary(sparklyr)\n\nspark_install()\n\nPlease be aware that after installation, Spark is not running. The next section will explain how to start a single node Spark cluster in your machine.",
    "crumbs": [
      "Get Started",
      "Install"
    ]
  },
  {
    "objectID": "get-started/index.html#connect-to-spark",
    "href": "get-started/index.html#connect-to-spark",
    "title": "Install",
    "section": "Connect to Spark",
    "text": "Connect to Spark\nYou can use spark_connect() to connect to Spark clusters. The arguments passed to this functions depend on the type of Spark cluster you are connecting to. There are several different types of Spark clusters, such as YARN, Stand Alone and Kubernetes.\nspark_connect() is able to both start, and connect to, the single node Spark cluster in your machine. In order to do that, pass “local” as the argument for master:\n\nlibrary(sparklyr)\n\nsc &lt;- spark_connect(master = \"local\")\n\nThe sc variable now contains all of the connection information needed to interact with the cluster.\nTo learn how to connect to other types of Spark clusters, see the Deployment section of this site.",
    "crumbs": [
      "Get Started",
      "Install"
    ]
  },
  {
    "objectID": "get-started/index.html#disconnect-from-spark",
    "href": "get-started/index.html#disconnect-from-spark",
    "title": "Install",
    "section": "Disconnect from Spark",
    "text": "Disconnect from Spark\nFor “local” connection, spark_disconnect() will shut down the single node Spark environment in your machine, and tell R that the connection is no longer valid. For other types of Spark clusters, spark_disconnect() will only end the Spark session, it will not shut down the Spark cluster itself.\n\nspark_disconnect(sc)",
    "crumbs": [
      "Get Started",
      "Install"
    ]
  },
  {
    "objectID": "get-started/index.html#clusters",
    "href": "get-started/index.html#clusters",
    "title": "Install",
    "section": "Clusters",
    "text": "Clusters\nHere are some examples of how to use spark_connect() to connect to different types of Spark clusters:\nHadoop YARN:\nsc &lt;- spark_connect(master = \"yarn\")\nMesos:\nsc &lt;- spark_connect(master = \"mesos://host:port\")\nKubernetes:\nsc &lt;- spark_connect(master = \"k8s://https://server\")\nApache Livy:\nsc &lt;- spark_connect(master = \"http://server/livy\", method = \"livy\")\nStand Alone:\nsc &lt;- spark_connect(master = \"spark://master-url:7077\")\nQubole: (for more info visit the Qubole page on this site)\nsc &lt;- spark_connect(method = \"qubole\")\nDatabricks - Visit the Databricks page on this site to review the connection options",
    "crumbs": [
      "Get Started",
      "Install"
    ]
  }
]