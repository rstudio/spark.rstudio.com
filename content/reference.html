<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Function reference â€¢ sparklyr</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous" />

<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script>

<!-- bootstrap-toc -->
<link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="Function reference" />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body data-spy="scroll" data-target="#toc">
    <div class="container template-reference-index">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index">sparklyr</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">1.4.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index">Reference</a>
</li>
<li>
  <a href="../news/index">Changelog</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/sparklyr/sparklyr/">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="contents col-md-9">
    <div class="page-header">
      <h1>Reference</h1>
    </div>

    <table class="ref-index">

    <colgroup>
      
      <col class="alias" />
      <col class="title" />
    </colgroup>

    <tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-spark-operations" class="hasAnchor"><a href="#section-spark-operations" class="anchor"></a>Spark Operations</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="spark_config">spark_config()</a></code> </p>
        </td>
        <td><p>Read Spark Configuration</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark-connections">spark_connect()</a></code> <code><a href="spark-connections">spark_connection_is_open()</a></code> <code><a href="spark-connections">spark_disconnect()</a></code> <code><a href="spark-connections">spark_disconnect_all()</a></code> <code><a href="spark-connections">spark_submit()</a></code> </p>
        </td>
        <td><p>Manage Spark Connections</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_install">spark_install_find()</a></code> <code><a href="spark_install">spark_install()</a></code> <code><a href="spark_install">spark_uninstall()</a></code> <code><a href="spark_install">spark_install_dir()</a></code> <code><a href="spark_install">spark_install_tar()</a></code> <code><a href="spark_install">spark_installed_versions()</a></code> <code><a href="spark_install">spark_available_versions()</a></code> </p>
        </td>
        <td><p>Find a given Spark installation by version.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_log">spark_log()</a></code> </p>
        </td>
        <td><p>View Entries in the Spark Log</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_web">spark_web()</a></code> </p>
        </td>
        <td><p>Open the Spark web interface</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="connection_is_open">connection_is_open()</a></code> </p>
        </td>
        <td><p>Check whether the connection is open</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="connection_spark_shinyapp">connection_spark_shinyapp()</a></code> </p>
        </td>
        <td><p>A Shiny app that can be used to construct a <code>spark_connect</code> statement</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_configuration">spark_session_config()</a></code> </p>
        </td>
        <td><p>Runtime configuration interface for the Spark Session</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="checkpoint_directory">spark_set_checkpoint_dir()</a></code> <code><a href="checkpoint_directory">spark_get_checkpoint_dir()</a></code> </p>
        </td>
        <td><p>Set/Get Spark checkpoint directory</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_table_name">spark_table_name()</a></code> </p>
        </td>
        <td><p>Generate a Table Name from Expression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_version_from_home">spark_version_from_home()</a></code> </p>
        </td>
        <td><p>Get the Spark Version Associated with a Spark Installation</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_versions">spark_versions()</a></code> </p>
        </td>
        <td><p>Retrieves a dataframe available Spark versions that van be installed.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_config_kubernetes">spark_config_kubernetes()</a></code> </p>
        </td>
        <td><p>Kubernetes Configuration</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_config_settings">spark_config_settings()</a></code> </p>
        </td>
        <td><p>Retrieve Available Settings</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_connection_find">spark_connection_find()</a></code> </p>
        </td>
        <td><p>Find Spark Connection</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_dependency_fallback">spark_dependency_fallback()</a></code> </p>
        </td>
        <td><p>Fallback to Spark Dependency</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_extension">spark_extension()</a></code> </p>
        </td>
        <td><p>Create Spark Extension</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_load_table">spark_load_table()</a></code> </p>
        </td>
        <td><p>Reads from a Spark Table into a Spark DataFrame.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_libsvm">spark_read_libsvm()</a></code> </p>
        </td>
        <td><p>Read libsvm file into a Spark DataFrame.</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-spark-data" class="hasAnchor"><a href="#section-spark-data" class="anchor"></a>Spark Data</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="spark_read">spark_read()</a></code> </p>
        </td>
        <td><p>Read file(s) into a Spark DataFrame using a custom reader</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_avro">spark_read_avro()</a></code> </p>
        </td>
        <td><p>Read Apache Avro data into a Spark DataFrame.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_csv">spark_read_csv()</a></code> </p>
        </td>
        <td><p>Read a CSV file into a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_delta">spark_read_delta()</a></code> </p>
        </td>
        <td><p>Read from Delta Lake into a Spark DataFrame.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_jdbc">spark_read_jdbc()</a></code> </p>
        </td>
        <td><p>Read from JDBC connection into a Spark DataFrame.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_json">spark_read_json()</a></code> </p>
        </td>
        <td><p>Read a JSON file into a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_parquet">spark_read_parquet()</a></code> </p>
        </td>
        <td><p>Read a Parquet file into a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_source">spark_read_source()</a></code> </p>
        </td>
        <td><p>Read from a generic source into a Spark DataFrame.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_table">spark_read_table()</a></code> </p>
        </td>
        <td><p>Reads from a Spark Table into a Spark DataFrame.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_orc">spark_read_orc()</a></code> </p>
        </td>
        <td><p>Read a ORC file into a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_text">spark_read_text()</a></code> </p>
        </td>
        <td><p>Read a Text file into a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_save_table">spark_save_table()</a></code> </p>
        </td>
        <td><p>Saves a Spark DataFrame as a Spark table</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write">spark_write()</a></code> </p>
        </td>
        <td><p>Write Spark DataFrame to file using a custom writer</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_avro">spark_write_avro()</a></code> </p>
        </td>
        <td><p>Serialize a Spark DataFrame into Apache Avro format</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_orc">spark_write_orc()</a></code> </p>
        </td>
        <td><p>Write a Spark DataFrame to a ORC file</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_text">spark_write_text()</a></code> </p>
        </td>
        <td><p>Write a Spark DataFrame to a Text file</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_csv">spark_write_csv()</a></code> </p>
        </td>
        <td><p>Write a Spark DataFrame to a CSV</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_delta">spark_write_delta()</a></code> </p>
        </td>
        <td><p>Writes a Spark DataFrame into Delta Lake</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_jdbc">spark_write_jdbc()</a></code> </p>
        </td>
        <td><p>Writes a Spark DataFrame into a JDBC table</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_json">spark_write_json()</a></code> </p>
        </td>
        <td><p>Write a Spark DataFrame to a JSON file</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_parquet">spark_write_parquet()</a></code> </p>
        </td>
        <td><p>Write a Spark DataFrame to a Parquet file</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_source">spark_write_source()</a></code> </p>
        </td>
        <td><p>Writes a Spark DataFrame into a generic source</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_table">spark_write_table()</a></code> </p>
        </td>
        <td><p>Writes a Spark DataFrame into a Spark table</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-spark-tables" class="hasAnchor"><a href="#section-spark-tables" class="anchor"></a>Spark Tables</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="src_databases">src_databases()</a></code> </p>
        </td>
        <td><p>Show database list</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="tbl_cache">tbl_cache()</a></code> </p>
        </td>
        <td><p>Cache a Spark Table</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="tbl_change_db">tbl_change_db()</a></code> </p>
        </td>
        <td><p>Use specific database</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="tbl_uncache">tbl_uncache()</a></code> </p>
        </td>
        <td><p>Uncache a Spark Table</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-spark-dataframes" class="hasAnchor"><a href="#section-spark-dataframes" class="anchor"></a>Spark DataFrames</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="sdf_along">sdf_along()</a></code> </p>
        </td>
        <td><p>Create DataFrame for along Object</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_bind">sdf_bind_rows()</a></code> <code><a href="sdf_bind">sdf_bind_cols()</a></code> </p>
        </td>
        <td><p>Bind multiple Spark DataFrames by row and column</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_broadcast">sdf_broadcast()</a></code> </p>
        </td>
        <td><p>Broadcast hint</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_checkpoint">sdf_checkpoint()</a></code> </p>
        </td>
        <td><p>Checkpoint a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_coalesce">sdf_coalesce()</a></code> </p>
        </td>
        <td><p>Coalesces a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_copy_to">sdf_copy_to()</a></code> <code><a href="sdf_copy_to">sdf_import()</a></code> </p>
        </td>
        <td><p>Copy an Object into Spark</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_drop_duplicates">sdf_drop_duplicates()</a></code> </p>
        </td>
        <td><p>Remove duplicates from a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_from_avro">sdf_from_avro()</a></code> </p>
        </td>
        <td><p>Convert column(s) from avro format</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_len">sdf_len()</a></code> </p>
        </td>
        <td><p>Create DataFrame for Length</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_num_partitions">sdf_num_partitions()</a></code> </p>
        </td>
        <td><p>Gets number of partitions of a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_random_split">sdf_random_split()</a></code> <code><a href="sdf_random_split">sdf_partition()</a></code> </p>
        </td>
        <td><p>Partition a Spark Dataframe</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_pivot">sdf_pivot()</a></code> </p>
        </td>
        <td><p>Pivot a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf-transform-methods">sdf_predict()</a></code> <code><a href="sdf-transform-methods">sdf_transform()</a></code> <code><a href="sdf-transform-methods">sdf_fit()</a></code> <code><a href="sdf-transform-methods">sdf_fit_and_transform()</a></code> </p>
        </td>
        <td><p>Spark ML -- Transform, fit, and predict methods (sdf_ interface)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_read_column">sdf_read_column()</a></code> </p>
        </td>
        <td><p>Read a Column from a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_register">sdf_register()</a></code> </p>
        </td>
        <td><p>Register a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_repartition">sdf_repartition()</a></code> </p>
        </td>
        <td><p>Repartition a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_residuals">sdf_residuals()</a></code> </p>
        </td>
        <td><p>Model Residuals</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_sample">sdf_sample()</a></code> </p>
        </td>
        <td><p>Randomly Sample Rows from a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_separate_column">sdf_separate_column()</a></code> </p>
        </td>
        <td><p>Separate a Vector Column into Scalar Columns</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_seq">sdf_seq()</a></code> </p>
        </td>
        <td><p>Create DataFrame for Range</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_sort">sdf_sort()</a></code> </p>
        </td>
        <td><p>Sort a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_to_avro">sdf_to_avro()</a></code> </p>
        </td>
        <td><p>Convert column(s) to avro format</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_with_unique_id">sdf_with_unique_id()</a></code> </p>
        </td>
        <td><p>Add a Unique ID Column to a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_collect">sdf_collect()</a></code> </p>
        </td>
        <td><p>Collect a Spark DataFrame into R.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_crosstab">sdf_crosstab()</a></code> </p>
        </td>
        <td><p>Cross Tabulation</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_debug_string">sdf_debug_string()</a></code> </p>
        </td>
        <td><p>Debug Info for Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_describe">sdf_describe()</a></code> </p>
        </td>
        <td><p>Compute summary statistics for columns of a data frame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_dim">sdf_dim()</a></code> <code><a href="sdf_dim">sdf_nrow()</a></code> <code><a href="sdf_dim">sdf_ncol()</a></code> </p>
        </td>
        <td><p>Support for Dimension Operations</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_is_streaming">sdf_is_streaming()</a></code> </p>
        </td>
        <td><p>Spark DataFrame is Streaming</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_last_index">sdf_last_index()</a></code> </p>
        </td>
        <td><p>Returns the last index of a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf-saveload">sdf_save_table()</a></code> <code><a href="sdf-saveload">sdf_load_table()</a></code> <code><a href="sdf-saveload">sdf_save_parquet()</a></code> <code><a href="sdf-saveload">sdf_load_parquet()</a></code> </p>
        </td>
        <td><p>Save / Load a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_persist">sdf_persist()</a></code> </p>
        </td>
        <td><p>Persist a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_project">sdf_project()</a></code> </p>
        </td>
        <td><p>Project features onto principal components</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_quantile">sdf_quantile()</a></code> </p>
        </td>
        <td><p>Compute (Approximate) Quantiles with a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_schema">sdf_schema()</a></code> </p>
        </td>
        <td><p>Read the Schema of a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_sql">sdf_sql()</a></code> </p>
        </td>
        <td><p>Spark DataFrame from SQL</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_with_sequential_id">sdf_with_sequential_id()</a></code> </p>
        </td>
        <td><p>Add a Sequential ID Column to a Spark DataFrame</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-spark-machine-learning" class="hasAnchor"><a href="#section-spark-machine-learning" class="anchor"></a>Spark Machine Learning</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="ml_decision_tree">ml_decision_tree_classifier()</a></code> <code><a href="ml_decision_tree">ml_decision_tree()</a></code> <code><a href="ml_decision_tree">ml_decision_tree_regressor()</a></code> </p>
        </td>
        <td><p>Spark ML -- Decision Trees</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_generalized_linear_regression">ml_generalized_linear_regression()</a></code> </p>
        </td>
        <td><p>Spark ML -- Generalized Linear Regression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_gradient_boosted_trees">ml_gbt_classifier()</a></code> <code><a href="ml_gradient_boosted_trees">ml_gradient_boosted_trees()</a></code> <code><a href="ml_gradient_boosted_trees">ml_gbt_regressor()</a></code> </p>
        </td>
        <td><p>Spark ML -- Gradient Boosted Trees</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_kmeans">ml_kmeans()</a></code> <code><a href="ml_kmeans">ml_compute_cost()</a></code> </p>
        </td>
        <td><p>Spark ML -- K-Means Clustering</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_lda">ml_lda()</a></code> <code><a href="ml_lda">ml_describe_topics()</a></code> <code><a href="ml_lda">ml_log_likelihood()</a></code> <code><a href="ml_lda">ml_log_perplexity()</a></code> <code><a href="ml_lda">ml_topics_matrix()</a></code> </p>
        </td>
        <td><p>Spark ML -- Latent Dirichlet Allocation</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_linear_regression">ml_linear_regression()</a></code> </p>
        </td>
        <td><p>Spark ML -- Linear Regression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_logistic_regression">ml_logistic_regression()</a></code> </p>
        </td>
        <td><p>Spark ML -- Logistic Regression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_model_data">ml_model_data()</a></code> </p>
        </td>
        <td><p>Extracts data associated with a Spark ML model</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_multilayer_perceptron_classifier">ml_multilayer_perceptron_classifier()</a></code> <code><a href="ml_multilayer_perceptron_classifier">ml_multilayer_perceptron()</a></code> </p>
        </td>
        <td><p>Spark ML -- Multilayer Perceptron</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_naive_bayes">ml_naive_bayes()</a></code> </p>
        </td>
        <td><p>Spark ML -- Naive-Bayes</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_one_vs_rest">ml_one_vs_rest()</a></code> </p>
        </td>
        <td><p>Spark ML -- OneVsRest</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_pca">ft_pca()</a></code> <code><a href="ft_pca">ml_pca()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- PCA (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_random_forest">ml_random_forest_classifier()</a></code> <code><a href="ml_random_forest">ml_random_forest()</a></code> <code><a href="ml_random_forest">ml_random_forest_regressor()</a></code> </p>
        </td>
        <td><p>Spark ML -- Random Forest</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_aft_survival_regression">ml_aft_survival_regression()</a></code> <code><a href="ml_aft_survival_regression">ml_survival_regression()</a></code> </p>
        </td>
        <td><p>Spark ML -- Survival Regression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_add_stage">ml_add_stage()</a></code> </p>
        </td>
        <td><p>Add a Stage to a Pipeline</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_als">ml_als()</a></code> <code><a href="ml_als">ml_recommend()</a></code> </p>
        </td>
        <td><p>Spark ML -- ALS</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_lsh_utils">ml_approx_nearest_neighbors()</a></code> <code><a href="ft_lsh_utils">ml_approx_similarity_join()</a></code> </p>
        </td>
        <td><p>Utility functions for LSH models</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_fpgrowth">ml_fpgrowth()</a></code> <code><a href="ml_fpgrowth">ml_association_rules()</a></code> <code><a href="ml_fpgrowth">ml_freq_itemsets()</a></code> </p>
        </td>
        <td><p>Frequent Pattern Mining -- FPGrowth</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_evaluator">ml_binary_classification_evaluator()</a></code> <code><a href="ml_evaluator">ml_binary_classification_eval()</a></code> <code><a href="ml_evaluator">ml_multiclass_classification_evaluator()</a></code> <code><a href="ml_evaluator">ml_classification_eval()</a></code> <code><a href="ml_evaluator">ml_regression_evaluator()</a></code> </p>
        </td>
        <td><p>Spark ML - Evaluators</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_bisecting_kmeans">ml_bisecting_kmeans()</a></code> </p>
        </td>
        <td><p>Spark ML -- Bisecting K-Means Clustering</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_call_constructor">ml_call_constructor()</a></code> </p>
        </td>
        <td><p>Wrap a Spark ML JVM object</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_chisquare_test">ml_chisquare_test()</a></code> </p>
        </td>
        <td><p>Chi-square hypothesis testing for categorical data.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_clustering_evaluator">ml_clustering_evaluator()</a></code> </p>
        </td>
        <td><p>Spark ML - Clustering Evaluator</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml-model-constructors">new_ml_model_prediction()</a></code> <code><a href="ml-model-constructors">new_ml_model()</a></code> <code><a href="ml-model-constructors">new_ml_model_classification()</a></code> <code><a href="ml-model-constructors">new_ml_model_regression()</a></code> <code><a href="ml-model-constructors">new_ml_model_clustering()</a></code> <code><a href="ml-model-constructors">ml_supervised_pipeline()</a></code> <code><a href="ml-model-constructors">ml_clustering_pipeline()</a></code> <code><a href="ml-model-constructors">ml_construct_model_supervised()</a></code> <code><a href="ml-model-constructors">ml_construct_model_clustering()</a></code> </p>
        </td>
        <td><p>Constructors for `ml_model` Objects</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_corr">ml_corr()</a></code> </p>
        </td>
        <td><p>Compute correlation matrix</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml-tuning">ml_sub_models()</a></code> <code><a href="ml-tuning">ml_validation_metrics()</a></code> <code><a href="ml-tuning">ml_cross_validator()</a></code> <code><a href="ml-tuning">ml_train_validation_split()</a></code> </p>
        </td>
        <td><p>Spark ML -- Tuning</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_default_stop_words">ml_default_stop_words()</a></code> </p>
        </td>
        <td><p>Default stop words</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_evaluate">ml_evaluate()</a></code> </p>
        </td>
        <td><p>Evaluate the Model on a Validation Set</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_feature_importances">ml_feature_importances()</a></code> <code><a href="ml_feature_importances">ml_tree_feature_importance()</a></code> </p>
        </td>
        <td><p>Spark ML - Feature Importance for Tree Models</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_word2vec">ft_word2vec()</a></code> <code><a href="ft_word2vec">ml_find_synonyms()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Word2Vec (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml-transform-methods">is_ml_transformer()</a></code> <code><a href="ml-transform-methods">is_ml_estimator()</a></code> <code><a href="ml-transform-methods">ml_fit()</a></code> <code><a href="ml-transform-methods">ml_transform()</a></code> <code><a href="ml-transform-methods">ml_fit_and_transform()</a></code> <code><a href="ml-transform-methods">ml_predict()</a></code> </p>
        </td>
        <td><p>Spark ML -- Transform, fit, and predict methods (ml_ interface)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_gaussian_mixture">ml_gaussian_mixture()</a></code> </p>
        </td>
        <td><p>Spark ML -- Gaussian Mixture clustering.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml-params">ml_is_set()</a></code> <code><a href="ml-params">ml_param_map()</a></code> <code><a href="ml-params">ml_param()</a></code> <code><a href="ml-params">ml_params()</a></code> </p>
        </td>
        <td><p>Spark ML -- ML Params</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_isotonic_regression">ml_isotonic_regression()</a></code> </p>
        </td>
        <td><p>Spark ML -- Isotonic Regression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_string_indexer">ft_string_indexer()</a></code> <code><a href="ft_string_indexer">ml_labels()</a></code> <code><a href="ft_string_indexer">ft_string_indexer_model()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- StringIndexer (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_linear_svc">ml_linear_svc()</a></code> </p>
        </td>
        <td><p>Spark ML -- LinearSVC</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml-persistence">ml_save()</a></code> <code><a href="ml-persistence">ml_load()</a></code> </p>
        </td>
        <td><p>Spark ML -- Model Persistence</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_pipeline">ml_pipeline()</a></code> </p>
        </td>
        <td><p>Spark ML -- Pipelines</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_stage">ml_stage()</a></code> <code><a href="ml_stage">ml_stages()</a></code> </p>
        </td>
        <td><p>Spark ML -- Pipeline stage extraction</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_standardize_formula">ml_standardize_formula()</a></code> </p>
        </td>
        <td><p>Standardize Formula Input for `ml_model`</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_summary">ml_summary()</a></code> </p>
        </td>
        <td><p>Spark ML -- Extraction of summary metrics</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_uid">ml_uid()</a></code> </p>
        </td>
        <td><p>Spark ML -- UID</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_count_vectorizer">ft_count_vectorizer()</a></code> <code><a href="ft_count_vectorizer">ml_vocabulary()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- CountVectorizer (Estimator)</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-spark-feature-transformers" class="hasAnchor"><a href="#section-spark-feature-transformers" class="anchor"></a>Spark Feature Transformers</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="ft_binarizer">ft_binarizer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Binarizer (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_bucketizer">ft_bucketizer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Bucketizer (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_count_vectorizer">ft_count_vectorizer()</a></code> <code><a href="ft_count_vectorizer">ml_vocabulary()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- CountVectorizer (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_dct">ft_dct()</a></code> <code><a href="ft_dct">ft_discrete_cosine_transform()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Discrete Cosine Transform (DCT) (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_elementwise_product">ft_elementwise_product()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- ElementwiseProduct (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_index_to_string">ft_index_to_string()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- IndexToString (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_one_hot_encoder">ft_one_hot_encoder()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- OneHotEncoder (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_quantile_discretizer">ft_quantile_discretizer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- QuantileDiscretizer (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sql-transformer">ft_sql_transformer()</a></code> <code><a href="sql-transformer">ft_dplyr_transformer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- SQLTransformer</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_string_indexer">ft_string_indexer()</a></code> <code><a href="ft_string_indexer">ml_labels()</a></code> <code><a href="ft_string_indexer">ft_string_indexer_model()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- StringIndexer (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_vector_assembler">ft_vector_assembler()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- VectorAssembler (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_tokenizer">ft_tokenizer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Tokenizer (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_regex_tokenizer">ft_regex_tokenizer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- RegexTokenizer (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_lsh">ft_bucketed_random_projection_lsh()</a></code> <code><a href="ft_lsh">ft_minhash_lsh()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- LSH (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_chisq_selector">ft_chisq_selector()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- ChiSqSelector (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_feature_hasher">ft_feature_hasher()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- FeatureHasher (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_hashing_tf">ft_hashing_tf()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- HashingTF (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_idf">ft_idf()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- IDF (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_imputer">ft_imputer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Imputer (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_interaction">ft_interaction()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Interaction (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_max_abs_scaler">ft_max_abs_scaler()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- MaxAbsScaler (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_min_max_scaler">ft_min_max_scaler()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- MinMaxScaler (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_ngram">ft_ngram()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- NGram (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_normalizer">ft_normalizer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Normalizer (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- OneHotEncoderEstimator (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_pca">ft_pca()</a></code> <code><a href="ft_pca">ml_pca()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- PCA (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_polynomial_expansion">ft_polynomial_expansion()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- PolynomialExpansion (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_r_formula">ft_r_formula()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- RFormula (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_standard_scaler">ft_standard_scaler()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- StandardScaler (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_stop_words_remover">ft_stop_words_remover()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- StopWordsRemover (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_vector_indexer">ft_vector_indexer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- VectorIndexer (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_vector_slicer">ft_vector_slicer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- VectorSlicer (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_word2vec">ft_word2vec()</a></code> <code><a href="ft_word2vec">ml_find_synonyms()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Word2Vec (Estimator)</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-spark-machine-learning-utilities" class="hasAnchor"><a href="#section-spark-machine-learning-utilities" class="anchor"></a>Spark Machine Learning Utilities</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="ml_evaluator">ml_binary_classification_evaluator()</a></code> <code><a href="ml_evaluator">ml_binary_classification_eval()</a></code> <code><a href="ml_evaluator">ml_multiclass_classification_evaluator()</a></code> <code><a href="ml_evaluator">ml_classification_eval()</a></code> <code><a href="ml_evaluator">ml_regression_evaluator()</a></code> </p>
        </td>
        <td><p>Spark ML - Evaluators</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_feature_importances">ml_feature_importances()</a></code> <code><a href="ml_feature_importances">ml_tree_feature_importance()</a></code> </p>
        </td>
        <td><p>Spark ML - Feature Importance for Tree Models</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_als_tidiers">tidy(<i>&lt;ml_model_als&gt;</i>)</a></code> <code><a href="ml_als_tidiers">augment(<i>&lt;ml_model_als&gt;</i>)</a></code> <code><a href="ml_als_tidiers">glance(<i>&lt;ml_model_als&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML ALS</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_glm_tidiers">tidy(<i>&lt;ml_model_generalized_linear_regression&gt;</i>)</a></code> <code><a href="ml_glm_tidiers">tidy(<i>&lt;ml_model_linear_regression&gt;</i>)</a></code> <code><a href="ml_glm_tidiers">augment(<i>&lt;ml_model_generalized_linear_regression&gt;</i>)</a></code> <code><a href="ml_glm_tidiers">augment(<i>&lt;ml_model_linear_regression&gt;</i>)</a></code> <code><a href="ml_glm_tidiers">glance(<i>&lt;ml_model_generalized_linear_regression&gt;</i>)</a></code> <code><a href="ml_glm_tidiers">glance(<i>&lt;ml_model_linear_regression&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML linear models</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_isotonic_regression_tidiers">tidy(<i>&lt;ml_model_isotonic_regression&gt;</i>)</a></code> <code><a href="ml_isotonic_regression_tidiers">augment(<i>&lt;ml_model_isotonic_regression&gt;</i>)</a></code> <code><a href="ml_isotonic_regression_tidiers">glance(<i>&lt;ml_model_isotonic_regression&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML Isotonic Regression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_lda_tidiers">tidy(<i>&lt;ml_model_lda&gt;</i>)</a></code> <code><a href="ml_lda_tidiers">augment(<i>&lt;ml_model_lda&gt;</i>)</a></code> <code><a href="ml_lda_tidiers">glance(<i>&lt;ml_model_lda&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML LDA models</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_linear_svc_tidiers">tidy(<i>&lt;ml_model_linear_svc&gt;</i>)</a></code> <code><a href="ml_linear_svc_tidiers">augment(<i>&lt;ml_model_linear_svc&gt;</i>)</a></code> <code><a href="ml_linear_svc_tidiers">glance(<i>&lt;ml_model_linear_svc&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML linear svc</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_logistic_regression_tidiers">tidy(<i>&lt;ml_model_logistic_regression&gt;</i>)</a></code> <code><a href="ml_logistic_regression_tidiers">augment(<i>&lt;ml_model_logistic_regression&gt;</i>)</a></code> <code><a href="ml_logistic_regression_tidiers">glance(<i>&lt;ml_model_logistic_regression&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML Logistic Regression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_multilayer_perceptron_tidiers">tidy(<i>&lt;ml_model_multilayer_perceptron_classification&gt;</i>)</a></code> <code><a href="ml_multilayer_perceptron_tidiers">augment(<i>&lt;ml_model_multilayer_perceptron_classification&gt;</i>)</a></code> <code><a href="ml_multilayer_perceptron_tidiers">glance(<i>&lt;ml_model_multilayer_perceptron_classification&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML MLP</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_naive_bayes_tidiers">tidy(<i>&lt;ml_model_naive_bayes&gt;</i>)</a></code> <code><a href="ml_naive_bayes_tidiers">augment(<i>&lt;ml_model_naive_bayes&gt;</i>)</a></code> <code><a href="ml_naive_bayes_tidiers">glance(<i>&lt;ml_model_naive_bayes&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML Naive Bayes</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_pca_tidiers">tidy(<i>&lt;ml_model_pca&gt;</i>)</a></code> <code><a href="ml_pca_tidiers">augment(<i>&lt;ml_model_pca&gt;</i>)</a></code> <code><a href="ml_pca_tidiers">glance(<i>&lt;ml_model_pca&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML Principal Component Analysis</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_survival_regression_tidiers">tidy(<i>&lt;ml_model_aft_survival_regression&gt;</i>)</a></code> <code><a href="ml_survival_regression_tidiers">augment(<i>&lt;ml_model_aft_survival_regression&gt;</i>)</a></code> <code><a href="ml_survival_regression_tidiers">glance(<i>&lt;ml_model_aft_survival_regression&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML Survival Regression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_tree_tidiers">tidy(<i>&lt;ml_model_decision_tree_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers">tidy(<i>&lt;ml_model_decision_tree_regression&gt;</i>)</a></code> <code><a href="ml_tree_tidiers">augment(<i>&lt;ml_model_decision_tree_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers">augment(<i>&lt;ml_model_decision_tree_regression&gt;</i>)</a></code> <code><a href="ml_tree_tidiers">glance(<i>&lt;ml_model_decision_tree_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers">glance(<i>&lt;ml_model_decision_tree_regression&gt;</i>)</a></code> <code><a href="ml_tree_tidiers">tidy(<i>&lt;ml_model_random_forest_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers">tidy(<i>&lt;ml_model_random_forest_regression&gt;</i>)</a></code> <code><a href="ml_tree_tidiers">augment(<i>&lt;ml_model_random_forest_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers">augment(<i>&lt;ml_model_random_forest_regression&gt;</i>)</a></code> <code><a href="ml_tree_tidiers">glance(<i>&lt;ml_model_random_forest_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers">glance(<i>&lt;ml_model_random_forest_regression&gt;</i>)</a></code> <code><a href="ml_tree_tidiers">tidy(<i>&lt;ml_model_gbt_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers">tidy(<i>&lt;ml_model_gbt_regression&gt;</i>)</a></code> <code><a href="ml_tree_tidiers">augment(<i>&lt;ml_model_gbt_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers">augment(<i>&lt;ml_model_gbt_regression&gt;</i>)</a></code> <code><a href="ml_tree_tidiers">glance(<i>&lt;ml_model_gbt_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers">glance(<i>&lt;ml_model_gbt_regression&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML tree models</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_unsupervised_tidiers">tidy(<i>&lt;ml_model_kmeans&gt;</i>)</a></code> <code><a href="ml_unsupervised_tidiers">augment(<i>&lt;ml_model_kmeans&gt;</i>)</a></code> <code><a href="ml_unsupervised_tidiers">glance(<i>&lt;ml_model_kmeans&gt;</i>)</a></code> <code><a href="ml_unsupervised_tidiers">tidy(<i>&lt;ml_model_bisecting_kmeans&gt;</i>)</a></code> <code><a href="ml_unsupervised_tidiers">augment(<i>&lt;ml_model_bisecting_kmeans&gt;</i>)</a></code> <code><a href="ml_unsupervised_tidiers">glance(<i>&lt;ml_model_bisecting_kmeans&gt;</i>)</a></code> <code><a href="ml_unsupervised_tidiers">tidy(<i>&lt;ml_model_gaussian_mixture&gt;</i>)</a></code> <code><a href="ml_unsupervised_tidiers">augment(<i>&lt;ml_model_gaussian_mixture&gt;</i>)</a></code> <code><a href="ml_unsupervised_tidiers">glance(<i>&lt;ml_model_gaussian_mixture&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML unsupervised models</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-extensions" class="hasAnchor"><a href="#section-extensions" class="anchor"></a>Extensions</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="compile_package_jars">compile_package_jars()</a></code> </p>
        </td>
        <td><p>Compile Scala sources into a Java Archive (jar)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="connection_config">connection_config()</a></code> </p>
        </td>
        <td><p>Read configuration values for a connection</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="download_scalac">download_scalac()</a></code> </p>
        </td>
        <td><p>Downloads default Scala Compilers</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="find_scalac">find_scalac()</a></code> </p>
        </td>
        <td><p>Discover the Scala Compiler</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark-api">spark_context()</a></code> <code><a href="spark-api">java_context()</a></code> <code><a href="spark-api">hive_context()</a></code> <code><a href="spark-api">spark_session()</a></code> </p>
        </td>
        <td><p>Access the Spark API</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="hive_context_config">hive_context_config()</a></code> </p>
        </td>
        <td><p>Runtime configuration interface for Hive</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="invoke">invoke()</a></code> <code><a href="invoke">invoke_static()</a></code> <code><a href="invoke">invoke_new()</a></code> </p>
        </td>
        <td><p>Invoke a Method on a JVM Object</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="register_extension">register_extension()</a></code> <code><a href="register_extension">registered_extensions()</a></code> </p>
        </td>
        <td><p>Register a Package that Implements a Spark Extension</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_compilation_spec">spark_compilation_spec()</a></code> </p>
        </td>
        <td><p>Define a Spark Compilation Specification</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_default_compilation_spec">spark_default_compilation_spec()</a></code> </p>
        </td>
        <td><p>Default Compilation Specification for Spark Extensions</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_context_config">spark_context_config()</a></code> </p>
        </td>
        <td><p>Runtime configuration interface for the Spark Context.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_dataframe">spark_dataframe()</a></code> </p>
        </td>
        <td><p>Retrieve a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_dependency">spark_dependency()</a></code> </p>
        </td>
        <td><p>Define a Spark dependency</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_home_set">spark_home_set()</a></code> </p>
        </td>
        <td><p>Set the SPARK_HOME environment variable</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_jobj">spark_jobj()</a></code> </p>
        </td>
        <td><p>Retrieve a Spark JVM Object Reference</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_version">spark_version()</a></code> </p>
        </td>
        <td><p>Get the Spark Version Associated with a Spark Connection</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-distributed-computing" class="hasAnchor"><a href="#section-distributed-computing" class="anchor"></a>Distributed Computing</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="spark_apply">spark_apply()</a></code> </p>
        </td>
        <td><p>Apply an R Function in Spark</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_apply_bundle">spark_apply_bundle()</a></code> </p>
        </td>
        <td><p>Create Bundle for Spark Apply</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_apply_log">spark_apply_log()</a></code> </p>
        </td>
        <td><p>Log Writer for Spark Apply</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="registerDoSpark">registerDoSpark()</a></code> </p>
        </td>
        <td><p>Register a Prallel Backend</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-livy" class="hasAnchor"><a href="#section-livy" class="anchor"></a>Livy</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="livy_install">livy_install()</a></code> <code><a href="livy_install">livy_available_versions()</a></code> <code><a href="livy_install">livy_install_dir()</a></code> <code><a href="livy_install">livy_installed_versions()</a></code> <code><a href="livy_install">livy_home_dir()</a></code> </p>
        </td>
        <td><p>Install Livy</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="livy_config">livy_config()</a></code> </p>
        </td>
        <td><p>Create a Spark Configuration for Livy</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="livy_service">livy_service_start()</a></code> <code><a href="livy_service">livy_service_stop()</a></code> </p>
        </td>
        <td><p>Start Livy</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-streaming" class="hasAnchor"><a href="#section-streaming" class="anchor"></a>Streaming</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="stream_find">stream_find()</a></code> </p>
        </td>
        <td><p>Find Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_generate_test">stream_generate_test()</a></code> </p>
        </td>
        <td><p>Generate Test Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_id">stream_id()</a></code> </p>
        </td>
        <td><p>Spark Stream's Identifier</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_name">stream_name()</a></code> </p>
        </td>
        <td><p>Spark Stream's Name</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_read_csv">stream_read_csv()</a></code> </p>
        </td>
        <td><p>Read CSV Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_read_json">stream_read_json()</a></code> </p>
        </td>
        <td><p>Read JSON Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_read_delta">stream_read_delta()</a></code> </p>
        </td>
        <td><p>Read Delta Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_read_kafka">stream_read_kafka()</a></code> </p>
        </td>
        <td><p>Read Kafka Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_read_orc">stream_read_orc()</a></code> </p>
        </td>
        <td><p>Read ORC Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_read_parquet">stream_read_parquet()</a></code> </p>
        </td>
        <td><p>Read Parquet Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_read_socket">stream_read_socket()</a></code> </p>
        </td>
        <td><p>Read Socket Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_read_text">stream_read_text()</a></code> </p>
        </td>
        <td><p>Read Text Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_render">stream_render()</a></code> </p>
        </td>
        <td><p>Render Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_stats">stream_stats()</a></code> </p>
        </td>
        <td><p>Stream Statistics</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_stop">stream_stop()</a></code> </p>
        </td>
        <td><p>Stops a Spark Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_trigger_continuous">stream_trigger_continuous()</a></code> </p>
        </td>
        <td><p>Spark Stream Continuous Trigger</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_trigger_interval">stream_trigger_interval()</a></code> </p>
        </td>
        <td><p>Spark Stream Interval Trigger</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_view">stream_view()</a></code> </p>
        </td>
        <td><p>View Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_watermark">stream_watermark()</a></code> </p>
        </td>
        <td><p>Watermark Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_console">stream_write_console()</a></code> </p>
        </td>
        <td><p>Write Console Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_csv">stream_write_csv()</a></code> </p>
        </td>
        <td><p>Write CSV Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_delta">stream_write_delta()</a></code> </p>
        </td>
        <td><p>Write Delta Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_json">stream_write_json()</a></code> </p>
        </td>
        <td><p>Write JSON Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_kafka">stream_write_kafka()</a></code> </p>
        </td>
        <td><p>Write Kafka Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_memory">stream_write_memory()</a></code> </p>
        </td>
        <td><p>Write Memory Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_orc">stream_write_orc()</a></code> </p>
        </td>
        <td><p>Write a ORC Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_parquet">stream_write_parquet()</a></code> </p>
        </td>
        <td><p>Write Parquet Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_text">stream_write_text()</a></code> </p>
        </td>
        <td><p>Write Text Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="reactiveSpark">reactiveSpark()</a></code> </p>
        </td>
        <td><p>Reactive spark reader</p></td>
      </tr>
    </tbody>
    </table>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top">
      <h2 data-toc-skip>Contents</h2>
    </nav>
  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Javier Luraschi, Kevin Kuo, Kevin Ushey, JJ Allaire, Hossein Falaki, Lu Wang, Andy Zhang, Yitao Li,  The Apache Software Foundation.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


