---
title: "Using sparklyr with Databricks"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
params:
  width: 600
---



<div id="overview" class="section level2">
<h2>Overview</h2>
<p>This documentation demonstrates how to use <code>sparklyr</code> with RStudio Server Pro,
RStudio Connect, and RStudio Package Manager to work with Databricks.</p>
</div>
<div id="best-practices" class="section level2">
<h2>Best practices</h2>
<ul>
<li><strong>Maintain separate installation environments</strong> - Install RStudio Server Pro,
RStudio Connect, and RStudio Package Manager outside of the Databricks cluster
so that they are not limited to the compute resources or ephemeral nature of
Databricks clusters.</li>
<li><strong>Connect to Databricks remotely</strong> - Work with Databricks as a remote compute
resource, similar to how you would connect remotely to external databases,
data sources, and storage systems. This can be accomplished using Databricks
Connect (as described in the
<a href="#option-1---connecting-to-databricks-remotely">Connecting to Databricks remotely</a>
section below) or by performing SQL queries with JDBC/ODBC using the
Databricks Spark SQL Driver on
<a href="https://docs.databricks.com/integrations/bi/jdbc-odbc-bi.html">AWS</a> or
<a href="https://docs.microsoft.com/en-us/azure/databricks/integrations/bi/jdbc-odbc-bi">Azure</a>.</li>
<li><strong>Restrict workloads to interactive analysis</strong> - Only perform workloads
related to exploratory or interactive analysis with Spark, then write the
results to a database, file system, or cloud storage for more efficient
retrieval in apps, reports, and APIs.</li>
<li><strong>Load and query results efficiently</strong> - Because of the nature of Spark
computations and the associated overhead, Shiny apps that use Spark on the
backend tend to have performance and runtime issues; consider reading the
results from a database, file system, or cloud storage instead.</li>
</ul>
</div>
<div id="using-rstudio-server-pro-with-databricks" class="section level2">
<h2>Using RStudio Server Pro with Databricks</h2>
<p>There are two options for using <code>sparklyr</code> and RStudio Server Pro with
Databricks:</p>
<ul>
<li>Option 1:
<a href="#option-1---connecting-to-databricks-remotely">Connecting to Databricks remotely</a>
(Recommended Option)</li>
<li>Option 2:
<a href="#option-2---working-inside-of-databricks">Working inside of Databricks</a>
(Alternative Option)</li>
</ul>
<div id="option-1---connecting-to-databricks-remotely" class="section level3">
<h3>Option 1 - Connecting to Databricks remotely</h3>
<p>With this configuration, RStudio Server Pro is installed outside of the Spark
cluster and allows users to connect to Spark remotely using <code>sparklyr</code> with
<a href="https://docs.databricks.com/dev-tools/databricks-connect.html">Databricks Connect</a>.</p>
<p>This is the recommended configuration because it targets separate environments,
involves a typical configuration process, avoids resource contention, and allows
RStudio Server Pro to connect to Databricks as well as other remote storage and
compute resources.</p>
<a href="/examples/databricks-cluster-remote">
<h2>
View steps for connecting to Databricks remotely
</h2>
<p></a></p>
<p><a href="/examples/databricks-cluster-remote">
<img src="/images/deployment/databricks/rstudio-databricks-remote.png" width='800px' align='center'/>
</a></p>
</div>
<div id="option-2---working-inside-of-databricks" class="section level3">
<h3>Option 2 - Working inside of Databricks</h3>
<p>If you cannot work with Spark remotely, you should install RStudio Server Pro on
the Driver node of a long-running, persistent Databricks cluster as opposed to a
worker node or an ephemeral cluster.</p>
<p>With this configuration, RStudio Server Pro is installed on the Spark driver
node and allows users to connect to Spark locally using <code>sparklyr</code>.</p>
<p>This configuration can result in increased complexity, limited connectivity to
other storage and compute resources, resource contention between RStudio Server
Pro and Databricks, and maintenance concerns due to the ephemeral nature of
Databricks clusters.</p>
<a href="/examples/databricks-cluster-local">
<h2>
View steps for working inside of Databricks
</h2>
<p></a></p>
<p><a href="/examples/databricks-cluster-local">
<img src="/images/deployment/databricks/rstudio-databricks-local.png" width='800px' align='center'/>
</a></p>
</div>
</div>
<div id="using-rstudio-connect-with-databricks" class="section level2">
<h2>Using RStudio Connect with Databricks</h2>
<p>The server environment within Databricks clusters is not permissive enough to
support RStudio Connect or the process sandboxing mechanisms that it uses to
isolate published content.</p>
<p>Therefore, the only supported configuration is to install RStudio Connect
outside of the Databricks cluster and connect to Databricks remotely.</p>
<p>Whether RStudio Server Pro is installed outside of the Databricks cluster
(Recommended Option) or within the Databricks cluster (Alternative Option), you
can publish content to RStudio Connect as long as HTTP/HTTPS network traffic is
allowed from RStudio Server Pro to RStudio Connect.</p>
<p>There are two options for using RStudio Connect with Databricks:</p>
<ol style="list-style-type: decimal">
<li>Performing SQL queries with JDBC/ODBC using the Databricks Spark SQL Driver
on <a href="https://docs.databricks.com/integrations/bi/jdbc-odbc-bi.html">AWS</a> or
<a href="https://docs.microsoft.com/en-us/azure/databricks/integrations/bi/jdbc-odbc-bi">Azure</a>
(Recommended Option)</li>
<li>Adding calls in your R code to create and run Databricks jobs
<a href="https://github.com/RafiKurlansik/bricksteR/">with bricksteR and the Databricks Jobs API</a>
(Alternative Option)</li>
</ol>
<p><img src="/images/deployment/databricks/rstudio-connect-databricks.png" width='800px' align='center'/></p>
</div>
<div id="using-rstudio-package-manager-with-databricks" class="section level2">
<h2>Using RStudio Package Manager with Databricks</h2>
<p>Whether RStudio Server Pro is installed outside of the Databricks cluster
(Recommended Option) or within the Databricks cluster (Alternative Option), you
can install packages from repositories in RStudio Package Manager as long as
HTTP/HTTPS network traffic is allowed from RStudio Server Pro to RStudio Package
Manager.</p>
</div>
