{
  "hash": "d5cecddd46f73d897c3d6b05514436ee",
  "result": {
    "markdown": "---\ntitle: \"Spark ML Pipelines\"\nformat:\n  html:\n    theme: default\n    toc: true\neditor_options: \n  markdown: \n    wrap: 72\nexecute:\n  freeze: true\n  eval: true\n---\n\nSpark's **ML Pipelines** provide a way to easily combine multiple\ntransformations and algorithms into a single workflow, or pipeline.\n\nFor R users, the insights gathered during the interactive sessions with\nSpark can now be converted to a formal pipeline. This makes the hand-off\nfrom Data Scientists to Big Data Engineers a lot easier, this is because\nthere should not be additional changes needed to be made by the later\ngroup.\n\nThe final list of selected variables, data manipulation, feature\ntransformations and modeling can be easily re-written into a\n`ml_pipeline()` object, saved, and ultimately placed into a Production\nenvironment. The `sparklyr` output of a saved Spark ML Pipeline object\nis in Scala code, which means that the code can be added to the\nscheduled Spark ML jobs, and without any dependencies in R.\n\n## Introduction to ML Pipelines\n\nThe official Apache Spark site contains a more complete overview of [ML\nPipelines](http://spark.apache.org/docs/latest/ml-pipeline.html). This\narticle will focus in introducing the basic concepts and steps to work\nwith ML Pipelines via `sparklyr`.\n\nThere are two important stages in building an ML Pipeline. The first one\nis creating a **Pipeline**. A good way to look at it, or call it, is as\nan **\"empty\" pipeline**. This step just builds the steps that the data\nwill go through. This is the somewhat equivalent of doing this in R:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\nr_pipeline <-  . %>% mutate(cyl = paste0(\"c\", cyl)) %>% lm(am ~ cyl + mpg, data = .)\nr_pipeline\n```\n\n::: {.cell-output-stdout}\n```\nFunctional sequence with the following components:\n\n 1. mutate(., cyl = paste0(\"c\", cyl))\n 2. lm(am ~ cyl + mpg, data = .)\n\nUse 'functions' to extract the individual functions. \n```\n:::\n:::\n\nThe `r_pipeline` object has all the steps needed to transform and fit\nthe model, but it has not yet transformed any data. The second step, is\nto pass data through the pipeline, which in turn will output a fitted\nmodel. That is called a **PipelineModel**. The **PipelineModel** can\nthen be used to produce predictions.\n\n::: {.cell}\n\n```{.r .cell-code}\nr_model <- r_pipeline(mtcars)\nr_model\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = am ~ cyl + mpg, data = .)\n\nCoefficients:\n(Intercept)        cylc6        cylc8          mpg  \n   -0.54388      0.03124     -0.03313      0.04767  \n```\n:::\n:::\n\n### Taking advantage of Pipelines and PipelineModels\n\nThe two stage ML Pipeline approach produces two final data products:\n\n-   A **PipelineModel** that can be added to the daily Spark jobs which\n    will produce new predictions for the incoming data, and again, with\n    no R dependencies.\n\n-   A **Pipeline** that can be **easily re-fitted** on a regular\n    interval, say every month. All that is needed is to pass a new\n    sample to obtain the new coefficients.\n\n## Pipeline\n\nAn additional goal of this article is that the reader can follow along,\nso the data, transformations and Spark connection in this example will\nbe kept as easy to reproduce as possible.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nycflights13)\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\")\n\nspark_flights <- copy_to(sc, flights)\n```\n:::\n\n\n### Feature Transformers\n\nPipelines make heavy use of [Feature\nTransformers](http://spark.rstudio.com/reference/#section-spark-feature-transformers).\nIf new to Spark, and `sparklyr`, it would be good to review what these\ntransformers do. These functions use the Spark API directly to transform\nthe data, and may be faster at making the data manipulations that a\n`dplyr` (SQL) transformation.\n\nIn `sparklyr` the `ft` functions are essentially are wrappers to\noriginal [Spark feature\ntransformer](http://spark.apache.org/docs/latest/ml-features.html).\n\n### ft\\_dplyr\\_transformer\n\nThis example will start with `dplyr` transformations, which are\nultimately SQL transformations, loaded into the `df` variable.\n\nIn `sparklyr`, there is one feature transformer that is not available in\nSpark, `ft_dplyr_transformer()`. The goal of this function is to convert\nthe `dplyr` code to a SQL Feature Transformer that can then be used in a\nPipeline.\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- spark_flights %>%\n  filter(!is.na(dep_delay)) %>%\n  mutate(\n    month = paste0(\"m\", month),\n    day = paste0(\"d\", day)\n  ) %>%\n  select(dep_delay, sched_dep_time, month, day, distance) \n```\n:::\n\n\nThis is the resulting pipeline stage produced from the `dplyr` code:\n\n::: {.cell}\n\n```{.r .cell-code}\nft_dplyr_transformer(sc, df)\n```\n\n::: {.cell-output-stdout}\n```\nSQLTransformer (Transformer)\n<dplyr_transformer__8ba1e82c_e9b0_4914_b571_2a71c8e1f4ea> \n (Parameters -- Column Names)\n```\n:::\n:::\n\n\nUse the `ml_param()` function to extract the “statement” attribute. That\nattribute contains the finalized SQL statement. Notice that the\n`flights` table name has been replace with `__THIS__`. This allows the\npipeline to accept different table names as its source, making the\npipeline very modular.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nft_dplyr_transformer(sc, df) %>%\n  ml_param(\"statement\")\n```\n\n::: {.cell-output-stdout}\n```\n[1] \"SELECT `dep_delay`, `sched_dep_time`, CONCAT(\\\"m\\\", `month`) AS `month`, CONCAT(\\\"d\\\", `day`) AS `day`, `distance`\\nFROM `__THIS__`\\nWHERE (NOT(((`dep_delay`) IS NULL)))\"\n```\n:::\n:::\n\n\n### Creating the Pipeline\n\nThe following step will create a 5 stage pipeline:\n\n1.  SQL transformer - Resulting from the `ft_dplyr_transformer()`\n    transformation\n2.  Binarizer - To determine if the flight should be considered delay.\n    The eventual outcome variable.\n3.  Bucketizer - To split the day into specific hour buckets\n4.  R Formula - To define the model’s formula\n5.  Logistic Model\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_pipeline <- ml_pipeline(sc) %>%\n  ft_dplyr_transformer(\n    tbl = df\n    ) %>%\n  ft_binarizer(\n    input_col = \"dep_delay\",\n    output_col = \"delayed\",\n    threshold = 15\n  ) %>%\n  ft_bucketizer(\n    input_col = \"sched_dep_time\",\n    output_col = \"hours\",\n    splits = c(400, 800, 1200, 1600, 2000, 2400)\n  )  %>%\n  ft_r_formula(delayed ~ month + day + hours + distance) %>% \n  ml_logistic_regression()\n```\n:::\n\n\nAnother nice feature for ML Pipelines in `sparklyr`, is the print-out.\nIt makes it really easy to how each stage is setup:\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_pipeline\n```\n\n::: {.cell-output-stdout}\n```\nPipeline (Estimator) with 5 stages\n<pipeline__e4745ba2_5bf8_4555_88d9_4aa66aa7b6ba> \n  Stages \n  |--1 SQLTransformer (Transformer)\n  |    <dplyr_transformer__e0e59acc_d897_4a93_9760_474ebc786add> \n  |     (Parameters -- Column Names)\n  |--2 Binarizer (Transformer)\n  |    <binarizer__5b3946fe_1209_4d5d_805c_ee7c79bea1f2> \n  |     (Parameters -- Column Names)\n  |      input_col: dep_delay\n  |      output_col: delayed\n  |--3 Bucketizer (Transformer)\n  |    <bucketizer__84db4fe8_4994_4ad3_9e04_a6bbb70e5cec> \n  |     (Parameters -- Column Names)\n  |      input_col: sched_dep_time\n  |      output_col: hours\n  |--4 RFormula (Estimator)\n  |    <r_formula__b6e8b330_5824_4719_9417_d14bd05de34f> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |     (Parameters)\n  |      force_index_label: FALSE\n  |      formula: delayed ~ month + day + hours + distance\n  |      handle_invalid: error\n  |      stringIndexerOrderType: frequencyDesc\n  |--5 LogisticRegression (Estimator)\n  |    <logistic_regression__3991e32e_fa8c_46f8_b337_1ea749ffb337> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |      prediction_col: prediction\n  |      probability_col: probability\n  |      raw_prediction_col: rawPrediction\n  |     (Parameters)\n  |      aggregation_depth: 2\n  |      elastic_net_param: 0\n  |      family: auto\n  |      fit_intercept: TRUE\n  |      max_iter: 100\n  |      maxBlockSizeInMB: 0\n  |      reg_param: 0\n  |      standardization: TRUE\n  |      threshold: 0.5\n  |      tol: 1e-06\n```\n:::\n:::\n\n\nNotice that there are no *coefficients* defined yet. That’s because no\ndata has been actually processed. Even though `df` uses\n`spark_flights()`, recall that the final SQL transformer makes that\nname, so there’s no data to process yet.\n\nPipelineModel\n-------------\n\nA quick partition of the data is created for this exercise.\n\n::: {.cell}\n\n```{.r .cell-code}\npartitioned_flights <- sdf_random_split(\n  spark_flights,\n  training = 0.01,\n  testing = 0.01,\n  rest = 0.98\n)\n```\n:::\n\n\nThe `ml_fit()` function produces the PipelineModel. The `training`\npartition of the `partitioned_flights` data is used to train the model:\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted_pipeline <- ml_fit(\n  flights_pipeline,\n  partitioned_flights$training\n)\nfitted_pipeline\n```\n\n::: {.cell-output-stdout}\n```\nPipelineModel (Transformer) with 5 stages\n<pipeline__e4745ba2_5bf8_4555_88d9_4aa66aa7b6ba> \n  Stages \n  |--1 SQLTransformer (Transformer)\n  |    <dplyr_transformer__e0e59acc_d897_4a93_9760_474ebc786add> \n  |     (Parameters -- Column Names)\n  |--2 Binarizer (Transformer)\n  |    <binarizer__5b3946fe_1209_4d5d_805c_ee7c79bea1f2> \n  |     (Parameters -- Column Names)\n  |      input_col: dep_delay\n  |      output_col: delayed\n  |--3 Bucketizer (Transformer)\n  |    <bucketizer__84db4fe8_4994_4ad3_9e04_a6bbb70e5cec> \n  |     (Parameters -- Column Names)\n  |      input_col: sched_dep_time\n  |      output_col: hours\n  |--4 RFormulaModel (Transformer)\n  |    <r_formula__b6e8b330_5824_4719_9417_d14bd05de34f> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |     (Transformer Info)\n  |      formula:  chr \"delayed ~ month + day + hours + distance\" \n  |--5 LogisticRegressionModel (Transformer)\n  |    <logistic_regression__3991e32e_fa8c_46f8_b337_1ea749ffb337> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |      prediction_col: prediction\n  |      probability_col: probability\n  |      raw_prediction_col: rawPrediction\n  |     (Transformer Info)\n  |      coefficient_matrix:  num [1, 1:43] 0.47246 0.8963 0.23965 0.00185 -0.03954 ... \n  |      coefficients:  num [1:43] 0.47246 0.8963 0.23965 0.00185 -0.03954 ... \n  |      intercept:  num -2.8 \n  |      intercept_vector:  num -2.8 \n  |      num_classes:  int 2 \n  |      num_features:  int 43 \n  |      threshold:  num 0.5 \n  |      thresholds:  num [1:2] 0.5 0.5 \n```\n:::\n:::\n\nNotice that the print-out for the fitted pipeline now displays the\nmodel’s coefficients.\n\nThe `ml_transform()` function can be used to run predictions, in other\nwords it is used instead of `predict()` or `sdf_predict()`.\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions <- ml_transform(\n  fitted_pipeline,\n  partitioned_flights$testing\n)\n\npredictions %>%\n  group_by(delayed, prediction) %>%\n  tally()\n```\n\n::: {.cell-output-stdout}\n```\n# Source: spark<?> [?? x 3]\n# Groups: delayed\n  delayed prediction     n\n    <dbl>      <dbl> <dbl>\n1       0          1    35\n2       0          0  2569\n3       1          0   650\n4       1          1    56\n```\n:::\n:::\n\nSave the pipelines to disk\n--------------------------\n\nThe `ml_save()` command can be used to save the Pipeline and\nPipelineModel to disk. The resulting output is a folder with the\nselected name, which contains all of the necessary Scala scripts:\n\n::: {.cell}\n\n```{.r .cell-code}\nml_save(\n  flights_pipeline,\n  \"flights_pipeline\",\n  overwrite = TRUE\n)\n```\n\n::: {.cell-output-stderr}\n```\nModel successfully saved.\n```\n:::\n\n```{.r .cell-code}\nml_save(\n  fitted_pipeline,\n  \"flights_model\",\n  overwrite = TRUE\n)\n```\n\n::: {.cell-output-stderr}\n```\nModel successfully saved.\n```\n:::\n:::\n\n\nUse an existing PipelineModel\n-----------------------------\n\nThe `ml_load()` command can be used to re-load Pipelines and\nPipelineModels. The saved ML Pipeline files can only be loaded into an\nopen Spark session.\n\n::: {.cell}\n\n```{.r .cell-code}\nreloaded_model <- ml_load(sc, \"flights_model\")\n```\n:::\n\nA simple query can be used as the table that will be used to make the\nnew predictions. This of course, does not have to done in R, at this\ntime the “flights\\_model” can be loaded into an independent Spark\nsession outside of R.\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_df <- spark_flights %>%\n  filter(\n    month == 7,\n    day == 5\n  )\n\nml_transform(reloaded_model, new_df)\n```\n\n::: {.cell-output-stdout}\n```\n# Source: spark<?> [?? x 12]\n   dep_delay sched_dep_time month day   distance delayed hours features   label\n       <dbl>          <int> <chr> <chr>    <dbl>   <dbl> <dbl> <list>     <dbl>\n 1        39           2359 m7    d5        1617       1     4 <dbl [43]>     1\n 2       141           2245 m7    d5        2475       1     4 <dbl [43]>     1\n 3         0            500 m7    d5         529       0     0 <dbl [43]>     0\n 4        -5            536 m7    d5        1400       0     0 <dbl [43]>     0\n 5        -2            540 m7    d5        1089       0     0 <dbl [43]>     0\n 6        -7            545 m7    d5        1416       0     0 <dbl [43]>     0\n 7        -3            545 m7    d5        1576       0     0 <dbl [43]>     0\n 8        -7            600 m7    d5        1076       0     0 <dbl [43]>     0\n 9        -7            600 m7    d5          96       0     0 <dbl [43]>     0\n10        -6            600 m7    d5         937       0     0 <dbl [43]>     0\n# … with more rows, and 3 more variables: rawPrediction <list>,\n#   probability <list>, prediction <dbl>\n```\n:::\n:::\n\n \nRe-fit an existing Pipeline\n---------------------------\n\nFirst, reload the pipeline into an open Spark session:\n\n::: {.cell}\n\n```{.r .cell-code}\nreloaded_pipeline <- ml_load(sc, \"flights_pipeline\")\n```\n:::\n\nUse `ml_fit()` again to pass new data, in this case, `sample_frac()` is\nused instead of `sdf_partition()` to provide the new data. The idea\nbeing that the re-fitting would happen at a later date than when the\nmodel was initially fitted.\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_model <-  ml_fit(reloaded_pipeline, sample_frac(spark_flights, 0.01))\n\nnew_model\n```\n\n::: {.cell-output-stdout}\n```\nPipelineModel (Transformer) with 5 stages\n<pipeline__e4745ba2_5bf8_4555_88d9_4aa66aa7b6ba> \n  Stages \n  |--1 SQLTransformer (Transformer)\n  |    <dplyr_transformer__e0e59acc_d897_4a93_9760_474ebc786add> \n  |     (Parameters -- Column Names)\n  |--2 Binarizer (Transformer)\n  |    <binarizer__5b3946fe_1209_4d5d_805c_ee7c79bea1f2> \n  |     (Parameters -- Column Names)\n  |      input_col: dep_delay\n  |      output_col: delayed\n  |--3 Bucketizer (Transformer)\n  |    <bucketizer__84db4fe8_4994_4ad3_9e04_a6bbb70e5cec> \n  |     (Parameters -- Column Names)\n  |      input_col: sched_dep_time\n  |      output_col: hours\n  |--4 RFormulaModel (Transformer)\n  |    <r_formula__b6e8b330_5824_4719_9417_d14bd05de34f> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |     (Transformer Info)\n  |      formula:  chr \"delayed ~ month + day + hours + distance\" \n  |--5 LogisticRegressionModel (Transformer)\n  |    <logistic_regression__3991e32e_fa8c_46f8_b337_1ea749ffb337> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |      prediction_col: prediction\n  |      probability_col: probability\n  |      raw_prediction_col: rawPrediction\n  |     (Transformer Info)\n  |      coefficient_matrix:  num [1, 1:43] 0.111 -0.798 -0.626 -0.355 -0.19 ... \n  |      coefficients:  num [1:43] 0.111 -0.798 -0.626 -0.355 -0.19 ... \n  |      intercept:  num -2.08 \n  |      intercept_vector:  num -2.08 \n  |      num_classes:  int 2 \n  |      num_features:  int 43 \n  |      threshold:  num 0.5 \n  |      thresholds:  num [1:2] 0.5 0.5 \n```\n:::\n:::\n\nThe new model can be saved using `ml_save()`. A new name is used in this\ncase, but the same name as the existing PipelineModel to replace it.\n\n::: {.cell}\n\n```{.r .cell-code}\nml_save(new_model, \"new_flights_model\", overwrite = TRUE)\n```\n\n::: {.cell-output-stderr}\n```\nModel successfully saved.\n```\n:::\n:::\n\nFinally, this example is complete by closing the Spark session.\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_disconnect(sc)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}