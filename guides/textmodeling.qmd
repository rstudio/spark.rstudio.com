---
title: "Text modeling"
execute:
  eval: false
  freeze: true
---

```{r setup}
#| include: false
library(sparklyr)
library(dplyr)
library(modeldata)
```

For this example we will use a local Spark connection, version 3.3

```{r}
library(sparklyr)

sc <- spark_connect(master = "local", version = "3.3")
```

## Data

```{r}
library(modeldata)

data("small_fine_foods")

training_data %>% 
  head(1) %>% 
  as.list()
```

```{r}
sff_training_data <- copy_to(sc, training_data)

sff_testing_data <- copy_to(sc, testing_data)
```

## Text transformers

1.  We will split each review into individual words, or tokens. The `ft_tokenizer()` function returns a in-line list containing the individual words.

    ```{r}
    sff_training_data %>% 
      ft_tokenizer(
        input_col = "review",
        output_col = "word_list"
      ) %>% 
      select(3:4)
    ```

2.  There are words very common in text, words such as: "the", "and", "or", etc. These are called "stop words". Most often, stop words are not useful in analysis and modeling so it is necessary to remove them. That is exactly what `ft_stop_words_remover()` does. Spark contains a list of stop words for several languages, not only English.

    ```{r}
    sff_training_data %>% 
      ft_tokenizer(
        input_col = "review",
        output_col = "word_list"
      ) %>% 
      ft_stop_words_remover(
        input_col = "word_list", 
        output_col = "wo_stop_words"
        ) %>% 
      select(3:5) 
    ```

3.  Text hashing maps a sequence of words, or "terms", to their frequencies. The number of terms that are mapped can be controlled using the `num_features` argument in `ft_hashing_ft()`. Because we are eventually going to use a logistic regression model, we will need to override the frequencies from their original value to 1. This is accomplished by setting the `binary` argument to `TRUE`.

    ```{r}
    sff_training_data %>%
      ft_tokenizer(
        input_col = "review",
        output_col = "word_list"
      ) %>% 
      ft_stop_words_remover(
        input_col = "word_list", 
        output_col = "wo_stop_words"
        ) %>% 
      ft_hashing_tf(
        input_col = "wo_stop_words", 
        output_col = "hashed_features", 
        binary = TRUE, 
        num_features = 1024
        ) %>%
      select(3:6) 

    ```

4.  Finally, we normalize the hashed column using `ft_normalizer()` .

    ```{r}
    sff_training_data %>% 
      ft_tokenizer(
        input_col = "review",
        output_col = "word_list"
      ) %>% 
      ft_stop_words_remover(
        input_col = "word_list", 
        output_col = "wo_stop_words"
        ) %>% 
      ft_hashing_tf(
        input_col = "wo_stop_words", 
        output_col = "hashed_features", 
        binary = TRUE, 
        num_features = 1024
        ) %>%
      ft_normalizer(
        input_col = "hashed_features", 
        output_col = "normal_features"
        ) %>% 
      select(3:7) 
    ```

:::{.callout-tip}
## Important concept

The `ft_hashing_tf()` outputs the index and frequency of each term. This can be thought of as how "dummy variables" are created for each discrete value of a
categorical variable.  This means that for modeling, we will only need to use
only one "column", `hashed_features`. But, we will use `normal_features` for the
model because it is derived from `hashed_features`.

:::

## Pipeline

```{r}
sff_pipeline <- ml_pipeline(sc) %>% 
  ft_tokenizer(
    input_col = "review",
    output_col = "word_list"
  ) %>% 
  ft_stop_words_remover(
    input_col = "word_list", 
    output_col = "wo_stop_words"
    ) %>% 
  ft_hashing_tf(
    input_col = "wo_stop_words", 
    output_col = "hashed_features", 
    binary = TRUE, 
    num_features = 1024
    ) %>%
  ft_normalizer(
    input_col = "hashed_features", 
    output_col = "normal_features"
    ) %>% 
  ft_r_formula(score ~ normal_features) %>% 
  ml_logistic_regression()  

sff_pipeline
```

```{r}
sff_pipeline_model <- ml_fit(sff_pipeline, sff_training_data)

sff_pipeline_model
```

```{r}
sff_test_predictions <- sff_pipeline_model %>% 
  ml_transform(sff_testing_data) 

glimpse(sff_test_predictions)
```

```{r}
ml_metrics_binary(sff_test_predictions)
```

```{r}
spark_disconnect(sc)
```
