{
  "hash": "46aa3bb85f952c5efc3a8285f26805f5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Tune Tidymodel in Databricks\nexecute:\n  eval: true\n  freeze: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n## Introduction\n\n`sparklyr` enables you to offload Tidymodels grid search tuning to\nDatabricks. [Tidymodels](https://www.tidymodels.org/) is a collection of\npackages designed to work together to provide everything from\nresampling, to preprocessing, to model tuning, to performance\nmeasurement.\n\nModel tuning is a time-consuming process because of the number of tuning\nparameter combinations that need to be processed. Running the\ncombinations in parallel saves a significant amount of time. While\nTidymodels supports parallel tuning, it does not natively integrate with\nSpark. `sparklyr` bridges this gap, making distributed tuning on a\ncluster straightforward to execute.\n\nIn Tidymodels, the\n[`tune_grid()`](https://tune.tidymodels.org/reference/tune_grid.html)\nfunction is called to execute the grid search locally. To run in\nDatabricks, simply call `sparklyr`'s `tune_grid_spark()` instead. It\naccepts the exact same arguments as `tune_grid()` does, see @fig-code.\n\n::::::: {#fig-code}\n:::::: columns\n::: {.column width=\"38%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\ntune::tune_grid(\n  object = my_model, \n  preprocessor = my_recipe, \n  resamples = my_resamples\n  \n  )\n```\n:::\n\n\nRun locally\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"58%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nsparklyr::tune_grid_spark(\n  object = my_model, \n  preprocessor = my_recipe, \n  resamples = my_resamples,\n  sc = my_conn # Only additional requirement\n  )\n```\n:::\n\n\nRun remotely in Databricks\n:::\n::::::\n\nComparing tune and sparklyr function calls\n:::::::\n\n`sparklyr` will automatically upload the needed R object to Databricks,\nsuch as the resample object, model specification, and the preprocessing\nsteps. It will then run the tuning in parallel taking advantage of the R\nintegration in Databricks. Lastly, it will collect all of the results\nback to your local R session, and return a `tune_results` object that is\nindistinguishable from one made directly by Tidymodels; see\n@fig-diagram.\n\n::: {#fig-diagram}\n![](/images/guides/tidymodels-db/diagram-1.png)\n\nHow *tune_grid_spark()* works with Databricks\n:::\n\n## Example\n\nIn Tidymodels, there are some specific elements needed to perform a grid\nsearch tuning:\n\n-   Model specification (`parsnip`)\n\n-   Resampled data (`rsample`)\n\n-   Data preprocessor (`recipe`)\n\n-   Post processor (`tailor`) *optional*\n\nIn this example, we define the modeling components exactly as we would\nfor local tuning with `tune_grid()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(readmission)\nset.seed(1)\nreadmission_splits <- initial_split(readmission, strata = readmitted)\n\n# Resampling -------------------------------------------------------\nreadmission_folds <- vfold_cv(\n  data = training(readmission_splits),\n  strata = readmitted\n  )\n\n# Preprocessing ----------------------------------------------------\nrecipe_basic <- recipe(readmitted ~ ., data = readmission) |>\n  step_mutate(\n    race = factor(case_when(\n      !(race %in% c(\"Caucasian\", \"African American\")) ~ \"Other\",\n      .default = race\n    ))\n  ) |>\n  step_unknown(all_nominal_predictors()) |>\n  step_YeoJohnson(all_numeric_predictors()) |>\n  step_normalize(all_numeric_predictors()) |>\n  step_dummy(all_nominal_predictors())\n\n# Model specification -----------------------------------------------\nspec_bt <- boost_tree(\n  mode = \"classification\",\n  mtry = tune(),       # Parameter to be tuned\n  learn_rate = tune(), # Parameter to be tuned\n  trees = 10\n  )\n```\n:::\n\n\nThe only required step before tuning the model in Databricks is to\nconnect to a Databricks' Spark cluster. Although we could have connected\nearlier, this demonstrates how easily a local tuning workflow can be\npivoted to a Databricks cluster.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n\nsc <- spark_connect(\n  method = \"databricks_connect\", \n  cluster_id = \"1218-000327-q970zsow\" # Replace with your own cluster's ID\n  )\n```\n:::\n\n\nThe next step is to call `tune_grid_spark()`. We will pass the three\nelements created in the first step, and the Databricks connection\nvariable. The optional `control_grid()` call is included here to display\nthe `sparklyr` output during the tuning process.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_results <- tune_grid_spark(\n  sc = sc, \n  object = spec_bt, \n  preprocessor = recipe_basic, \n  resamples = readmission_folds,\n  control = control_grid(verbose = TRUE)\n  )\n#> i Creating pre-processing data to finalize 1 unknown parameter: \"mtry\"\n#> ℹ Uploading model, pre-processor, and other info to the Spark session\n#> ✔ Uploading model, pre-processor, and other info to the Spark session [663ms]\n#> \n#> ℹ Uploading the re-samples to the Spark session\n#> ✔ Uploading the re-samples to the Spark session [2.8s]\n#> \n#> ℹ Copying the grid to the Spark session\n#> ✔ Copying the grid to the Spark session [160ms]\n#> \n#> ℹ Executing the model tuning in Spark\n#> ✔ Executing the model tuning in Spark [16.6s]\n#> \n```\n:::\n\n\nOnce complete, the `spark_results` object is fully compatible with\nstandard `tune` functions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(spark_results)\n```\n\n::: {.cell-output-display}\n![](databricks_tune_tidymodels_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## R and Python libraries prerequisites\n\nBefore tuning in Databricks, the following Python and R packages\n**must** be pre-installed on the cluster:\n\n-   `rpy2` **(Python)**\n-   `tidymodels` **(R)**\n-   `reticulate` **(R)**\n-   Any R packages required by the `parsnip` engine (e.g., `xgboost` or\n    `ranger`).\n\nThere are two options to accomplish the installation, the first via the\nDatabricks portal, or by installing the libraries programmatically.\n\n#### Option 1 - Databricks Web Portal\n\nThe most common method is to use the Databricks UI. It is a\nstraightforward process managed through the cluster's Libraries tab.\nDetailed instructions can be found here: [Databricks - Cluster\nLibraries](https://docs.databricks.com/en/libraries/cluster-libraries.html).\n\n#### Option 2 - Programmatic installation via `brickster`\n\nFor those who prefer a scripted approach, the\n[`brickster`](https://databrickslabs.github.io/brickster/) package is a\ncomplete toolkit for interacting with Databricks. It can be used to\nprogrammatically install the Python and R libraries, provided the user\nhas permissions to modify the target cluster.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brickster)\n\n# DBRs have a fixed snapshot date to source the R packages. Redirecting  \n# to the 'latest' snapshot to get the most recent package versions.\nrepo <- \"https://databricks.packagemanager.posit.co/cran/__linux__/noble/latest\"\n\n# Builds the list of R packages, pointing them to the 'latest' snapshot\nr_libs <- libraries(\n  lib_cran(\"reticulate\", repo = repo),\n  lib_cran(\"tidymodels\", repo = repo),\n  lib_cran(\"xgboost\", repo = repo) # (optional) Used in the example\n)\n\ndb_libs_install(\"1218-000327-q970zsow\", r_libs)\n\n# Builds the Python package object. Specifying the version of `rpy2` to install.\npy_lib <- lib_pypi(\"rpy2==3.6.4\") |>\n  libraries()\n\ndb_libs_install(\"1218-000327-q970zsow\", py_lib)\n```\n:::\n\n\nAs time goes by, there may be other modeling R packages that need to be\ninstalled in Spark. Here is a template that can be used to install a\nsingle package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brickster)\n\nlib_cran(\n  package = \"[Missing package]\", \n  repo = \"https://databricks.packagemanager.posit.co/cran/__linux__/noble/latest\"\n  ) |> \n  libraries() |> \n  db_libs_install(\"[Your cluster ID]\", libraries =  _)\n\n```\n:::\n\n\n## Considerations\n\n-   **Local vs Remote results** - It is possible that results from\n    tuning a model locally will differ from those returned from\n    Databricks. Differences in Operating Systems, R run-time, and\n    libraries between the local machine and those in the Databricks\n    cluster will affect the calculations. However, re-running the same\n    tuning in the same Databricks cluster will return the same results\n    if a seed is set in the local R session. `sparklyr` ensures that the\n    seeds are set the same way Tidymodels does for the local tuning.\n-   **Parallelism** - There are two ways to set the parallelism for\n    tuning performed in Databricks. The default way is to tune over the\n    resamples, and the second way is over all combinations of resamples\n    and parameters. For a full explanation of how parallelism works in\n    this kind of distributed computing, please refer to [Tune Tidymodels\n    in Spark - Parallel\n    Processing](/guides/tune_tidymodels_spark.html#parallel-processing-in-spark)\n-   **Size of the data** – The main goal of `tune_grid_spark()` is to\n    accelerate the search, not to process \"big data.\" If you need to\n    tune models using data too large to fit in local memory, consider\n    using [Spark](/guides/model_tuning_text.qmd)[\n    ML](model_tuning_text.qmd) directly. The downside is that Spark ML\n    requires an entirely different setup and codebase than Tidymodels.\n-   **Retrieving predictions** - `sparklyr` supports the feature that\n    downloads the predictions from each parameter and resample\n    combination. Keep in mind that this will roughly double the amount\n    of data downloaded from Databricks. In most cases this is not a\n    problem, but for a very large resampled data set it may become one.\n    To learn how to turn that feature on, see [Tune Tidymodels in\n    Spark - Retrieving\n    predictions](/guides/tune_tidymodels_spark.html#retrieving-predictions-a.k.a-save_pred-true)\n\n## Conclusion\n\nWe’ve done a lot of work to ensure that `tune_grid_spark()` works\nexactly like `tune_grid()`. There’s no need to recode your logic or\nlearn a new syntax; you just connect to the cluster and submit the job.\nSince many companies already provide access to Databricks, this workflow\nallows you to take full advantage of existing infrastructure with\nminimal effort.\n\nOffloading heavy grid searches to a cluster saves significant time you\ncan reinvest in deeper experimentation or presenting results to\nstakeholders faster. The next time you need to tune a model, give this a\ntry and see how much faster your workflow becomes.\n",
    "supporting": [
      "databricks_tune_tidymodels_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}