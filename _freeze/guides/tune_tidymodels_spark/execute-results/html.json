{
  "hash": "6342701b753fccb17d4efdf516acd5e4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Tune `tidymodels` remotely in Spark Connect\"\nexecute:\n  eval: true\n  freeze: true\n---\n\n\n\n\n\n\n## Intro\n\nUse the `tidymodels` objects created locally in R, to run the hyper-parameter\ntuning in Spark.  With one call, `sparklyr` will upload \nthe `tidymodels` objects,  execute the parallel jobs run remotely, and return\nthe finalized tuned object to the local R session.\n\n::: {#fig-cluster}\n```{mermaid}\n%%| fig-width: 6\nflowchart RL\n  subgraph sc [Spark Connect]\n   p[Parallel processes]\n  end\n  r[local R]  -- Re-samples --> sc\n  sc -.- rs[Results] -.-> r\n  r -- Model --> sc\n  r -- Pre-processor --> sc\n  style sc fill:#F0B675,stroke:#666,color:#000;\n  style p fill:#F8DDBF,stroke:#666,color:#000;\n  style r fill:#75C7F0,stroke:#666,color:#000;\n  style rs fill:#333,stroke:#333,color:#fff;\n  linkStyle default stroke:#666,color:#000\n```\n\nModel tuning in a Spark cluster\n:::\n\nTo do this use `tune_grid_spark()` instead of calling `tune`'s `tune_grid()`\nfunction.  The function accepts the exact same arguments as `tune_grid()`. The \nonly required added argument is the Spark session.\n\n**It is important to note that we are not speaking of \"big data\", but rather of \nsomething more like  \"big processing\"**. \nProcessing numerous tuning combinations is what makes the process take\na long time. Each combination has to be individually fitted, the resulting model\nis then used run predictions, and lastly the results of the predictions to\nestimate metrics. The more of these individual combinations that can run in \nparallel the more time is saved, and Spark is particularly good at this sort\nof processing. Additionally, Spark is a more widely available distributed \ncomputing infrastructure, as opposed to something such a grid computer This \nopens a new avenue for us to take advantage of vast resources, and without\nhaving to change existing code, or having to learn a new API.\n\n## Installation\n\nThe latest versions of `sparklyr`, `pysparklyr` and `tune` are needed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# TODO: Replace with actual installation instructions at release time\n\nremotes::install_github(\"sparklyr/sparklyr\", ref = \"grid2\")\nremotes::install_github(\"tidymodels/tune\", ref = \"grid2\")\nremotes::install_github(\"mlverse/pysparklyr\", ref = \"grid2\")\n```\n:::\n\n\n\n## Prepare locally\n\nThe standard framework for Machine Learning in R is  [Tidymodels](https://www.tidymodels.org/). \nIt is a collection of packages that are designed to work together to provide \neverything from re-sampling, to pre-processing, to model tuning, to performance \nmeasurement. \n\nIn this article, we will follow an example of tuning a model of hospital \nreadmission data. The following code prepares the re-sampling, pre-processing\nand model specification. All of these steps are processed locally in R. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(readmission)\n\nset.seed(1)\n\n# Data set resampling \nreadmission_splits <- initial_split(readmission, strata = readmitted)\n\nreadmission_folds <- vfold_cv(\n  data = training(readmission_splits),\n  strata = readmitted\n  )\n\n# Data pre-processing\nrecipe_basic <- recipe(readmitted ~ ., data = readmission) |>\n  step_mutate(\n    race = factor(case_when(\n      !(race %in% c(\"Caucasian\", \"African American\")) ~ \"Other\",\n      .default = race\n    ))\n  ) |>\n  step_unknown(all_nominal_predictors()) |>\n  step_YeoJohnson(all_numeric_predictors()) |>\n  step_normalize(all_numeric_predictors()) |>\n  step_dummy(all_nominal_predictors())\n\n# Model specification\nspec_bt <- boost_tree(\n  mode = \"classification\",\n  mtry = tune(), # Part of the hyper-parameters to tune\n  learn_rate = tune(), # Part of the hyper-parameters to tune\n  trees = 10\n  )\n```\n:::\n\n\nTypically, the next step is calling `tune_grid()` to run the hyper-parameter\ntuning. This is where we will diverge in order to use Spark, instead of our \nlaptop, to tune the model.\n\n## Tune in Spark Connect\n\nRemote model tuning is only available for Spark Connect connections,\nand its vendor variants such as [Databricks Connect](../deployment/databricks-connect.qmd). \n\n### Spark requirements\n\nIn all of its nodes, the Spark cluster will need an R run-time, and recent \ninstallations of some R packages. There is also a Python package requirement:\n\n**R**\n\n- `tidymodels`\n\n- `reticulate`\n\n**Python**\n\n- `rpy2`\n\nIf tuning on a model not available in base R, then the package containing that\nmodel will need to be installed. In the example on this article, the default\nfor `boost_tree()` is XGboost. This means that the `xbgoost` package should be\navailable in the cluster.\n\n### Connect to Spark\n\nTo show a full example in this article, we have included the steps to\nstart a *local* Spark Connect service. Keep in mind that it is not necessary\nto create anything local if you already have a Spark Connect service available\nsomewhere else, such as a vendor provided cluster. \n\n1. Install Spark locally if the version you wish to use is not available locally \nyet:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n\nspark_install(\"4.1.0\")\n```\n:::\n\n\n2. Start the Spark Connect service locally. Make sure to match the Spark version\nyou recently installed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npysparklyr::spark_connect_service_start(\"4.1.0\", python_version = \"3.12\")\n#> Starting Spark Connect locally ...\n#> openjdk version \"17.0.14\" 2025-01-21\n#> OpenJDK Runtime Environment Homebrew (build 17.0.14+0)\n#> OpenJDK 64-Bit Server VM Homebrew (build 17.0.14+0, mixed mode, sharing)\n#> ℹ Attempting to load 'r-sparklyr-pyspark-4.1'\n#> ✔ Python environment: 'r-sparklyr-pyspark-4.1' [643ms]\n#> \n#>   starting org.apache.spark.sql.connect.service.SparkConnectServer, logging to\n#>   /Users/edgar/spark/spark-4.1.0-bin-hadoop3/logs/spark-edgar-org.apache.spark.sql.connect.service.SparkConnectServer-1-edgarruiz-WL57.out\n```\n:::\n\n\n3. Connect to Spark:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsc <- spark_connect(\n  \"sc://localhost\",\n  method = \"spark_connect\",\n  version = \"4.1.0\"\n  )\n#> ℹ Attempting to load 'r-sparklyr-pyspark-4.1'\n#> ✔ Python environment: 'r-sparklyr-pyspark-4.1' [5ms]\n#> \n```\n:::\n\n\n### Tune the model\n\nOnce connected to Spark, to execute the tuning run: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_results <- tune_grid_spark(\n  sc = sc, # Spark connection\n  object = spec_bt, # Model specification (`parsnip`)\n  preprocessor = recipe_basic, \n  resamples = readmission_folds \n  )\n#> i Creating pre-processing data to finalize 1 unknown parameter: \"mtry\"\n```\n:::\n\n\nAs part of adding this feature, `sparklyr` ported a lot of the functionality\ndirectly from the `tidymodels` packages in order to offer an equivalent \nexperience, as well as to perform the same checks. The resulting object will \nlook indistinguishable from that created directly by `tidymodels` locally in R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_results\n#> # Tuning results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 × 4\n#>    splits               id     .metrics          .notes          \n#>    <list>               <chr>  <list>            <list>          \n#>  1 <split [48272/5364]> Fold01 <tibble [30 × 6]> <tibble [0 × 4]>\n#>  2 <split [48272/5364]> Fold02 <tibble [30 × 6]> <tibble [0 × 4]>\n#>  3 <split [48272/5364]> Fold03 <tibble [30 × 6]> <tibble [0 × 4]>\n#>  4 <split [48272/5364]> Fold04 <tibble [30 × 6]> <tibble [0 × 4]>\n#>  5 <split [48272/5364]> Fold05 <tibble [30 × 6]> <tibble [0 × 4]>\n#>  6 <split [48272/5364]> Fold06 <tibble [30 × 6]> <tibble [0 × 4]>\n#>  7 <split [48273/5363]> Fold07 <tibble [30 × 6]> <tibble [0 × 4]>\n#>  8 <split [48273/5363]> Fold08 <tibble [30 × 6]> <tibble [0 × 4]>\n#>  9 <split [48273/5363]> Fold09 <tibble [30 × 6]> <tibble [0 × 4]>\n#> 10 <split [48273/5363]> Fold10 <tibble [30 × 6]> <tibble [0 × 4]>\n```\n:::\n\n\nThe object can now be used to inspect and used to finalize the model \nselection.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(spark_results)\n```\n\n::: {.cell-output-display}\n![](tune_tidymodels_spark_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n## Comparing results\n\nThe steps in this section should not be part of an every day workflow. It is\nincluded here for comparison purposes. We are comparing the results from \nSpark versus local R. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- tune_grid(spec_bt, recipe_basic, readmission_folds)\n#> i Creating pre-processing data to finalize 1 unknown parameter: \"mtry\"\n\n# Use show_best() to compare the recommendations\nshow_best(results)\n#> Warning in show_best(results): No value of `metric` was given; \"roc_auc\" will\n#> be used.\n#> # A tibble: 5 × 8\n#>    mtry learn_rate .metric .estimator  mean     n std_err .config         \n#>   <int>      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>           \n#> 1     4     0.0880 roc_auc binary     0.600    10 0.00425 pre0_mod02_post0\n#> 2     7     0.001  roc_auc binary     0.600    10 0.00434 pre0_mod03_post0\n#> 3    10     0.0129 roc_auc binary     0.599    10 0.00467 pre0_mod04_post0\n#> 4    13     0.167  roc_auc binary     0.596    10 0.00391 pre0_mod05_post0\n#> 5    20     0.0245 roc_auc binary     0.596    10 0.00509 pre0_mod07_post0\n\nshow_best(spark_results)\n#> Warning in show_best(spark_results): No value of `metric` was given; \"roc_auc\"\n#> will be used.\n#> # A tibble: 5 × 8\n#>    mtry learn_rate .metric .estimator  mean     n std_err .config         \n#>   <int>      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>           \n#> 1     4    0.0880  roc_auc binary     0.600    10 0.00425 pre0_mod02_post0\n#> 2     7    0.00100 roc_auc binary     0.600    10 0.00434 pre0_mod03_post0\n#> 3    10    0.0129  roc_auc binary     0.599    10 0.00467 pre0_mod04_post0\n#> 4    13    0.167   roc_auc binary     0.596    10 0.00391 pre0_mod05_post0\n#> 5    20    0.0245  roc_auc binary     0.596    10 0.00509 pre0_mod07_post0\n```\n:::\n\n\nThe results are identical because Spark is running locally. It should not be\nsurprising if the results are different if using Spark remotely. Differences in\nOperating System, R, and packages can affect the results. Reproducibility is not\nimpacted because the remote job will always return the same results.  `sparklyr`\nreplicates how `tidymodels` sets the seed for each run. \n\n## Parallel processing in Spark\n\nIn Spark, the term **job** differs from the term **task**. A single job\ncontains multiple tasks that can run in parallel. Spark will run as many \n*concurrent tasks* as there are CPU cores that the cluster makes available to\nthat particular job. In the example on @fig-job-task, a single job is running\n4 tasks in parallel thanks to each running in a separate core.\n\n::: {#fig-job-task}\n```{mermaid}\n%%| fig-width: 6\nflowchart RL\n  subgraph j [<b>Job</b>]\n   t1[<b>Task 1</b> <br/> Core 1]\n   t2[<b>Task 2</b> <br/> Core 2]\n   t3[<b>Task 3</b> <br/> Core 3]\n   t4[<b>Task 4</b> <br/> Core 4]\n  end\n  style j fill:#F8DDBF,stroke:#666,color:#000;\n  style t1 fill:#F7EADC,stroke:#666,color:#444;\n  style t2 fill:#F7EADC,stroke:#666,color:#444;\n  style t3 fill:#F7EADC,stroke:#666,color:#444;\n  style t4 fill:#F7EADC,stroke:#666,color:#444;\n  linkStyle default stroke:#666,color:#000\n```\n\nJob vs. Task in Spark example\n:::\n\nHow this relates to `tune_grid_spark()` depends on how the parallel processing\nflag is set. As with `tune_grid()`, that is driven by setting a value for\n`parallel_over` in the control grid: \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncntrl <- control_grid(parallel_over = \"resamples\")\n\nspark_results <- tune_grid_spark(\n  sc = sc,\n  object = spec_bt,\n  preprocessor = recipe_basic, \n  resamples = readmission_folds,\n  control = cntrl\n  )\n```\n:::\n\n\nThere are two possible values to use in `parallel_over`: \n\n1. **resamples** *(default)* - The tuning is performed in parallel over the \nre-samples alone. In the example, `readmission_folds` has 10 folds/re-samples. \nIf we had 4 tasks available from Spark, then the 10 folds would be distributed\nacross that number of tasks. This means that some two tasks will run 2 folds, \nthe the other two will run 3 folds, see @fig-resamples. Inside the tasks, the\n2 or 3 folds will run sequentially, but the 4 tasks will start processing\nfolds at the same time. \n\n::: {#fig-resamples}\n```{mermaid}\n%%| fig-width: 6\nflowchart RL\n  subgraph j [<b>Job</b>]\n   t1[<b>Task 1</b> <br/> Fold 1 <br/> Fold 2 <br/> Fold 3]\n   t2[<b>Task 2</b> <br/> Fold 4 <br/> Fold 5 <br/> Fold 6]\n   t3[<b>Task 3</b> <br/> Fold 7 <br/> Fold 8]\n   t4[<b>Task 4</b> <br/> Fold 9 <br/> Fold 10]\n  end\n  style j fill:#F8DDBF,stroke:#666,color:#000;\n  style t1 fill:#F7EADC,stroke:#666,color:#444;\n  style t2 fill:#F7EADC,stroke:#666,color:#444;\n  style t3 fill:#F7EADC,stroke:#666,color:#444;\n  style t4 fill:#F7EADC,stroke:#666,color:#444;\n  linkStyle default stroke:#666,color:#000\n```\n\nHow \"resamples\" works in Spark\n:::\n\n2. **everything** - The tuning will be performed in parallel for each \nindividual permutation of re-samples and tuning combinations.  In the example,\nthere are 10 tune combinations, which are then combined with the 10 \nfolds/re-samples, giving us 100 total combinations.  If we had 4 tasks available\nfrom Spark, then each will run 25 combinations.\n\n::: {#fig-everything}\n```{mermaid}\n%%| fig-width: 6\nflowchart RL\n  subgraph j [<b>Job</b>]\n   t1[<b>Task 1</b> <br/> Fold 1 - Combo 1 <br/> ... <br/> ...  <br/> Fold 3 - Combo 5]\n   t2[<b>Task 2</b> <br/> Fold 3 - Combo 6 <br/> ... <br/> ...  <br/> Fold 5 - Combo 10]\n   t3[<b>Task 3</b> <br/> Fold 6 - Combo 1 <br/> ... <br/> ...  <br/> Fold 8 - Combo 5]\n   t4[<b>Task 4</b> <br/> Fold 8 - Combo 6 <br/> ... <br/> ...  <br/> Fold 10 - Combo 10]\n  end\n  style j fill:#F8DDBF,stroke:#666,color:#000;\n  style t1 fill:#F7EADC,stroke:#666,color:#444;\n  style t2 fill:#F7EADC,stroke:#666,color:#444;\n  style t3 fill:#F7EADC,stroke:#666,color:#444;\n  style t4 fill:#F7EADC,stroke:#666,color:#444;\n  linkStyle default stroke:#666,color:#000\n```\n\nHow \"everything\" works in Spark\n:::\n\nIn either case, despite the number of re-samples, or number of tuning +\nre-samples combinations, the limit of how many processes run in parallel is set\nby how many tasks are created for the job.  The more nodes and cores there are \navailable in the Spark cluster, the more possible parallel tasks could be \nschedule to run for the job. \n\n## Retrieving predictions a.k.a `save_pred = TRUE`\n\n`sparklyr` supports getting the out-of-sample predictions if they are saved. \nThat is done by using `control_grid()` when calling `tune_grid_spark()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncntrl <- control_grid(save_pred = TRUE)\n\nspark_results <- tune_grid_spark(\n  sc = sc,\n  object = spec_bt,\n  preprocessor = recipe_basic, \n  resamples = readmission_folds,\n  control = cntrl\n  )\n#> i Creating pre-processing data to finalize 1 unknown parameter: \"mtry\"\n```\n:::\n\n\nThere will be a second Spark job that executes. The job simply reads the files \ncontaining the predictions created during the tuning job, and return the\ndata back to the local R session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_predictions(spark_results)\n#> # A tibble: 536,360 × 9\n#>    .pred_class .pred_Yes .pred_No id     readmitted  .row  mtry learn_rate\n#>    <chr>           <dbl>    <dbl> <chr>  <chr>      <int> <int>      <dbl>\n#>  1 No             0.0882    0.912 Fold01 No            10     1    0.00681\n#>  2 No             0.0889    0.911 Fold01 Yes           31     1    0.00681\n#>  3 No             0.0883    0.912 Fold01 No            51     1    0.00681\n#>  4 No             0.0879    0.912 Fold01 No            52     1    0.00681\n#>  5 No             0.0889    0.911 Fold01 No            62     1    0.00681\n#>  6 No             0.0879    0.912 Fold01 No            89     1    0.00681\n#>  7 No             0.0884    0.912 Fold01 Yes          102     1    0.00681\n#>  8 No             0.0878    0.912 Fold01 No           112     1    0.00681\n#>  9 No             0.0881    0.912 Fold01 No           146     1    0.00681\n#> 10 No             0.0879    0.912 Fold01 No           150     1    0.00681\n#> # ℹ 536,350 more rows\n#> # ℹ 1 more variable: .config <chr>\n```\n:::\n\n\n",
    "supporting": [
      "tune_tidymodels_spark_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}