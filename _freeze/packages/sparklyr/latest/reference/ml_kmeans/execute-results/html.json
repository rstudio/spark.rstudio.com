{
  "hash": "d239f73d06eb83a045133f374d79274c",
  "result": {
    "markdown": "---\ntitle: \"Spark ML -- K-Means Clustering\"\nexecute:\n  freeze: true\n---\n\n\n\n\n*R/ml_clustering_kmeans.R, R/ml_model_kmeans.R*\n\n## ml_kmeans\n\n## Description\n K-means clustering with support for k-means|| initialization proposed by Bahmani et al.   Using `ml_kmeans()` with the formula interface requires Spark 2.0+. \n\n\n## Usage\n```r\n \nml_kmeans( \n  x, \n  formula = NULL, \n  k = 2, \n  max_iter = 20, \n  tol = 1e-04, \n  init_steps = 2, \n  init_mode = \"k-means||\", \n  seed = NULL, \n  features_col = \"features\", \n  prediction_col = \"prediction\", \n  uid = random_string(\"kmeans_\"), \n  ... \n) \n \nml_compute_cost(model, dataset) \n \nml_compute_silhouette_measure( \n  model, \n  dataset, \n  distance_measure = c(\"squaredEuclidean\", \"cosine\") \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | A `spark_connection`, `ml_pipeline`, or a `tbl_spark`. |\n| formula | Used when `x` is a `tbl_spark`. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details. |\n| k | The number of clusters to create |\n| max_iter | The maximum number of iterations to use. |\n| tol | Param for the convergence tolerance for iterative algorithms. |\n| init_steps | Number of steps for the k-means|| initialization mode. This is an advanced setting -- the default of 2 is almost always enough. Must be > 0. Default: 2. |\n| init_mode | Initialization algorithm. This can be either \"random\" to choose random points as initial cluster centers, or \"k-means||\" to use a parallel variant of k-means++ (Bahmani et al., Scalable K-Means++, VLDB 2012). Default: k-means||. |\n| seed | A random seed. Set this value if you need your results to be reproducible across repeated calls. |\n| features_col | Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by `ft_r_formula`. |\n| prediction_col | Prediction column name. |\n| uid | A character string used to uniquely identify the ML estimator. |\n| ... | Optional arguments, see Details. |\n| model | A fitted K-means model returned by `ml_kmeans()` |\n| dataset | Dataset on which to calculate K-means cost |\n| distance_measure | Distance measure to apply when computing the Silhouette measure. |\n\n\n\n## Value\n\n The object returned depends on the class of `x`.     \n\n- `spark_connection`: When `x` is a `spark_connection`, the function returns an instance of a `ml_estimator` object. The object contains a pointer to   a Spark `Estimator` object and can be used to compose   `Pipeline` objects.    \n\n- `ml_pipeline`: When `x` is a `ml_pipeline`, the function returns a `ml_pipeline` with   the clustering estimator appended to the pipeline.    \n\n- `tbl_spark`: When `x` is a `tbl_spark`, an estimator is constructed then   immediately fit with the input `tbl_spark`, returning a clustering model.    \n\n- `tbl_spark`, with `formula` or `features` specified: When `formula`     is specified, the input `tbl_spark` is first transformed using a     `RFormula` transformer before being fit by     the estimator. The object returned in this case is a `ml_model` which is a     wrapper of a `ml_pipeline_model`. This signature does not apply to `ml_lda()`. \n\n  `ml_compute_cost()` returns the K-means cost (sum of   squared distances of points to their nearest center) for the model   on the given data.  `ml_compute_silhouette_measure()` returns the Silhouette measure   of the clustering on the given data. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n \nsc <- spark_connect(master = \"local\") \niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \nml_kmeans(iris_tbl, Species ~ .) \n#> K-means clustering with 2 clusters\n#> \n#> Cluster centers:\n#>   Sepal_Length Sepal_Width Petal_Length Petal_Width\n#> 1     6.301031    2.886598     4.958763    1.695876\n#> 2     5.005660    3.369811     1.560377    0.290566\n#> \n#> Within Set Sum of Squared Errors =  not computed.\n```\n:::\n\n\n## See Also\n See [https://spark.apache.org/docs/latest/ml-clustering.html](https://spark.apache.org/docs/latest/ml-clustering.html) for   more information on the set of clustering algorithms.  Other ml clustering algorithms:  `ml_bisecting_kmeans()`, `ml_gaussian_mixture()`, `ml_lda()` \n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}