---
title: "Get Started"
format:
  html:
    theme: default
    toc: true
execute:
  eval: true
  freeze: true
---

```{r, include = FALSE}
library(sparklyr) 
library(dplyr)
```


::: {.panel-tabset}

## Install

### Install the package

You can install the `sparklyr` package from [CRAN](https://CRAN.r-project.org) as follows:

```{r}
#| eval: false

install.packages("sparklyr")
```

### Install Spark locally

This section is meant for developers new to `sparklyr`.  You will need a running
Spark environment to connect to.  `sparklyr` can install Spark in your computer. 
The installed Spark environment is meant for learning and prototyping purposes.
The installation will work on all the major Operating Systems that R works on,
including Linux, MacOS, and Windows.  

```{r}
#| eval: false

library(sparklyr)

spark_install()
```

Please be aware that after installation, Spark is **not** running. The next section
will explain how to start a single node Spark cluster in your machine.

### Connect to Spark

You can use `spark_connect()` to connect to Spark clusters.  The arguments passed to
this functions depend on the type of Spark cluster you are connecting to. There are
several different types of Spark clusters, such as YARN, Stand Alone and Kubernetes. 

`spark_connect()` is able to both start, and connect to, the single node Spark
cluster in your machine.  In order to do that, pass "local" as the argument
for `master`:

```{r}
#| eval: false

library(sparklyr)

sc <- spark_connect(master = "local")
```

The `sc` variable now contains all of the connection information needed to
interact with the cluster.

To learn how to connect to other types of Spark clusters, see the 
[Deployment](/deployment/index.qmd) section of this site. 

For "local" connection, `spark_disconnect()` will shut down the single node
Spark environment in your machine, and tell R that the connection is no longer 
valid.  For other types of Spark clusters, `spark_disconnect()` will only 
end the Spark session, it will not shut down the Spark cluster itself.

```{r}
#| eval: false
spark_disconnect(sc)
```


## Read Data

A new Spark session will contain no data. The first step is to either load data
into your Spark session's memory, or point Spark to the location of the data
so it can access the data on-demand.

For this exercise, we will start a "local" Spark session, and then transfer data 
from our R environment to the Spark session's memory.  To do that, we will 
use the `copy_to()` command:

```{r}
library(sparklyr)

sc <- spark_connect(master = "local")

tbl_mtcars <- copy_to(sc, mtcars, "spark_mtcars")
```

If you are using the RStudio IDE, you will notice a new table in the Connections
pane.  The name of that table is **spark_mtcars**.  That is the name of the data
set inside the Spark memory. The `tbl_mtcars` variable does not contain any
`mtcars` data, this variable contains the info that points to the location where 
the Spark session loaded the data to. 

Calling the `tbl_mtcars` variable in R will download the first 1,000 records and
display them :

```{r}
tbl_mtcars
```
Notice that at the top of the data print out, it is noted that records were 
downloaded from Spark: *Source: spark...*.

To clean up the session, we will now stop the Spark session:

```{r}
spark_disconnect(sc)
```

### Files

In a formal Spark environment, it will be rare when we would have to upload data
from R into Spark.  

Using `sparklyr`, you can tell Spark to read and write data. Spark is able to interact
with multiple types of file systems, such as HDFS, S3 and local. Additionally, 
Spark is able to read several file types such as CSV, Parquet, Delta and JSON.
`sparklyr` provides functions that makes it easy to access these features.  See
the [Spark Data](/packages/sparklyr/latest/reference/#spark-data) section for a 
full list of available functions.  

The following command will tell Spark to read a CSV file, and to also load it
into Spark memory. 

```{r}
#| eval: false
# Do not run the next following command. It is for example purposes only.
spark_read_csv(sc, name = "test_table",  path = "/test/path/test_file.csv")
```


## Prepare Data

There are three methods for working with data: 

- Using `dplyr` commands
- Using SQL
- Using Spark's feature transformers


For this example start a local session of Spark. We'll start by copying a 
data set from R into the Spark cluster (note that you may need to install the 
`nycflights13`)

```{r}
library(sparklyr)
sc <- spark_connect(master = "local")
flights_tbl <- copy_to(sc, nycflights13::flights, "spark_flights")
```

### Using `dplyr`

We can now use all of the available `dplyr` verbs against the tables within the cluster.

```{r}
delay <- flights_tbl %>%
  group_by(tailnum) %>%
  summarise(
    count = n(), 
    dist = mean(distance, na.rm = TRUE), 
    delay = mean(arr_delay, na.rm = TRUE)
    ) %>%
  filter(count > 20, dist < 2000, !is.na(delay)) 

delay
```

For additional documentation on using dplyr with Spark see the [Manipulating Data with `dplyr`](/get-started/dplyr.qmd) article in 
this site

### Using SQL

It's also possible to execute SQL queries directly against tables within a Spark cluster. The `spark_connection()` object implements a [DBI](https://dbi.r-dbi.org/) interface for Spark, so you can use `dbGetQuery()` to execute SQL and return the result as an R data frame:

```{r sql-dbi}
library(DBI)

dbGetQuery(sc, "SELECT carrier, sched_dep_time, dep_time, dep_delay FROM spark_flights LIMIT 5")

```
```{r}
spark_disconnect(sc)
```


## Model Data

You can orchestrate machine learning algorithms in a Spark cluster via the [machine learning](https://spark.apache.org/docs/latest/mllib-guide.html) functions within **sparklyr**. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning workflows.

Here's an example where we use [ml_linear_regression](https://spark.rstudio.com/reference/ml_linear_regression/) to fit a linear regression model. We'll use the built-in `mtcars` dataset, and see if we can predict a car's fuel consumption (`mpg`) based on its weight (`wt`), and the number of cylinders the engine contains (`cyl`). We'll assume in each case that the relationship between `mpg` and each of our features is linear.

```{r}
#| eval: false

# copy mtcars into spark
mtcars_tbl <- copy_to(sc, mtcars, overwrite = TRUE)

# transform our data set, and then partition into 'training', 'test'
partitions <- mtcars_tbl %>%
  filter(hp >= 100) %>%
  mutate(cyl8 = cyl == 8) %>%
  sdf_partition(training = 0.5, test = 0.5, seed = 1099)

# fit a linear model to the training dataset
fit <- partitions$training %>%
  ml_linear_regression(response = "mpg", features = c("wt", "cyl"))
fit
```

For linear regression models produced by Spark, we can use `summary()` to learn a bit more about the quality of our fit, and the statistical significance of each of our predictors.

```{r}
#| eval: false

summary(fit)
```

Spark machine learning supports a wide array of algorithms and feature transformations and as illustrated above it's easy to chain these functions together with dplyr pipelines. To learn more see the [machine learning](https://spark.rstudio.com/mlib/) section.


:::