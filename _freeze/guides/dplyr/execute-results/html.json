{
  "hash": "ed883ca83adc99a4a7fcc91e656b9a94",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Manipulating Data with `dplyr`\"\nexecute:\n    eval: true\n    freeze: true\naliases:\n  - /dplyr\n---\n\n## Overview\n\n[**`dplyr`**](https://cran.r-project.org/web/packages/dplyr/index.html) is\nan R package for working with structured data both in and outside of R.\ndplyr makes data manipulation for R users easy, consistent, and\nperformant. With `dplyr` as an interface to manipulating Spark DataFrames,\nyou can:\n\n  - Select, filter, and aggregate data\n  - Use window functions (e.g. for sampling)\n  - Perform joins on `DataFrames`\n  - Collect data from Spark into R\n\nStatements in dplyr can be chained together using pipes defined by the\n[magrittr](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html)\nR package. dplyr also supports [non-standard\nevalution](https://cran.r-project.org/web/packages/dplyr/vignettes/nse.html)\nof its arguments. For more information on dplyr, see the\n[introduction](https://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html),\na guide for connecting to\n[databases](https://cran.r-project.org/web/packages/dplyr/vignettes/databases.html),\nand a variety of\n[vignettes](https://cran.r-project.org/web/packages/dplyr/index.html).\n\n\n### Flights Data\n\nThis guide will demonstrate some of the basic data manipulation verbs of\ndplyr by using data from the `nycflights13` R package. This package\ncontains data for all 336,776 flights departing New York City in 2013.\nIt also includes useful metadata on airlines, airports, weather, and\nplanes. The data comes from the US [Bureau of Transportation\nStatistics](http://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&Link=0),\nand is documented in `?nycflights13`\n\nConnect to the cluster and copy the flights data using the `copy_to()`\nfunction. Caveat: The flight data in `nycflights13` is convenient for\ndplyr demonstrations because it is small, but in practice large data\nshould rarely be copied directly from R objects.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nsc <- spark_connect(master=\"local\")\n\nflights_tbl <- copy_to(sc, nycflights13::flights, \"flights\")\n\nairlines_tbl <- copy_to(sc, nycflights13::airlines, \"airlines\")\n\n```\n:::\n\n\n\n## dplyr Verbs\n\nVerbs are `dplyr` commands for manipulating data. When connected to a\nSpark DataFrame, `dplyr` translates the commands into **Spark SQL**\nstatements. Remote data sources use exactly the same five verbs as local\ndata sources. Here are the five verbs with their corresponding SQL\ncommands:\n\n  - `select()` ~ `SELECT`\n  - `filter()` ~ `WHERE`\n  - `arrange()` ~ `ORDER`\n  - `summarise()` ~ `aggregators: sum, min, sd, etc.`\n  - `mutate()` ~ `operators: +, *, log, etc.`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nselect(flights_tbl, year:day, arr_delay, dep_delay)\n#> # Source:   SQL [?? x 5]\n#> # Database: spark_connection\n#>     year month   day arr_delay dep_delay\n#>    <int> <int> <int>     <dbl>     <dbl>\n#>  1  2013     1     1        11         2\n#>  2  2013     1     1        20         4\n#>  3  2013     1     1        33         2\n#>  4  2013     1     1       -18        -1\n#>  5  2013     1     1       -25        -6\n#>  6  2013     1     1        12        -4\n#>  7  2013     1     1        19        -5\n#>  8  2013     1     1       -14        -3\n#>  9  2013     1     1        -8        -3\n#> 10  2013     1     1         8        -2\n#> # ℹ more rows\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter(flights_tbl, dep_delay > 1000)\n#> # Source:   SQL [?? x 19]\n#> # Database: spark_connection\n#>    year month   day dep_time sched_dep_time dep_delay\n#>   <int> <int> <int>    <int>          <int>     <dbl>\n#> 1  2013     1     9      641            900      1301\n#> 2  2013     1    10     1121           1635      1126\n#> 3  2013     6    15     1432           1935      1137\n#> 4  2013     7    22      845           1600      1005\n#> 5  2013     9    20     1139           1845      1014\n#> # ℹ 13 more variables: arr_time <int>,\n#> #   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#> #   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#> #   air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\narrange(flights_tbl, desc(dep_delay))\n#> # Source:     SQL [?? x 19]\n#> # Database:   spark_connection\n#> # Ordered by: desc(dep_delay)\n#>     year month   day dep_time sched_dep_time dep_delay\n#>    <int> <int> <int>    <int>          <int>     <dbl>\n#>  1  2013     1     9      641            900      1301\n#>  2  2013     6    15     1432           1935      1137\n#>  3  2013     1    10     1121           1635      1126\n#>  4  2013     9    20     1139           1845      1014\n#>  5  2013     7    22      845           1600      1005\n#>  6  2013     4    10     1100           1900       960\n#>  7  2013     3    17     2321            810       911\n#>  8  2013     6    27      959           1900       899\n#>  9  2013     7    22     2257            759       898\n#> 10  2013    12     5      756           1700       896\n#> # ℹ more rows\n#> # ℹ 13 more variables: arr_time <int>,\n#> #   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#> #   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#> #   air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarise(\n  flights_tbl, \n  mean_dep_delay = mean(dep_delay, na.rm = TRUE)\n  )\n#> # Source:   SQL [?? x 1]\n#> # Database: spark_connection\n#>   mean_dep_delay\n#>            <dbl>\n#> 1           12.6\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmutate(flights_tbl, speed = distance / air_time * 60)\n#> # Source:   SQL [?? x 20]\n#> # Database: spark_connection\n#>     year month   day dep_time sched_dep_time dep_delay\n#>    <int> <int> <int>    <int>          <int>     <dbl>\n#>  1  2013     1     1      517            515         2\n#>  2  2013     1     1      533            529         4\n#>  3  2013     1     1      542            540         2\n#>  4  2013     1     1      544            545        -1\n#>  5  2013     1     1      554            600        -6\n#>  6  2013     1     1      554            558        -4\n#>  7  2013     1     1      555            600        -5\n#>  8  2013     1     1      557            600        -3\n#>  9  2013     1     1      557            600        -3\n#> 10  2013     1     1      558            600        -2\n#> # ℹ more rows\n#> # ℹ 14 more variables: arr_time <int>,\n#> #   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#> #   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#> #   air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>, speed <dbl>\n```\n:::\n\n\n\n## Laziness\n\nWhen working with databases, `dplyr` tries to be as lazy as possible:\n\n  - It never pulls data into R unless you explicitly ask for it.\n\n  - It delays doing any work until the last possible moment: it collects\n    together everything you want to do and then sends it to the database\n    in one step.\n\nFor example, take the following\ncode:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc1 <- filter(\n  flights_tbl, \n  day == 17, month == 5, carrier %in% c('UA', 'WN', 'AA', 'DL')\n  )\n\nc2 <- select(c1, year, month, day, carrier, dep_delay, air_time, distance)\n\nc3 <- mutate(c2, air_time_hours = air_time / 60)\n\nc4 <- arrange(c3, year, month, day, carrier)\n\n```\n:::\n\n\nThis sequence of operations never actually touches the database. It’s\nnot until you ask for the data (e.g. by printing `c4`) that dplyr\nrequests the results from the database.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc4\n#> # Source:     SQL [?? x 8]\n#> # Database:   spark_connection\n#> # Ordered by: year, month, day, carrier\n#>     year month   day carrier dep_delay air_time distance\n#>    <int> <int> <int> <chr>       <dbl>    <dbl>    <dbl>\n#>  1  2013     5    17 AA              2       35      187\n#>  2  2013     5    17 AA             -4      313     2475\n#>  3  2013     5    17 AA             -3      117      733\n#>  4  2013     5    17 AA             -2      294     2248\n#>  5  2013     5    17 AA              6      184     1389\n#>  6  2013     5    17 AA             -6      143     1096\n#>  7  2013     5    17 AA              0      196     1598\n#>  8  2013     5    17 AA             -2      146     1085\n#>  9  2013     5    17 AA             -5      314     2475\n#> 10  2013     5    17 AA             -3      193     1598\n#> # ℹ more rows\n#> # ℹ 1 more variable: air_time_hours <dbl>\n```\n:::\n\n\n## Piping\n\nYou can use\n[magrittr](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html)\npipes to write cleaner syntax. Using the same example from above, you\ncan write a much cleaner version like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc4 <- flights_tbl %>%\n  filter(month == 5, day == 17, carrier %in% c('UA', 'WN', 'AA', 'DL')) %>%\n  select(carrier, dep_delay, air_time, distance) %>%\n  mutate(air_time_hours = air_time / 60) %>% \n  arrange(carrier) \n```\n:::\n\n\n\n## Grouping\n\nThe `group_by()` function corresponds to the `GROUP BY` statement in SQL.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_tbl %>% \n  group_by(carrier) %>%\n  summarize(\n    count = n(), \n    mean_dep_delay = mean(dep_delay, na.rm = FALSE)\n    )\n#> Warning: Missing values are always removed in SQL aggregation functions.\n#> Use `na.rm = TRUE` to silence this warning\n#> This warning is displayed once every 8 hours.\n#> # Source:   SQL [?? x 3]\n#> # Database: spark_connection\n#>    carrier count mean_dep_delay\n#>    <chr>   <dbl>          <dbl>\n#>  1 WN      12275          17.7 \n#>  2 VX       5162          12.9 \n#>  3 YV        601          19.0 \n#>  4 DL      48110           9.26\n#>  5 OO         32          12.6 \n#>  6 B6      54635          13.0 \n#>  7 F9        685          20.2 \n#>  8 EV      54173          20.0 \n#>  9 US      20536           3.78\n#> 10 UA      58665          12.1 \n#> 11 MQ      26397          10.6 \n#> 12 AA      32729           8.59\n#> 13 FL       3260          18.7 \n#> 14 AS        714           5.80\n#> 15 9E      18460          16.7 \n#> 16 HA        342           4.90\n```\n:::\n\n\n\n## Collecting to R\n\nYou can copy data from Spark into R’s memory by using `collect()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncarrierhours <- collect(c4)\n```\n:::\n\n\n`collect()` executes the Spark query and returns the results to R for\nfurther analysis and visualization.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Test the significance of pairwise differences and plot the results\n\nwith(carrierhours, pairwise.t.test(air_time, carrier))\n#> \n#> \tPairwise comparisons using t tests with pooled SD \n#> \n#> data:  air_time and carrier \n#> \n#>    AA      DL      UA     \n#> DL 0.25057 -       -      \n#> UA 0.07957 0.00044 -      \n#> WN 0.07957 0.23488 0.00041\n#> \n#> P value adjustment method: holm\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncarrierhours %>% \n  ggplot() + \n  geom_boxplot(aes(carrier, air_time_hours))\n```\n\n::: {.cell-output-display}\n![](dplyr_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n## SQL Translation\n\nIt’s relatively straightforward to translate R code to SQL (or indeed to\nany programming language) when doing simple mathematical operations of\nthe form you normally use when *filtering*, *mutating* and *summarizing.*\n`dplyr` knows how to convert the following R functions to Spark SQL:\n\n``` r\n# Basic math operators\n+, -, *, /, %%, ^\n  \n# Math functions\nabs, acos, asin, asinh, atan, atan2, ceiling, cos, cosh, exp, floor, log, \nlog10, round, sign, sin, sinh, sqrt, tan, tanh\n\n# Logical comparisons\n<, <=, !=, >=, >, ==, %in%\n\n# Boolean operations\n&, &&, |, ||, !\n\n# Character functions\npaste, tolower, toupper, nchar\n\n# Casting\nas.double, as.integer, as.logical, as.character, as.date\n\n# Basic aggregations\nmean, sum, min, max, sd, var, cor, cov, n\n```\n\n`dplyr` supports Spark SQL window functions. Window functions are used in\nconjunction with mutate and filter to solve a wide range of problems.\nYou can compare the `dplyr` syntax to the query it has generated by using\n`dplyr::show_query()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Rank each flight within a daily\nranked <- flights_tbl %>%\n  group_by(year, month, day) %>%\n  select(dep_delay) %>% \n  mutate(rank = rank(desc(dep_delay)))\n#> Adding missing grouping variables: `year`, `month`, and `day`\n\ndplyr::show_query(ranked)\n#> <SQL>\n#> SELECT\n#>   `year`,\n#>   `month`,\n#>   `day`,\n#>   `dep_delay`,\n#>   CASE\n#> WHEN (NOT((`dep_delay` IS NULL))) THEN RANK() OVER (PARTITION BY `year`, `month`, `day`, (CASE WHEN ((`dep_delay` IS NULL)) THEN 1 ELSE 0 END) ORDER BY `dep_delay` DESC)\n#> END AS `rank`\n#> FROM `flights`\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nranked \n#> # Source:   SQL [?? x 5]\n#> # Database: spark_connection\n#> # Groups:   year, month, day\n#>     year month   day dep_delay  rank\n#>    <int> <int> <int>     <dbl> <int>\n#>  1  2013     1     1       853     1\n#>  2  2013     1     1       379     2\n#>  3  2013     1     1       290     3\n#>  4  2013     1     1       285     4\n#>  5  2013     1     1       260     5\n#>  6  2013     1     1       255     6\n#>  7  2013     1     1       216     7\n#>  8  2013     1     1       192     8\n#>  9  2013     1     1       157     9\n#> 10  2013     1     1       155    10\n#> # ℹ more rows\n```\n:::\n\n\n\n## Peforming Joins\n\nIt’s rare that a data analysis involves only a single table of data. In\npractice, you’ll normally have many tables that contribute to an\nanalysis, and you need flexible tools to combine them. In `dplyr`, there\nare three families of verbs that work with two tables at a time:\n\n  - Mutating joins, which add new variables to one table from matching\n    rows in another.\n\n  - Filtering joins, which filter observations from one table based on\n    whether or not they match an observation in the other table.\n\n  - Set operations, which combine the observations in the data sets as\n    if they were set elements.\n\nAll two-table verbs work similarly. The first two arguments are `x` and\n`y`, and provide the tables to combine. The output is always a new table\nwith the same type as `x`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_tbl %>% \n  left_join(airlines_tbl, by = \"carrier\") %>% \n  select(name, flight, dep_time)\n#> # Source:   SQL [?? x 3]\n#> # Database: spark_connection\n#>    name                 flight dep_time\n#>    <chr>                 <int>    <int>\n#>  1 Delta Air Lines Inc.    461      554\n#>  2 Delta Air Lines Inc.   1919      602\n#>  3 JetBlue Airways         725      544\n#>  4 JetBlue Airways         507      555\n#>  5 JetBlue Airways          79      557\n#>  6 JetBlue Airways          49      558\n#>  7 JetBlue Airways          71      558\n#>  8 JetBlue Airways        1806      559\n#>  9 JetBlue Airways         371      600\n#> 10 JetBlue Airways         343      601\n#> # ℹ more rows\n```\n:::\n\n\n\n## Sampling\n\nYou can use `sample_n()` and `sample_frac()` to take a random sample of\nrows: use `sample_n()` for a fixed number and `sample_frac()` for a\nfixed fraction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_n(flights_tbl, 10) %>% \n  select(1:4)\n#> # Source:   SQL [?? x 4]\n#> # Database: spark_connection\n#>     year month   day dep_time\n#>    <int> <int> <int>    <int>\n#>  1  2013    10    25     1419\n#>  2  2013    12    31      616\n#>  3  2013    12    28     1333\n#>  4  2013     2     3     1018\n#>  5  2013     6    12     1953\n#>  6  2013    10    11     1055\n#>  7  2013     2    27      700\n#>  8  2013     4    20      711\n#>  9  2013     7    24     1818\n#> 10  2013     9     9     1809\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_frac(flights_tbl, 0.01) %>% \n  count()\n#> # Source:   SQL [?? x 1]\n#> # Database: spark_connection\n#>       n\n#>   <dbl>\n#> 1  3368\n```\n:::\n\n\n## Hive Functions\n\nMany of Hive’s built-in functions (UDF) and built-in aggregate functions\n(UDAF) can be called inside dplyr’s mutate and summarize. The [Languange\nReference\nUDF](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF)\npage provides the list of available functions.\n\nThe following example uses the **datediff** and **current\\_date** Hive\nUDFs to figure the difference between the flight\\_date and the current\nsystem date:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflights_tbl %>% \n  mutate(\n    flight_date = paste(year,month,day,sep=\"-\"),\n    days_since = datediff(current_date(), flight_date)\n    ) %>%\n  group_by(flight_date,days_since) %>%\n  count() %>%\n  arrange(-days_since)\n#> # Source:     SQL [?? x 3]\n#> # Database:   spark_connection\n#> # Groups:     flight_date, days_since\n#> # Ordered by: -days_since\n#>    flight_date days_since     n\n#>    <chr>            <int> <dbl>\n#>  1 2013-1-1          4664   842\n#>  2 2013-1-2          4663   943\n#>  3 2013-1-3          4662   914\n#>  4 2013-1-4          4661   915\n#>  5 2013-1-5          4660   720\n#>  6 2013-1-6          4659   832\n#>  7 2013-1-7          4658   933\n#>  8 2013-1-8          4657   899\n#>  9 2013-1-9          4656   902\n#> 10 2013-1-10         4655   932\n#> # ℹ more rows\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_disconnect(sc)\n```\n:::\n\n\n\n",
    "supporting": [
      "dplyr_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}