{
  "hash": "17522e26c37fbc12176fddca1a133491",
  "result": {
    "markdown": "---\ntitle: \"Apply an R Function in Spark\"\nexecute:\n  freeze: true\n---\n\n\n\n\n*R/spark_apply.R*\n\n## spark_apply\n\n## Description\n Applies an R function to a Spark object (typically, a Spark DataFrame). \n\n\n## Usage\n```r\n \nspark_apply( \n  x, \n  f, \n  columns = NULL, \n  memory = TRUE, \n  group_by = NULL, \n  packages = NULL, \n  context = NULL, \n  name = NULL, \n  barrier = NULL, \n  fetch_result_as_sdf = TRUE, \n  partition_index_param = \"\", \n  arrow_max_records_per_batch = NULL, \n  auto_deps = FALSE, \n  ... \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | An object (usually a `spark_tbl`) coercable to a Spark DataFrame. |\n| f | A function that transforms a data frame partition into a data frame.   The function `f` has signature `f(df, context, group1, group2, ...)` where   `df` is a data frame with the data to be processed, `context`   is an optional object passed as the `context` parameter and `group1` to   `groupN` contain the values of the `group_by` values. When   `group_by` is not specified, `f` takes only one argument.    Can also be an `rlang` anonymous function. For example, as `~ .x + 1`   to define an expression that adds one to the given `.x` data frame. |\n| columns | A vector of column names or a named vector of column types for the transformed object. When not specified, a sample of 10 rows is taken to infer out the output columns automatically, to avoid this performance penalty, specify the column types. The sample size is configurable using the `sparklyr.apply.schema.infer` configuration option. |\n| memory | Boolean; should the table be cached into memory? |\n| group_by | Column name used to group by data frame partitions. |\n| packages | Boolean to distribute `.libPaths()` packages to each node,   a list of packages to distribute, or a package bundle created with   `spark_apply_bundle()`.    Defaults to `TRUE` or the `sparklyr.apply.packages` value set in   `spark_config()`.    For clusters using Yarn cluster mode, `packages` can point to a package   bundle created using `spark_apply_bundle()` and made available as a Spark   file using `config$sparklyr.shell.files`. For clusters using Livy, packages   can be manually installed on the driver node.    For offline clusters where `available.packages()` is not available,   manually download the packages database from  https://cran.r-project.org/web/packages/packages.rds and set   `Sys.setenv(sparklyr.apply.packagesdb = \"<pathl-to-rds>\")`. Otherwise,   all packages will be used by default.    For clusters where R packages already installed in every worker node,   the `spark.r.libpaths` config entry can be set in `spark_config()`   to the local packages library. To specify multiple paths collapse them   (without spaces) with a comma delimiter (e.g., `\"/lib/path/one,/lib/path/two\"`). |\n| context | Optional object to be serialized and passed back to `f()`. |\n| name | Optional table name while registering the resulting data frame. |\n| barrier | Optional to support Barrier Execution Mode in the scheduler. |\n| fetch_result_as_sdf | Whether to return the transformed results in a Spark   Dataframe (defaults to `TRUE`). When set to `FALSE`, results will be   returned as a list of R objects instead.    NOTE: `fetch_result_as_sdf` must be set to `FALSE` when the transformation   function being applied is returning R objects that cannot be stored in a Spark   Dataframe (e.g., complex numbers or any other R data type that does not have an   equivalent representation among Spark SQL data types). |\n| partition_index_param | Optional if non-empty, then `f` also receives   the index of the partition being processed as a named argument with this name, in   addition to all positional argument(s) it will receive    NOTE: when `fetch_result_as_sdf` is set to `FALSE`, object returned from the   transformation function also must be serializable by the `base::serialize`   function in R. |\n| arrow_max_records_per_batch | Maximum size of each Arrow record batch, ignored if Arrow serialization is not enabled. |\n| auto_deps | [Experimental] Whether to infer all required R packages by examining the closure `f()` and only distribute required R and their transitive dependencies to Spark worker nodes (default: FALSE). NOTE: this option will only take effect if `packages` is set to `TRUE` or is a character vector of R package names. If `packages` is a character vector of R package names, then both the set of packages specified by `packages` and the set of inferred packages will be distributed to Spark workers. |\n| ... | Optional arguments; currently unused. |\n\n\n## Section\n\n## Configuration\n\n   `spark_config()` settings can be specified to change the workers environment.  For instance, to set additional environment variables to each worker node use the `sparklyr.apply.env.*` config, to launch workers without `--vanilla` use `sparklyr.apply.options.vanilla` set to `FALSE`, to run a custom script before launching Rscript use `sparklyr.apply.options.rscript.before`. \n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n \n \nlibrary(sparklyr) \nsc <- spark_connect(master = \"local[3]\") \n \n# creates an Spark data frame with 10 elements then multiply times 10 in R \nsdf_len(sc, 10) %>% spark_apply(function(df) df * 10) \n#> # Source: spark<?> [?? x 1]\n#>       id\n#>    <dbl>\n#>  1    10\n#>  2    20\n#>  3    30\n#>  4    40\n#>  5    50\n#>  6    60\n#>  7    70\n#>  8    80\n#>  9    90\n#> 10   100\n \n# using barrier mode \nsdf_len(sc, 3, repartition = 3) %>% \n  spark_apply(nrow, barrier = TRUE, columns = c(id = \"integer\")) %>% \n  collect() \n#> # A tibble: 3 Ã— 1\n#>      id\n#>   <int>\n#> 1     1\n#> 2     1\n#> 3     1\n```\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}