{
  "hash": "d99ae6ea9813de33d9de78db6484b424",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Tune `tidymodels` remotely in Databricks Connect\"\nexecute:\n  eval: true\n  freeze: true\n---\n\n\n\n## Intro\n\n`sparklyr` makes it easy to take advantage of Databricks Connect to tune\nTidymodels. \nSimply call `tune_spark_grid()` instead of `tune_spark()`. `sparklyr` will take \ncare of all the setup and parallel job scheduling inside Databricks, see @fig-diagram. \nThe output from `tune_spark_grid()` is the exact same type of R object\n`tune_grid()` returns. \n\n:::{#fig-diagram}\n![](/images/guides/tidymodels-db/diagram-1.png)\n\nHow *tune_grid_spark()* works with Databricks\n:::\n\n`sparklyr` accepts the exact same arguments as the function in `tune`. The only\nadditional required argument is the Databricks connection object. In @fig-code\nwe show how the exact same parsnip model, recipe and resamples can be passed\nto both.\n\n:::{#fig-code}\n:::{.columns}\n:::{.column width=\"48%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\ntune::tune_grid(\n  object = my_model, \n  preprocessor = my_recipe, \n  resamples = my_resamples\n  \n  )\n```\n:::\n\n\nRun locally\n\n:::\n:::{.column width=\"4%\"}\n:::\n:::{.column width=\"48%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nsparklyr::tune_grid_spark(\n  object = my_model, \n  preprocessor = my_recipe, \n  resamples = my_resamples,\n  sc = my_conn # Only additional requirement\n  )\n```\n:::\n\n\nRun remotely in Databricks\n\n:::\n:::\nComparing tune and sparklyr function calls\n:::\n\n## Example\n\n### Setup the Tidymodels objects\n\nThe model building, the data pre-processing and re-sampling steps remain the \nexact same. `sparklyr` will upload the R objects resulting from each step onto\nthe Databricks cluster, so that it can process the hyper-tuning using the exact\nsame information.\n\nTo demonstrate the functionality in this article, we start with a standard\ntuning workflow. In other words, there is nothing different to do in Tidymodels\nto setup for the actual model tuning process:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(readmission)\n\nset.seed(1)\n\n# Data set resampling \nreadmission_splits <- initial_split(readmission, strata = readmitted)\n\nreadmission_folds <- vfold_cv(\n  data = training(readmission_splits),\n  strata = readmitted\n  )\n\n# Data pre-processing\nrecipe_basic <- recipe(readmitted ~ ., data = readmission) |>\n  step_mutate(\n    race = factor(case_when(\n      !(race %in% c(\"Caucasian\", \"African American\")) ~ \"Other\",\n      .default = race\n    ))\n  ) |>\n  step_unknown(all_nominal_predictors()) |>\n  step_YeoJohnson(all_numeric_predictors()) |>\n  step_normalize(all_numeric_predictors()) |>\n  step_dummy(all_nominal_predictors())\n\n# Model specification\nspec_bt <- boost_tree(\n  mode = \"classification\",\n  mtry = tune(), # Part of the hyper-parameters to tune\n  learn_rate = tune(), # Part of the hyper-parameters to tune\n  trees = 10\n  )\n```\n:::\n\n\n### Tune in Databricks Connect\n\nThe first step is to connect to the Databricks cluster. This operation is\nonly needed one time during the R session: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n\nsc <- spark_connect(\n  method = \"databricks_connect\", \n  cluster_id = \"1218-000327-q970zsow\" # Replace with your own cluster's ID\n  )\n#> ℹ Retrieving info for cluster:'1218-000327-q970zsow'\n#> ✔ Cluster: '1218-000327-q970zsow' | DBR: '17.3' [501ms]\n#> \n#> ℹ Attempting to load 'r-sparklyr-databricks-17.3'\n#> ✔ Python environment: 'r-sparklyr-databricks-17.3' [1.3s]\n#> \n#> ℹ Connecting to '8 cores'\n#> ✔ Connected to: '8 cores' [6ms]\n#> \n```\n:::\n\n\nThe next step is to call `tune_grid_spark()`. We will pass the model, recipe\nand re-sample objects prepared in the previous section. The only addition is\nto have the process run in `verbose` mode, this is an optional argument. It \nwill display the steps and timings for each:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_results <- tune_grid_spark(\n  sc = sc, \n  object = spec_bt, \n  preprocessor = recipe_basic, \n  resamples = readmission_folds,\n  control = control_grid(verbose = TRUE)\n  )\n#> i Creating pre-processing data to finalize 1 unknown parameter: \"mtry\"\n#> ℹ Uploading model, pre-processor, and other info to the Spark session\n#> ✔ Uploading model, pre-processor, and other info to the Spark session [650ms]\n#> \n#> ℹ Uploading the re-samples to the Spark session\n#> ✔ Uploading the re-samples to the Spark session [2.3s]\n#> \n#> ℹ Copying the grid to the Spark session\n#> ✔ Copying the grid to the Spark session [177ms]\n#> \n#> ℹ Executing the model tuning in Spark\n#> ✔ Executing the model tuning in Spark [26.3s]\n#> \n```\n:::\n\n\nThe `spark_results` object can be now used as if it was a tuned object that\nwas prepared locally. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(spark_results)\n```\n\n::: {.cell-output-display}\n![](tune_tidymodels_databricks_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n## R and Python libraries\n\nThere are Python and R packages that need to be pre-installed in the Databricks\ncluster:\n\n**Python**\n\n- `rpy2`\n\n**R**\n\n- `tidymodels`\n\n- `reticulate`\n\n- The modeling package used by `parsnip` in the tuning \n\nInstallation is a simple operation that is done via your Databricks web portal. \nHere are the instructions that shows you how to do that: [Databricks - Cluster\nLibraries](https://docs.databricks.com/en/libraries/cluster-libraries.html).\n\n\n### Install programmatically (optional)\n\nThe [`brickster`](https://databrickslabs.github.io/brickster/) package is a \ncomplete toolkit for interacting with Databricks. It can be used to \nprogrammatically install the Python and R libraries. It requires that the\nuser has rights to modify the libraries in target cluster.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brickster)\n\n# DBRs have a fixed snapshot date to source the R packages. Redirecting  \n# to the 'latest' snapshot to get the most recent package versions.\nrepo <- \"https://databricks.packagemanager.posit.co/cran/__linux__/noble/latest\"\n\n# Builds the list of R packages, pointing them to the 'latest' snapshot\nr_libs <- libraries(\n  lib_cran(\"reticulate\", repo = repo),\n  lib_cran(\"tidymodels\", repo = repo),\n  lib_cran(\"xgboost\", repo = repo) # (optional) Used in the example\n)\n\ndb_libs_install(\"1218-000327-q970zsow\", r_libs)\n\n# Builds the Python package object. Specifying the version of `rpy2` to install.\npy_lib <- lib_pypi(\"rpy2==3.6.4\") |>\n  libraries()\n\ndb_libs_install(\"1218-000327-q970zsow\", py_lib)\n```\n:::\n\n\nAs time goes by, there may be other modeling R packages that need to be installed\nin Spark. Here is a template that can be used to install a single package: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brickster)\n\nlib_cran(\n  package = \"[Missing package]\", \n  repo = \"https://databricks.packagemanager.posit.co/cran/__linux__/noble/latest\"\n  ) |> \n  libraries() |> \n  db_libs_install(\"[Your cluster ID]\", libraries =  _)\n\n```\n:::\n\n",
    "supporting": [
      "tune_tidymodels_databricks_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}