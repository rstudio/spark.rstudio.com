{
  "hash": "fe1dfef6aa853e6254bde9f4760ffd3b",
  "result": {
    "markdown": "---\ntitle: Databricks Connect v2\nformat:\n  html:\n    theme: default\n    toc: true\nexecute:\n    eval: true\n    freeze: true\neditor: \n  markdown: \n    wrap: 72\naliases:\n  - /examples/databricks-cluster\n  - /deployment/databricks-cluster.html\n  - /deployment/databricks-cluster-odbc.html  \n  - /deployment/databricks-cluster-local.html\n  - /deployment/databricks-cluster-remote.html\n  - /deployment/databricks-spark-connect.html  \n---\n\n\n\n\n*Last updated: Thu Dec  7 14:21:48 2023*\n\n## Intro\n\n[Spark\nConnect](https://spark.apache.org/docs/latest/spark-connect-overview.html)\nintroduced a decoupled client-server architecture that allows remote\nconnectivity to Spark clusters using the DataFrame API. The separation\nbetween client and server allows Spark to be leveraged from everywhere,\nand this would allow R users to interact with a cluster from the comfort\nof their preferred mathine environment. The new [Databricks\nConnect](https://docs.databricks.com/dev-tools/databricks-connect.html), often \nreferred to as **\"Databricks Connect v2\"**, \navailable in Databricks Runtime version 13 and above, is based on this\nnew architecture.\n\n#### Solution architecture \n\nThe Spark Connect API is very different than the \"legacy\" Spark, using the\nSpark shell is no longer an option. We have decided to use Python as the\ninterface to this new API. In turn, Python uses *gRPC* to interact with Spark. \n\n::: {#fig-connect}\n\n\n```{mermaid}\n%%| fig-width: 10\nflowchart LR\n  subgraph lp[test]\n    subgraph r[R]\n      sr[sparklyr]\n      rt[reticulate]\n    end\n    subgraph ps[Python]\n      dc[Databricks Connect]\n      g1[gRPC]\n    end\n  end   \n  subgraph db[Databricks]\n    sp[Spark]   \n  end\n  sr <--> rt\n  rt <--> dc\n  g1 <-- Internet<br>Connection --> sp\n  dc <--> g1\n  \n  style r   fill:#fff,stroke:#666,color:#000\n  style sr  fill:#fff,stroke:#666,color:#000\n  style rt  fill:#fff,stroke:#666,color:#000\n  style ps  fill:#fff,stroke:#666,color:#000\n  style lp  fill:#fff,stroke:#666,color:#fff\n  style db  fill:#fff,stroke:#666,color:#000\n  style sp  fill:#fff,stroke:#666,color:#000\n  style g1  fill:#fff,stroke:#666,color:#000\n  style dc  fill:#fff,stroke:#666,color:#000\n```\n\n\nHow `sparklyr` communicates with Databricks Connect\n:::\n\nWe are using `reticulate` to interact with the Python API. `sparklyr` extends \nthe functionality, and user experience, by providing the `dplyr`back-end, `DBI` \nback-end, RStudio's Connection pane integration.\n\nIn order to quickly iterate on enhancements and bug fixes, we have\ndecided to isolate the Python integration into its own package. The new\npackage, called `pysparklyr`, is an extension of `sparklyr`.\n\n\n\n## Getting Started\n\n### Package Installation\n\nTo access Databricks Connect, you will need the following two packages:\n\n- `sparklyr` - 1.8.4\n- `pysparklyr` - 0.1.2\n\n``` r\ninstall.packages(\"sparklyr\")\ninstall.packages(\"pysparklyr\")\n```\n\n### Setup credentials\n\nTo use with Databricks Connect, in run-time 13 or above, you will need\nthree configuration items:\n\n1.  Your [Workspace Instance\n    URL](https://docs.databricks.com/workspace/workspace-details.html#workspace-url) \n1.  Your \n    [Personal Authentication Token](https://docs.databricks.com/dev-tools/auth.html#pat) (PAT)\n1.   Your \n    [cluster's](https://docs.databricks.com/workspace/workspace-details.html#cluster-url-and-id)\n    ID \n\nWe have developed this solution to align with other R, and non-R, applications that \nintegrate with Databricks. All applications need, at minimum, a work space *(1)*, \nand the authentication token *(2)*. For default values, those applications initially \nlook for these environment variables: \n\n-   `DATABRICKS_HOST` - Your Workspace Instance URL\n-   `DATABRICKS_TOKEN` - Your Personal Authentication Token\n\nEnvironment variables work well, because they rarely vary between projects. The \nthing that will change more often is the cluster you are connecting to. It also \nmakes connection safer, because the token's contents will not be in your code in\nplain text. We recommend that you set  these two variables at your **user level**. \nTo do this run:\n\n```r\nusethis::edit_r_environ()\n```\n\nThat command will open a text file that controls the environment variables at\nthe **user level**. If missing, insert the entries for the two variables:\n\n```bash\nDATABRICKS_HOST=\"Enter here your Workspace URL\"\nDATABRICKS_TOKEN=\"Enter here your personal token\"\n```\n\n**This is a one time operation.** After saving and closing the file, restart your\nR session. \n\n### First time connecting\n\nAfter setting up your Host and Token environment variables, you can now connect\nto your cluster by simply providing the cluster's ID, and the method to \n`spark_connect()`:\n\n``` r\nlibrary(sparklyr)\n\nsc <- spark_connect(\n  cluster_id = \"Enter here your cluster ID\",\n  method = \"databricks_connect\"\n)\n```\n\nIn order to connect and interact with Databricks, you will need a specific set\nof [Python libraries](#python-libraries) installed and available. \nTo make it easier to get started, we provide functionality that will \nautomatically do the following:\n\n- Create, or re-create, a Python environment. Based on your OS, it will choose \nto create a Virtual Environment, or Conda.\n\n- Install the needed Python libraries\n\n`spark_connect()` will check to see if you have the expected Python environment,\nand prompt you to accept its installation if missing. Here is an example of \nthe code and output you would expect to see: \n\n```r\nsc <- spark_connect(\n    cluster_id = \"1026-175310-7cpsh3g8\",\n    method = \"databricks_connect\"\n)\n\n#> ! Retrieving version from cluster '1026-175310-7cpsh3g8' \n#> Cluster version: '14.1' \n#> ! No viable Python Environment was identified for Databricks Connect version 14.1 \n#> Do you wish to install Databricks Connect version 14.1? \n#>  \n#> 1: Yes \n#> 2: No \n#> 3: Cancel \n#>  \n#> Selection: 1 \n```\n\nAfter accepting, the Python environment will be created with a specific name,\nand all of the needed Python libraries will be installed within. After it is done,\nit will attempt to connect to your cluster. Here is an abbreviated example of the\noutput that occurs when selecting \"Yes\": \n\n```r\n#> ✔ Automatically naming the environment:'r-sparklyr-databricks-14.1' \n#> Using Python: /Users/edgar/.pyenv/versions/3.10.13/bin/python3.10 \n#> Creating virtual environment 'r-sparklyr-databricks-14.1' ... \n#> + /Users/edgar/.pyenv/versions/3.10.13/bin/python3.10 -m venv /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1 \n#> Done! \n#>   Installing packages: pip, wheel, setuptools \n#> + /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python -m pip install --upgrade pip wheel setuptools \n#> Requirement already satisfied: pip in /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/lib/python3.10/site-packages (23.0.1) \n#> Collecting pip \n#> Using cached pip-23.3.1-py3-none-any.whl (2.1 MB) \n#> Collecting wheel \n#> Using cached wheel-0.42.0-py3-none-any.whl (65 kB) \n#> Requirement already satisfied: setuptools in /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/lib/python3.10/site-packages (65.5.0) \n...\n...\n...\n#> Successfully installed PyArrow-14.0.1 cachetools-5.3.2 certifi-2023.11.17 charset-normalizer-3.3.2 databricks-connect-14.1.0 databricks-sdk-0.14.0 google-api-core-2.14.0 google-api-python-client-2.109.0 google-auth-2.25.0 google-auth-httplib2-0.1.1 googleapis-common-protos-1.61.0 grpcio-1.59.3 grpcio_status-1.59.3 httplib2-0.22.0 idna-3.6 numpy-1.26.2 pandas-2.1.3 protobuf-4.25.1 py4j-0.10.9.7 pyasn1-0.5.1 pyasn1-modules-0.3.0 pyparsing-3.1.1 python-dateutil-2.8.2 pytz-2023.3.post1 requests-2.31.0 rsa-4.9 six-1.16.0 tzdata-2023.3 uritemplate-4.1.1 urllib3-2.1.0 \n#> ✔ Using the 'r-sparklyr-databricks-14.1' Python environment \n#> Path: /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python \n```\n\n## Interacting with the cluster\n\n### RStudio's Connection pane\n\nThanks to the new way we are integrating with Spark, it is now possible\nto display the same structure displayed in the Databricks Data Explorer\npage. In Databricks, the current data structure levels are:\n\n-   Catalog\n    -   Database\n        -   Table\n\nIn RStudio, you can navigate the data structure by expanding from the\ntop level, all the way down to the table you wish to explore. Once\nexpanded, the table's fields, and their types are displayed.\n\n![](/images/deployment/connect/rstudio-connection.png)\n\nIn the Connection Pane, you can click on the **table** icon, situated to\nthe right of the table name, to preview the first 1,000 rows:\n\n![](/images/deployment/connect/preview.png)\n\n### Using the Connection to Access Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(sparklyr)\n\nsc <- spark_connect(\n    cluster_id = \"1026-175310-7cpsh3g8\",\n    method = \"databricks_connect\"\n)\n#> ! Sanitizing Databricks Host (`master`) entry:\n#>   Original: rstudio-partner-posit-default.cloud.databricks.com\n#>   Using: https://rstudio-partner-posit-default.cloud.databricks.com\n#>   Set `host_sanitize = FALSE` in `spark_connect()` to avoid this change\n#> ! Retrieving version from cluster '1026-175310-7cpsh3g8'\n#>   Cluster version: '14.1'\n#> ✔ Using the 'r-sparklyr-databricks-14.1' Python environment\n#>   Path: /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python\n```\n:::\n\n\nAfter connecting, you can use `dbplyr`'s `in_catalog()` function to\naccess any table in your data catalog. You will only need to pass the\nrespective names of the three levels as comma separated character\nentries to `in_catalog()` in this order: Catalog, Database, and Table.\n\nHere is an example of using `tbl()` and `in_catalog()` to point to the\n**trips** table, which is inside **nyctaxi** database, which is a\ndatabase inside the **samples** catalog:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrips <- tbl(sc, in_catalog(\"samples\", \"nyctaxi\", \"trips\"))\n\ntrips\n#> # Source: spark<trips> [?? x 6]\n#>    tpep_pickup_datetime tpep_dropoff_datetime trip_distance fare_amount\n#>    <dttm>               <dttm>                        <dbl>       <dbl>\n#>  1 2016-02-14 10:52:13  2016-02-14 11:16:04            4.94        19  \n#>  2 2016-02-04 12:44:19  2016-02-04 12:46:00            0.28         3.5\n#>  3 2016-02-17 11:13:57  2016-02-17 11:17:55            0.7          5  \n#>  4 2016-02-18 04:36:07  2016-02-18 04:41:45            0.8          6  \n#>  5 2016-02-22 08:14:41  2016-02-22 08:31:52            4.51        17  \n#>  6 2016-02-05 00:45:02  2016-02-05 00:50:26            1.8          7  \n#>  7 2016-02-15 09:03:28  2016-02-15 09:18:45            2.58        12  \n#>  8 2016-02-25 13:09:26  2016-02-25 13:24:50            1.4         11  \n#>  9 2016-02-13 10:28:18  2016-02-13 10:36:36            1.21         7.5\n#> 10 2016-02-13 18:03:48  2016-02-13 18:10:24            0.6          6  \n#> # ℹ more rows\n#> # ℹ 2 more variables: pickup_zip <int>, dropoff_zip <int>\n```\n:::\n\n\nAfter pointing `tbl()` to that specific table, you can then use `dplyr`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrips %>%\n  group_by(pickup_zip) %>%\n  summarise(\n    count = n(),\n    avg_distance = mean(trip_distance, na.rm = TRUE)\n  )\n#> # Source: spark<?> [?? x 3]\n#>    pickup_zip count avg_distance\n#>         <int> <dbl>        <dbl>\n#>  1      10032    15         4.49\n#>  2      10013   273         2.98\n#>  3      10022   519         2.00\n#>  4      10162   414         2.19\n#>  5      10018  1012         2.60\n#>  6      11106    39         2.03\n#>  7      10011  1129         2.29\n#>  8      11103    16         2.75\n#>  9      11237    15         3.31\n#> 10      11422   429        15.5 \n#> # ℹ more rows\n```\n:::\n\n\n## Machine Learning \n\nMachine Learning capabilities are currently available starting with Databricks\nRuntime version 14.1.  Compared to \"legacy\" Spark, Spark Connect's ML capabilities\nare limited. At this time, there is only one supported model, Logistic Regression,\nand two scaler transformers, namely Standard Scaler and Max Abs Scaler. `sparklyr`\nmakes that functionality available. \n\n### Using for the first time\n\nBy default, the Python environment that `sparklyr` creates, does not include\nthe libraries that relate to Machine Learning. These include Torch, and \n\"scikit-learn\". Some of the libraries are large in size, and sometimes they\nmay have some Python requirements that may make it challenging to new users.\nAdditionally , we have noticed that, at this time, there are not many users\nthat need to utilize ML capabilities. \n\nThe first time an ML function is accessed through `sparklyr`, you will be prompted\nto install the additional Python libraries which are needed to access such\nML capabilities.\n\n```r\nml_logistic_regression(tbl_mtcars, am ~ .)\n#> ! Required Python libraries to run ML functions are missing\n#>   Could not find: torch, torcheval, and scikit-learn\n#>   Do you wish to install? (This will be a one time operation)\n#> \n#> 1: Yes\n#> 2: Cancel\n#> \n#> Selection: 1\n#> Using virtual environment '/Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1' ...\n#> + /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python -m pip install --upgrade --no-user torch torcheval scikit-learn\n#> Collecting torch\n...\n```\n\n::: {.callout-note}\nIt is possible to install the ML libraries along with the required libraries. \nThere may be several reasons to do this, including trying to recreate the environment\nafter upgrading Python in your machine. Just pass `install_ml=TRUE` to the\ninstallation function:\n\n```r\ninstall_databricks(cluster_id = \"Enter your cluster's ID\", install_ml = TRUE)\n```\n\nor\n\n```r\ninstall_databricks(version = \"14.1\", install_ml = TRUE)\n```\n\n:::\n\n### Easily fit and use\n\nAt this time, Logistic Regression is the only model supported. As usual, there\nare specific data preparation steps in order to run. `sparklyr` automates those\nsteps, so all you have to do is pass the Spark data frame, and the formula\nto use:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars <- copy_to(sc, mtcars)\n\nmodel1 <- ml_logistic_regression(tbl_mtcars, am ~ .)\n```\n:::\n\n\nThe output for Spark Connect based models has been upgraded. It will display the\nmodel parameters. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1\n#> \n#> ── ML Connect model:\n#> Logistic Regression\n#> \n#> ── Parameters:\n#> ◼ batchSize:       32            ◼ momentum:        0.9        \n#> ◼ featuresCol:     features      ◼ numTrainWorkers: 1          \n#> ◼ fitIntercept:    TRUE          ◼ predictionCol:   prediction \n#> ◼ labelCol:        label         ◼ probabilityCol:  probability\n#> ◼ learningRate:    0.001         ◼ seed:            0          \n#> ◼ maxIter:         100           ◼ tol:             1e-06\n```\n:::\n\n\nAs shown in the following screenshot, the new output features a first-of-its-kind\ntooltip, it will popup the description of the parameter when hovered over. This \nfunctionality works when used in RStudio, and any console that supports this kind\nof enhanced user experience.\n\n![](/images/deployment/databricks/model-output.png)\nTo use the model, you can run `ml_predict()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nml_predict(model1, tbl_mtcars)\n#> # Source: spark<?> [?? x 13]\n#>      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb prediction\n#>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>      <dbl>\n#>  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4          0\n#>  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4          0\n#>  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1          1\n#>  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1          0\n#>  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2          0\n#>  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1          0\n#>  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4          0\n#>  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2          0\n#>  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2          0\n#> 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4          1\n#> # ℹ more rows\n#> # ℹ 1 more variable: probability <chr>\n```\n:::\n\n\n### Using feature transformers \n\nThese are the two feature transformers currently supported:\n\n- Standard Scaler - `ft_standard_scaler()`\n- Max Abs Caler - `ft_max_abs_scaler()`\n\nTo access simply call the function by passing a vector of column names. \nPlease note that it will create a single column with an array field that \ncontains all of the newly scaled values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars %>% \n  ft_standard_scaler(c(\"wt\", \"mpg\"), \"features\") %>% \n  select(wt, mpg, features)\n#> # Source: spark<?> [?? x 3]\n#>       wt   mpg features                                  \n#>    <dbl> <dbl> <chr>                                     \n#>  1  2.62  21   c(-0.610399567481535, 0.150884824647656)  \n#>  2  2.88  21   c(-0.349785269100972, 0.150884824647656)  \n#>  3  2.32  22.8 c(-0.917004624399845, 0.449543446630647)  \n#>  4  3.22  21.4 c(-0.00229953792688741, 0.217253407310543)\n#>  5  3.44  18.7 c(0.227654254761845, -0.230734525663943)  \n#>  6  3.46  18.1 c(0.248094591889732, -0.330287399658273)  \n#>  7  3.57  14.3 c(0.360516446093113, -0.960788934955698)  \n#>  8  3.19  24.4 c(-0.0278499593367465, 0.715017777282194) \n#>  9  3.15  22.8 c(-0.0687306335925211, 0.449543446630647) \n#> 10  3.44  19.2 c(0.227654254761845, -0.147773797335335)  \n#> # ℹ more rows\n```\n:::\n\n\nWhen you are done with you queries and computations, you should\ndisconnect from the cluster.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_disconnect(sc)\n```\n:::\n\n\n## Reported Problems\n\nAs it is with any new implementations, we are seeing some early adopters report\nissues with their installation or connections.  Here are some of the issues that \nhave been reported to us, and the recommended solution, or workaround.\nEach issue is **collapsed**, just expand it to see the background and \nrecommendation. Each one is titled based on the full error message \nor, the distinctive part of the error message.\n\nBefore reviewing the issues, please make sure to have the latest versions of \n`sparklyr`, and `pysparklyr` from CRAN. These are the current version levels:\n\n- `sparklyr` - 1.8.4\n- `pysparklyr` - 0.1.2\n\n\n::: {.callout-note collapse=\"true\"}\n## `...reticulate can only bind to copies of Python built with '--enable-shared'.`\n\nError message contains:\n\n```r\n... reticulate can only bind to copies of Python built with '--enable-shared'.\n```\n\nThis is happening because there is no Python installation that `reticulate` \ncan use. `pysparklyr` depends on that package to function. \n\n### Solution\n\nThe best way to resolve is to install an acceptable version of Python. You can run:\n\n```r\nreticulate::install_python()\n```\n\nIf the output mentions `--skip-existing [version number]` then you will have two options:\n\n1. If you're ok with replacing the existing Python, then use:\n   ```r\n    reticulate::install_python(version = \"[version number]\", force = TRUE)\n   ``` \n\n2. If you would like to keep that installation of Python then use a different version number:\n   ```r\n    reticulate::install_python(version = \"[slightly different version number]\")\n   ``` \n    For example, if the version number is `3.9.12`, then use `3.9.18`: \n  \n    ```r\n    reticulate::install_python(version = \"3.9.18\")\n    ``` \n:::\n\n::: {.callout-note collapse=\"true\"}\n##  `spark_connect()` returns `! Version '14.2' does not exist`\n\nError message contains: \n\n```r\n  Checking if provided version is valid against PyPi.org\n  Error in `install_environment()`\n  ! Version '14.2' does not exist\n```\n\nThis happens when `pysparklyr` gets the cluster DBR version automatically. Usually\nwhen running `spark_connect()` or `install_databricks()`. This occurs because\nthe latest version of DBR, as of today 14.2, is ahead of what the `databricks.connect`\nversion available in `databricks.connect`.\n\n### Solution\n\nCurrent workdaround is to say \"No\" when prompted to install version 14.2. And if \nmissing in your machine, install 14.1 using: \n\n```r\npysparklyr::install_databricks(\"14.1\")\n```\n\n:::\n\n## Environments\n\n### Install different version of `databricks.connect`\n\nHere are three different options to create a custom Python environment, that\nwill contain the needed Python libraries to interact with Databricks Connect:\n\n- To install the latest versions of all the needed libraries, use:\n  ```r\n  pysparklyr::install_databricks()\n  ```\n  `sparklyr` will query PyPi.org to get the latest version of `databricks.connect`\n  and installs that version. \n\n\n- It is recommended that the version of the `databricks.connect` library, matches \nthe DBR version of your cluster.   To do this, pass the DBR version in the\n`version` argument, for example:\n  ```r\n  pysparklyr::install_databricks(\"14.0\")\n  ```\n  This will create a Python environment and install `databricks.connect` version\n  14.0, and it will automatically name it `r-sparklyr-databricks-14.0`.  By using\n  this name, `sparklyr` is able to know what version of `databricks.connect` is\n  available inside this particular Python environment.\n\n- If you are not sure about the version of the cluster you want to interact with,\nthen use the `cluster_id` argument. We have added a way to pull the cluster's\ninformation, without starting a Spark connect. This allows us to query the\ncluster and get the DBR version:\n\n  ```r\n  pysparklyr::install_databricks(cluster_id = \"[Your cluster's ID]\")\n  ```\n\n### Restricted Python environments \n\nIf your organization restricts Python environment creation, you can \npoint `sparklyr` to the designated Python installation.  To do this, pass the\npath to the environment in the `envname` argument of `spark_connect()`:\n\n```r\nlibrary(sparklyr)\n\nsc <- spark_connect(\n  method = \"databricks_connect\",\n  cluster_id = \"Enter here your cluster ID\",\n  envname = \"Enter here the path to your Python environment\"\n)\n```\n\nTo successfully connect to a Databricks cluster, you will need to match the\nproper version of the `databricks.connect` Python library, to the Databricks\nRuntime (DBR) version in the cluster. For example, if you are trying to use a\nDatabricks cluster with a DBR version 14.0, then `databricks.connect` will also\nneed to be version 14.0. Failure to do so, can result in instability, or even\nthe inability to connect. \n\nBesides `datbricks.connect`, the Python environment will also need to have other\nPython libraries installed. The full list is in the [Python Libraries](#python-libraries)\nsection.\n\n:::{.callout-important}\nIf your server, or machine, has only one Python installation and, no ability to create\nConda or Virtual environments, then you will encounter issues when connecting to\na Databricks cluster with a mismatched version of `databricks.connect` to DBR.\n:::\n\n**Important** -  This step needs only to be **done one time**. If you need to connect to a different\ncluster, but that has the same DBR version, `sparklyr` will use the same\nPython environment.  If the new cluster has a different DBR version, then it is\nrecommended that you run the installation function using the new DBR version, or\ncluster ID.\n\n### Python Libraries\n\nHere is the list of the Python libraries needed in order to work with the cluster:\n\nRequired libraries: \n\n- `databricks-connect`\n- `delta-spark` \n- `pandas`\n- `PyArrow`\n- `grpcio`\n- `google-api-python-client`\n- `grpcio_status`\n\nML libraries (Optional): \n\n- `torch` \n- `torcheval`\n- `scikit-learn`\n\n## Deploying to Posit Connect\n\nTODO\n\n## What is supported\n\nHere is a list of what we currently support, and do not support via\n`sparklyr` and Connect:\n\n**Supported**:\n\n-   Integration with most of the `dplyr`, and `DBI`, APIs\n-   Integration with the `invoke()` command\n-   RStudio Connections Pane navigation\n-   Support for Personal Access Token security authentication for\n    Databricks Connect\n-   Support for most read and write commands. These have only been\n    tested in Spark Connect.\n\n**Not supported**:\n\n-   **ML functions** - Very few functions, in `sparklyr`, that have the `ml_`\n    and `ft_` are currently not supported. The reason is that Spark 3.4\n    does not currently support MLlib. We expect that some ML support\n    will be available in Spark 3.5. At that time we will work on\n    integrating the new ML routines from Connect into `sparklyr`.\n\n-   **SDF functions** - Most of these functions require SparkSession,\n    which is currently not supported in Spark 3.4.\n\n-   **`tidyr`** - This is ongoing work that we are focusing on in\n    `sparklyr`. We are implementing these functions using PySpark\n    DataFrame commands, instead of depending on the Scala\n    implementation.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}