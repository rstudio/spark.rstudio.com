{
  "hash": "794b50e029f9a68fe6f2e0a5ae9033a3",
  "result": {
    "markdown": "---\ntitle: \"Model Tuning - Part II\"\nexecute:\n  eval: false\n  freeze: true\n---\n\n\n\n\n## Goals\n\nThe aim of this article is to build on the knowledge from [Model Tuning](model_tuning.qmd){target=\"_blank\"} \nby covering the following: \n\n- Show how to re-use the code of an existing ML Pipeline as the base \nof the hyper-parameter tuning \n- Show how tuning parameters are not limited to only model's parameters. We will\ncover how to tune data transformation parameters\n \n## Recreating \"Tuning Text Analysis\"\n\nThe example in this article is based on the [Tuning Text Analysis](https://tune.tidymodels.org/articles/extras/text_analysis.html){target=\"_blank\"}\narticle found in the `tidymodels`' `tune` website. In that article, they use \n*Amazon's Fine Foods Reviews* text data to perform hyper parameter tuning. \n\nAs its name suggest, Model Tuning has two main phases: the modeling, and the tuning.\nIn `tidymodels`, the modeling is done using the `recepies` and `parsnip` packages,\nwhile the tuning is done with `tune`. `tune` is able to modify the \nthe `recipe` and model's arguments for each experiment. \n\nIn Spark, the modeling is done with an ML Pipeline. In itself, preparing the data, \nand setting up the model can be complex enough to merit its own walk-through. \nFor the walk-through of the model used in this article, please see\n[Text Modeling](textmodeling.qmd){target=\"_blank\"}.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nlibrary(modeldata)\n\ndata(\"small_fine_foods\")\n\nsc <- spark_connect(master = \"local\", version = \"3.3\")\n\nsff_training_data <- copy_to(sc, training_data)\nsff_testing_data <- copy_to(sc, testing_data)\n```\n:::\n\n\n## Pipeline\n\nThe [Text Modeling](textmodeling.qmd#prepare-the-model-with-an-ml-pipeline){target=\"_blank\"}\narticle was \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_pipeline <- ml_pipeline(sc) %>% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %>% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %>% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 1024\n    ) %>%\n  ft_normalizer(\n    input_col = \"hashed_features\", \n    output_col = \"normal_features\"\n    ) %>% \n  ft_r_formula(score ~ normal_features) %>% \n  ml_logistic_regression()\n```\n:::\n\n\n## Grid\n\n[Grid Search](https://tune.tidymodels.org/articles/extras/text_analysis.html#grid-search-1){target=\"_blank\"}\nsection in the `tidymodels` article. \n\n  penalty = 10^seq(-3, 0, length = 20)\n  mixture = seq(0, 1, length = 5)\n  num_terms = 2^c(8, 10, 12)\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_grid <-  list(\n    hashing_tf = list(\n      num_features = 2^c(8, 10, 12)  \n    ),\n    logistic_regression = list(\n      elastic_net_param = 10^seq(-3, 0, length = 20), \n      reg_param = seq(0, 1, length = 5)    \n    )\n  )\n\nsff_grid\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_grid <-  list(\n    hashing_tf = list(\n      num_features = 2^c(12)  \n    ),\n    logistic_regression = list(\n      elastic_net_param = 10^seq(-3, 0, length = 5), \n      reg_param = seq(0, 1, length = 3)    \n    )\n  )\n\nsff_grid\n```\n:::\n\n\n## Evaluator\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_evaluator <- ml_binary_classification_evaluator(sc)\n```\n:::\n\n\n## Model Tuning\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_cv <- ml_cross_validator(\n  x = sc,\n  estimator = sff_pipeline, \n  estimator_param_maps = sff_grid,\n  evaluator = sff_evaluator,\n  num_folds = 3,\n  parallelism = 4\n)\n\nsff_cv\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_model <- ml_fit(\n  x = sff_cv, \n  dataset = sff_training_data\n  )\n```\n:::\n\n\n## Validation metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_metrics <- ml_validation_metrics(sff_model)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nsff_metrics %>% \n  mutate(reg_param_1 = as.factor(reg_param_1)) %>% \n  ggplot() +\n  geom_line(aes(\n    x = elastic_net_param_1, \n    y = areaUnderROC, \n    color = reg_param_1\n    )) +\n  facet_wrap(~ num_features_2) +\n  theme_light()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\nsff_metrics %>% \n  arrange(desc(areaUnderROC)) %>% \n  head()\n```\n:::\n\n\n## Model selection\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_pipeline <- ml_pipeline(sc) %>% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %>% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %>% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 4096\n    ) %>%\n  ft_normalizer(\n    input_col = \"hashed_features\", \n    output_col = \"normal_features\"\n    ) %>% \n  ft_r_formula(score ~ normal_features) %>% \n  ml_logistic_regression(elastic_net_param = 0.018, reg_param = 0.5)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_fitted <- new_sff_pipeline %>% \n  ml_fit(sff_training_data)\n```\n:::\n\n\n## Test data metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_fitted %>% \n  ml_transform(sff_testing_data) %>% \n  ml_metrics_binary()\n```\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}