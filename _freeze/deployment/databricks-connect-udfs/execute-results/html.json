{
  "hash": "b69f24653b54f8882ae001c24bb27ab3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Run R inside Databricks Connect\nformat:\n  html:\n    toc: true\nexecute:\n    eval: false \n    freeze: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n*Last updated: Wed Feb  4 08:40:27 2026*\n\n## Intro\n\nSupport for `spark_apply()` is available starting with the following package\nversions: \n\n-   `sparklyr` - 1.8.5\n-   `pysparklyr` - 0.1.4\n\nDatabricks Connect is now able to run regular Python code inside Spark.\n`sparklyr` takes advantage of this capability by having Python transport\nand run the R code. It does this via the `rpy2` Python library. Using\nthis library also guarantees Arrow support.\n\n::: {#fig-connect}\n```{mermaid}\n%%| fig-width: 6\n%%| eval: true\nflowchart LR\n  subgraph mm [\"My machine\"]\n  nb1(\"` `\")\n    subgraph mmr[\"`R _(sparklyr)_`\"]\n    nb2(\"` `\")\n      subgraph mmrr[\"`reticulate`\"]\n      nb3(\"` `\")\n        subgraph mmp[\"`Python`\"]\n          nb4(\"` `\")\n          subgraph mmrp2[\"`rpy2`\"]\n             nb5(\"`_rpy2 'packages' the R code_`\")\n             mmrc[\"R code\"]\n          end\n        end\n      end\n    end\n  end\n  \n  subgraph db[Databricks]\n    nb6(\"` `\")\n    subgraph sr[Spark]\n      nb7(\"` `\")\n      subgraph pt[Python]\n        nb8(\"` `\")\n        subgraph dbrp2[rpy2]\n          nb9(\"`_rpy2 runs the R code_`\")\n          subgraph dbr[R]\n            dbrc[\"R code\"]\n          end\n        end\n      end\n    end\n  end\n\n\nmmrc --> dbrc\n\nstyle nb1 fill:#fff,stroke-width:0\nstyle nb2 fill:#fff,stroke-width:0\nstyle nb3 fill:#fff,stroke-width:0\nstyle nb4 fill:#fff,stroke-width:0\nstyle nb5 fill:#fff,stroke-width:0\nstyle nb6 fill:#fff,stroke-width:0\nstyle nb7 fill:#fff,stroke-width:0\nstyle nb8 fill:#fff,stroke-width:0\nstyle nb9 fill:#fff,stroke-width:0\nstyle mm  fill:#fff,stroke:#666,color:#000\nstyle mmr fill:#fff,stroke:#666,color:#000\nstyle mmrr fill:#fff,stroke:#666,color:#000\nstyle mmp fill:#fff,stroke:#666,color:#000\nstyle mmrp2 fill:#fff,stroke:#666,color:#000\nstyle mmr fill:#fff,stroke:#666,color:#000\nstyle db fill:#fff,stroke:#666,color:#000\nstyle sr fill:#fff,stroke:#666,color:#000\nstyle pt fill:#fff,stroke:#666,color:#000\nstyle dbr fill:#fff,stroke:#666,color:#000\nstyle dbrp2 fill:#fff,stroke:#666,color:#000\n```\n\nHow `sparklyr` uses rpy2 to run R code in Databricks Connect\n:::\n\n## Getting started\n\nIf you have been using `sparklyr` with Databricks Connect v2 already,\nthen after upgrading the packages, you will be prompted to install\n`rpy2` in your Python environment. The prompt will occur the first time\nyou use `spark_apply()` in an interactive R session. If this is the\nfirst time you are using `sparklyr` with Databricks Connect v2, please\nrefer to our intro article[\"Databricks Connect\nv2\"](/deployment/databricks-connect.qmd) to learn how to setup your\nenvironment.\n\nAs shown in the diagram on the previous section, `rpy2` is needed on the\nDatabricks cluster you plan to use. This means that you will need to\n\"manually\" install the library in the cluster. This is a simple\noperation that is done via your Databricks web portal. Here are the\ninstructions that shows you how to do that: [Databricks - Cluster\nLibraries](https://docs.databricks.com/en/libraries/cluster-libraries.html).\n\n## What is supported in `spark_apply()` - At a glance\n\n| Argument                      | Supported? | Notes                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n|---------|---------|------------------------------------------------------|\n| `x`                           | Yes        |                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| `f`                           | Yes        |                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| `columns`                     | Yes        | Requires a string entry that contains the name of the column and its Spark variable type. Accepted values are: `long`, `decimal`, `string`, `datetime` and `bool`. Example: `columns = \"x long, y string\"`. If not provided, `sparklyr` will automatically create one, by examining the first 10 records of `x`, and it will provide a `columns` spec you can use when running `spark_apply()` again. See: [Providing a schema](#providing-a-schema) |\n| `memory`                      | Yes        |                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| `group_by`                    | Yes        |                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| `packages`                    | **No**     | You will need to pre-install the needed R packages in your cluster via the Databricks web portal, see [R packages](#r-packages)                                                                                                                                                                                                                                                                                                                      |\n| `context`                     | **No**     |                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| `name`                        | Yes        |                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| `barrier`                     | Yes        | Support only on ungrouped data. In other words, it is valid when the `group_by` argument is used.                                                                                                                                                                                                                                                                                                                                                    |\n| `fetch_result_as_sdf`         | Yes        | At this time, `spark_apply()` inside Databricks Connect only supports rectangular data, so seeing to `FALSE` will always return a data frame.                                                                                                                                                                                                                                                                                                        |\n| `partition_index_param`       | **No**     |                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| `arrow_max_records_per_batch` | Yes        | Support only on ungrouped data. In other words, it is valid when the `group_by` argument is used.                                                                                                                                                                                                                                                                                                                                                    |\n| `auto_deps`                   | **No**     |                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| `...`                         |            |                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n\n## R packages {#r-packages}\n\nIf your `spark_apply()` call uses specific R packages, you will need to\npre-install those specific packages in your target cluster. This is a\nsimple operation, because you can do this via your Databricks web\nportal, please see [Databricks - Cluster\nLibraries](https://docs.databricks.com/en/libraries/cluster-libraries.html)\nto learn how to do this.\n\n::: callout-caution\n## Only CRAN packages supported\n\nThe Databricks cluster library interface is able to source packages from\nCRAN only. This means that packages installed from GitHub, or another\nalternative sources, will not be available.\n:::\n\n#### Additional background\n\nIn previous implementation, `spark_apply()` was able to easily copy the\nlocally installed R packages in order to ensure that your code will run\nin the cluster. This was possible because R, and RStudio, was running in\none of the matching servers in the Spark cluster. Because `sparklyr` is\nrunning on a remote machine, more likely a laptop, this is no longer an\noption. In the vast majority of cases, the remote machine will be on\ndifferent a Operating System than the cluster. Additionally,\ntransmitting the unpacked, compiled, R packages would take a long time\nover a broadband Internet connection.\n\n## Providing a schema {#providing-a-schema}\n\nPassing a schema in `columns` will make`spark_apply()` run faster.\nBecause if not provided, `sparklyr` has to collect the first 10 rows,\nand run the R code in order to try and determine the names and types of\nyour resulting data set. As a convenience, `sparklyr` will output a\nmessage with the schema it used as the schema. If you are going to rerun\nyour `spark_apply()` command again, you can copy and paste the output of\nthe message to you code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_apply(\n  tbl_mtcars,\n  nrow,\n  group_by = \"am\"\n)\n```\n:::\n\n\nPassing the `columns` argument, silences the message:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_apply(\n  tbl_mtcars,\n  nrow,\n  group_by = \"am\", \n  columns = \"am double, x long\"\n)\n```\n:::\n\n\n## Partition data\n\nTypically, with un-grouped data, the number of parallel jobs will\ncorrespond with the number of partitions of the data. For Databricks\nconnections, `sparklyr` will, by default, attempt to use Apache Arrow.\nThe Databricks Connect clusters come with Arrow installed. This approach\nalso changes how Spark will partition your data. Instead of the number\nof partitions, Spark will use the value in the \"Arrow Max Records per\nBach\" option. This option can be controlled directly in the\n`spark_apply()` call by setting the `arrow_max_records_per_batch`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_apply(tbl_mtcars, nrow, arrow_max_records_per_batch = 4, columns = \"x long\")\n```\n:::\n\n\nIf you pass a different Arrow Batch size than what the option is set to\ncurrently, `sparklyr` will change the value of that option, and will\nnotify you of that:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_apply(tbl_mtcars, nrow, arrow_max_records_per_batch = 2, columns = \"x long\")\n```\n:::\n\n\n## Limitations\n\n`spark_apply()` will not work on the [Serverless compute](https://docs.databricks.com/aws/en/compute/serverless/) \nbecause that service does not include an installation of R. \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}