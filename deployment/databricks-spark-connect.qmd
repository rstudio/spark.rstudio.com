---
title: "Spark Connect, and Databricks Connect v2"
format:
  html:
    theme: default
    toc: true
execute:
    eval: true
    freeze: true
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| include: false

library(remotes)
library(dplyr)
library(dbplyr)
library(sparklyr)
library(pysparklyr)

```

*Last updated: `r lubridate::date()`*

## Intro

[Spark
Connect](https://spark.apache.org/docs/latest/spark-connect-overview.html)
introduced a decoupled client-server architecture that allows remote
connectivity to Spark clusters using the DataFrame API. The separation
between client and server allows Spark to be leveraged from everywhere,
and this would allow R users to interact with a cluster from the comfort
of their preferred environment, laptop or otherwise. 

The new [Databricks
Connect](https://docs.databricks.com/dev-tools/databricks-connect.html), often 
referred to as "Databricks Connect v2", 
available in Databricks Runtime version 13 and above, is based on this
new architecture.

We are working on enhancing `sparklyr` so that it can bring the benefits
of Spark Connect to the R user community.

## The Solution

The Spark Connect API is very different than the "legacy" Spark, using the
Spark shell is no longer an option. We have decided to use Python as the
interface to this new API. In turn, Python uses *gRPC* to interact with Spark. 

::: {#fig-connect}
```{mermaid}
flowchart LR
  subgraph lp[User's machine]
    sr[sparklyr]
    rt[reticulate]
    ps[PySpark]
    g1[gRPC]
  end
  sp[Spark]
  
  sr --> rt
  rt --> ps
  g1 <-. Network .-> sp
  ps --> g1
  
  style sr  fill:#d0efb1,stroke:#666
  style rt  fill:#d0efb1,stroke:#666
  style ps  fill:#eff,stroke:#666
  style lp  fill:#fff,stroke:#666
  style sp  fill:#f4c430,stroke:#666
  style g1  fill:#447099,stroke:#666,color:#fff
```

How `sparklyr` communicates with Spark Connect, and Databricks Connect
:::

We are using `reticulate` to interact with the Python API. `sparklyr` extends 
the functionality, and user experience, by providing the `dplyr`back-end, `DBI` 
back-end, RStudio's Connection pane integration.

In order to quickly iterate on enhancements and bug fixes, we have
decided to isolate the Python integration into its own package. The new
package, called `pysparklyr`, is an extension of `sparklyr`.

## Package Installation

To access Spark Connect, you will need the following two packages:

- `sparklyr` - 1.8.4
- `pysparklyr` - 0.1.2

``` r
install.packages("sparklyr")
install.packages("pysparklyr")
```

## Databricks Connect (ML 13+) 

### Getting Started

To use with Databricks Connect, in run-time 13 or above, you will need
three configuration items:

1.  Your [Workspace Instance
    URL](https://docs.databricks.com/workspace/workspace-details.html#workspace-url) 
1.  Your 
    [Personal Authentication Token](https://docs.databricks.com/dev-tools/auth.html#pat) (PAT)
1.   Your 
    [cluster's](https://docs.databricks.com/workspace/workspace-details.html#cluster-url-and-id)
    ID 

We have developed this solution to align with other R, and non-R, applications that 
integrate with Databricks. All applications need, at minimum, a work space *(1)*, 
and the authentication token *(2)*. For default values, those applications initially 
look for these environment variables: 

-   `DATABRICKS_HOST` - Your Workspace Instance URL
-   `DATABRICKS_TOKEN` - Your Personal Authentication Token

Environment variables work well, because they rarely vary between projects. The 
thing that will change more often is the cluster you are connecting to. It also 
makes connection safer, because the token's contents will not be in your code in
plain text. We recommend that you set  these two variables at your **user level**. 
To do this run:

```r
usethis::edit_r_environ()
```

That command will open a text file that controls the environment variables at
the **user level**. If missing, insert the entries for the two variables:

```bash
DATABRICKS_HOST="Enter here your Workspace URL"
DATABRICKS_TOKEN="Enter here your personal token"
```

**This is a one time operation.** After saving and closing the file, restart your
R session. 

### First time connecting

After setting up your Host and Token environment variables, you can now connect
to your cluster by simply providing the cluster's ID, and the method to 
`spark_connect()`:

``` r
library(sparklyr)

sc <- spark_connect(
  cluster_id = "Enter here your cluster ID",
  method = "databricks_connect"
)
```

In order to connect and interact with Databricks, you will need a specific set
of [Python libraries](#additional-setup-details) installed and available. 
To make it easier to get started, we provide functionality that will 
automatically do the following:

- Create, or re-create, a Python environment. Based on your OS, it will choose 
to create a Virtual Environment, or Conda.

- Install the needed Python libraries

`spark_connect()` will check to see if you have the expected Python environment,
and prompt you to accept its installation if missing. Here is an example of 
the code and output you would expect to see: 

```r
sc <- spark_connect(
    cluster_id = "1026-175310-7cpsh3g8",
    method = "databricks_connect"
)

#> ! Retrieving version from cluster '1026-175310-7cpsh3g8' 
#> Cluster version: '14.1' 
#> ! No viable Python Environment was identified for Databricks Connect version 14.1 
#> Do you wish to install Databricks Connect version 14.1? 
#>  
#> 1: Yes 
#> 2: No 
#> 3: Cancel 
#>  
#> Selection: 1 
```

After accepting, the Python environment will be created with a specific name,
and all of the needed Python libraries will be installed within. After it is done,
it will attempt to connect to your cluster. Here is an abbreviated example of the
output that occurs when selecting "Yes": 

```r
#> ✔ Automatically naming the environment:'r-sparklyr-databricks-14.1' 
#> Using Python: /Users/edgar/.pyenv/versions/3.10.13/bin/python3.10 
#> Creating virtual environment 'r-sparklyr-databricks-14.1' ... 
#> + /Users/edgar/.pyenv/versions/3.10.13/bin/python3.10 -m venv /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1 
#> Done! 
#>   Installing packages: pip, wheel, setuptools 
#> + /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python -m pip install --upgrade pip wheel setuptools 
#> Requirement already satisfied: pip in /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/lib/python3.10/site-packages (23.0.1) 
#> Collecting pip 
#> Using cached pip-23.3.1-py3-none-any.whl (2.1 MB) 
#> Collecting wheel 
#> Using cached wheel-0.42.0-py3-none-any.whl (65 kB) 
#> Requirement already satisfied: setuptools in /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/lib/python3.10/site-packages (65.5.0) 
...
...
...
#> Successfully installed PyArrow-14.0.1 cachetools-5.3.2 certifi-2023.11.17 charset-normalizer-3.3.2 databricks-connect-14.1.0 databricks-sdk-0.14.0 google-api-core-2.14.0 google-api-python-client-2.109.0 google-auth-2.25.0 google-auth-httplib2-0.1.1 googleapis-common-protos-1.61.0 grpcio-1.59.3 grpcio_status-1.59.3 httplib2-0.22.0 idna-3.6 numpy-1.26.2 pandas-2.1.3 protobuf-4.25.1 py4j-0.10.9.7 pyasn1-0.5.1 pyasn1-modules-0.3.0 pyparsing-3.1.1 python-dateutil-2.8.2 pytz-2023.3.post1 requests-2.31.0 rsa-4.9 six-1.16.0 tzdata-2023.3 uritemplate-4.1.1 urllib3-2.1.0 
#> ✔ Using the 'r-sparklyr-databricks-14.1' Python environment 
#> Path: /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python 
```

### RStudio's Connection pane

Thanks to the new way we are integrating with Spark, it is now possible
to display the same structure displayed in the Databricks Data Explorer
page. In Databricks, the current data structure levels are:

-   Catalog
    -   Database
        -   Table

In RStudio, you can navigate the data structure by expanding from the
top level, all the way down to the table you wish to explore. Once
expanded, the table's fields, and their types are displayed.

![](/images/deployment/connect/rstudio-connection.png)

In the Connection Pane, you can click on the **table** icon, situated to
the right of the table name, to preview the first 1,000 rows:

![](/images/deployment/connect/preview.png)

### Using the Connection to Access Data

After connecting, you can use `dbplyr`'s `in_catalog()` function to
access any table in your data catalog. You will only need to pass the
respective names of the three levels as comma separated character
entries to `in_catalog()` in this order: Catalog, Database, and Table.

Here is an example of using `tbl()` and `in_catalog()` to point to the
**trips** table, which is inside **nyctaxi** database, which is a
database inside the **samples** catalog:

```{r}
library(dplyr)
library(dbplyr)
library(sparklyr)

sc <- spark_connect(
    cluster_id = "1026-175310-7cpsh3g8",
    method = "databricks_connect"
)

trips <- tbl(sc, in_catalog("samples", "nyctaxi", "trips"))

trips
```

After pointing `tbl()` to that specific table, you can then use `dplyr`.

```{r}
trips %>%
  group_by(pickup_zip) %>%
  summarise(
    count = n(),
    avg_distance = mean(trip_distance, na.rm = TRUE)
  )
```

When you are done with you queries and computations, you should
disconnect from the cluster.

```{r}
spark_disconnect(sc)
```

### Restricted Python environments 

If your organization restricts Python environment creation, you can 
point `sparklyr` to the designated Python installation.  To do this, pass the
path to the environment in the `envname` argument of `spark_connect()`:

```r
library(sparklyr)

sc <- spark_connect(
  method = "databricks_connect",
  cluster_id = "Enter here your cluster ID",
  envname = "Enter here the path to your Python environment"
)
```

To successfully connect to a Databricks cluster, you will need to match the
proper version of the `databricks.connect` Python library, to the Databricks
Runtime (DBR) version in the cluster. For example, if you are trying to use a
Databricks cluster with a DBR version 14.0, then `databricks.connect` will also
need to be version 14.0. Failure to do so, can result in instability, or even
the inability to connect. 

Besides `datbricks.connect`, the Python environment will also need to have other
Python libraries installed. The full list is in the [Additional setup details](#additional-setup-details)
section.

:::{.callout-important}
If your server, or machine, has only one Python installation and, no ability to create
Conda or Virtual environments, then you will encounter issues when connecting to
a Databricks cluster with a mismatched version of `databricks.connect` to DBR.
:::

### Reported Problems

As it is with any new implementations, we are seeing some early adopters report
issues with their installation or connections.  Here are some of the issues that 
have been reported to us, and the recommended solution, or workaround.
Each issue is **collapsed**, just expand it to see the background and 
recommendation. Each one is titled based on the full error message 
or, the distinctive part of the error message.

Before reviewing the issues, please make sure to have the latest versions of 
`sparklyr`, and `pysparklyr` from CRAN. These are the current version levels:

- `sparklyr` - 1.8.4
- `pysparklyr` - 0.1.2


::: {.callout-note collapse="true"}
## `...reticulate can only bind to copies of Python built with '--enable-shared'.`

Error message contains:

```r
... reticulate can only bind to copies of Python built with '--enable-shared'.
```

This is happening because there is no Python installation that `reticulate` 
can use. `pysparklyr` depends on that package to function. 

### Solution

The best way to resolve is to install an acceptable version of Python. You can run:

```r
reticulate::install_python()
```

If the output mentions `--skip-existing [version number]` then you will have two options:

1. If you're ok with replacing the existing Python, then use:
   ```r
    reticulate::install_python(version = "[version number]", force = TRUE)
   ``` 

2. If you would like to keep that installation of Python then use a different version number:
   ```r
    reticulate::install_python(version = "[slightly different version number]")
   ``` 
    For example, if the version number is `3.9.12`, then use `3.9.18`: 
  
    ```r
    reticulate::install_python(version = "3.9.18")
    ``` 
:::


### Initial setup

Starting with `pysparklyr` version 0.1.2, this step is no longer needed. You 
will be prompted to install the proper Python environment if one is not found. 
If this is the first time you are using this solution, please go to the 
[Connecting](#connecting) section. 

`sparklyr` will need specific Python libraries in order to connect, and interact
with Databricks Connect. We provide a convenience function that will automatically
do the following:

- Create, or re-create, a Python environment. Based on your OS, it will choose
to create a Virtual Environment, or Conda. 

- Install the needed Python libraries

To install the latest versions of all the libraries, use:

```r
pysparklyr::install_databricks()
```

`sparklyr` will query PyPi.org to get the latest version of `databricks.connect`
and installs that version. It is recommended that the version of the 
`databricks.connect` library, matches  the DBR version of your cluster.  
To do this, pass the DBR version in the `version` argument, for example:

```r
pysparklyr::install_databricks("14.0")
```

This will create a Python environment and install `databricks.connect` version
14.0, and it will automatically name it `r-sparklyr-databricks-14.0`.  By using
this name, `sparklyr` is able to know what version of `databricks.connect` is
available inside this particular Python environment.

If you are not sure about the version of the cluster you want to interact with,
then use the `cluster_id` argument. We have added a way to pull the cluster's
information, without starting a Spark connect. This allows us to query the
cluster and get the DBR version:

```r
pysparklyr::install_databricks(cluster_id = "[Your cluster's ID]")
```


**Important** -  This step needs only to be **done one time**. If you need to connect to a different
cluster, but that has the same DBR version, `sparklyr` will use the same
Python environment.  If the new cluster has a different DBR version, then it is
recommended that you run the installation function using the new DBR version, or
cluster ID.

### Safely connect




## Spark Connect

### Initial setup

`sparklyr` will need specific Python libraries in order to connect, and interact
with Spark Connect. We provide a convenience function that will automatically
do the following:

- Create, or re-create, a Python environment. Based on your OS, it will choose
to create a Virtual Environment, or Conda. 

- Install the needed Python libraries

To install the latest versions of all the libraries, use:

```r
pysparklyr::install_pyspark()
```

`sparklyr` will query PyPi.org to get the latest version of PySpark
and installs that version. It is recommended that the version of the
PySpark library matches the Spark version of your cluster. 
To do this, pass the Spark version in the  `version` argument, for example:

```r
pysparklyr::install_pyspark("3.4")
```

We have seen Spark sessions crash, when the version of PySpark and the version
of Spark do not match. Specially, when using a newer version of PySpark is used
against an older version of Spark.  If you are having issues with your connection, 
definitely consider running the `install_pyspark()` to match that cluster's 
specific Spark version.

### Connecting

To start a session with a open source Spark cluster, via Spark Connect,
you will need to set the `master`, and `method`. The `master` will be an IP,
and maybe a port that you will need to pass. The protocol to use to put
together the proper connection URL is "sc://". For `method`, use
"spark_connect". Here is an example:

``` r
library(sparklyr)

sc <- spark_connect(
  master = "sc://[Host IP(:Host Port - optional)]", 
  method = "spark_connect"
  version = "[Version that matches your cluster]"
  )
```

If `version` is not passed, then `sparklyr` will automatically choose the 
installed Python environment with the highest PySpark version. In a console 
message, `sparklyr` will let you know which environment it will use.

### Run locally

It is possible to run Spark Connect in your machine We provide helper
functions that let you setup, and start/stop the services in locally.

If you wish to try this out, first install Spark 3.4 or above:

``` r
spark_install("3.4")
```

After installing, start the Spark Connect using:

```r
pysparklyr::spark_connect_service_start()
```

To connect to your local Spark Connect, use **localhost** as the address for 
`master`:


```r
sc <- spark_connect(
  master = "sc://localhost", 
  method = "spark_connect"
  )
```

Now, you are able to interact with your local Spark session:

```r
library(dplyr)

tbl_mtcars <- copy_to(sc, mtcars)

tbl_mtcars %>% 
  group_by(am) %>% 
  summarise(mpg = mean(mpg, na.rm = TRUE))
```

When done, you can disconnect from Spark Connect:

```r
spark_disconnect(sc)
```

The regular version of local Spark would terminate the local cluster
when the you pass `spark_disconnect()`. For Spark Connect, the local
cluster needs to be stopped independently.

```r
pysparklyr::spark_connect_service_stop()
```

## What is supported

Here is a list of what we currently support, and do not support via
`sparklyr` and Connect:

**Supported**:

-   Integration with most of the `dplyr`, and `DBI`, APIs
-   Integration with the `invoke()` command
-   RStudio Connections Pane navigation
-   Support for Personal Access Token security authentication for
    Databricks Connect
-   Support for most read and write commands. These have only been
    tested in Spark Connect.

**Not supported**:

-   **ML functions** - All functions, in `sparklyr`, that have the `ml_`
    and `ft_` are currently not supported. The reason is that Spark 3.4
    does not currently support MLlib. We expect that some ML support
    will be available in Spark 3.5. At that time we will work on
    integrating the new ML routines from Connect into `sparklyr`.

-   **SDF functions** - Most of these functions require SparkSession,
    which is currently not supported in Spark 3.4.

-   **`tidyr`** - This is ongoing work that we are focusing on in
    `sparklyr`. We are implementing these functions using PySpark
    DataFrame commands, instead of depending on the Scala
    implementation.

A more detailed list of our progress, which includes individual
functions we have confirmed that work, do not work, or will work
differently, is available here: [`pysparklyr`
progress](https://github.com/mlverse/pysparklyr/blob/main/progress.md)

## Additional setup details

If you wish to use your own Python environment, then just make sure to
load it before calling `spark_connect()`. If there is a Python
environment already loaded when you connect to your Spark cluster, then
`sparklyr` will use that environment instead. If you use your own Python
environment you will need the following libraries installed:

-   `pyspark`
-   `pandas`
-   `PyArrow`
-   `grpcio`
-   `google-api-python-client`
-   `grpcio_status`
-   `torch` *(Spark 3.5+)*
-   `torcheval` *(Spark 3.5+)*

If connecting to Databrics Connect avoid installing `pyspark`, and install 
these instead: 

-   `databricks-connect`
-   `delta-spark`


## TODO

- Posit Connect write up
- ML Section (including the installation prompt)
- 