{
  "hash": "94e6bebf676ecc24a4abf0d65314a779",
  "result": {
    "markdown": "---\ntitle: \"Grid Search Tuning\"\nexecute:\n  eval: true\n  freeze: true\neditor_options: \n  markdown: \n    wrap: 72\n---\n\n\n\n\nThis article will cover the following:\n\n-   Overview of Grid Search, and Cross Validation\n\n-   Through an example, show how easy it is to setup and run this kind\n    of validation\n\n-   Provide a compelling reason to use ML Pipelines in our daily work\n\n-   Highlight the advantages of using Spark, and `sparklyr`, for model\n    tuning\n\n## Grid Search \n\nThe main goal of hyper-parameter tuning is to find the ideal set of\nparameters to use when fitting our model. For example, finding out the\nideal number of trees to use for a model. We use model tuning to try\nseveral specific, and increasing, number of trees. That will tell us at\nwhat point a higher number of trees does not improve the model's\nperformance.\n\nIn Grid Search, we provide a provide a set of specific parameters, and\nspecific values to test for each parameter. The total number of\ncombinations will be the product of all the specific values of each\nparameter.\n\nFor example, suppose we are going to try **two** parameters. For the\nfirst parameter we provide 5 values to try, and for the second parameter\nwe provide 10 values to try. The total number of combinations will be\n50. The number of combinations grows quickly the more parameters we use.\nIn our example, adding a **third** parameter, with only 2 values, the\nnumber of combinations would double to 100 (5x10x2).\n\nThe model tuning returns a performance metric for each combination. We\ncan compare the results, and decide which model to use.\n\nIn Spark, we use an **ML Pipeline**, and a list of the parameters and\nthe values to try (**Grid**). We also specify the **metric** it should\nuse to measure performance (See @fig-grid). Spark will then take care of\nfiguring out the combinations, and fits the corresponding models.\n\n::: {#fig-grid}\n\n```{mermaid}\n%%| fig-width: 6.5\nflowchart LR\n  subgraph id1 [ ]\n    subgraph id2 [ ]\n      subgraph id3 [ML Pipeline]\n        d[Prepare] --> m[Model]\n      end\n      subgraph id4 [Grid Search Tuning]\n        m --> gv[Grid]\n        gv-- Combo 1 -->ft1[Fit Models]\n        ft1 --> ev1[Metric]\n        gv-- Combo 2 -->ft2[Fit Models]\n        ft2 --> ev2[Metric]     \n        gv-- Combo 3 -->ft3[Fit Models]\n        ft3 --> ev3[Metric]  \n        gv-- Combo n -->ft4[Fit Models]\n        ft4 --> ev4[Metric]          \n      end\n    end\n  end\n  style id1 fill:#eee,stroke:#eee\n  style id2 fill:#eee,stroke:#eee\n  style d fill:#99ccff,stroke#666\n  style m fill:#99ffcc,stroke:#666\n  style gv fill:#ffcc99,stroke:#666\n  style ft1 fill:#ccff99,stroke:#666\n  style ft2 fill:#ccff99,stroke:#666\n  style ft3 fill:#ccff99,stroke:#666\n  style ft4 fill:#ccff99,stroke:#666\n```\n\n\nGrid Search Tuning in Spark\n:::\n\n## Cross Validation\n\nIn Cross Validation, multiple models are fitted with the same\ncombination of parameters. The difference is the data used for training,\nand validation. These are called folds. The training data, and the\nvalidation data is different for each fold, that is called re-sampling.\n\nThe model is fitted with the current fold's training data, and then it\nis evaluated using the validation data. The average of the evaluation\nresults become the official metric value of the combination of\nparameters. @fig-cv, is a \"zoomed\" look of what happens inside\n`Fit Models` of @fig-grid.\n\nThe total number of models, will be the total number of combinations\ntimes the number of folds. For example, if we use 3 parameters, with 5\nvalues each, that would be 125 combinations. If tune with 3 folds, Cross\nValidation will fit, and validate, a total of 375 models.\n\nIn Spark, running **the 375 discrete models can be distributed across\nthe entire cluster**, thus significantly reducing the amount of time we\nwould have to wait to see the results.\n\n::: {#fig-cv}\n\n```{mermaid}\n%%| fig-width: 6.5\nflowchart LR\n  subgraph id1 [ ]\n    subgraph id2 [ ]\n      gv[Grid] -- Combo n -->re[Resample]\n      re -- Fold 1 --> ft1[Fit]\n      subgraph id4 [Fit Models with Cross Validation]\n        re -- Fold 2 --> ft2[Fit]\n        re -- Fold 3 --> ft3[Fit]\n        ft1 --> ev1[Evaluate]\n        ft2 --> ev2[Evaluate]     \n        ft3 --> ev3[Evaluate]  \n        ev1 --> eva[Avgerage]\n        ev2 --> eva\n        ev3 --> eva\n      end\n      eva --> mt[Metric]\n    end\n  end\n  style id1 fill:#eee,stroke:#eee\n  style id4 fill:#ccff99,stroke:#666\n  style gv fill:#ffcc99,stroke:#666\n  style ft1 fill:#ccffff,stroke:#666\n  style ft2 fill:#ccffff,stroke:#666\n  style ft3 fill:#ccffff,stroke:#666\n  style re fill:#ffccff,stroke:#666\n  style ev1 fill:#ffff99,stroke:#666\n  style ev2 fill:#ffff99,stroke:#666\n  style ev3 fill:#ffff99,stroke:#666\n  style eva fill:#ffff66,stroke:#666\n```\n\n\nCross Validation in Spark\n:::\n\n## Reproducing \"Tuning Text Analysis\" in Spark\n\nIn this article, we will reproduce the *Grid Search* tuning example\nfound in the `tune` package's website: [Tuning Text\nAnalysis](https://tune.tidymodels.org/articles/extras/text_analysis.html){target=\"_blank\"}.\nThat example analyzes *Amazon's Fine Food Reviews* text data. The goal\nis to tune the model using the exact same tuning parameters, and values,\nthat were used in the `tune`'s website examle.\n\n::: callout-tip\nThis article builds on the knowledge of two previous articles, [Text\nModeling](textmodeling.qmd){target=\"_blank\"} and [Intro to Model\nTuning](model_tuning.qmd){target=\"_blank\"}. We encourage you to\nfamiliarize yourself with the concepts and code from those articles.\n:::\n\n## Spark and Data Setup\n\nFor this example, we will start a local Spark session, and then copy the\n*Fine Food Reviews* data to it. For more information about the data,\nplease see the [Data](textmodeling.qmd#data) section of the *Text\nModeling* article.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nlibrary(modeldata)\n\ndata(\"small_fine_foods\")\n\nsc <- spark_connect(master = \"local\", version = \"3.3\")\n\nsff_training_data <- copy_to(sc, training_data)\nsff_testing_data <- copy_to(sc, testing_data)\n```\n:::\n\n\n## ML Pipeline\n\nAs mentioned before, the data preparation and modeling in [Text\nModeling](textmodeling.qmd#recipe-and-model-specifications-1){target=\"_blank\"}\nare based on the same example from the [`tune`\nwebsite](https://tune.tidymodels.org/articles/extras/text_analysis.html){target=\"_blank\"}'s\narticle. The `recipe` steps, and `parsnip` model are recreated with\nFeature Transformers, and an ML model respectively.\n\nUnlike `tidymodels`, there is no need to \"pre-define\" the arguments that\nwill need tuning. At execution, Spark will automatically override the\nparameters specified in the [grid](#grid). *This means that it doesn't\nmatter that we use the exact same code for developing, and tuning the\npipeline. We can literally copy-paste, and run the resulting pipeline\ncode from [Text\nModeling](textmodeling.qmd#prepare-the-model-with-an-ml-pipeline)*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_pipeline <- ml_pipeline(sc) %>% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %>% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %>% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 1024\n    ) %>%\n  ft_normalizer(\n    input_col = \"hashed_features\", \n    output_col = \"normal_features\"\n    ) %>% \n  ft_r_formula(score ~ normal_features) %>% \n  ml_logistic_regression()\n\nsff_pipeline\n#> Pipeline (Estimator) with 6 stages\n#> <pipeline__446ecae5_835a_4814_89fd_bd3a69c16930> \n#>   Stages \n#>   |--1 Tokenizer (Transformer)\n#>   |    <tokenizer__39822f1c_8ab0_49d6_9ffc_ccfa27f5eddf> \n#>   |     (Parameters -- Column Names)\n#>   |      input_col: review\n#>   |      output_col: word_list\n#>   |--2 StopWordsRemover (Transformer)\n#>   |    <stop_words_remover__4710871f_fb93_4c16_8ce4_22a5ac3d35ef> \n#>   |     (Parameters -- Column Names)\n#>   |      input_col: word_list\n#>   |      output_col: wo_stop_words\n#>   |--3 HashingTF (Transformer)\n#>   |    <hashing_tf__db6289ce_c0b3_4120_973d_09acd24bf2f4> \n#>   |     (Parameters -- Column Names)\n#>   |      input_col: wo_stop_words\n#>   |      output_col: hashed_features\n#>   |--4 Normalizer (Transformer)\n#>   |    <normalizer__92d5682c_7a37_4094_bf26_f3ada353b0ab> \n#>   |     (Parameters -- Column Names)\n#>   |      input_col: hashed_features\n#>   |      output_col: normal_features\n#>   |--5 RFormula (Estimator)\n#>   |    <r_formula__07231e4f_5f23_46c6_805a_a0f426e7fea6> \n#>   |     (Parameters -- Column Names)\n#>   |      features_col: features\n#>   |      label_col: label\n#>   |     (Parameters)\n#>   |      force_index_label: FALSE\n#>   |      formula: score ~ normal_features\n#>   |      handle_invalid: error\n#>   |      stringIndexerOrderType: frequencyDesc\n#>   |--6 LogisticRegression (Estimator)\n#>   |    <logistic_regression__ab9bfeb3_d76c_4ca3_b799_fc2db28a86a1> \n#>   |     (Parameters -- Column Names)\n#>   |      features_col: features\n#>   |      label_col: label\n#>   |      prediction_col: prediction\n#>   |      probability_col: probability\n#>   |      raw_prediction_col: rawPrediction\n#>   |     (Parameters)\n#>   |      aggregation_depth: 2\n#>   |      elastic_net_param: 0\n#>   |      family: auto\n#>   |      fit_intercept: TRUE\n#>   |      max_iter: 100\n#>   |      maxBlockSizeInMB: 0\n#>   |      reg_param: 0\n#>   |      standardization: TRUE\n#>   |      threshold: 0.5\n#>   |      tol: 1e-06\n```\n:::\n\n\nIt is also worth pointing out that in a\"real life\" exercise,\n`sff_pipeline` would probably already be loaded into our environment.\nThat is because we just finished modeling and, decided to test to see if\nwe could tune the model. Spark can re-use the exact same ML Pipeline\nobject for the cross validation step.\n\n## Grid {#grid}\n\nThere is a big advantage to transforming, and modeling the data in a\nsingle ML Pipeline. It opens the door for Spark to also alter parameters\nused for data transformation, in addition to the model's parameters.\nThis means that we can include the parameters of the tokenization,\ncleaning, hashing, and normalization steps as possible candidates for\nthe model tuning.\n\nThe *Tuning Text Analysis* article uses three tuning parameters. Two are\nin the model, and one is in the hashing step. Here are the parameters,\nand how they map between `tidymodels` and `sparklyr`:\n\n| Parameter                             | `tidymodels` | `sparklyr`          |\n|---------------------------------------|--------------|---------------------|\n| Number of Terms to Hash               | `num_terms`  | `num_features`      |\n| Amount of regularization in the model | `penalty`    | `elastic_net_param` |\n| Proportion of pure vs ridge Lasso     | `mixture`    | `reg_param`         |\n\nUsing partial name matching, we map the parameters to the steps we want\nto tune:\n\n-   `hashing_ft` will be the name of the list object containing the\n    `num_features` values\n\n-   `logistic_regression` will be the of the list object containing the\n    values of the other two parameters\n\nFor more about partial name matching, see in the [Intro Model\nTuning](model_tuning.qmd#grid) article. For the parameters values, we\ncan copy the exact same values from the [`tune`\nwebsite](https://tune.tidymodels.org/articles/extras/text_analysis.html#grid-search-1){target=\"_blank\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_grid <-  list(\n    hashing_tf = list(\n      num_features = 2^c(8, 10, 12)  \n    ),\n    logistic_regression = list(\n      elastic_net_param = 10^seq(-3, 0, length = 20), \n      reg_param = seq(0, 1, length = 5)    \n    )\n  )\n\nsff_grid\n#> $hashing_tf\n#> $hashing_tf$num_features\n#> [1]  256 1024 4096\n#> \n#> \n#> $logistic_regression\n#> $logistic_regression$elastic_net_param\n#>  [1] 0.001000000 0.001438450 0.002069138 0.002976351 0.004281332 0.006158482\n#>  [7] 0.008858668 0.012742750 0.018329807 0.026366509 0.037926902 0.054555948\n#> [13] 0.078475997 0.112883789 0.162377674 0.233572147 0.335981829 0.483293024\n#> [19] 0.695192796 1.000000000\n#> \n#> $logistic_regression$reg_param\n#> [1] 0.00 0.25 0.50 0.75 1.00\n```\n:::\n\n\n## Evaluate\n\nIn the [`tune`\nwebsite](https://tune.tidymodels.org/articles/extras/text_analysis.html){target=\"_blank\"}'s\narticle, ROC AUC is used to measure performance. The is the default\nmetric of `ml_binary_classification_evaluator()` , so we only need to\npass the connection variable to the evaluator function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_evaluator <- ml_binary_classification_evaluator(sc)\n```\n:::\n\n\n## Model Tuning\n\nWe will use `ml_cross_validator()` to prepare a tuning specification\ninside Spark. We recommend to set the `seed` argument in order to\nincrease reproducibility.\n\nSpark will automatically create the grid combinations when tuning the\nmodel. In this case, `sff_grid` contains three parameters:\n\n-   `num_features` has 3 values\n\n-   `elastic_net_param` has 20 values\n\n-   `reg_parm` has 5 values\n\nThis means that there will be 300 combinations for the tuning parameters\n(3 x 20 x 5). Because we set the number of folds to 3 (`num_folds`),\nSpark will run a total of 900 models (3 x 300).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_cv <- ml_cross_validator(\n  x = sc,\n  estimator = sff_pipeline, \n  estimator_param_maps = sff_grid,\n  evaluator = sff_evaluator,\n  num_folds = 3,\n  parallelism = 4,\n  seed = 100\n)\n\nsff_cv\n#> CrossValidator (Estimator)\n#> <cross_validator__0f3039f9_8869_48de_932d_0b037693527d> \n#>  (Parameters -- Tuning)\n#>   estimator: Pipeline\n#>              <pipeline__446ecae5_835a_4814_89fd_bd3a69c16930> \n#>   evaluator: BinaryClassificationEvaluator\n#>              <binary_classification_evaluator__c791acef_d5d1_4d71_852e_8a6f06947514> \n#>     with metric areaUnderROC \n#>   num_folds: 3 \n#>   [Tuned over 300 hyperparameter sets]\n```\n:::\n\n\nThis is the step that will take the longest time. The `ml_fit()`\nfunction will run the 900 models using the training data. There is no\nneed to pre-prepare the re-sampling folds, Spark will take care of that.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_model <- ml_fit(\n  x = sff_cv, \n  dataset = sff_training_data\n  )\n```\n:::\n\n\n## Metrics\n\nWe can now extract the metrics from `sff_model` using\n`ml_validation_metrics()`. The ROC AUC values will be in a column called\n`areaUnderROC`. We can then take a look at the best performing models\nusing `dplyr`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_metrics <- ml_validation_metrics(sff_model)\n\nlibrary(dplyr)\n\nsff_metrics %>% \n  arrange(desc(areaUnderROC)) %>% \n  head()\n#>   areaUnderROC reg_param_1 elastic_net_param_1 num_features_2\n#> 1    0.7858727        0.25          0.05455595           4096\n#> 2    0.7847232        0.50          0.02636651           4096\n#> 3    0.7835850        0.75          0.01832981           4096\n#> 4    0.7830411        0.50          0.01832981           4096\n#> 5    0.7830230        0.25          0.03792690           4096\n#> 6    0.7828651        0.75          0.01274275           4096\n```\n:::\n\n\nWe will now plot the results. We will match the approach used in the\n[Grid\nSearch](https://tune.tidymodels.org/articles/extras/text_analysis.html#grid-search-1){target=\"_blank\"}\nsection of the *Tuning Text Analysis* article.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nsff_metrics %>% \n  mutate(reg_param_1 = as.factor(reg_param_1)) %>% \n  ggplot(aes(\n    x = elastic_net_param_1, \n    y = areaUnderROC, \n    color = reg_param_1\n    )) +\n  geom_line() +\n  geom_point(size = 0.5) +\n  scale_x_continuous(trans = \"log10\") +\n  facet_wrap(~ num_features_2) +\n  theme_light(base_size = 9)\n```\n\n::: {.cell-output-display}\n![](model_tuning_text_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nIn the plot, we can see the effects of the three parameters, and the\nvalues that look to be the best. These effects are very similar to the\noriginal `tune`'s website article.\n\n## Model selection\n\nWe can create a new ML Pipeline using the same code as the original\npipeline. We only need to change the 3 parameters values, with values\nthat performed best.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_pipeline <- ml_pipeline(sc) %>% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %>% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %>% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 4096      \n    ) %>%\n  ft_normalizer(\n    input_col = \"hashed_features\", \n    output_col = \"normal_features\"\n    ) %>% \n  ft_r_formula(score ~ normal_features) %>% \n  ml_logistic_regression(\n    elastic_net_param = 0.05,\n    reg_param = 0.25  \n    )\n```\n:::\n\n\nNow, we create a final model using the new ML Pipeline.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_fitted <- new_sff_pipeline %>% \n  ml_fit(sff_training_data)\n```\n:::\n\n\n## Test data metrics\n\nThe test data set is now used to confirm that the performance gains\nhold. We use it to run predictions with thew new ML Pipeline Model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_fitted %>% \n  ml_transform(sff_testing_data) %>% \n  ml_metrics_binary()\n#> # A tibble: 2 Ã— 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.783\n#> 2 pr_auc  binary         0.653\n```\n:::\n\n\nThe results show an increase performance in contrast with running the ML\nPipeline Model, those results are in [Fit and\nPredict](textmodeling.qmd#fit-and-predict) section of the *Text\nModeling* article.\n\n## Advantages of Spark\n\nWe will show that Spark, and `sparklyr`, provide an easy way to go from\nexploration, to modeling, to tuning. Even without a \"formal\" Spark\ncluster, it is possible to take advantage of these capabilities right\nfrom our personal computer.\n\nIf we add an actual cluster to the mix, the advantage of using Spark\nraises dramatically. **Usually, we talk about Spark for \"big data\"\nanalysis, but in this case, we can leverage it to \"parallelize\" hundreds\nof models across multiple machines.** The ability to distribute the\nmodels across the cluster will cut down the tuning processing time (@fig-cluster). The\nresources available to the cluster, and the given Spark session, will\nalso determine the the amount of time saved. There is really no other\nopen-source technology that is capable of this.\n\n::: {#fig-cluster}\n\n```{mermaid}\n%%| fig-width: 6\n\nclassDiagram\n  class Driver {\n  }\n  class Node1{\n    Job 1 - Model 1\n    Job 2 - Model 2\n    Job 3 - Model 3\n    Job 4 - Model 4    \n  }\n  class Node2{\n    Job 1 - Model 5\n    Job 2 - Model 6\n    Job 3 - Model 7\n    Job 4 - Model 8       \n  }\n  class Node3{\n    Job 1 - Model 9\n    Job 2 - Model 10\n    Job 3 - Model 11\n    Job 4 - Model 12      \n  }  \n  class Node4{\n    Job 1 - Model 13\n    Job 2 - Model 14\n    Job 3 - Model 15\n    Job 4 - Model 16      \n  }    \n  Driver --> Node1\n  Driver --> Node2\n  Driver --> Node3\n  Driver --> Node4\n\n```\n\nModel tuning in Spark\n:::\n\n\n\n",
    "supporting": [
      "model_tuning_text_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}