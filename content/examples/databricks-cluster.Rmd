---
title: "Using sparklyr with Databricks"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
params:
  width: 600
---

## Overview

This documentation demonstrates how to use `sparklyr` with a Databricks cluster
in the context of RStudio Server Pro, RStudio Connect and RStudio Package
Manager.

## Best practices

- **Maintain separate installation environments** - Install RStudio Server Pro,
  RStudio Connect, and RStudio Package Manager outside of the Databricks cluster
  so that they are not limited to the resources or ephemeral nature of
  Databricks clusters.
- **Connect to Databricks remotely** - Work with Databricks as a remote compute
  resource in RStudio Server Pro, similar to how you would connect remotely to
  external databases, data sources, and storage systems. This can be
  accomplished using Databricks Connect for Spark as described in the
  [Connecting to Databricks remotely](#option-1---connecting-to-databricks-remotely)
  section below, or by using the
  [RStudio Professional Drivers](https://docs.rstudio.com/pro-drivers/) for Hive
  and Impala.
- **Restrict workloads to interactive analysis** - Only perform workloads
  related to exploratory or interactive analysis with Spark, then write the
  results to a database, file system, or cloud storage for more efficient
  retrieval in apps, reports, and APIs.
- **Load and query results efficiently** - Because of the nature of Spark
  computations and the associated overhead, Shiny apps that use Spark on the
  backend tend to have performance and runtime issues; consider reading the
  results from a database, file system, or cloud storage instead.

## Options for using RStudio Server Pro with Databricks

There are two options for using `sparklyr` and RStudio Server Pro with
Databricks:

- Option 1:
  [Connecting to Databricks remotely](#option-1---connecting-to-databricks-remotely)
  (Recommended Option)
- Option 2:
  [Working inside of Databricks](#option-2---working-inside-of-databricks)
  (Alternative Option)

## Option 1 - Connecting to Databricks remotely

With this configuration, RStudio Server Pro is installed outside of the Spark
cluster and allows users to connect to Spark remotely using
[Databricks Connect](https://docs.databricks.com/dev-tools/databricks-connect.html).

This is the recommended configuration because it targets separate environments,
involves simpler configuration, avoids resource contention, and allows RStudio
Server Pro to connect to Databricks as well as other remote storage and compute
resources.

<a href="/examples/databricks-cluster-remote">
  <h2>View steps for connecting to Databricks remotely</h2>
</a>

<a href="/examples/databricks-cluster-remote">
  <img src="/images/deployment/databricks/rstudio-databricks-remote.png" width='800px' align='center'/>
</a>

## Option 2 - Working inside of Databricks

If you cannot work with Spark remotely, you should install RStudio Server Pro on
the Driver node of a long-running, persistent Databricks cluster as opposed to a
worker node or an ephemeral cluster.

With this configuration, RStudio Server Pro is installed on the Spark driver
node and allows users to work locally with Spark.

This configuration can result in increased complexity, limited connectivity to
other storage and compute sources, resource contention between RStudio Server
Pro and Databricks, and maintenance concerns due to the ephemeral nature of
Databricks clusters.

<a href="/examples/databricks-cluster-local">
  <h2>View steps for working inside of Databricks</h2>
</a>

<a href="/examples/databricks-cluster-local">
  <img src="/images/deployment/databricks/rstudio-databricks-local.png" width='800px' align='center'/>
</a>

## Using RStudio Connect with Databricks

The server environments in Databricks clusters are not permissive enough to
support RStudio Connect or its process sandboxing mechanisms used for published
content. Therefore, the only supported configuration is to install RStudio
Connect outside of the Databricks cluster.

There are two options for using `sparklyr` and RStudio Connect with Databricks:

1. By connecting to Databricks remotely using
   [Databricks Connect](https://docs.databricks.com/dev-tools/databricks-connect.html)
   (Recommended Option)
2. By adding calls in your R code to create and run Databricks jobs
   [via the Databricks REST API](https://github.com/RafiKurlansik/bricksteR/)
   (Alternative Option)

## Using RStudio Package Manager with Databricks

Whether RStudio Server Pro is installed outside of the Databricks cluster
(Recommended Option) or within the Databricks cluster, you can install packages
from repositories in RStudio Package Manager as long as HTTP/HTTPS network
traffic is allowed from RStudio Server Pro to RStudio Package Manager.
