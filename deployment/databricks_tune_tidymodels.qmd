---
title: Tune Tidymodel in Databricks
execute:
  eval: true
  freeze: true
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| include: false
run_r <- TRUE

library(tidymodels)
library(sparklyr)

sc <- spark_connect(method = "databricks_connect", cluster_id = "1218-000327-q970zsow")
```

## Introduction

`sparklyr` enables you to offload Tidymodels grid search tuning to
Databricks. [Tidymodels](https://www.tidymodels.org/) is a collection of
packages designed to work together to provide everything from
resampling, to preprocessing, to model tuning, to performance
measurement.

Model tuning is a time-consuming process because of the number of tuning
parameter combinations that need to be processed. Running the
combinations in parallel saves a significant amount of time. While
Tidymodels supports parallel tuning, it does not natively integrate with
Spark. `sparklyr` bridges this gap, making distributed tuning on a
cluster straightforward to execute.

In Tidymodels, the
[`tune_grid()`](https://tune.tidymodels.org/reference/tune_grid.html)
function is called to execute the grid search locally. To run in
Databricks, simply call `sparklyr`'s `tune_grid_spark()` instead. It
accepts the exact same arguments as `tune_grid()` does, see @fig-code.

::::::: {#fig-code}
:::::: columns
::: {.column width="38%"}
```{r}
#| eval: false
tune::tune_grid(
  object = my_model, 
  preprocessor = my_recipe, 
  resamples = my_resamples
  
  )
```

Run locally
:::

::: {.column width="4%"}
:::

::: {.column width="58%"}
```{r}
#| eval: false
sparklyr::tune_grid_spark(
  object = my_model, 
  preprocessor = my_recipe, 
  resamples = my_resamples,
  sc = my_conn # Only additional requirement
  )
```

Run remotely in Databricks
:::
::::::

Comparing tune and sparklyr function calls
:::::::

`sparklyr` will automatically upload the needed R object to Databricks,
such as the resample object, model specification, and the preprocessing
steps. It will then run the tuning in parallel taking advantage of the R
integration in Databricks. Lastly, it will collect all of the results
back to your local R session, and return a `tune_results` object that is
indistinguishable from one made directly by Tidymodels; see
@fig-diagram.

::: {#fig-diagram}
![](/images/guides/tidymodels-db/diagram-1.png)

How *tune_grid_spark()* works with Databricks
:::

## Example

In Tidymodels, there are some specific elements needed to perform a grid
search tuning:

-   Model specification (`parsnip`)

-   Resampled data (`rsample`)

-   Data preprocessor (`recipe`)

-   Post processor (`tailor`) *optional*

In this example, we define the modeling components exactly as we would
for local tuning with `tune_grid()`.

```{r}
#| eval: !expr 'run_r'

library(tidymodels)
library(readmission)
set.seed(1)
readmission_splits <- initial_split(readmission, strata = readmitted)

# Resampling -------------------------------------------------------
readmission_folds <- vfold_cv(
  data = training(readmission_splits),
  strata = readmitted
  )

# Preprocessing ----------------------------------------------------
recipe_basic <- recipe(readmitted ~ ., data = readmission) |>
  step_mutate(
    race = factor(case_when(
      !(race %in% c("Caucasian", "African American")) ~ "Other",
      .default = race
    ))
  ) |>
  step_unknown(all_nominal_predictors()) |>
  step_YeoJohnson(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors())

# Model specification -----------------------------------------------
spec_bt <- boost_tree(
  mode = "classification",
  mtry = tune(),       # Parameter to be tuned
  learn_rate = tune(), # Parameter to be tuned
  trees = 10
  )
```

The only required step before tuning the model in Databricks is to
connect to a Databricks' Spark cluster. Although we could have connected
earlier, this demonstrates how easily a local tuning workflow can be
pivoted to a Databricks cluster.

```{r}
#| eval: FALSE
library(sparklyr)

sc <- spark_connect(
  method = "databricks_connect", 
  cluster_id = "1218-000327-q970zsow" # Replace with your own cluster's ID
  )
```

The next step is to call `tune_grid_spark()`. We will pass the three
elements created in the first step, and the Databricks connection
variable. The optional `control_grid()` call is included here to display
the `sparklyr` output during the tuning process.

```{r}
#| eval: !expr 'run_r'
spark_results <- tune_grid_spark(
  sc = sc, 
  object = spec_bt, 
  preprocessor = recipe_basic, 
  resamples = readmission_folds,
  control = control_grid(verbose = TRUE)
  )
```

Once complete, the `spark_results` object is fully compatible with
standard `tune` functions.

```{r}
#| eval: !expr 'run_r'
autoplot(spark_results)
```

## R and Python libraries prerequisites

Before tuning in Databricks, the following Python and R packages
**must** be pre-installed on the cluster:

-   `rpy2` **(Python)**
-   `tidymodels` **(R)**
-   `reticulate` **(R)**
-   Any R packages required by the `parsnip` engine (e.g., `xgboost` or
    `ranger`).

There are two options to accomplish the installation, the first one is
using the web portal in the Databricks website, or by installing the
libraries programmatically.

#### Option 1 - Databricks Web Portal

The most common method is to use the Databricks UI. It is a
straightforward process managed through the cluster's Libraries tab.
Detailed instructions can be found here: [Databricks - Cluster
Libraries](https://docs.databricks.com/en/libraries/cluster-libraries.html).

#### Option 2 - Programmatic installation via `brickster`

For those who prefer a scripted approach, the
[`brickster`](https://databrickslabs.github.io/brickster/) package is a
complete toolkit for interacting with Databricks. It can be used to
programmatically install the Python and R libraries, provided the user
has permissions to modify the target cluster.

```{r}
#| eval: false
library(brickster)

# DBRs have a fixed snapshot date to source the R packages. Redirecting  
# to the 'latest' snapshot to get the most recent package versions.
repo <- "https://databricks.packagemanager.posit.co/cran/__linux__/noble/latest"

# Builds the list of R packages, pointing them to the 'latest' snapshot
r_libs <- libraries(
  lib_cran("reticulate", repo = repo),
  lib_cran("tidymodels", repo = repo),
  lib_cran("xgboost", repo = repo) # (optional) Used in the example
)

db_libs_install("1218-000327-q970zsow", r_libs)

# Builds the Python package object. Specifying the version of `rpy2` to install.
py_lib <- lib_pypi("rpy2==3.6.4") |>
  libraries()

db_libs_install("1218-000327-q970zsow", py_lib)
```

As time goes by, there may be other modeling R packages that need to be
installed in Spark. Here is a template that can be used to install a
single package:

```{r}
#| eval: false
library(brickster)

lib_cran(
  package = "[Missing package]", 
  repo = "https://databricks.packagemanager.posit.co/cran/__linux__/noble/latest"
  ) |> 
  libraries() |> 
  db_libs_install("[Your cluster ID]", libraries =  _)

```
