{
  "hash": "eec38c0b57252c1a772eaf2c6ee04021",
  "result": {
    "markdown": "---\ntitle: \"Read the Schema of a Spark DataFrame\"\nexecute:\n  freeze: true\n---\n\n\n\n\n*R/sdf_wrapper.R*\n\n## sdf_schema\n\n## Description\nRead the schema of a Spark DataFrame. \n\n\n## Usage\n```r\nsdf_schema(x, expand_nested_cols = FALSE, expand_struct_cols = FALSE) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | A `spark_connection`, `ml_pipeline`, or a `tbl_spark`. |\n| expand_nested_cols | Whether to expand columns containing nested array of structs (which are usually created by tidyr::nest on a Spark data frame) |\n| expand_struct_cols | Whether to expand columns containing structs |\n\n## Details\n\nThe `type` column returned gives the string representation of the underlying Spark  type for that column; for example, a vector of numeric values would be returned with the type `\"DoubleType\"`. Please see the [Spark Scala API Documentation](https://spark.apache.org/docs/latest/api/scala/index.html)\n\nfor information on what types are available and exposed by Spark. \n\n\n## Value\nAn `R` `list`, with each `list` element describing the   `name` and `type` of a column. \n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}