      <div class="row">
  <div class="contents col-md-9">
    <div id="sparklyr-r-interface-for-apache-spark" class="section level1">
<div class="page-header"><h1 class="hasAnchor"><a href="#sparklyr-r-interface-for-apache-spark" class="anchor"> </a>sparklyr: R interface for Apache Spark</h1></div>

<p><img src="home/README_files/images/sparklyr-illustration.png" width="364" height="197" align="right"></p>
<ul><li>Connect to <a href="http://spark.apache.org/">Spark</a> from R. The sparklyr package provides a <br> complete <a href="https://github.com/hadley/dplyr">dplyr</a> backend.</li>
<li>Filter and aggregate Spark datasets then bring them into R for <br> analysis and visualization.</li>
<li>Use Spark&rsquo;s distributed <a href="http://spark.apache.org/docs/latest/mllib-guide.html">machine learning</a> library from R.</li>
<li>Create <a href="http://spark.rstudio.com/extensions.html">extensions</a> that call the full Spark API and provide <br> interfaces to Spark packages.</li>
</ul><div id="installation" class="section level2">
<h2 class="hasAnchor">
<a href="#installation" class="anchor"> </a>Installation</h2>
<p>You can install the <strong>sparklyr</strong> package from CRAN as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">install.packages</span>(<span class="st">"sparklyr"</span>)</code></pre></div>
<p>You should also install a local version of Spark for development purposes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)
<span class="kw"><a href="reference/spark_install.html">spark_install</a></span>(<span class="dt">version =</span> <span class="st">"1.6.2"</span>)</code></pre></div>
<p>To upgrade to the latest version of sparklyr, run the following command and restart your r session:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools::<span class="kw">install_github</span>(<span class="st">"rstudio/sparklyr"</span>)</code></pre></div>
<p>If you use the RStudio IDE, you should also download the latest <a href="https://www.rstudio.com/products/rstudio/download/preview/">preview release</a> of the IDE which includes several enhancements for interacting with Spark (see the <a href="#rstudio-ide">RStudio IDE</a> section below for more details).</p>
</div>
<div id="connecting-to-spark" class="section level2">
<h2 class="hasAnchor">
<a href="#connecting-to-spark" class="anchor"> </a>Connecting to Spark</h2>
<p>You can connect to both local instances of Spark as well as remote Spark clusters. Here we&rsquo;ll connect to a local instance of Spark via the <a href="http://spark.rstudio.com/reference/sparklyr/latest/spark_connect.html">spark_connect</a> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)
sc &lt;-<span class="st"> </span><span class="kw"><a href="reference/spark-connections.html">spark_connect</a></span>(<span class="dt">master =</span> <span class="st">"local"</span>)</code></pre></div>
<p>The returned Spark connection (<code>sc</code>) provides a remote dplyr data source to the Spark cluster.</p>
<p>For more information on connecting to remote Spark clusters see the <a href="http://spark.rstudio.com/deployment.html">Deployment</a> section of the sparklyr website.</p>
</div>
<div id="using-dplyr" class="section level2">
<h2 class="hasAnchor">
<a href="#using-dplyr" class="anchor"> </a>Using dplyr</h2>
<p>We can new use all of the available dplyr verbs against the tables within the cluster.</p>
<p>We&rsquo;ll start by copying some datasets from R into the Spark cluster (note that you may need to install the nycflights13 and Lahman packages in order to execute this code):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">install.packages</span>(<span class="kw">c</span>(<span class="st">"nycflights13"</span>, <span class="st">"Lahman"</span>))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)
iris_tbl &lt;-<span class="st"> </span><span class="kw"><a href="reference/reexports.html">copy_to</a></span>(sc, iris)
flights_tbl &lt;-<span class="st"> </span><span class="kw"><a href="reference/reexports.html">copy_to</a></span>(sc, nycflights13::flights, <span class="st">"flights"</span>)
batting_tbl &lt;-<span class="st"> </span><span class="kw"><a href="reference/reexports.html">copy_to</a></span>(sc, Lahman::Batting, <span class="st">"batting"</span>)
<span class="kw">src_tbls</span>(sc)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## [1] "batting" "flights" "iris"</code></pre></div>
<p>To start with here&rsquo;s a simple filtering example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># filter by departure delay and print the first few records</span>
flights_tbl %&gt;%<span class="st"> </span><span class="kw">filter</span>(dep_delay ==<span class="st"> </span><span class="dv">2</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## Source:   query [6,233 x 19]
## Database: spark connection master=local[8] app=sparklyr local=TRUE
## 
##     year month   day dep_time sched_dep_time dep_delay arr_time
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;
## 1   2013     1     1      517            515         2      830
## 2   2013     1     1      542            540         2      923
## 3   2013     1     1      702            700         2     1058
## 4   2013     1     1      715            713         2      911
## 5   2013     1     1      752            750         2     1025
## 6   2013     1     1      917            915         2     1206
## 7   2013     1     1      932            930         2     1219
## 8   2013     1     1     1028           1026         2     1350
## 9   2013     1     1     1042           1040         2     1325
## 10  2013     1     1     1231           1229         2     1523
## # ... with 6,223 more rows, and 12 more variables: sched_arr_time &lt;int&gt;,
## #   arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;,
## #   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,
## #   minute &lt;dbl&gt;, time_hour &lt;dbl&gt;</code></pre></div>
<p><a href="https://CRAN.R-project.org/package=dplyr">Introduction to dplyr</a> provides additional dplyr examples you can try. For example, consider the last example from the tutorial which plots data on flight delays:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">delay &lt;-<span class="st"> </span>flights_tbl %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(tailnum) %&gt;%
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">count =</span> <span class="kw">n</span>(), <span class="dt">dist =</span> <span class="kw">mean</span>(distance), <span class="dt">delay =</span> <span class="kw">mean</span>(arr_delay)) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(count &gt;<span class="st"> </span><span class="dv">20</span>, dist &lt;<span class="st"> </span><span class="dv">2000</span>, !<span class="kw">is.na</span>(delay)) %&gt;%
<span class="st">  </span>collect

<span class="co"># plot delays</span>
<span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(delay, <span class="kw">aes</span>(dist, delay)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">size =</span> count), <span class="dt">alpha =</span> <span class="dv">1</span>/<span class="dv">2</span>) +
<span class="st">  </span><span class="kw">geom_smooth</span>() +
<span class="st">  </span><span class="kw">scale_size_area</span>(<span class="dt">max_size =</span> <span class="dv">2</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## `geom_smooth()` using method = 'gam'</code></pre></div>
<p><img src="home/README_files/figure-markdown_github/ggplot2-1.png"></p>
<div id="window-functions" class="section level3">
<h3 class="hasAnchor">
<a href="#window-functions" class="anchor"> </a>Window Functions</h3>
<p>dplyr <a href="https://CRAN.R-project.org/package=dplyr">window functions</a> are also supported, for example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">batting_tbl %&gt;%
<span class="st">  </span><span class="kw">select</span>(playerID, yearID, teamID, G, AB:H) %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(playerID, yearID, teamID) %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(playerID) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">min_rank</span>(<span class="kw">desc</span>(H)) &lt;=<span class="st"> </span><span class="dv">2</span> &amp;<span class="st"> </span>H &gt;<span class="st"> </span><span class="dv">0</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## Source:   query [2.562e+04 x 7]
## Database: spark connection master=local[8] app=sparklyr local=TRUE
## Groups: playerID
## 
##     playerID yearID teamID     G    AB     R     H
##        &lt;chr&gt;  &lt;int&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1  abbotpa01   2000    SEA    35     5     1     2
## 2  abbotpa01   2004    PHI    10    11     1     2
## 3  abnersh01   1992    CHA    97   208    21    58
## 4  abnersh01   1990    SDN    91   184    17    45
## 5  abreujo02   2015    CHA   154   613    88   178
## 6  abreujo02   2014    CHA   145   556    80   176
## 7  acevejo01   2001    CIN    18    34     1     4
## 8  acevejo01   2004    CIN    39    43     0     2
## 9  adamsbe01   1919    PHI    78   232    14    54
## 10 adamsbe01   1918    PHI    84   227    10    40
## # ... with 2.561e+04 more rows</code></pre></div>
<p>For additional documentation on using dplyr with Spark see the <a href="http://spark.rstudio.com/dplyr.html">dplyr</a> section of the sparklyr website.</p>
</div>
</div>
<div id="using-sql" class="section level2">
<h2 class="hasAnchor">
<a href="#using-sql" class="anchor"> </a>Using SQL</h2>
<p>It&rsquo;s also possible to execute SQL queries directly against tables within a Spark cluster. The <code>spark_connection</code> object implements a <a href="https://github.com/rstats-db/DBI">DBI</a> interface for Spark, so you can use <code>dbGetQuery</code> to execute SQL and return the result as an R data frame:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(DBI)
iris_preview &lt;-<span class="st"> </span><span class="kw">dbGetQuery</span>(sc, <span class="st">"SELECT * FROM iris LIMIT 10"</span>)
iris_preview</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species
## 1           5.1         3.5          1.4         0.2  setosa
## 2           4.9         3.0          1.4         0.2  setosa
## 3           4.7         3.2          1.3         0.2  setosa
## 4           4.6         3.1          1.5         0.2  setosa
## 5           5.0         3.6          1.4         0.2  setosa
## 6           5.4         3.9          1.7         0.4  setosa
## 7           4.6         3.4          1.4         0.3  setosa
## 8           5.0         3.4          1.5         0.2  setosa
## 9           4.4         2.9          1.4         0.2  setosa
## 10          4.9         3.1          1.5         0.1  setosa</code></pre></div>
</div>
<div id="machine-learning" class="section level2">
<h2 class="hasAnchor">
<a href="#machine-learning" class="anchor"> </a>Machine Learning</h2>
<p>You can orchestrate machine learning algorithms in a Spark cluster via the <a href="http://spark.apache.org/docs/latest/mllib-guide.html">machine learning</a> functions within <strong>sparklyr</strong>. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning workflows.</p>
<p>Here&rsquo;s an example where we use <a href="http://spark.rstudio.com/reference/sparklyr/latest/ml_linear_regression.html">ml_linear_regression</a> to fit a linear regression model. We&rsquo;ll use the built-in <code>mtcars</code> dataset, and see if we can predict a car&rsquo;s fuel consumption (<code>mpg</code>) based on its weight (<code>wt</code>), and the number of cylinders the engine contains (<code>cyl</code>). We&rsquo;ll assume in each case that the relationship between <code>mpg</code> and each of our features is linear.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># copy mtcars into spark</span>
mtcars_tbl &lt;-<span class="st"> </span><span class="kw"><a href="reference/reexports.html">copy_to</a></span>(sc, mtcars)

<span class="co"># transform our data set, and then partition into 'training', 'test'</span>
partitions &lt;-<span class="st"> </span>mtcars_tbl %&gt;%
<span class="st">  </span><span class="kw">filter</span>(hp &gt;=<span class="st"> </span><span class="dv">100</span>) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">cyl8 =</span> cyl ==<span class="st"> </span><span class="dv">8</span>) %&gt;%
<span class="st">  </span><span class="kw"><a href="reference/sdf_partition.html">sdf_partition</a></span>(<span class="dt">training =</span> <span class="fl">0.5</span>, <span class="dt">test =</span> <span class="fl">0.5</span>, <span class="dt">seed =</span> <span class="dv">1099</span>)

<span class="co"># fit a linear model to the training dataset</span>
fit &lt;-<span class="st"> </span>partitions$training %&gt;%
<span class="st">  </span><span class="kw"><a href="reference/ml_linear_regression.html">ml_linear_regression</a></span>(<span class="dt">response =</span> <span class="st">"mpg"</span>, <span class="dt">features =</span> <span class="kw">c</span>(<span class="st">"wt"</span>, <span class="st">"cyl"</span>))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## * No rows dropped by 'na.omit' call</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## Call: ml_linear_regression(., response = "mpg", features = c("wt", "cyl"))
## 
## Coefficients:
## (Intercept)          wt         cyl 
##   37.066699   -2.309504   -1.639546</code></pre></div>
<p>For linear regression models produced by Spark, we can use <code>summary()</code> to learn a bit more about the quality of our fit, and the statistical significance of each of our predictors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(fit)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## Call: ml_linear_regression(., response = "mpg", features = c("wt", "cyl"))
## 
## Deviance Residuals::
##     Min      1Q  Median      3Q     Max 
## -2.6881 -1.0507 -0.4420  0.4757  3.3858 
## 
## Coefficients:
##             Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 37.06670    2.76494 13.4059 2.981e-07 ***
## wt          -2.30950    0.84748 -2.7252   0.02341 *  
## cyl         -1.63955    0.58635 -2.7962   0.02084 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-Squared: 0.8665
## Root Mean Squared Error: 1.799</code></pre></div>
<p>Spark machine learning supports a wide array of algorithms and feature transformations and as illustrated above it&rsquo;s easy to chain these functions together with dplyr pipelines. To learn more see the <a href="mllib.html">machine learning</a> section.</p>
</div>
<div id="reading-and-writing-data" class="section level2">
<h2 class="hasAnchor">
<a href="#reading-and-writing-data" class="anchor"> </a>Reading and Writing Data</h2>
<p>You can read and write data in CSV, JSON, and Parquet formats. Data can be stored in HDFS, S3, or on the lcoal filesystem of cluster nodes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">temp_csv &lt;-<span class="st"> </span><span class="kw">tempfile</span>(<span class="dt">fileext =</span> <span class="st">".csv"</span>)
temp_parquet &lt;-<span class="st"> </span><span class="kw">tempfile</span>(<span class="dt">fileext =</span> <span class="st">".parquet"</span>)
temp_json &lt;-<span class="st"> </span><span class="kw">tempfile</span>(<span class="dt">fileext =</span> <span class="st">".json"</span>)

<span class="kw"><a href="reference/spark_write_csv.html">spark_write_csv</a></span>(iris_tbl, temp_csv)
iris_csv_tbl &lt;-<span class="st"> </span><span class="kw"><a href="reference/spark_read_csv.html">spark_read_csv</a></span>(sc, <span class="st">"iris_csv"</span>, temp_csv)

<span class="kw"><a href="reference/spark_write_parquet.html">spark_write_parquet</a></span>(iris_tbl, temp_parquet)
iris_parquet_tbl &lt;-<span class="st"> </span><span class="kw"><a href="reference/spark_read_parquet.html">spark_read_parquet</a></span>(sc, <span class="st">"iris_parquet"</span>, temp_parquet)

<span class="kw"><a href="reference/spark_write_json.html">spark_write_json</a></span>(iris_tbl, temp_json)
iris_json_tbl &lt;-<span class="st"> </span><span class="kw"><a href="reference/spark_read_json.html">spark_read_json</a></span>(sc, <span class="st">"iris_json"</span>, temp_json)

<span class="kw">src_tbls</span>(sc)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## [1] "batting"      "flights"      "iris"         "iris_csv"    
## [5] "iris_json"    "iris_parquet" "mtcars"</code></pre></div>
</div>
<div id="extensions" class="section level2">
<h2 class="hasAnchor">
<a href="#extensions" class="anchor"> </a>Extensions</h2>
<p>The facilities used internally by sparklyr for its dplyr and machine learning interfaces are available to extension packages. Since Spark is a general purpose cluster computing system there are many potential applications for extensions (e.g.&nbsp;interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).</p>
<p>Here&rsquo;s a simple example that wraps a Spark text file line counting function with an R function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># write a CSV </span>
tempfile &lt;-<span class="st"> </span><span class="kw">tempfile</span>(<span class="dt">fileext =</span> <span class="st">".csv"</span>)
<span class="kw">write.csv</span>(nycflights13::flights, tempfile, <span class="dt">row.names =</span> <span class="ot">FALSE</span>, <span class="dt">na =</span> <span class="st">""</span>)

<span class="co"># define an R interface to Spark line counting</span>
count_lines &lt;-<span class="st"> </span>function(sc, path) {
  <span class="kw"><a href="reference/spark-api.html">spark_context</a></span>(sc) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw"><a href="reference/invoke.html">invoke</a></span>(<span class="st">"textFile"</span>, path, 1L) %&gt;%<span class="st"> </span>
<span class="st">      </span><span class="kw"><a href="reference/invoke.html">invoke</a></span>(<span class="st">"count"</span>)
}

<span class="co"># call spark to count the lines of the CSV</span>
<span class="kw">count_lines</span>(sc, tempfile)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## [1] 336777</code></pre></div>
<p>To learn more about creating extensions see the <a href="http://spark.rstudio.com/extensions.html">Extensions</a> section of the sparklyr website.</p>
</div>
<div id="table-utilities" class="section level2">
<h2 class="hasAnchor">
<a href="#table-utilities" class="anchor"> </a>Table Utilities</h2>
<p>You can cache a table into memory with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/tbl_cache.html">tbl_cache</a></span>(sc, <span class="st">"batting"</span>)</code></pre></div>
<p>and unload from memory using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/tbl_uncache.html">tbl_uncache</a></span>(sc, <span class="st">"batting"</span>)</code></pre></div>
</div>
<div id="connection-utilities" class="section level2">
<h2 class="hasAnchor">
<a href="#connection-utilities" class="anchor"> </a>Connection Utilities</h2>
<p>You can view the Spark web console using the <code>spark_web</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/spark_web.html">spark_web</a></span>(sc)</code></pre></div>
<p>You can show the log using the <code>spark_log</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/spark_log.html">spark_log</a></span>(sc, <span class="dt">n =</span> <span class="dv">10</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## 16/12/19 13:43:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 91 (/var/folders/fz/v6wfsg2x1fb1rw4f6r0x4jwm0000gn/T//RtmpcsSFgF/file82259906243.csv MapPartitionsRDD[363] at textFile at NativeMethodAccessorImpl.java:-2)
## 16/12/19 13:43:42 INFO TaskSchedulerImpl: Adding task set 91.0 with 1 tasks
## 16/12/19 13:43:42 INFO TaskSetManager: Starting task 0.0 in stage 91.0 (TID 177, localhost, partition 0,PROCESS_LOCAL, 2429 bytes)
## 16/12/19 13:43:42 INFO Executor: Running task 0.0 in stage 91.0 (TID 177)
## 16/12/19 13:43:42 INFO HadoopRDD: Input split: file:/var/folders/fz/v6wfsg2x1fb1rw4f6r0x4jwm0000gn/T/RtmpcsSFgF/file82259906243.csv:0+33313106
## 16/12/19 13:43:42 INFO Executor: Finished task 0.0 in stage 91.0 (TID 177). 2082 bytes result sent to driver
## 16/12/19 13:43:42 INFO TaskSetManager: Finished task 0.0 in stage 91.0 (TID 177) in 119 ms on localhost (1/1)
## 16/12/19 13:43:42 INFO DAGScheduler: ResultStage 91 (count at NativeMethodAccessorImpl.java:-2) finished in 0.119 s
## 16/12/19 13:43:42 INFO TaskSchedulerImpl: Removed TaskSet 91.0, whose tasks have all completed, from pool 
## 16/12/19 13:43:42 INFO DAGScheduler: Job 61 finished: count at NativeMethodAccessorImpl.java:-2, took 0.121816 s</code></pre></div>
<p>Finally, we disconnect from Spark:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/spark-connections.html">spark_disconnect</a></span>(sc)</code></pre></div>
</div>
<div id="rstudio-ide" class="section level2">
<h2 class="hasAnchor">
<a href="#rstudio-ide" class="anchor"> </a>RStudio IDE</h2>
<p>The latest RStudio <a href="https://www.rstudio.com/products/rstudio/download/preview/">Preview Release</a> of the RStudio IDE includes integrated support for Spark and the sparklyr package, including tools for:</p>
<ul><li>Creating and managing Spark connections</li>
<li>Browsing the tables and columns of Spark DataFrames</li>
<li>Previewing the first 1,000 rows of Spark DataFrames</li>
</ul><p>Once you&rsquo;ve installed the sparklyr package, you should find a new <strong>Spark</strong> pane within the IDE. This pane includes a <strong>New Connection</strong> dialog which can be used to make connections to local or remote Spark instances:</p>
<p><img src="home/README_files/images/spark-connect.png" class="screenshot" width="639" height="447/"></p>
<p>Once you&rsquo;ve connected to Spark you&rsquo;ll be able to browse the tables contained within the Spark cluster:</p>
<p><img src="home/README_files/images/spark-tab.png" class="screenshot" width="639" height="393/"></p>
<p>The Spark DataFrame preview uses the standard RStudio data viewer:</p>
<p><img src="home/README_files/images/spark-dataview.png" class="screenshot" width="639" height="446/"></p>
<p>The RStudio IDE features for sparklyr are available now as part of the <a href="https://www.rstudio.com/products/rstudio/download/preview/">RStudio Preview Release</a>.</p>
</div>
<div id="connecting-through-livy" class="section level2">
<h2 class="hasAnchor">
<a href="#connecting-through-livy" class="anchor"> </a>Connecting through Livy</h2>
<p><a href="https://github.com/cloudera/livy">Livy</a> enables remote connections to Apache Spark clusters. Connecting to Spark clusters through Livy is <strong>under experimental development</strong> in <code>sparklyr</code>. Please post any feedback or questions as a GitHub issue as needed.</p>
<p>Before connecting to Livy, you will need the connection information to an existing service running Livy. Otherwise, to test <code>livy</code> in your local environment, you can install it and run it locally as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/livy_install.html">livy_install</a></span>()</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/livy_service_start.html">livy_service_start</a></span>()</code></pre></div>
<p>To connect, use the Livy service address as <code>master</code> and <code>method = "livy"</code> in <code>spark_connect</code>. Once connection completes, use <code>sparklyr</code> as usual, for instance:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sc &lt;-<span class="st"> </span><span class="kw"><a href="reference/spark-connections.html">spark_connect</a></span>(<span class="dt">master =</span> <span class="st">"http://localhost:8998"</span>, <span class="dt">method =</span> <span class="st">"livy"</span>)
<span class="kw"><a href="reference/reexports.html">copy_to</a></span>(sc, iris)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode R"><code class="sourceCode r">## Source:   query [150 x 5]
## Database: spark connection master=http://localhost:8998 app= local=FALSE
## 
##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species
##           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;   &lt;chr&gt;
## 1           5.1         3.5          1.4         0.2  setosa
## 2           4.9         3.0          1.4         0.2  setosa
## 3           4.7         3.2          1.3         0.2  setosa
## 4           4.6         3.1          1.5         0.2  setosa
## 5           5.0         3.6          1.4         0.2  setosa
## 6           5.4         3.9          1.7         0.4  setosa
## 7           4.6         3.4          1.4         0.3  setosa
## 8           5.0         3.4          1.5         0.2  setosa
## 9           4.4         2.9          1.4         0.2  setosa
## 10          4.9         3.1          1.5         0.1  setosa
## # ... with 140 more rows</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/spark-connections.html">spark_disconnect</a></span>(sc)</code></pre></div>
<p>Once you are done using <code>livy</code> locally, you should stop this service with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/livy_service_start.html">livy_service_stop</a></span>()</code></pre></div>
<p>To connect to remote <code>livy</code> clusters that support basic authentication connect as:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">config &lt;-<span class="st"> </span><span class="kw">livy_config_auth</span>(<span class="st">"&lt;username&gt;"</span>, <span class="st">"&lt;password"</span>&gt;)
sc &lt;-<span class="st"> </span><span class="kw"><a href="reference/spark-connections.html">spark_connect</a></span>(<span class="dt">master =</span> <span class="st">"&lt;address&gt;"</span>, <span class="dt">method =</span> <span class="st">"livy"</span>, <span class="dt">config =</span> config)
<span class="kw"><a href="reference/spark-connections.html">spark_disconnect</a></span>(sc)</code></pre></div>
</div>
</div>
  </div>

  <div class="col-md-3" id="sidebar">
    <h2>Links</h2><ul class="list-unstyled"><li>Download from CRAN at <br><a href="https://cran.r-project.org/package=sparklyr">https://&#8203;cran.r-project.org/&#8203;package=sparklyr</a></li>
<li>Report a bug at <br><a href="https://github.com/rstudio/sparklyr/issues">https://&#8203;github.com/&#8203;rstudio/&#8203;sparklyr/&#8203;issues</a></li>
</ul><h2>License</h2>
<p>Apache License 2.0 | file <a href="LICENSE">LICENSE</a></p>
<h2>Developers</h2><ul class="list-unstyled"><li>Javier Luraschi <br><small class="roles"> Author, maintainer </small> </li>
<li>Kevin Ushey <br><small class="roles"> Author </small> </li>
<li>JJ Allaire <br><small class="roles"> Author </small> </li>
<li> The Apache Software Foundation <br><small class="roles"> Author, copyright&nbsp;holder </small> </li>
<li><a href="authors.html">All authors...</a></li>
</ul><html><body><h2>Dev status</h2><ul class="list-unstyled"><li><a href="https://travis-ci.org/rstudio/sparklyr">
  <img src="https://travis-ci.org/rstudio/sparklyr.svg?branch=master" alt="Build Status"></a></li>
<li><a href="https://cran.r-project.org/package=sparklyr">
  <img src="https://www.r-pkg.org/badges/version/sparklyr" alt="CRAN_Status_Badge"></a></li>
</ul></body></html></div>
</div>


      <footer><div class="copyright">
  <p>Developed by Javier Luraschi, Kevin Ushey, JJ Allaire,  The Apache Software Foundation.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer></div>

  </body></html>
