{
  "hash": "fbb52a84ad21eaaba99094d286a9d132",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Write Spark DataFrame to RDS files\"\nexecute:\n  freeze: true\n---\n\n\n\n*R/data_interface.R*\n\n## spark_write_rds\n\n## Description\n Write Spark dataframe to RDS files. Each partition of the dataframe will be exported to a separate RDS file so that all partitions can be processed in parallel. \n\n\n## Usage\n```r\n \nspark_write_rds(x, dest_uri) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | A Spark DataFrame to be exported |\n| dest_uri | Can be a URI template containing `partitionId` (e.g., `\"hdfs://my_data_part_{partitionId}.rds\"`) where `partitionId` will be substituted with ID of each partition using `glue`, or a list of URIs to be assigned to RDS output from all partitions (e.g., `\"hdfs://my_data_part_0.rds\"`, `\"hdfs://my_data_part_1.rds\"`, and so on) If working with a Spark instance running locally, then all URIs should be in `\"file://<local file path>\"` form. Otherwise the scheme of the URI should reflect the underlying file system the Spark instance is working with (e.g., \"hdfs://\"). If the resulting list of URI(s) does not contain unique values, then it will be post-processed with `make.unique()` to ensure uniqueness. |\n\n\n\n## Value\n A tibble containing partition ID and RDS file location for each   partition of the input Spark dataframe. \n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}