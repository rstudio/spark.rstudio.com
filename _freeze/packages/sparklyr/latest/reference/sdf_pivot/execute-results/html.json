{
  "hash": "8617ac6b6b12847b05cf064f0606b71d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Pivot a Spark DataFrame\"\nexecute:\n  freeze: true\n---\n\n\n\n*R/sdf_wrapper.R*\n\n## sdf_pivot\n\n## Description\n Construct a pivot table over a Spark Dataframe, using a syntax similar to that from `reshape2::dcast`. \n\n\n## Usage\n```r\n \nsdf_pivot(x, formula, fun.aggregate = \"count\") \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | A `spark_connection`, `ml_pipeline`, or a `tbl_spark`. |\n| formula | A two-sided `R` formula of the form `x_1 + x_2 + ... ~ y_1`. The left-hand side of the formula indicates which variables are used for grouping, and the right-hand side indicates which variable is used for pivoting. Currently, only a single pivot column is supported. |\n| fun.aggregate | How should the grouped dataset be aggregated? Can be a length-one character vector, giving the name of a Spark aggregation function to be called; a named `R` list mapping column names to an aggregation method, or an `R` function that is invoked on the grouped dataset. |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n \nlibrary(sparklyr) \nlibrary(dplyr) \n \nsc <- spark_connect(master = \"local\") \niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \n# aggregating by mean \niris_tbl %>% \n  mutate(Petal_Width = ifelse(Petal_Width > 1.5, \"High\", \"Low\")) %>% \n  sdf_pivot(Petal_Width ~ Species, \n    fun.aggregate = list(Petal_Length = \"mean\") \n  ) \n \n# aggregating all observations in a list \niris_tbl %>% \n  mutate(Petal_Width = ifelse(Petal_Width > 1.5, \"High\", \"Low\")) %>% \n  sdf_pivot(Petal_Width ~ Species, \n    fun.aggregate = list(Petal_Length = \"collect_list\") \n  ) \n \n \n```\n:::\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}