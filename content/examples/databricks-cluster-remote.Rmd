---
title: "Option 1 - Connecting to Databricks remotely"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
params:
  width: 600
---

## Overview

With this configuration, RStudio Server Pro is installed outside of the Spark
cluster and connects to Spark remotely using Databricks Connect.

<img src="/images/deployment/databricks/rstudio-databricks-remote.png" width='800px' align='center'/>

This is the recommended configuration because it targets separate environments,
involves simpler configuration, avoids resource contention, and allows RStudio
Server Pro to connect to Databricks as well as other remote storage and compute
resources.

## Advantages and limitations

Advantages:

- RStudio Server Pro will remain functional if Databricks clusters are
  terminated
- Provides the ability to comminicate with one or more Databricks clusters as a
  remote compute resource
- Avoids resource contention between RStudio Server Pro and Databricks

Limitations:

- Cannot easily or efficiently load data from outside of the Spark cluster

## Configure RStudio Server Pro to connect to Databricks remotely

## Create a Databricks cluster

In the Databricks console, create a new Databricks cluster.

## Use sparklyr

To work with a remote Databricks cluster, you need to have a local installation
of Spark that matches the version of Spark on the Databricks Cluster.

You can install Spark by running the following command in the RStudio IDE:

```
sparklyr::spark_install()
```

`SPARK_HOME` must be set to the output of `databricks-connect get-spark-home`.

Connect using the following:

```
library(sparklyr)

Sys.setenv(SPARK_HOME="<path-returned-by-databricks-connect-get-spark-home>")

sc <- spark_connect(master = "local", method = "databricks")
```
