---
title: Databricks
format:
  html:
    theme: default
    toc: true
execute:
    eval: true
    freeze: true
editor: 
  markdown: 
    wrap: 72
aliases:
  - /examples/databricks-cluster
  - /examples/databricks-cluster-odbc.html  
  - /examples/databricks-cluster-local.html
  - /examples/databricks-cluster-remote.html
  - /deployment/databricks-cluster.html
  - /deployment/databricks-cluster-odbc.html  
  - /deployment/databricks-cluster-local.html
  - /deployment/databricks-cluster-remote.html
  - /deployment/databricks-spark-connect.html  
---

```{r}
#| include: false

library(remotes)
library(dplyr)
library(dbplyr)
library(sparklyr)
library(pysparklyr)
```

*Last updated: `r lubridate::date()`*

## Intro

[Databricks Connect](https://docs.databricks.com/en/dev-tools/databricks-connect/index.html#)
enables the interaction with Spark clusters remotely. It is based on [Spark
Connect](https://spark.apache.org/docs/latest/spark-connect-overview.html),
which enables remote connectivity thanks to its new decoupled client-server
architecture. This allows users to interact with the Spark cluster without
having to run the jobs from a node. Additionally, it removes the requirement of
having Java components installed in the user's machine.

The API is very different than "legacy" Spark and using the Spark
shell is no longer an option. We have decided to use Python as the new
interface. In turn, Python uses *gRPC* to interact with Spark.

::: {#fig-connect}
```{mermaid}
%%| fig-width: 10
%%| eval: true
flowchart LR
  subgraph lp[test]
    subgraph r[R]
      sr[sparklyr]
      rt[reticulate]
    end
    subgraph ps[Python]
      dc[Databricks Connect]
      g1[gRPC]
    end
  end   
  subgraph db[Databricks]
    sp[Spark]   
  end
  sr <--> rt
  rt <--> dc
  g1 <-- Internet<br>Connection --> sp
  dc <--> g1
  
  style r   fill:#fff,stroke:#666,color:#000
  style sr  fill:#fff,stroke:#666,color:#000
  style rt  fill:#fff,stroke:#666,color:#000
  style ps  fill:#fff,stroke:#666,color:#000
  style lp  fill:#fff,stroke:#666,color:#fff
  style db  fill:#fff,stroke:#666,color:#000
  style sp  fill:#fff,stroke:#666,color:#000
  style g1  fill:#fff,stroke:#666,color:#000
  style dc  fill:#fff,stroke:#666,color:#000
```

How `sparklyr` communicates with Databricks Connect
:::

We are using `reticulate` to interact with the Python API. `sparklyr`
extends the functionality, and user experience, by providing the
`dplyr`back-end, `DBI` back-end, and integration with RStudio's
Connection pane.

In order to quickly iterate on enhancements and bug fixes, we have
decided to isolate the Python integration into its own package. The new
package, called [`pysparklyr`](https://github.com/mlverse/pysparklyr),
is an extension of `sparklyr`.

## Package Installation

To access Databricks Connect, you will need the following two packages:

-   `sparklyr` - 1.8.4
-   `pysparklyr` - 0.1.3

``` r
install.packages("sparklyr")
install.packages("pysparklyr")
```

## Setup credentials

To use with Databricks Connect, in run-time 13 or above, you will need
three configuration items:

1.  Your [Workspace Instance
    URL](https://docs.databricks.com/workspace/workspace-details.html#workspace-url)
2.  Your [Personal Authentication
    Token](https://docs.databricks.com/dev-tools/auth.html#pat) (PAT), or a 
    Posit Workbench instance configured to manage with Databricks services (see next section)
3.  Your [Cluster
    ID](https://docs.databricks.com/workspace/workspace-details.html#cluster-url-and-id)

### Posit Workbench

Posit Workbench can manage Databricks credentials on behalf of the user.
For users of Posit Workbench, this is the recommended approach to
setting up credentials as it provides an additional layer of security. If you
are not currently using Posit Workbench, feel free to skip this section. 

Details for how to setup and configure the Databricks integration can be found
[here](https://docs.posit.co/ide/server-pro/integration/databricks.html).

For users who have signed into a Databricks Workspace via Posit
Workbench, the credentials will be automatically configured and no
additional setup is required. The only thing that still needs to be
supplied when making a connection to Databricks is the cluster ID.

![](/images/deployment/databricks/workbench-session-start.png){width="600"}

### Environment Variables

We have developed this solution to align with other applications that
integrate with Databricks. All applications need, at minimum, a work
space *(1)*, and an authentication token *(2)*. For default values,
those applications initially look for these environment variables:

-   `DATABRICKS_HOST` - Your Workspace Instance URL
-   `DATABRICKS_TOKEN` - Your Personal Authentication Token

Environment variables work well, because they rarely vary between
projects. The thing that will change more often is the cluster you are
connecting to. Using environment variables also makes connection safer,
because token contents will not be in your code in plain text. We
recommend that you set these two variables at your **user level**. To do
this run:

``` r
usethis::edit_r_environ()
```

That command will open a text file that controls the environment
variables at the **user level**. If missing, insert the entries for the
two variables:

``` bash
DATABRICKS_HOST="Enter here your Workspace URL"
DATABRICKS_TOKEN="Enter here your personal token" # Not needed if using Posit Workbench
```

**This is a one time operation.** After saving and closing the file,
restart your R session.

::: callout-note
If you are using Posit Workbench and have signed into your Databricks Workspace
when starting your session, you do not need to set these environment variables.
:::

## First time connecting

After setting up your Host and Token environment variables, you can now
connect to your cluster by simply providing the cluster's ID, and the
method to `spark_connect()`:

``` r
library(sparklyr)

sc <- spark_connect(
  cluster_id = "Enter here your cluster ID",
  method = "databricks_connect"
)
```

In order to connect and interact with Databricks, you will need a
specific set of [Python libraries](#python-libraries) installed and
available. To make it easier to get started, we provide functionality
that will automatically do the following:

-   Create or re-create the necessary Python environment. Based on your
    OS, it will choose to create a Virtual Environment or use Conda.

-   Install the needed Python libraries into the new environment.

`spark_connect()` will check to see if you have the expected Python
environment and prompt you to accept its installation if missing. Here
is an example of the code and output you would expect to see:

``` r
sc <- spark_connect(
    cluster_id = "1026-175310-7cpsh3g8",
    method = "databricks_connect"
)

#> ! Retrieving version from cluster '1026-175310-7cpsh3g8' 
#> Cluster version: '14.1' 
#> ! No viable Python Environment was identified for Databricks Connect version 14.1 
#> Do you wish to install Databricks Connect version 14.1? 
#>  
#> 1: Yes 
#> 2: No 
#> 3: Cancel 
#>  
#> Selection: 1 
```

After accepting, the Python environment will be created with a specific
name, and all of the needed Python libraries will be installed within.
After it is done, it will attempt to connect to your cluster. Here is an
abbreviated example of the output that occurs when selecting "Yes":

``` r
#> ✔ Automatically naming the environment:'r-sparklyr-databricks-14.1' 
#> Using Python: /Users/edgar/.pyenv/versions/3.10.13/bin/python3.10 
#> Creating virtual environment 'r-sparklyr-databricks-14.1' ... 
#> + /Users/edgar/.pyenv/versions/3.10.13/bin/python3.10 -m venv /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1 
#> Done! 
#>   Installing packages: pip, wheel, setuptools 
#> + /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python -m pip install --upgrade pip wheel setuptools 
#> Requirement already satisfied: pip in /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/lib/python3.10/site-packages (23.0.1) 
#> Collecting pip 
#> Using cached pip-23.3.1-py3-none-any.whl (2.1 MB) 
#> Collecting wheel 
#> Using cached wheel-0.42.0-py3-none-any.whl (65 kB) 
#> Requirement already satisfied: setuptools in /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/lib/python3.10/site-packages (65.5.0) 
...
...
...
#> Successfully installed PyArrow-14.0.1 cachetools-5.3.2 certifi-2023.11.17 charset-normalizer-3.3.2 databricks-connect-14.1.0 databricks-sdk-0.14.0 google-api-core-2.14.0 google-api-python-client-2.109.0 google-auth-2.25.0 google-auth-httplib2-0.1.1 googleapis-common-protos-1.61.0 grpcio-1.59.3 grpcio_status-1.59.3 httplib2-0.22.0 idna-3.6 numpy-1.26.2 pandas-2.1.3 protobuf-4.25.1 py4j-0.10.9.7 pyasn1-0.5.1 pyasn1-modules-0.3.0 pyparsing-3.1.1 python-dateutil-2.8.2 pytz-2023.3.post1 requests-2.31.0 rsa-4.9 six-1.16.0 tzdata-2023.3 uritemplate-4.1.1 urllib3-2.1.0 
#> ✔ Using the 'r-sparklyr-databricks-14.1' Python environment 
#> Path: /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python 
```

## Interacting with the cluster

### RStudio's Connection pane

Thanks to the new way we are integrating with Spark, it is now possible
to display the same structure displayed in the Databricks Data Explorer.
In Databricks, the current data structure levels are:

-   Catalog
    -   Database
        -   Table

In the RStudio Connections Pane, you can navigate the data structure by
expanding from the top level, all the way down to the table you wish to
explore. Once expanded, the table's fields and their types are
displayed.

![](/images/deployment/connect/rstudio-connection.png)

You can also click on the **table** icon, situated to the right of the
table name, to preview the first 1,000 rows:

![](/images/deployment/connect/preview.png)

### Using the Connection to Access Data

```{r}
library(dplyr)
library(dbplyr)
library(sparklyr)

sc <- spark_connect(
    cluster_id = "1026-175310-7cpsh3g8",
    method = "databricks_connect"
)
```

After connecting, you can use `dbplyr`'s `in_catalog()` function to
access any table in your data catalog. You will only need to pass the
respective names of the three levels as comma separated character
entries to `in_catalog()` in this order: Catalog, Database, and Table.

Here is an example of using `tbl()` and `in_catalog()` to point to the
**trips** table, which is inside **nyctaxi** database, which is inside
the \***samples** catalog:

```{r}
trips <- tbl(sc, in_catalog("samples", "nyctaxi", "trips"))

trips
```

After pointing `tbl()` to that specific table, you can then use `dplyr`
to execute queries against the data.

```{r}
trips %>%
  group_by(pickup_zip) %>%
  summarise(
    count = n(),
    avg_distance = mean(trip_distance, na.rm = TRUE)
  )
```

### Posit Workench's 'Databricks Pane'

Posit Workbench provides users with a Databricks pane for direct access
to available Databricks clusters. From this pane, users can view details
about Databricks clusters and connect directly to them. More details
about this feature can be found
[here](https://docs.posit.co/ide/server-pro/user/rstudio-pro/guide/databricks.html).

![](/images/deployment/databricks/workbench-databricks-pane.png){width="500"}


## Machine Learning

Machine Learning capabilities are currently available starting with
Databricks Runtime version 14.1. Compared to "legacy" Spark, Spark
Connect's ML capabilities are limited. At this time, there is only one
supported model, Logistic Regression, and two scaler transformers,
namely Standard Scaler and Max Abs Scaler. `sparklyr` makes that
functionality available.

### Using for the first time

By default, the Python environment that `sparklyr` creates does not
include libraries that relate to Machine Learning. These include Torch
and "scikit-learn". Some of the libraries are large in size and they may
have Python requirements that are challenging to new users.
Additionally, we have noticed there are not many users that need to
utilize ML capabilities at this time.

The first time an ML function is accessed through `sparklyr`, you will
be prompted to install the additional Python libraries which are needed
to access such ML capabilities.

``` r
ml_logistic_regression(tbl_mtcars, am ~ .)
#> ! Required Python libraries to run ML functions are missing
#>   Could not find: torch, torcheval, and scikit-learn
#>   Do you wish to install? (This will be a one time operation)
#> 
#> 1: Yes
#> 2: Cancel
#> 
#> Selection: 1
#> Using virtual environment '/Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1' ...
#> + /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python -m pip install --upgrade --no-user torch torcheval scikit-learn
#> Collecting torch
...
```

::: callout-note
It is possible to install the ML libraries along with the required
libraries. There may be several reasons to do this, including trying to
recreate the environment after upgrading Python in your machine. Just
pass `install_ml=TRUE` to the installation function:

``` r
install_databricks(cluster_id = "Enter your cluster's ID", install_ml = TRUE)
```

or

``` r
install_databricks(version = "14.1", install_ml = TRUE)
```
:::

### Easily fit and use

At this time, Logistic Regression is the only model supported. As usual,
there are specific data preparation steps in order to run. `sparklyr`
automates those steps, so all you have to do is pass the Spark data
frame and the formula to use:

```{r}
tbl_mtcars <- copy_to(sc, mtcars)

model1 <- ml_logistic_regression(tbl_mtcars, am ~ .)
```

The output for Spark Connect based models has been upgraded. It will
display the model parameters.

```{r}
model1
```

As shown in the following screenshot, the new output features a
first-of-its-kind tooltip, it will popup the description of the
parameter when hovered over. This functionality works when used in
RStudio, and any console that supports this kind of enhanced user
experience.

![](/images/deployment/databricks/model-output.png)

To use the model, you can run `ml_predict()`:

```{r}
ml_predict(model1, tbl_mtcars)
```

### Using feature transformers

These are the two feature transformers currently supported:

-   Standard Scaler - `ft_standard_scaler()`
-   Max Abs Scaler - `ft_max_abs_scaler()`

To access simply call the function by passing a vector of column names.
Please note that it will create a single column with an array field that
contains all of the newly scaled values.

```{r}
tbl_mtcars %>% 
  ft_standard_scaler(c("wt", "mpg"), "features") %>% 
  select(wt, mpg, features)
```

When you are done with you queries and computations, you should
disconnect from the cluster.

```{r}
spark_disconnect(sc)
```


## Environments

### Install different version of `databricks.connect`

Here are three different options to create a custom Python environment,
that will contain the needed Python libraries to interact with
Databricks Connect:

-   To install the latest versions of all the needed libraries, use:

    ``` r
    pysparklyr::install_databricks()
    ```

    `sparklyr` will query PyPi.org to get the latest version of
    `databricks.connect` and installs that version.

-   It is recommended that the version of the `databricks.connect`
    library matches the DBR version of your cluster. To do this, pass
    the DBR version in the `version` argument:

    ``` r
    pysparklyr::install_databricks("14.0")
    ```

    This will create a Python environment and install
    `databricks.connect` version 14.0, and it will automatically name it
    `r-sparklyr-databricks-14.0`. By using this name, `sparklyr` is able
    to know what version of `databricks.connect` is available inside
    this particular Python environment.

-   If you are not sure about the version of the cluster you want to
    interact with, then use the `cluster_id` argument. We have added a
    way to pull the cluster's information without starting Spark
    Connect. This allows us to query the cluster and get the DBR
    version:

    ``` r
    pysparklyr::install_databricks(cluster_id = "[Your cluster's ID]")
    ```

### Restricted Python environments

If your organization restricts Python environment creation, you can
point `sparklyr` to the designated Python installation. To do this, pass
the path to the environment in the `envname` argument of
`spark_connect()`:

``` r
library(sparklyr)

sc <- spark_connect(
  method = "databricks_connect",
  cluster_id = "Enter here your cluster ID",
  envname = "Enter here the path to your Python environment"
)
```

To successfully connect to a Databricks cluster, you will need to match
the proper version of the `databricks.connect` Python library to the
Databricks Runtime (DBR) version in the cluster. For example, if you are
trying to use a Databricks cluster with a DBR version 14.0 then
`databricks.connect` will also need to be version 14.0. Failure to do so
can result in instability or even the inability to connect.

Besides `datbricks.connect`, the Python environment will also need to
have other Python libraries installed. The full list is in the [Python
Libraries](#python-libraries) section.

::: callout-important
If your server, or machine, has only one Python installation and no
ability to create Conda or Virtual environments, then you will encounter
issues when connecting to a Databricks cluster with a mismatched version
of `databricks.connect` to DBR.
:::

**Important** - This step needs only to be **done one time**. If you
need to connect to a different cluster that has the same DBR version,
`sparklyr` will use the same Python environment. If the new cluster has
a different DBR version, then it is recommended that you run the
installation function using the new DBR version or cluster ID.

## Python Libraries {#python-libraries}

Here is the list of the Python libraries needed in order to work with
the cluster:

Required libraries:

-   `databricks-connect`
-   `delta-spark`
-   `pandas`
-   `PyArrow`
-   `grpcio`
-   `google-api-python-client`
-   `grpcio_status`

ML libraries (Optional):

-   `torch`
-   `torcheval`
-   `scikit-learn`

To enable R User Defined Functions (UDFs):

-   `rpy2` (see [Run R code in
    Databricks](/deployment/databricks-connect-udfs.qmd))

## What is supported

Here is a list of what we currently support, and do not support via
`sparklyr` and Databricks Connect:

**Supported**:

-   Integration with most of the `dplyr`, and `DBI`, APIs
-   Integration with the `invoke()` command
-   RStudio Connections Pane navigation
-   Support for Personal Access Token security authentication for
    Databricks Connect
-   Support for most read and write commands. These have only been
    tested in Spark Connect.

**Not supported**:

-   **SDF functions** - Most of these functions require SparkSession,
    which is currently not supported in Spark 3.4.

-   **`tidyr`** - This is ongoing work that we are focusing on in
    `sparklyr`. We are implementing these functions using PySpark
    DataFrame commands instead of depending on the Scala implementation.
