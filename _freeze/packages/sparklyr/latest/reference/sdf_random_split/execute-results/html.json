{
  "hash": "555a92dacbe6699a6a1b68b88d1e5055",
  "result": {
    "markdown": "---\ntitle: \"Partition a Spark Dataframe\"\nexecute:\n  freeze: true\n---\n\n\n*R/sdf_ml.R*\n\n## sdf_random_split\n\n## Description\nPartition a Spark DataFrame into multiple groups. This routine is useful for splitting a DataFrame into, for example, training and test datasets. \n\n\n## Usage\n```r\nsdf_random_split( \n  x, \n  ..., \n  weights = NULL, \n  seed = sample(.Machine$integer.max, 1) \n) \n\nsdf_partition(x, ..., weights = NULL, seed = sample(.Machine$integer.max, 1)) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | An object coercable to a Spark DataFrame. |\n| ... | Named parameters, mapping table names to weights. The weights will be normalized such that they sum to 1. |\n| weights | An alternate mechanism for supplying weights -- when specified, this takes precedence over the `...` arguments. |\n| seed | Random seed to use for randomly partitioning the dataset. Set this if you want your partitioning to be reproducible on repeated runs. |\n\n## Details\n\nThe sampling weights define the probability that a particular observation will be assigned to a particular partition, not the resulting size of the partition. This implies that partitioning a DataFrame with, for example, \n\n`sdf_random_split(x, training = 0.5, test = 0.5)`\n\nis not guaranteed to produce `training` and `test` partitions of equal size. \n\n## Section\n\n## Transforming Spark DataFrames\n\nThe family of functions prefixed with `sdf_` generally access the Scala Spark DataFrame API directly, as opposed to the `dplyr` interface which uses Spark SQL. These functions will 'force' any pending SQL in a `dplyr` pipeline, such that the resulting `tbl_spark` object returned will no longer have the attached 'lazy' SQL operations. Note that the underlying Spark DataFrame **does** execute its operations lazily, so that even though the pending set of operations (currently) are not exposed at the `R` level, these operations will only be executed when you explicitly `collect()` the table. \n\n## Value\nAn `R` `list` of `tbl_spark`s. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n# randomly partition data into a 'training' and 'test' \n# dataset, with 60% of the observations assigned to the \n# 'training' dataset, and 40% assigned to the 'test' dataset \ndata(diamonds, package = \"ggplot2\") \ndiamonds_tbl <- copy_to(sc, diamonds, \"diamonds\") \npartitions <- diamonds_tbl %>% \n  sdf_random_split(training = 0.6, test = 0.4) \nprint(partitions) \n# alternate way of specifying weights \nweights <- c(training = 0.6, test = 0.4) \ndiamonds_tbl %>% sdf_random_split(weights = weights) \n```\n:::\n\n\n## See Also\nOther Spark data frames:  `sdf_copy_to()`, `sdf_distinct()`, `sdf_register()`, `sdf_sample()`, `sdf_sort()`, `sdf_weighted_sample()`\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}