{
  "hash": "8cf72efeb36459f3170b13d1a4610a2e",
  "result": {
    "markdown": "---\ntitle: \"Read Data\"\nformat:\n  html:\n    theme: default\n    toc: true\nexecute:\n  eval: true\n  freeze: true\n---\n\n\n\n\nA new Spark session will contain no data. The first step is to either load data\ninto your Spark session's memory, or point Spark to the location of the data\nso it can access the data on-demand.\n\n## Exercise \n\nFor this exercise, we will start a \"local\" Spark session, and then transfer data \nfrom our R environment to the Spark session's memory.  To do that, we will \nuse the `copy_to()` command:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\ntbl_mtcars <- copy_to(sc, mtcars, \"spark_mtcars\")\n```\n:::\n\n\nIf you are using the RStudio IDE, you will notice a new table in the Connections\npane.  The name of that table is **spark_mtcars**.  That is the name of the data\nset inside the Spark memory. The `tbl_mtcars` variable does not contain any\n`mtcars` data, this variable contains the info that points to the location where \nthe Spark session loaded the data to. \n\nCalling the `tbl_mtcars` variable in R will download the first 1,000 records and\ndisplay them :\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars\n#> # Source: spark<spark_mtcars> [?? x 11]\n#>      mpg   cyl  disp    hp  drat    wt  qsec    vs    am\n#>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#>  1  21       6  160    110  3.9   2.62  16.5     0     1\n#>  2  21       6  160    110  3.9   2.88  17.0     0     1\n#>  3  22.8     4  108     93  3.85  2.32  18.6     1     1\n#>  4  21.4     6  258    110  3.08  3.22  19.4     1     0\n#>  5  18.7     8  360    175  3.15  3.44  17.0     0     0\n#>  6  18.1     6  225    105  2.76  3.46  20.2     1     0\n#>  7  14.3     8  360    245  3.21  3.57  15.8     0     0\n#>  8  24.4     4  147.    62  3.69  3.19  20       1     0\n#>  9  22.8     4  141.    95  3.92  3.15  22.9     1     0\n#> 10  19.2     6  168.   123  3.92  3.44  18.3     1     0\n#> # â€¦ with more rows, and 2 more variables: gear <dbl>,\n#> #   carb <dbl>\n```\n:::\n\nNotice that at the top of the data print out, it is noted that records were \ndownloaded from Spark: *Source: spark<spark_mtcars>...*.\n\nTo clean up the session, we will now stop the Spark session:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_disconnect(sc)\n```\n:::\n\n\n## Working with files\n\nIn a formal Spark environment, it will be rare when we would have to upload data\nfrom R into Spark.  \n\nUsing `sparklyr`, you can tell Spark to read and write data. Spark is able to interact\nwith multiple types of file systems, such as HDFS, S3 and local. Additionally, \nSpark is able to read several file types such as CSV, Parquet, Delta and JSON.\n`sparklyr` provides functions that makes it easy to access these features.  See\nthe [Spark Data](/packages/sparklyr/latest/reference/#spark-data) section for a \nfull list of available functions.  \n\nThe following command will tell Spark to read a CSV file, and to also load it\ninto Spark memory. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Do not run the next following command. It is for example purposes only.\nspark_read_csv(sc, name = \"test_table\",  path = \"/test/path/test_file.csv\")\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}