{
  "hash": "1d31d5a4694cb139bf04d2dad8fac418",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Databricks\nformat:\n  html:\n    theme: default\n    toc: true\nexecute:\n    eval: true\n    freeze: true\neditor: \n  markdown: \n    wrap: 72\naliases:\n  - /examples/databricks-cluster\n  - /examples/databricks-cluster-odbc.html  \n  - /examples/databricks-cluster-local.html\n  - /examples/databricks-cluster-remote.html\n  - /deployment/databricks-cluster.html\n  - /deployment/databricks-cluster-odbc.html  \n  - /deployment/databricks-cluster-local.html\n  - /deployment/databricks-cluster-remote.html\n  - /deployment/databricks-spark-connect.html  \n---\n\n\n\n\n\n\n*Last updated: Wed Mar 26 15:58:07 2025*\n\n## Intro\n\n[Databricks Connect](https://docs.databricks.com/en/dev-tools/databricks-connect/index.html#)\nenables the interaction with Spark clusters remotely. It is based on [Spark\nConnect](https://spark.apache.org/docs/latest/spark-connect-overview.html),\nwhich enables remote connectivity thanks to its new decoupled client-server\narchitecture. This allows users to interact with the Spark cluster without\nhaving to run the jobs from a node. Additionally, it removes the requirement of\nhaving Java components installed in the user's machine. \n\nThe API is very different than \"legacy\" Spark and using the Spark\nshell is no longer an option. We have decided to use Python as the new\ninterface. In turn, Python uses *gRPC* to interact with Spark.\n\n::: {#fig-connect}\n\n\n\n```{mermaid}\n%%| fig-width: 10\n%%| eval: true\nflowchart LR\n  subgraph lp[test]\n    subgraph r[R]\n      sr[sparklyr]\n      rt[reticulate]\n    end\n    subgraph ps[Python]\n      dc[Databricks Connect]\n      g1[gRPC]\n    end\n  end   \n  subgraph db[Databricks]\n    sp[Spark]   \n  end\n  sr <--> rt\n  rt <--> dc\n  g1 <-- Internet<br>Connection --> sp\n  dc <--> g1\n  \n  style r   fill:#fff,stroke:#666,color:#000\n  style sr  fill:#fff,stroke:#666,color:#000\n  style rt  fill:#fff,stroke:#666,color:#000\n  style ps  fill:#fff,stroke:#666,color:#000\n  style lp  fill:#fff,stroke:#666,color:#fff\n  style db  fill:#fff,stroke:#666,color:#000\n  style sp  fill:#fff,stroke:#666,color:#000\n  style g1  fill:#fff,stroke:#666,color:#000\n  style dc  fill:#fff,stroke:#666,color:#000\n```\n\n\n\n\nHow `sparklyr` communicates with Databricks Connect\n:::\n\nWe are using `reticulate` to interact with the Python API. `sparklyr`\nextends the functionality, and user experience, by providing the\n`dplyr`back-end, `DBI` back-end, and integration with RStudio's\nConnection pane.\n\nIn order to quickly iterate on enhancements and bug fixes, we have\ndecided to isolate the Python integration into its own package. The new\npackage, called [`pysparklyr`](https://github.com/mlverse/pysparklyr),\nis an extension of `sparklyr`.\n\n## Package Installation\n\nTo access Databricks Connect, you will need the following two packages:\n\n-   `sparklyr` \n-   `pysparklyr`\n\n``` r\ninstall.packages(\"sparklyr\")\ninstall.packages(\"pysparklyr\")\n```\n\n## Setup credentials\n\nTo use with Databricks Connect, in run-time 13 or above, you will need\nthree configuration items:\n\n1.  Your [Workspace Instance\n    URL](https://docs.databricks.com/workspace/workspace-details.html#workspace-url)\n2.  Your [Personal Authentication\n    Token](https://docs.databricks.com/dev-tools/auth.html#pat) (PAT), or a \n    Posit Workbench instance configured to manage with Databricks services (see next section)\n3.  Your [Cluster\n    ID](https://docs.databricks.com/workspace/workspace-details.html#cluster-url-and-id)\n\n### Posit Workbench\n\nPosit Workbench can manage Databricks credentials on behalf of the user.\nFor users of Posit Workbench, this is the recommended approach to\nsetting up credentials as it provides an additional layer of security. If you\nare not currently using Posit Workbench, feel free to skip this section. \n\nDetails for how to setup and configure the Databricks integration can be found\n[here](https://docs.posit.co/ide/server-pro/integration/databricks.html).\n\nFor users who have signed into a Databricks Workspace via Posit\nWorkbench, the credentials will be automatically configured and no\nadditional setup is required. The only thing that still needs to be\nsupplied when making a connection to Databricks is the cluster ID.\n\n![](/images/deployment/databricks/workbench-session-start.png){width=\"600\"}\n\n### Environment Variables\n\nWe have developed this solution to align with other applications that\nintegrate with Databricks. All applications need, at minimum, a work\nspace *(1)*, and an authentication token *(2)*. For default values,\nthose applications initially look for these environment variables:\n\n-   `DATABRICKS_HOST` - Your Workspace Instance URL\n-   `DATABRICKS_TOKEN` - Your Personal Authentication Token\n\nEnvironment variables work well, because they rarely vary between\nprojects. The thing that will change more often is the cluster you are\nconnecting to. Using environment variables also makes connection safer,\nbecause token contents will not be in your code in plain text. We\nrecommend that you set these two variables at your **user level**. To do\nthis run:\n\n``` r\nusethis::edit_r_environ()\n```\n\nThat command will open a text file that controls the environment\nvariables at the **user level**. If missing, insert the entries for the\ntwo variables:\n\n``` bash\nDATABRICKS_HOST=\"Enter here your Workspace URL\"\nDATABRICKS_TOKEN=\"Enter here your personal token\" # Not needed if using Posit Workbench\n```\n\n**This is a one time operation.** After saving and closing the file,\nrestart your R session.\n\n::: callout-note\nIf you are using Posit Workbench and have signed into your Databricks Workspace\nwhen starting your session, you do not need to set these environment variables.\n:::\n\n## First time connecting\n\nAfter setting up your Host and Token environment variables, you can now\nconnect to your cluster by simply providing the cluster's ID, and the\nmethod to `spark_connect()`:\n\n``` r\nlibrary(sparklyr)\n\nsc <- spark_connect(\n  cluster_id = \"Enter here your cluster ID\",\n  method = \"databricks_connect\"\n)\n```\n\nIn order to connect and interact with Databricks, you will need a\nspecific set of [Python libraries](#python-libraries) installed and\navailable. To make it easier to get started, we provide functionality\nthat will automatically do the following:\n\n-   Create or re-create the necessary Python environment\n\n-   Install the needed Python libraries into the new environment.\n\n\nOn a new computer environment, first time installation will take some time. Under\nthe hood, `reticulate` is downloading, or selecting, the correct version of \nPython, and downloading the Python libraries. After connecting for the first\ntime, the session should start much faster, since all of the needed Python\ncomponents are now downloaded and cached. \n\n## Serveles Compute\n\n[Databricks' Serverless compute](https://docs.databricks.com/aws/en/compute/serverless/) \nlets you run workloads without having to provision a cluster. Databricks\ntakes care or managing the resources.\n\nTo use with `sparklyr`, set the `serverless` argument to `TRUE`. And make sure to\npass the `version` of the DBR you will connect to. `sparklyr` will use the value\nfrom `version` to determine which version of the [`databricks-connect`](https://pypi.org/project/databricks-connect/)\nPython project will be used for the session: \n\n``` r\nlibrary(sparklyr)\n\nsc <- spark_connect(\n  cluster_id = \"Enter here your cluster ID\",\n  method = \"databricks_connect\",\n  serverless = TRUE,\n  version = \"`databricks-connect` version to use\"\n)\n```\n\n## Interacting with the cluster\n\n### RStudio's Connection pane\n\nThanks to the new way we are integrating with Spark, it is now possible\nto display the same structure displayed in the Databricks Data Explorer.\nIn Databricks, the current data structure levels are:\n\n-   Catalog\n    -   Database\n        -   Table\n\nIn the RStudio Connections Pane, you can navigate the data structure by\nexpanding from the top level, all the way down to the table you wish to\nexplore. Once expanded, the table's fields and their types are\ndisplayed.\n\n![](/images/deployment/connect/rstudio-connection.png)\n\nYou can also click on the **table** icon, situated to the right of the\ntable name, to preview the first 1,000 rows:\n\n![](/images/deployment/connect/preview.png)\n\n### Using the Connection to Access Data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(sparklyr)\n\nsc <- spark_connect(\n    cluster_id = \"0320-233822-dge6gcj3\",\n    method = \"databricks_connect\"\n)\n#> ℹ Retrieving info for cluster:'0320-233822-dge6gcj3'\n#> ✔ Cluster: '0320-233822-dge6gcj3' | DBR: '15.4' [294ms]\n#> \n#> ℹ Attempting to load 'r-sparklyr-databricks-15.4'\n#> ✔ Python environment: 'r-sparklyr-databricks-15.4' [1.2s]\n#> \n#> ℹ Connecting to 'Edgar Ruiz's Personal Compute Cluster'\n#> ✔ Connected to: 'Edgar Ruiz's Personal Compute Cluster' [6ms]\n#> \n```\n:::\n\n\n\n\nAfter connecting, you can use `dbplyr`'s `in_catalog()` function to\naccess any table in your data catalog. You will only need to pass the\nrespective names of the three levels as comma separated character\nentries to `in_catalog()` in this order: Catalog, Database, and Table.\n\nHere is an example of using `tbl()` and `in_catalog()` to point to the\n**trips** table, which is inside **nyctaxi** database, which is inside\nthe \\***samples** catalog:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrips <- tbl(sc, in_catalog(\"samples\", \"nyctaxi\", \"trips\"))\n\ntrips\n#> # Source:   table<`samples`.`nyctaxi`.`trips`> [?? x 6]\n#> # Database: spark_connection\n#>    tpep_pickup_datetime tpep_dropoff_datetime trip_distance fare_amount\n#>    <dttm>               <dttm>                        <dbl>       <dbl>\n#>  1 2016-02-13 15:47:53  2016-02-13 15:57:15            1.4          8  \n#>  2 2016-02-13 12:29:09  2016-02-13 12:37:23            1.31         7.5\n#>  3 2016-02-06 13:40:58  2016-02-06 13:52:32            1.8          9.5\n#>  4 2016-02-12 13:06:43  2016-02-12 13:20:54            2.3         11.5\n#>  5 2016-02-23 04:27:56  2016-02-23 04:58:33            2.6         18.5\n#>  6 2016-02-12 18:41:43  2016-02-12 18:46:52            1.4          6.5\n#>  7 2016-02-18 17:49:53  2016-02-18 18:12:53           10.4         31  \n#>  8 2016-02-18 14:21:45  2016-02-18 14:38:23           10.2         28.5\n#>  9 2016-02-03 04:47:50  2016-02-03 05:07:06            3.27        15  \n#> 10 2016-02-18 19:26:39  2016-02-18 19:40:01            4.42        15  \n#> # ℹ more rows\n#> # ℹ 2 more variables: pickup_zip <int>, dropoff_zip <int>\n```\n:::\n\n\n\n\nAfter pointing `tbl()` to that specific table, you can then use `dplyr`\nto execute queries against the data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrips %>%\n  group_by(pickup_zip) %>%\n  summarise(\n    count = n(),\n    avg_distance = mean(trip_distance, na.rm = TRUE)\n  )\n#> # Source:   SQL [?? x 3]\n#> # Database: spark_connection\n#>    pickup_zip count avg_distance\n#>         <int> <dbl>        <dbl>\n#>  1      10468     2        0.545\n#>  2      10032    15        4.49 \n#>  3      10013   273        2.98 \n#>  4      10022   519        2.00 \n#>  5      10162   414        2.19 \n#>  6      10018  1012        2.60 \n#>  7      11106    39        2.03 \n#>  8      11237    15        3.31 \n#>  9      10011  1129        2.29 \n#> 10      11103    16        2.75 \n#> # ℹ more rows\n```\n:::\n\n\n\n\n### Posit Workench's 'Databricks Pane'\n\nPosit Workbench provides users with a Databricks pane for direct access\nto available Databricks clusters. From this pane, users can view details\nabout Databricks clusters and connect directly to them. More details\nabout this feature can be found\n[here](https://docs.posit.co/ide/server-pro/user/rstudio-pro/guide/databricks.html).\n\n![](/images/deployment/databricks/workbench-databricks-pane.png){width=\"500\"}\n\n\n## Machine Learning\n\nMachine Learning capabilities are currently available starting with\nDatabricks Runtime version 14.1. Compared to \"legacy\" Spark, Spark\nConnect's ML capabilities are limited. At this time, there is only one\nsupported model, Logistic Regression, and two scaler transformers,\nnamely Standard Scaler and Max Abs Scaler. `sparklyr` makes that\nfunctionality available.\n\n### Using for the first time\n\nBy default, the Python environment that `sparklyr` creates does not\ninclude libraries that relate to Machine Learning. These include Torch\nand \"scikit-learn\". Some of the libraries are large in size and they may\nhave Python requirements that are challenging to new users.\nAdditionally, we have noticed there are not many users that need to\nutilize ML capabilities at this time.\n\nThe first time an ML function is accessed through `sparklyr`, you will\nbe prompted to install the additional Python libraries which are needed\nto access such ML capabilities.\n\n``` r\nml_logistic_regression(tbl_mtcars, am ~ .)\n#> ! Required Python libraries to run ML functions are missing\n#>   Could not find: torch, torcheval, and scikit-learn\n#>   Do you wish to install? (This will be a one time operation)\n#> \n#> 1: Yes\n#> 2: Cancel\n#> \n#> Selection: 1\n#> Using virtual environment '/Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1' ...\n#> + /Users/edgar/.virtualenvs/r-sparklyr-databricks-14.1/bin/python -m pip install --upgrade --no-user torch torcheval scikit-learn\n#> Collecting torch\n...\n```\n\n::: callout-note\nIt is possible to install the ML libraries along with the required\nlibraries. There may be several reasons to do this, including trying to\nrecreate the environment after upgrading Python in your machine. Just\npass `install_ml=TRUE` to the installation function:\n\n``` r\ninstall_databricks(cluster_id = \"Enter your cluster's ID\", install_ml = TRUE)\n```\n\nor\n\n``` r\ninstall_databricks(version = \"14.1\", install_ml = TRUE)\n```\n:::\n\n### Easily fit and use\n\nAt this time, Logistic Regression is the only model supported. As usual,\nthere are specific data preparation steps in order to run. `sparklyr`\nautomates those steps, so all you have to do is pass the Spark data\nframe and the formula to use:\n\n```r\ntbl_mtcars <- copy_to(sc, mtcars)\n\nmodel1 <- ml_logistic_regression(tbl_mtcars, am ~ .)\n```\n\nThe output for Spark Connect based models has been upgraded. It will\ndisplay the model parameters.\n\nAs shown in the following screenshot, the new output features a\nfirst-of-its-kind tooltip, it will popup the description of the\nparameter when hovered over. This functionality works when used in\nRStudio, and any console that supports this kind of enhanced user\nexperience.\n\n![](/images/deployment/databricks/model-output.png)\n\nTo use the model, you can run `ml_predict()`:\n\n```r\nml_predict(model1, tbl_mtcars)\n```\n\n### Using feature transformers\n\nThese are the two feature transformers currently supported:\n\n-   Standard Scaler - `ft_standard_scaler()`\n-   Max Abs Scaler - `ft_max_abs_scaler()`\n\nTo access simply call the function by passing a vector of column names.\nPlease note that it will create a single column with an array field that\ncontains all of the newly scaled values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars <- copy_to(sc, mtcars)\n\ntbl_mtcars %>% \n  ft_standard_scaler(c(\"wt\", \"mpg\"), \"features\") %>% \n  select(wt, mpg, features)\n#> # Source:   SQL [?? x 3]\n#> # Database: spark_connection\n#>       wt   mpg features                                  \n#>    <dbl> <dbl> <chr>                                     \n#>  1  2.62  21   c(-0.610399567481535, 0.150884824647656)  \n#>  2  2.88  21   c(-0.349785269100972, 0.150884824647656)  \n#>  3  2.32  22.8 c(-0.917004624399845, 0.449543446630647)  \n#>  4  3.22  21.4 c(-0.00229953792688741, 0.217253407310543)\n#>  5  3.44  18.7 c(0.227654254761845, -0.230734525663943)  \n#>  6  3.46  18.1 c(0.248094591889732, -0.330287399658273)  \n#>  7  3.57  14.3 c(0.360516446093113, -0.960788934955698)  \n#>  8  3.19  24.4 c(-0.0278499593367465, 0.715017777282194) \n#>  9  3.15  22.8 c(-0.0687306335925211, 0.449543446630647) \n#> 10  3.44  19.2 c(0.227654254761845, -0.147773797335335)  \n#> # ℹ more rows\n```\n:::\n\n\n\n\nWhen you are done with you queries and computations, you should\ndisconnect from the cluster.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_disconnect(sc)\n```\n:::\n\n\n\n\n\n## Environments\n\n### Install different version of `databricks.connect`\n\nHere are three different options to create a custom Python environment,\nthat will contain the needed Python libraries to interact with\nDatabricks Connect:\n\n-   To install the latest versions of all the needed libraries, use:\n\n    ``` r\n    pysparklyr::install_databricks()\n    ```\n\n    `sparklyr` will query PyPi.org to get the latest version of\n    `databricks.connect` and installs that version.\n\n-   It is recommended that the version of the `databricks.connect`\n    library matches the DBR version of your cluster. To do this, pass\n    the DBR version in the `version` argument:\n\n    ``` r\n    pysparklyr::install_databricks(\"14.0\")\n    ```\n\n    This will create a Python environment and install\n    `databricks.connect` version 14.0, and it will automatically name it\n    `r-sparklyr-databricks-14.0`. By using this name, `sparklyr` is able\n    to know what version of `databricks.connect` is available inside\n    this particular Python environment.\n\n-   If you are not sure about the version of the cluster you want to\n    interact with, then use the `cluster_id` argument. We have added a\n    way to pull the cluster's information without starting Spark\n    Connect. This allows us to query the cluster and get the DBR\n    version:\n\n    ``` r\n    pysparklyr::install_databricks(cluster_id = \"[Your cluster's ID]\")\n    ```\n\n### Restricted Python environments\n\nIf your organization restricts Python environment creation, you can\npoint `sparklyr` to the designated Python installation. To do this, pass\nthe path to the environment in the `envname` argument of\n`spark_connect()`:\n\n``` r\nlibrary(sparklyr)\n\nsc <- spark_connect(\n  method = \"databricks_connect\",\n  cluster_id = \"Enter here your cluster ID\",\n  envname = \"Enter here the path to your Python environment\"\n)\n```\n\nTo successfully connect to a Databricks cluster, you will need to match\nthe proper version of the `databricks.connect` Python library to the\nDatabricks Runtime (DBR) version in the cluster. For example, if you are\ntrying to use a Databricks cluster with a DBR version 14.0 then\n`databricks.connect` will also need to be version 14.0. Failure to do so\ncan result in instability or even the inability to connect.\n\nBesides `datbricks.connect`, the Python environment will also need to\nhave other Python libraries installed. The full list is in the [Python\nLibraries](#python-libraries) section.\n\n::: callout-important\nIf your server, or machine, has only one Python installation and no\nability to create Conda or Virtual environments, then you will encounter\nissues when connecting to a Databricks cluster with a mismatched version\nof `databricks.connect` to DBR.\n:::\n\n**Important** - This step needs only to be **done one time**. If you\nneed to connect to a different cluster that has the same DBR version,\n`sparklyr` will use the same Python environment. If the new cluster has\na different DBR version, then it is recommended that you run the\ninstallation function using the new DBR version or cluster ID.\n\n## Python Libraries {#python-libraries}\n\nHere is the list of the Python libraries needed in order to work with\nthe cluster:\n\nRequired libraries:\n\n-   `databricks-connect`\n-   `delta-spark`\n-   `pandas`\n-   `PyArrow`\n-   `grpcio`\n-   `google-api-python-client`\n-   `grpcio_status`\n\nML libraries (Optional):\n\n-   `torch`\n-   `torcheval`\n-   `scikit-learn`\n\nTo enable R User Defined Functions (UDFs):\n\n-   `rpy2` (see [Run R code in\n    Databricks](/deployment/databricks-connect-udfs.qmd))\n\n## What is supported\n\nHere is a list of what we currently support, and do not support via\n`sparklyr` and Databricks Connect:\n\n**Supported**:\n\n-   Integration with most of the `dplyr`, and `DBI`, APIs\n-   Integration with the `invoke()` command\n-   RStudio Connections Pane navigation\n-   Support for Personal Access Token security authentication for\n    Databricks Connect\n-   Support for most read and write commands. These have only been\n    tested in Spark Connect.\n\n**Not supported**:\n\n-   **SDF functions** - Most of these functions require SparkSession,\n    which is currently not supported in Spark 3.4.\n\n-   **`tidyr`** - This is ongoing work that we are focusing on in\n    `sparklyr`. We are implementing these functions using PySpark\n    DataFrame commands instead of depending on the Scala implementation.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}