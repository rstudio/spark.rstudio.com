---
title: "`tidymodels` and Spark"
execute:
  eval: true
  freeze: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r}
#| include: false
options("pillar.width" = 60)
library(sparklyr)
library(parsnip)
library(stringr)
library(tidyr)
library(dplyr)
library(rlang)
library(purrr)
library(corrr)
library(gt)

get_title <- function(x, pkg) {
  map_chr(
    x, ~{
      ht <- help(.x, package = {{pkg}})
      if(length(as.character(ht)) == 0) {
        ""
      } else {
        htl <- utils:::.getHelpFile(as.character(ht))
        htl[[1]][[1]][[1]]        
      }
    }
  )
}

```

## Intro

`tidymodels` is a collection of packages for modeling and machine
learning. Just like `sparklyr`, `tidymodels` uses `tidyverse`
principles.

`sparklyr` allows us to use `dplyr` verbs to manipulate data. We use the
same commands in R when manipulating local data or Spark data.
Similarly, `sparklyr` and some packages in the `tidymodels` ecosystem
offer integration.

As with any evolving framework, the integration does not apply to all
functions. This article aims at enumerating what is available today, and
why should we consider using the `tidymodels` implementation in our
day-to-day work with Spark.

Our expectation is that this article will be constantly updated as the
integration between `tidymodels` and `sparklyr` grows and improves.

## Model specification with `parsnip`

`parsnip` provides a common interface to models. This enables us to run
the same model against multiple engines. `parsnip` contains translation
for each of these packages, so we do not have to remember, or find out,
how to setup each argument in the respective package.

### Why use in Spark?

In some cases, it is better to try out model parameters on a smaller,
local, data set in R. Once we are happy with the parameters, we can then
run the model over the entire data set in Spark.

For example, doing this for a linear regression model, we would use
`lm()` locally, and then we would have to re-write the model using
`ml_linear_regression()`. Both of these functions have different sets of
function arguments that we would need to set.

`parsnip` allows us to use the exact same set of functions and arguments
when running against either back-end. With a couple of small changes, we
can change the target engine (R vs Spark) and the target data set (local
vs remote). Here is an example of what the model fitting looks like
locally in R:

```{r}
#| eval: false

linear_reg() %>%
  set_engine("lm") %>%           # << Engine set to `lm`
  fit(mpg ~ ., data = mtcars)    # << Local `mtcars`
```

To switch to Spark, we just need to change the engine to `spark`, and
the training data set to the remote Spark data set:

```{r}
#| eval: false

linear_reg() %>%
  set_engine("spark") %>%           # << Engine set to `spark`
  fit(mpg ~ ., data = spark_mtcars) # << Remote `mtcars`
```

### List of supported models

There are six `parsnip` models that currently support `sparklyr`
equivalent models. Here is the list:

```{r}
#| include: false

pf <- ls("package:parsnip")

spark_engines <- map_dfr(pf, ~{
  x <- get_from_env(.x)
  if(!is.null(x)) x$model <- .x
  x
  }) %>% 
  filter(engine == "spark") %>% 
  select(-engine)

supported_models <- spark_engines %>% 
  mutate(supported = "Yes") %>% 
  pivot_wider(
    names_from = "mode", 
    values_from = "supported", 
    values_fill = ""
    ) %>% 
  mutate(title = get_title(model, "parsnip")) %>% 
  select(title, model, everything()) %>% 
  mutate(model = paste0("`", model, "()`"))
```

```{r}
#| echo: false
supported_models %>% 
  gt() %>% 
  cols_label(
    title = md("**Model**"),
    model = md("**`parsnip` function**"),
    classification = md("**Classification**"),
    regression = md("**Regression**")
  ) %>% 
  fmt_markdown(model)
```

### Examples

This article will use the same Spark session in all the examples.

```{r}
library(sparklyr)
library(dplyr)

sc <- spark_connect("local")
```

We will upload the `mtcars` data set to the Spark session:

```{r}
spark_mtcars <- copy_to(sc, mtcars)
```

A linear regression model is trained with `spark_mtcars`:

```{r}
library(parsnip)

mtcars_model <- linear_reg() %>%
  set_engine("spark") %>%
  fit(mpg ~ ., data = spark_mtcars)

mtcars_model
```

It is also possible to see how `parsnip` plans to translate the model
against the given engine. Use `translate()` so view the translation:

```{r}
linear_reg() %>%
  set_engine("spark") %>%
  translate()
```

Now, we will show an example with a classification model. We will fit a
random forest model. To start, we will copy the `iris` data set to the
Spark session:

```{r}
spark_iris <- copy_to(sc, iris)
```

We can prepare the model by piping the initial setup of 100 trees, then
then to set the mode to "classification", and then the engine to "spark"
and lastly, fit the model:

```{r}
iris_model <- rand_forest(trees = 100) %>% 
  set_mode("classification") %>% 
  set_engine("spark") %>% 
  fit(Species ~., data = spark_iris)

iris_model
```

## Model results with `broom`

The `broom` package offers great ways to get summarized information
about a fitted model. There is support for three `broom` functions in
`sparklyr`:

-   `tidy()` - Summarizes information about the components of a model. A
    model component might be a single term in a regression, a single
    hypothesis, a cluster, or a class.

-   `glance()` - Returns a data frame with exactly one row of model
    summaries. The summaries are typically goodness of fit measures,
    p-values for hypothesis tests on residuals, or model convergence
    information.

-   `augment()` - Adds the prediction columns to the data set. This
    function is similar to `ml_predict()`, but instead of returning only
    a vector of predictions (like `predict()`), it adds the new
    column(s) to the data set. `augment()`

### Why use in Spark?

`tidy()` and `glance()` offer a very good, concise way to view the model
results in a rectangular data frame. This is very helpful when we want
to compare different model runs side-by-side.

```{r}
#| include: false


af <- ls(getNamespace("sparklyr"), all.names = TRUE)

find_methods <- function(x, pattern) {
  y <- x[str_detect(x, pattern)] 
  y1 <- str_remove(y, pattern)
  y1[y1 != "ml_model"]
}

all_tidy <- find_methods(af, "tidy\\.")
all_glance <- find_methods(af, "glance\\.")

all_table <- table(c(all_tidy, all_glance))
if(length(unique(all_table)) > 1) stop("Mistmatched methods")
```

### List of supported models

Currently, 20 Spark models support `broom` via `sparklyr`. Here is the
current list of models and the corresponding `sparklyr` function:

```{r}
#| echo: false

tibble(obj_name = all_tidy) %>% 
  mutate(
    fname = str_replace(obj_name, "ml_model_", "ml_"),
    fname = str_remove(fname, "_classification"),
    fname = ifelse(fname == "ml_gbt", "ml_gradient_boosted_trees", fname)
    ) %>% 
  filter(
    fname != "ml_decision_tree_regression", 
    fname != "ml_gbt_regression", 
    fname != "ml_random_forest_regression"
    ) %>% 
  mutate(
    model = get_title(fname, "sparklyr"),
    model = str_remove(model, "Spark ML --"),
    model = str_remove(model, "Feature Transformation -- ")
    ) %>% 
  mutate(fname = md(paste0("`", fname,"()`"))) %>% 
  arrange(model) %>% 
  select(model, fname) %>%  
  gt() %>% 
  tab_header(md("**Models that support `glance()`, `tidy()`, and `augment()`**")) %>% 
  cols_label(
    model = md("**Model**"),
    fname = md("**Function**")  ) %>% 
  fmt_markdown(fname) %>% 
  cols_align("left")
```

### Examples

Using the same Spark session and models created in the previous section
we start by loading `broom`:

```{r}
library(broom)
```

To view the estimates for each term simply pass `mtcars_model` to the
`tidy()` function:

```{r}
#| include: false
tidy(mtcars_model)
```

```{r}
#| warnings: false
tidy(mtcars_model)
```

`glance()` returns the the models R Squared, error means, and variance:

```{r}
#| warnings: false
glance(mtcars_model)
```

```{r}
#| eval: false

augment(mtcars_model)
```

For our classification model, `tidy()` returns each feature's
importance:

```{r}
#| warnings: false
tidy(iris_model)
```

The `glance()` model returns the number of trees, nodes depth,
sub-sampling rate and impurtiy mode:

```{r}
#| warnings: false
glance(iris_model)
```

## `yardstick`-like metrics

The `metrics()` function, from the `yardstick` package, provides an easy to read
`tibble` with the relevant metrics.  It automatically detects the type of model
and it decides which metrics to show. 

### Why use in Spark?

In `sparklyr`, the family of `ml_metrics...` functions outputs a `tibble` with the 
same structure as `yardstick::metrics()`.  The functions also expect the same 
base arguments of `x`, `truth` and `estimate`.  In `sparklyr`, model detection is not
available yet, so based on the type of model, there are three functions to choose
from. 

The `ml_metrics...` functions expect a `tbl_spark` that was created by the
`ml_predict()` function.  These functions provide a `metrics` argument that allows
us to change the metrics to calculate.  All of the metrics that have an equivalent
in `yardstick` can be called using the same value, such as `f_meas`. For others,
they can be requested using Spark's designation. For more information, see the
help file of the specific `ml_metrics...` function.

- [ml_metrics_binary()](/packages/sparklyr/latest/reference/ml_metrics_binary.html)
- [ml_metrics_multiclass()](/packages/sparklyr/latest/reference/ml_metrics_multiclass.html)
- [ml_metrics_regression()](/packages/sparklyr/latest/reference/ml_metrics_regression.html)

#### How are they different form `ml_evaluate()`?

It is true that both sets of functions return metrics based on the results. The 
difference is that `ml_evaluate()` requires the original Spark model object in
order to work. `ml_metrics...` only required a table with the predictions, 
preferably, predictions created by `ml_predict()`.

### Example

Using `sdf_random_split()`, split the data into `training` and `test`. And then
fit a model.  In this case it will be a logistic regression model.

```{r}
prep_iris <- tbl(sc, "iris") %>%
  mutate(is_setosa = ifelse(Species == "setosa", 1, 0))

iris_split <- sdf_random_split(prep_iris, training = 0.5, test = 0.5)

model <- ml_logistic_regression(iris_split$training, "is_setosa ~ Sepal_Length")
```

With `ml_predict()`, create a new `tbl_spark` that contains the original data
and additional columns needed created by the prediction process.

```{r}
tbl_predictions <- ml_predict(model, iris_split$test)
```

The `ml_metrics_binary()` outputs a `tibble` with the ROC and PR AUC.

```{r}
ml_metrics_binary(tbl_predictions)
```


## Correlations using `corrr`

The `corrr` package helps with exploring data correlations in R. It
returns a data frame with all of the correlations.

### Why use in Spark?

For `sparklyr`, `corrr` wraps the `ml_cor()` function, and returns a
data frame with the exact same format as if the correlation would have
been calculated in R. This allows us to use all the other functions
inside `corrr`, such as filtering, and plotting without having to re-run
the correlation inside Spark.

### Example

We start by loading the package `corrr`:

```{r}
library(corrr)
```

We will pipe `spark_mtcars` into the `correlate()` function. That runs
the correlations inside Spark, and returning the results into R. Those
results are saved into a data frame:

```{r}
#| message: false
#| warning: false
corr_mtcars <- spark_mtcars %>% 
  correlate()
```

The `corr_mtcars` variable is now a local data set. So we do not need to
go back to Spark if we wish to use it for other things that `corrr` can
do:

```{r}
corr_mtcars
```

For example, `shave()` removes the duplicate correlations from the data
set, making it easier to read:

```{r}
#| message: false
#| warning: false
corr_mtcars %>% 
  shave()
```

`rplot()` provides a nice way to visualize the correlations. Again,
because `corr_mtcars`'s data it is currently locally in R, plotting
requires no extra steps:

```{r}
#| message: false
#| warning: false
corr_mtcars %>% 
  rplot()
```

```{r}
#| include: false
spark_disconnect(sc)
```
