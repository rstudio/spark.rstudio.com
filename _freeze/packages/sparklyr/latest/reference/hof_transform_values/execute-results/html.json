{
  "hash": "c466faadba703b304f6148bbf2f5b0da",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Transforms values of a map\"\nexecute:\n  freeze: true\n---\n\n\n\n*R/dplyr_hof.R*\n\n## hof_transform_values\n\n## Description\n Applies the transformation function specified to all values of a map (this is essentially a dplyr wrapper to the `transform_values(expr, func)` higher- order function, which is supported since Spark 3.0) \n\n\n## Usage\n```r\n \nhof_transform_values(x, func, expr = NULL, dest_col = NULL, ...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | The Spark data frame to be processed |\n| func | The transformation function to apply (it should take (key, value) as arguments and return a transformed value) |\n| expr | The map being transformed, could be any SQL expression evaluating to a map (default: the last column of the Spark data frame) |\n| dest_col | Column to store the transformed result (default: expr) |\n| ... | Additional params to dplyr::mutate |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n \n \nlibrary(sparklyr) \nsc <- spark_connect(master = \"local\", version = \"3.0.0\") \nsdf <- sdf_len(sc, 1) %>% dplyr::mutate(m = map(\"a\", 0L, \"b\", 2L, \"c\", -1L)) \ntransformed_sdf <- sdf %>% hof_transform_values(~ CONCAT(.x, \" == \", .y)) \n \n \n```\n:::\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}