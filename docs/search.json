[
  {
    "objectID": "learn-more.html#cheatsheet",
    "href": "learn-more.html#cheatsheet",
    "title": "Learn More",
    "section": "Cheatsheet",
    "text": "Cheatsheet\nA 2 page visual guide to sparklyr functions. It provides a concise description of each function. It also has diagrams of how several Feature Transformers work."
  },
  {
    "objectID": "learn-more.html#book",
    "href": "learn-more.html#book",
    "title": "Learn More",
    "section": "Book",
    "text": "Book\nThis free, online book, intends to take someone unfamiliar with Spark or R and help you become proficient by teaching you a set of tools, skills and practices applicable to large-scale data science."
  },
  {
    "objectID": "site-news.html",
    "href": "site-news.html",
    "title": "Site News",
    "section": "",
    "text": "Date\nEvent\nArticle / Page\n\n\n\n\n1/25/22\nNewly added\nGet Started - Model Data\n\n\n1/25/22\nNewly added\nGet Started - Prepare Data\n\n\n1/25/22\nUpdated\nSpark Machine Learning Library (MLlib)\n\n\n1/25/22\nUpdated\nManipulating Data with dplyr\n\n\n1/24/22\nNewly added\nGet Started - Read Data\n\n\n1/24/22\nNewly added\nGet Started - Install\n\n\n1/21/22\nUpdated\nUnderstanding Spark Caching\n\n\n1/21/22\nUpdated\nDistributing R Computations\n\n\n1/21/22\nUpdated\nIntro to Spark Streaming with sparklyr\n\n\n1/20/22\nUpdated\nText mining with Spark & sparklyr\n\n\n1/20/22\nUpdated\nSpark ML Pipelines"
  },
  {
    "objectID": "deployment/cloudera-aws.html#cdh-5",
    "href": "deployment/cloudera-aws.html#cdh-5",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "CDH 5",
    "text": "CDH 5\nWe will start with a Cloudera cluster CDH version 5.8.2 (free version) with an underlaying Ubuntu Linux distribution."
  },
  {
    "objectID": "deployment/cloudera-aws.html#spark-1.6",
    "href": "deployment/cloudera-aws.html#spark-1.6",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Spark 1.6",
    "text": "Spark 1.6\nThe default Spark 1.6.0 parcel is in installed and running"
  },
  {
    "objectID": "deployment/cloudera-aws.html#hive-data",
    "href": "deployment/cloudera-aws.html#hive-data",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Hive data",
    "text": "Hive data\nFor this demo, we have created and populated 3 tables in Hive. The table names are: flights, airlines and airports. Using Hue, we can see the loaded tables. For the links to the data files and their Hive import scripts please see Appendix A."
  },
  {
    "objectID": "deployment/cloudera-aws.html#cache-the-tables-into-memory",
    "href": "deployment/cloudera-aws.html#cache-the-tables-into-memory",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Cache the tables into memory",
    "text": "Cache the tables into memory\nUse tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame.\n\n# Cache flights Hive table into Spark\ntbl_cache(sc, 'flights')\nflights_tbl <- tbl(sc, 'flights')\n\n# Cache airlines Hive table into Spark\ntbl_cache(sc, 'airlines')\nairlines_tbl <- tbl(sc, 'airlines')\n\n# Cache airports Hive table into Spark\ntbl_cache(sc, 'airports')\nairports_tbl <- tbl(sc, 'airports')"
  },
  {
    "objectID": "deployment/cloudera-aws.html#create-a-model-data-set",
    "href": "deployment/cloudera-aws.html#create-a-model-data-set",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Create a model data set",
    "text": "Create a model data set\nFilter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight.\n\n# Filter records and create target variable 'gain'\nmodel_data <- flights_tbl %>%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>%\n  filter(depdelay > 15 & depdelay < 240) %>%\n  filter(arrdelay > -60 & arrdelay < 360) %>%\n  filter(year >= 2003 & year <= 2007) %>%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>%\n  mutate(gain = depdelay - arrdelay) %>%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain)\n\n# Summarize data by carrier\nmodel_data %>%\n  group_by(uniquecarrier) %>%\n  summarize(description = min(description), gain=mean(gain), \n            distance=mean(distance), depdelay=mean(depdelay)) %>%\n  select(description, gain, distance, depdelay) %>%\n  arrange(gain)\n\nSource:   query [?? x 4]\nDatabase: spark connection master=yarn-client app=sparklyr local=FALSE\n\n                    description       gain  distance depdelay\n                          <chr>      <dbl>     <dbl>    <dbl>\n1        ATA Airlines d/b/a ATA -5.5679651 1240.7219 61.84391\n2       Northwest Airlines Inc. -3.1134556  779.1926 48.84979\n3                     Envoy Air -2.2056576  437.0883 54.54923\n4             PSA Airlines Inc. -1.9267647  500.6955 55.60335\n5  ExpressJet Airlines Inc. (1) -1.5886314  537.3077 61.58386\n6               JetBlue Airways -1.3742524 1087.2337 59.80750\n7         SkyWest Airlines Inc. -1.1265678  419.6489 54.04198\n8          Delta Air Lines Inc. -0.9829374  956.9576 50.19338\n9        American Airlines Inc. -0.9631200 1066.8396 56.78222\n10  AirTran Airways Corporation -0.9411572  665.6574 53.38363\n# ... with more rows"
  },
  {
    "objectID": "deployment/cloudera-aws.html#train-a-linear-model",
    "href": "deployment/cloudera-aws.html#train-a-linear-model",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Train a linear model",
    "text": "Train a linear model\nPredict time gained or lost in flight as a function of distance, departure delay, and airline carrier.\n\n# Partition the data into training and validation sets\nmodel_partition <- model_data %>% \n  sdf_partition(train = 0.8, valid = 0.2, seed = 5555)\n\n# Fit a linear model\nml1 <- model_partition$train %>%\n  ml_linear_regression(gain ~ distance + depdelay + uniquecarrier)\n\n# Summarize the linear model\nsummary(ml1)\n\nCall: ml_linear_regression(., gain ~ distance + depdelay + uniquecarrier)\n\nDeviance Residuals: (approximate):\n     Min       1Q   Median       3Q      Max \n-302.343   -5.669    2.714    9.832  104.130 \n\nCoefficients:\n                    Estimate  Std. Error  t value  Pr(>|t|)    \n(Intercept)      -1.26566581  0.10385870 -12.1864 < 2.2e-16 ***\ndistance          0.00308711  0.00002404 128.4155 < 2.2e-16 ***\ndepdelay         -0.01397013  0.00028816 -48.4812 < 2.2e-16 ***\nuniquecarrier_AA -2.18483090  0.10985406 -19.8885 < 2.2e-16 ***\nuniquecarrier_AQ  3.14330242  0.29114487  10.7964 < 2.2e-16 ***\nuniquecarrier_AS  0.09210380  0.12825003   0.7182 0.4726598    \nuniquecarrier_B6 -2.66988794  0.12682192 -21.0523 < 2.2e-16 ***\nuniquecarrier_CO -1.11611186  0.11795564  -9.4621 < 2.2e-16 ***\nuniquecarrier_DL -1.95206198  0.11431110 -17.0767 < 2.2e-16 ***\nuniquecarrier_EV  1.70420830  0.11337215  15.0320 < 2.2e-16 ***\nuniquecarrier_F9 -1.03178176  0.15384863  -6.7065 1.994e-11 ***\nuniquecarrier_FL -0.99574060  0.12034738  -8.2739 2.220e-16 ***\nuniquecarrier_HA -1.16970713  0.34894788  -3.3521 0.0008020 ***\nuniquecarrier_MQ -1.55569040  0.10975613 -14.1741 < 2.2e-16 ***\nuniquecarrier_NW -3.58502418  0.11534938 -31.0797 < 2.2e-16 ***\nuniquecarrier_OH -1.40654797  0.12034858 -11.6873 < 2.2e-16 ***\nuniquecarrier_OO -0.39069404  0.11132164  -3.5096 0.0004488 ***\nuniquecarrier_TZ -7.26285217  0.34428509 -21.0955 < 2.2e-16 ***\nuniquecarrier_UA -0.56995737  0.11186757  -5.0949 3.489e-07 ***\nuniquecarrier_US -0.52000028  0.11218498  -4.6352 3.566e-06 ***\nuniquecarrier_WN  4.22838982  0.10629405  39.7801 < 2.2e-16 ***\nuniquecarrier_XE -1.13836940  0.11332176 -10.0455 < 2.2e-16 ***\nuniquecarrier_YV  3.17149538  0.11709253  27.0854 < 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nR-Squared: 0.02301\nRoot Mean Squared Error: 17.83"
  },
  {
    "objectID": "deployment/cloudera-aws.html#assess-model-performance",
    "href": "deployment/cloudera-aws.html#assess-model-performance",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Assess model performance",
    "text": "Assess model performance\nCompare the model performance using the validation data.\n\n# Calculate average gains by predicted decile\nmodel_deciles <- lapply(model_partition, function(x) {\n  sdf_predict(ml1, x) %>%\n    mutate(decile = ntile(desc(prediction), 10)) %>%\n    group_by(decile) %>%\n    summarize(gain = mean(gain)) %>%\n    select(decile, gain) %>%\n    collect()\n})\n\n# Create a summary dataset for plotting\ndeciles <- rbind(\n  data.frame(data = 'train', model_deciles$train),\n  data.frame(data = 'valid', model_deciles$valid),\n  make.row.names = FALSE\n)\n\n# Plot average gains by predicted decile\ndeciles %>%\n  ggplot(aes(factor(decile), gain, fill = data)) +\n  geom_bar(stat = 'identity', position = 'dodge') +\n  labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes')"
  },
  {
    "objectID": "deployment/cloudera-aws.html#visualize-predictions",
    "href": "deployment/cloudera-aws.html#visualize-predictions",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Visualize predictions",
    "text": "Visualize predictions\nCompare actual gains to predicted gains for an out of time sample.\n\n# Select data from an out of time sample\ndata_2008 <- flights_tbl %>%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>%\n  filter(depdelay > 15 & depdelay < 240) %>%\n  filter(arrdelay > -60 & arrdelay < 360) %>%\n  filter(year == 2008) %>%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>%\n  mutate(gain = depdelay - arrdelay) %>%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest)\n\n# Summarize data by carrier\ncarrier <- sdf_predict(ml1, data_2008) %>%\n  group_by(description) %>%\n  summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>%\n  filter(freq > 10000) %>%\n  collect\n\n# Plot actual gains and predicted gains by airline carrier\nggplot(carrier, aes(gain, prediction)) + \n  geom_point(alpha = 0.75, color = 'red', shape = 3) +\n  geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') +\n  geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) +\n  labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted')\n\n\nSome carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights."
  },
  {
    "objectID": "deployment/cloudera-aws.html#build-dashboard",
    "href": "deployment/cloudera-aws.html#build-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Build dashboard",
    "text": "Build dashboard\nAggregate the scored data by origin, destination, and airline. Save the aggregated data.\n\n# Summarize by origin, destination, and carrier\nsummary_2008 <- sdf_predict(ml1, data_2008) %>%\n  rename(carrier = uniquecarrier, airline = description) %>%\n  group_by(origin, dest, carrier, airline) %>%\n  summarize(\n    flights = n(),\n    distance = mean(distance),\n    avg_dep_delay = mean(depdelay),\n    avg_arr_delay = mean(arrdelay),\n    avg_gain = mean(gain),\n    pred_gain = mean(prediction)\n    )\n\n# Collect and save objects\npred_data <- collect(summary_2008)\nairports <- collect(select(airports_tbl, name, faa, lat, lon))\nml1_summary <- capture.output(summary(ml1))\nsave(pred_data, airports, ml1_summary, file = 'flights_pred_2008.RData')"
  },
  {
    "objectID": "deployment/cloudera-aws.html#publish-dashboard",
    "href": "deployment/cloudera-aws.html#publish-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Publish dashboard",
    "text": "Publish dashboard\nUse the saved data to build an R Markdown flexdashboard. Publish the flexdashboard\n\n#Appendix\n\nAppendix A - Data files\nRun the following script to download data from the web onto your master node. Download the yearly flight data and the airlines lookup table.\n\n# Make download directory\nmkdir /tmp/flights\n\n# Download flight data by year\nfor i in {2006..2008}\n  do\n    echo \"$(date) $i Download\"\n    fnam=$i.csv.bz2\n    wget -O /tmp/flights/$fnam http://stat-computing.org/dataexpo/2009/$fnam\n    echo \"$(date) $i Unzip\"\n    bunzip2 /tmp/flights/$fnam\n  done\n\n# Download airline carrier data\nwget -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS\n\n# Download airports data\nwget -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\n\n\n\nHive tables\nWe used the Hue interface, logged in as ‘admin’ to load the data into HDFS and then into Hive.\nCREATE EXTERNAL TABLE IF NOT EXISTS flights\n(\nyear int,\nmonth int,\ndayofmonth int,\ndayofweek int,\ndeptime int,\ncrsdeptime int,\narrtime int, \ncrsarrtime int,\nuniquecarrier string,\nflightnum int,\ntailnum string, \nactualelapsedtime int,\ncrselapsedtime int,\nairtime string,\narrdelay int,\ndepdelay int, \norigin string,\ndest string,\ndistance int,\ntaxiin string,\ntaxiout string,\ncancelled int,\ncancellationcode string,\ndiverted int,\ncarrierdelay string,\nweatherdelay string,\nnasdelay string,\nsecuritydelay string,\nlateaircraftdelay string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLINES TERMINATED BY '\\n'\nTBLPROPERTIES(\"skip.header.line.count\"=\"1\");\nLOAD DATA INPATH '/user/admin/flights/2006.csv/' INTO TABLE flights;\nLOAD DATA INPATH '/user/admin/flights/2007.csv/' INTO TABLE flights;\nLOAD DATA INPATH '/user/admin/flights/2008.csv/' INTO TABLE flights;\n# Create metadata for airlines\nCREATE EXTERNAL TABLE IF NOT EXISTS airlines\n(\nCode string,\nDescription string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE\ntblproperties(\"skip.header.line.count\"=\"1\");\nLOAD DATA INPATH '/user/admin/L_UNIQUE_CARRIERS.csv' INTO TABLE airlines;\nCREATE EXTERNAL TABLE IF NOT EXISTS airports\n(\nid string,\nname string,\ncity string,\ncountry string,\nfaa string,\nicao string,\nlat double,\nlon double,\nalt int,\ntz_offset double,\ndst string,\ntz_name string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE;\nLOAD DATA INPATH '/user/admin/airports.dat' INTO TABLE airports;"
  },
  {
    "objectID": "deployment/data-lakes.html#audience",
    "href": "deployment/data-lakes.html#audience",
    "title": "Data Lakes",
    "section": "Audience",
    "text": "Audience\nThis article explains how to take advantage of Apache Spark at organizations that have a Hadoop based Big Data Lake."
  },
  {
    "objectID": "deployment/data-lakes.html#introduction",
    "href": "deployment/data-lakes.html#introduction",
    "title": "Data Lakes",
    "section": "Introduction",
    "text": "Introduction\nWe have noticed that the types of questions we field after a demo of sparklyr to our customers were more about high-level architecture than how the package works. To answer those questions, we put together a set of slides that illustrate and discuss important concepts, to help customers see where Spark, R, and sparklyr fit in a Big Data Platform implementation. In this article, we’ll review those slides and provide a narrative that will help you better envision how you can take advantage of our products."
  },
  {
    "objectID": "deployment/data-lakes.html#r-for-data-science",
    "href": "deployment/data-lakes.html#r-for-data-science",
    "title": "Data Lakes",
    "section": "R for Data Science",
    "text": "R for Data Science\nIt is very important to preface the Use Case review with some background information about where RStudio focuses its efforts when developing packages and products. Many vendors offer R integration, but in most cases, what this means is that they will add a model built in R to their pipeline or interface, and pass new inputs to that model to generate outputs that can be used in the next step in the pipeline, or in a calculation for the interface.\nIn contrast, our focus is on the process that happens before that: the discipline that produces the model, meaning Data Science.\n\nIn their R for Data Science book, Hadley Wickham and Garrett Grolemund provide a great diagram that nicely illustrates the Data Science process:\n\nWe import data into memory with R\nClean and tidy the data\nDive into a cyclical process called understand, which helps us to get to know our data, and hopefully find the answer to the question we started with. This cycle typically involves making transformations to our tidied data, using the transformed data to fit models, and visualizing results.\nOnce we find an answer to our question, we then communicate the results.\n\nData Scientists like using R because it allows them to complete a Data Science project from beginning to end inside the R environment, and in memory."
  },
  {
    "objectID": "deployment/data-lakes.html#hadoop-as-a-data-source",
    "href": "deployment/data-lakes.html#hadoop-as-a-data-source",
    "title": "Data Lakes",
    "section": "Hadoop as a Data Source",
    "text": "Hadoop as a Data Source\nWhat happens when the data that needs to be analyzed is very large, like the data sets found in a Hadoop cluster? It would be impossible to fit these in memory, so workarounds are normally used. Possible workarounds include using a comparatively minuscule data sample, or download as much data as possible. This becomes disruptive to Data Scientists because either the small sample may not be representative, or they have to wait a long time in every iteration of importing a lot of data, exploring a lot of data, and modeling a lot of data."
  },
  {
    "objectID": "deployment/data-lakes.html#spark-as-an-analysis-engine",
    "href": "deployment/data-lakes.html#spark-as-an-analysis-engine",
    "title": "Data Lakes",
    "section": "Spark as an Analysis Engine",
    "text": "Spark as an Analysis Engine\nWe noticed that a very important mental leap to make is to see Spark not just as a gateway to Hadoop (or worse, as an additional data source), but as a computing engine. As such, it is an excellent vehicle to scale our analytics. Spark has many capabilities that makes it ideal for Data Science in a data lake, such as close integration with Hadoop and Hive, the ability to cache data into memory across multiple nodes, data transformers, and its Machine Learning libraries.\nThe approach, then, is to push as much compute to the cluster as possible, using R primarily as an interface to Spark for the Data Scientist, which will then collect as few results as possible back into R memory, mostly to visualize and communicate. As shown in the slide, the more import, tidy, transform and modeling work we can push to Spark, the faster we can analyze very large data sets."
  },
  {
    "objectID": "deployment/data-lakes.html#cluster-setup",
    "href": "deployment/data-lakes.html#cluster-setup",
    "title": "Data Lakes",
    "section": "Cluster Setup",
    "text": "Cluster Setup\nHere is an illustration of how R, RStudio, and sparklyr can be added to the YARN managed cluster. The highlights are:\n\nR, RStudio, and sparklyr need to be installed on one node only, typically an edge node\nThe Data Scientist can access R, Spark, and the cluster via a web browser by navigating to the RStudio IDE inside the edge node"
  },
  {
    "objectID": "deployment/data-lakes.html#considerations",
    "href": "deployment/data-lakes.html#considerations",
    "title": "Data Lakes",
    "section": "Considerations",
    "text": "Considerations\nThere are some important considerations to keep in mind when combining your Data Lake and R for large scale analytics:\n\nSpark’s Machine Learning libraries may not contain specific models that a Data Scientist needs. For those cases, workarounds would include using a sparklyr extension such as H2O, or collecting a sample of the data into R memory for modeling.\nSpark does not have visualization functionality; currently, the best approach is to collect pre-calculated data into R for plotting. A good way to drastically reduce the number of rows being brought back into memory is to push as much computation as possible to Spark, and return just the results to be plotted. For example, the bins of a Histogram can be calculated in Spark, so that only the final bucket values would be returned to R for visualization.\nA particular use case may require a different way of scaling analytics. We have published an article that provides a very good overview of the options that are available: R for Enterprise: How to Scale Your Analytics Using R"
  },
  {
    "objectID": "deployment/data-lakes.html#r-for-data-science-toolchain-with-spark",
    "href": "deployment/data-lakes.html#r-for-data-science-toolchain-with-spark",
    "title": "Data Lakes",
    "section": "R for Data Science Toolchain with Spark",
    "text": "R for Data Science Toolchain with Spark\nWith sparklyr, the Data Scientist will be able to access the Data Lake’s data, and also gain an additional, very powerful understand layer via Spark. sparklyr, along with the RStudio IDE and the tidyverse packages, provides the Data Scientist with an excellent toolbox to analyze data, big and small."
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#overview",
    "href": "deployment/databricks-cluster-local.html#overview",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Overview",
    "text": "Overview\nIf the recommended path of connecting to Spark remotely with Databricks Connect does not apply to your use case, then you can install RStudio Workbench directly within a Databricks cluster as described in the sections below.\nWith this configuration, RStudio Workbench is installed on the Spark driver node and allows users to work locally with Spark using sparklyr.\n\nThis configuration can result in increased complexity, limited connectivity to other storage and compute resources, resource contention between RStudio Workbench and Databricks, and maintenance concerns due to the ephemeral nature of Databricks clusters.\nFor additional details, refer to the FAQ for RStudio in the Databricks Documentation."
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#advantages-and-limitations",
    "href": "deployment/databricks-cluster-local.html#advantages-and-limitations",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Advantages and limitations",
    "text": "Advantages and limitations\nAdvantages:\n\nAbility for users to connect sparklyr to Spark without configuring remote connectivity\nProvides a high-bandwidth connection between R and the Spark JVM processes because they are running on the same machine\nCan load data from the cluster directly into an R session since RStudio Workbench is installed within the Databricks cluster\n\nLimitations:\n\nIf the Databricks cluster is restarted or terminated, then the instance of RStudio Workbench will be terminated and its configuration will be lost\nIf users do not persist their code through version control or the Databricks File System, then you risk losing user’s work if the cluster is restarted or terminated\nRStudio Workbench (and other RStudio products) installed within a Databricks cluster will be limited to the compute resources and lifecycle of that particular Spark cluster\nNon-Spark jobs will use CPU and RAM resources within the Databricks cluster\nNeed to install one instance of RStudio Workbench per Spark cluster that you want to run jobs on"
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#requirements",
    "href": "deployment/databricks-cluster-local.html#requirements",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Requirements",
    "text": "Requirements\n\nA running Databricks cluster with a runtime version 4.1 or above\nThe cluster must not have “table access control” or “automatic termination” enabled\nYou must have “Can Attach To” permission for the Databricks cluster"
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#preparation",
    "href": "deployment/databricks-cluster-local.html#preparation",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Preparation",
    "text": "Preparation\nThe following steps walk through the process to install RStudio Workbench on the Spark driver node within your Databricks cluster.\nThe recommended method for installing RStudio Workbench to the Spark driver node is via SSH. However, an alternative method is available if you are not able to access the Spark driver node via SSH."
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#configure-ssh-access-to-the-spark-driver-node",
    "href": "deployment/databricks-cluster-local.html#configure-ssh-access-to-the-spark-driver-node",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Configure SSH access to the Spark driver node",
    "text": "Configure SSH access to the Spark driver node\nConfigure SSH access to the Spark driver node in Databricks by following the steps in the SSH access to clusters section of the Databricks Cluster configurations documentation.\nNote: If you are unable to configure SSH access or connect to the Spark driver node via SSH, then you can follow the steps in the Get started with RStudio Workbench section of the RStudio on Databricks documentation to install RStudio Workbench from a Databricks notebook, then skip to the access RStudio Workbench section of this documentation."
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#connect-to-the-spark-driver-node-via-ssh",
    "href": "deployment/databricks-cluster-local.html#connect-to-the-spark-driver-node-via-ssh",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Connect to the Spark driver node via SSH",
    "text": "Connect to the Spark driver node via SSH\nConnect to the Spark driver node via SSH on port 2200 by using the following command on your local machine:\nssh ubuntu@<spark-driver-node-address> -p 2200 -i <path-to-private-SSH-key>\nReplace <spark-driver-node-address> with the DNS name or IP address of the Spark driver node, and <path-to-private-SSH-key> with the path to your private SSH key on your local machine."
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#install-rstudio-workbench-on-the-spark-driver-node",
    "href": "deployment/databricks-cluster-local.html#install-rstudio-workbench-on-the-spark-driver-node",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Install RStudio Workbench on the Spark driver node",
    "text": "Install RStudio Workbench on the Spark driver node\nAfter you SSH into the Spark driver node, then you can follow the typical steps to install RStudio Workbench in the RStudio documentation. In the installation steps, you can select Ubuntu as the target Linux distribution."
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#configure-rstudio-workbench",
    "href": "deployment/databricks-cluster-local.html#configure-rstudio-workbench",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Configure RStudio Workbench",
    "text": "Configure RStudio Workbench\nThe following configuration steps are required to be able to use RStudio Workbench with Databricks.\nAdd the following configuration lines to /etc/rstudio/rserver.conf to use proxied authentication with Databricks and enable the administrator dashboard:\nauth-proxy=1\nauth-proxy-user-header-rewrite=^(.*)$ $1\nauth-proxy-sign-in-url=<domain>/login.html\nadmin-enabled=1\nAdd the following configuration line to /etc/rstudio/rsession-profile to set the PATH to be used with RStudio Workbench:\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:$PATH\nAdd the following configuration lines to /etc/rstudio/rsession.conf to configure sessions in RStudio Workbench to work with Databricks:\nsession-rprofile-on-resume-default=1\nallow-terminal-websockets=0\nRestart RStudio Workbench:\nsudo rstudio-server restart"
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#access-rstudio-workbench",
    "href": "deployment/databricks-cluster-local.html#access-rstudio-workbench",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Access RStudio Workbench",
    "text": "Access RStudio Workbench\nFrom the Databricks console, click on the Databricks cluster that you want to work with:\n\nFrom within the Databricks cluster, click on the Apps tab:\n\nClick on the Set up RStudio button:\n\nTo access RStudio Workbench, click on the link to Open RStudio:\n\nIf you configured proxied authentication in RStudio Workbench as described in the previous section, then you do not need to use the username or password that is displayed. Instead, RStudio Workbench will automatically login and start a new RStudio session as your logged-in Databricks user:\n\nOther users can access RStudio Workbench from the Databricks console by following the same steps described above. You do not need to create those users in RStudio Workbench or their home directory beforehand."
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#configure-sparklyr",
    "href": "deployment/databricks-cluster-local.html#configure-sparklyr",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Configure sparklyr",
    "text": "Configure sparklyr\nUse the following R code to establish a connection from sparklyr to the Databricks cluster:\nSparkR::sparkR.session()\nlibrary(sparklyr)\nsc <- spark_connect(method = \"databricks\")"
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#additional-information",
    "href": "deployment/databricks-cluster-local.html#additional-information",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Additional information",
    "text": "Additional information\nFor more information on using RStudio Workbench inside of Databricks, refer to the sections on RStudio on Databricks (AWS) or RStudio on Databricks (Azure) in the Databricks documentation."
  },
  {
    "objectID": "deployment/databricks-cluster-odbc.html#overview",
    "href": "deployment/databricks-cluster-odbc.html#overview",
    "title": "Using an ODBC connection with Databricks",
    "section": "Overview",
    "text": "Overview\nThis configuration details how to connect to Databricks using an ODBC connection. With this setup, R can connect to Databricks using the odbc and DBI R packages. This type of configuration is the recommended approach for connecting to Databricks from RStudio Connect and can also be used from RStudio Workbench."
  },
  {
    "objectID": "deployment/databricks-cluster-odbc.html#advantages-and-limitations",
    "href": "deployment/databricks-cluster-odbc.html#advantages-and-limitations",
    "title": "Using an ODBC connection with Databricks",
    "section": "Advantages and limitations",
    "text": "Advantages and limitations\nAdvantages:\n\nODBC connections tend to be more stable than Spark connections. This is especially beneficial for content published to RStudio Connect.\nIf code is developed using a Spark connection and sparklyr, it is easy to swap out the connection type for an ODBC connection and the remaining code will still run.\nThe Spark ODBC driver provided by Databricks was benchmarked against a native Spark connection and the performance of the two is very comparable.\n\nLimitations: - Not all Spark features and functions are available through an ODBC connection."
  },
  {
    "objectID": "deployment/databricks-cluster-odbc.html#driver-installation",
    "href": "deployment/databricks-cluster-odbc.html#driver-installation",
    "title": "Using an ODBC connection with Databricks",
    "section": "Driver installation",
    "text": "Driver installation\nDownload and install the Spark ODBC driver from Databricks"
  },
  {
    "objectID": "deployment/databricks-cluster-odbc.html#driver-configuration",
    "href": "deployment/databricks-cluster-odbc.html#driver-configuration",
    "title": "Using an ODBC connection with Databricks",
    "section": "Driver configuration",
    "text": "Driver configuration\nCreate a DSN for Databricks.\n[Databricks-Spark]\nDriver=Simba\nServer=<server-hostname>\nHOST=<server-hostname>\nPORT=<port>\nSparkServerType=3\nSchema=default\nThriftTransport=2\nSSL=1\nAuthMech=3\nUID=token\nPWD=<personal-access-token>\nHTTPPath=<http-path>"
  },
  {
    "objectID": "deployment/databricks-cluster-odbc.html#connect-to-databricks",
    "href": "deployment/databricks-cluster-odbc.html#connect-to-databricks",
    "title": "Using an ODBC connection with Databricks",
    "section": "Connect to Databricks",
    "text": "Connect to Databricks\nThe connection can be tested from the command line using isql -v Databricks-Spark where Databricks-Spark is the DSN name for the connection. If that connects successfully, then the following code can be used to create a connection from an R session:\nlibrary(DBI)\nlibrary(odbc)\n\ncon <- dbConnect(odbc(), \"Databricks-Spark\")"
  },
  {
    "objectID": "deployment/databricks-cluster-odbc.html#additional-information",
    "href": "deployment/databricks-cluster-odbc.html#additional-information",
    "title": "Using an ODBC connection with Databricks",
    "section": "Additional information",
    "text": "Additional information\nFor more information about ODBC connections from R, please visit db.rstudio.com."
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#overview",
    "href": "deployment/databricks-cluster-remote.html#overview",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Overview",
    "text": "Overview\nWith this configuration, RStudio Workbench is installed outside of the Spark cluster and allows users to connect to Spark remotely using sparklyr with Databricks Connect.\n\nThis is the recommended configuration because it targets separate environments, involves a typical configuration process, avoids resource contention, and allows RStudio Workbench to connect to Databricks as well as other remote storage and compute resources."
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#advantages-and-limitations",
    "href": "deployment/databricks-cluster-remote.html#advantages-and-limitations",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Advantages and limitations",
    "text": "Advantages and limitations\nAdvantages:\n\nRStudio Workbench will remain functional if Databricks clusters are terminated\nProvides the ability to communicate with one or more Databricks clusters as a remote compute resource\nAvoids resource contention between RStudio Workbench and Databricks\n\nLimitations:\n\nDatabricks Connect does not currently support the following APIs from sparklyr: Broom APIs, Streaming APIs, Broadcast APIs, Most MLlib APIs, csv_file serialization mode, and the spark_submit API\nDatabricks Connect does not support structured streaming\nDatabricks Connect does not support running arbitrary code that is not a part of a Spark job on the remote cluster\nDatabricks Connect does not support Scala, Python, and R APIs for Delta table operations\nDatabricks Connect does not support most utilities in Databricks Utilities. However, dbutils.fs and dbutils.secrets are supported\n\nFor more information on the limitations of Databricks Connect, refer to the Limitation section of the Databricks Connect documentation."
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#requirements",
    "href": "deployment/databricks-cluster-remote.html#requirements",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Requirements",
    "text": "Requirements\n\nRStudio Workbench installed outside of the Databricks cluster\nJava 8 installed on the machine with RStudio Workbench\nA running Databricks cluster with a runtime version 5.5 or above"
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#install-python",
    "href": "deployment/databricks-cluster-remote.html#install-python",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Install Python",
    "text": "Install Python\nThe Databricks Connect client is provided as a Python library. The minor version of your Python installation must be the same as the minor Python version of your Databricks cluster.\nRefer to the steps in the install Python section of the RStudio Documentation to install Python on the same server where RStudio Workbench is installed.\nNote that you can either install Python for all users in a global location (as an administrator) or in a home directory (as an end user)."
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#install-databricks-connect",
    "href": "deployment/databricks-cluster-remote.html#install-databricks-connect",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Install Databricks Connect",
    "text": "Install Databricks Connect\nRun the following command to install Databricks Connect on the server with RStudio Workbench:\npip install -U databricks-connect==6.3.*  # or a different version to match your Databricks cluster\nNote that you can either install this library for all users in a global Python environment (as an administrator) or for an individual user in their Python environment (e.g., using the pip --user option or installing into a conda environment or virtual environment)."
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#configure-databricks-connect",
    "href": "deployment/databricks-cluster-remote.html#configure-databricks-connect",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Configure Databricks Connect",
    "text": "Configure Databricks Connect\nTo configure the Databricks Connect client, you can run the following command in a terminal when logged in as a user in RStudio Workbench:\ndatabricks-connect configure\nIn the prompts that follow, enter the following information:\n\n\n\n\n\n\n\n\nParameter\nDescription\nExample Value\n\n\n\n\nDatabricks Host\nBase address of your Databricks console URL\nhttps://dbc-01234567-89ab.cloud.databricks.com\n\n\nDatabricks Token\nUser token generated from the Databricks Console under your “User Settings”\ndapi24g06bdd96f2700b09dd336d5444c1yz\n\n\nCluster ID\nCluster ID in the Databricks console under Advanced Options > Tags > ClusterId\n0308-033548-colt989\n\n\nOrg ID\nFound in the ?o=orgId portion of your Databricks Console URL\n8498623428173033\n\n\nPort\nThe port that Databricks Connect connects to\n15001\n\n\n\nAfter you’ve completed the configuration process for Databricks Connect, you can run the following command in a terminal to test the connectivity of Databricks Connect to your Databricks cluster:\ndatabricks-connect test"
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#install-sparklyr",
    "href": "deployment/databricks-cluster-remote.html#install-sparklyr",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Install sparklyr",
    "text": "Install sparklyr\nThe integration of sparklyr with Databricks Connect is currently being added to the development version of sparklyr. To use this functionality now, you’ll need to install the development version of sparklyr by running the following command in an R console:\ndevtools::install_github(\"sparklyr/sparklyr\")"
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#install-spark",
    "href": "deployment/databricks-cluster-remote.html#install-spark",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Install Spark",
    "text": "Install Spark\nTo work with a remote Databricks cluster, you need to have a local installation of Spark that matches the version of Spark on the Databricks Cluster.\nYou can install Spark by running the following command in an R console:\nlibrary(sparklyr)\nsparklyr::spark_install()"
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#use-sparklyr",
    "href": "deployment/databricks-cluster-remote.html#use-sparklyr",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Use sparklyr",
    "text": "Use sparklyr\nIn order to connect to Databricks using sparklyr and databricks-connect, SPARK_HOME must be set to the output of the databricks-connect get-spark-home command.\nYou can set SPARK_HOME as an environment variable or directly within spark_connect(). The following R code demonstrates connecting to Databricks, copying some data into the cluster, summarizing that data using sparklyr, and disconnecting:\nlibrary(sparklyr)\nlibrary(dplyr)\n\ndatabricks_connect_spark_home <- system(\"databricks-connect get-spark-home\", intern = TRUE)\nsc <- spark_connect(method = \"databricks\", spark_home = databricks_connect_spark_home)\n\ncars_tbl <- copy_to(sc, mtcars, overwrite = TRUE)\n\ncars_tbl %>% \n  group_by(cyl) %>% \n  summarise(\n    mean_mpg = mean(mpg, na.rm = TRUE),\n    mean_hp  = mean(hp, na.rm = TRUE)\n    )\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#additional-information",
    "href": "deployment/databricks-cluster-remote.html#additional-information",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Additional information",
    "text": "Additional information\nFor more information on the setup, configuration, troubleshooting, and limitations of Databricks Connect, refer to the Databricks Connect section of the Databricks documentation."
  },
  {
    "objectID": "deployment/databricks-cluster.html#overview",
    "href": "deployment/databricks-cluster.html#overview",
    "title": "Using sparklyr with Databricks",
    "section": "Overview",
    "text": "Overview\nThis documentation demonstrates how to use sparklyr with Apache Spark in Databricks along with RStudio Team, RStudio Workbench, RStudio Connect, and RStudio Package Manager."
  },
  {
    "objectID": "deployment/databricks-cluster.html#using-rstudio-team-with-databricks",
    "href": "deployment/databricks-cluster.html#using-rstudio-team-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Using RStudio Team with Databricks",
    "text": "Using RStudio Team with Databricks\nRStudio Team is a bundle of our popular professional software for developing data science projects, publishing data products, and managing packages.\nRStudio Team and sparklyr can be used with Databricks to work with large datasets and distributed computations with Apache Spark. The most common use case is to perform interactive analysis and exploratory development with RStudio Workbench and sparklyr; write out the results to a database, file system, or cloud storage; then publish apps, reports, and APIs to RStudio Connect that query and access the results.\n\nThe sections below describe best practices and different options for configuring specific RStudio products to work with Databricks."
  },
  {
    "objectID": "deployment/databricks-cluster.html#best-practices-for-working-with-databricks",
    "href": "deployment/databricks-cluster.html#best-practices-for-working-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Best practices for working with Databricks",
    "text": "Best practices for working with Databricks\n\nMaintain separate installation environments - Install RStudio Workbench, RStudio Connect, and RStudio Package Manager outside of the Databricks cluster so that they are not limited to the compute resources or ephemeral nature of Databricks clusters.\nConnect to Databricks remotely - Work with Databricks as a remote compute resource, similar to how you would connect remotely to external databases, data sources, and storage systems. This can be accomplished using Databricks Connect (as described in the Connecting to Databricks remotely section below) or by performing SQL queries with JDBC/ODBC using the Databricks Spark SQL Driver on AWS or Azure.\nRestrict workloads to interactive analysis - Only perform workloads related to exploratory or interactive analysis with Spark, then write the results to a database, file system, or cloud storage for more efficient retrieval in apps, reports, and APIs.\nLoad and query results efficiently - Because of the nature of Spark computations and the associated overhead, Shiny apps that use Spark on the backend tend to have performance and runtime issues; consider reading the results from a database, file system, or cloud storage instead."
  },
  {
    "objectID": "deployment/databricks-cluster.html#using-rstudio-workbench-with-databricks",
    "href": "deployment/databricks-cluster.html#using-rstudio-workbench-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Using RStudio Workbench with Databricks",
    "text": "Using RStudio Workbench with Databricks\nThere are two options for using sparklyr and RStudio Workbench with Databricks:\n\nOption 1: Connecting to Databricks remotely (Recommended Option)\nOption 2: Working inside of Databricks (Alternative Option)\n\n\nOption 1 - Connecting to Databricks remotely\nWith this configuration, RStudio Workbench is installed outside of the Spark cluster and allows users to connect to Spark remotely using sparklyr with Databricks Connect.\nThis is the recommended configuration because it targets separate environments, involves a typical configuration process, avoids resource contention, and allows RStudio Workbench to connect to Databricks as well as other remote storage and compute resources.\nFollow this article for steps on how to do this: Option 1 - Connecting to Databricks remotely\n\n\nOption 2 - Working inside of Databricks\nIf you cannot work with Spark remotely, you should install RStudio Workbench on the Driver node of a long-running, persistent Databricks cluster as opposed to a worker node or an ephemeral cluster.\nWith this configuration, RStudio Workbench is installed on the Spark driver node and allows users to connect to Spark locally using sparklyr.\nThis configuration can result in increased complexity, limited connectivity to other storage and compute resources, resource contention between RStudio Workbench and Databricks, and maintenance concerns due to the ephemeral nature of Databricks clusters.\nFollow this article for steps on how to do this: Option 2 - Working inside of Databricks"
  },
  {
    "objectID": "deployment/databricks-cluster.html#using-rstudio-connect-with-databricks",
    "href": "deployment/databricks-cluster.html#using-rstudio-connect-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Using RStudio Connect with Databricks",
    "text": "Using RStudio Connect with Databricks\nThe server environment within Databricks clusters is not permissive enough to support RStudio Connect or the process sandboxing mechanisms that it uses to isolate published content.\nTherefore, the only supported configuration is to install RStudio Connect outside of the Databricks cluster and connect to Databricks remotely.\nWhether RStudio Workbench is installed outside of the Databricks cluster (Recommended Option) or within the Databricks cluster (Alternative Option), you can publish content to RStudio Connect as long as HTTP/HTTPS network traffic is allowed from RStudio Workbench to RStudio Connect.\nThere are two options for using RStudio Connect with Databricks:\n\nPerforming SQL queries with ODBC using the Databricks Spark SQL Driver (Recommended Option).\nAdding calls in your R code to create and run Databricks jobs with bricksteR and the Databricks Jobs API (Alternative Option)\n\n[])(/images/deployment/databricks/rstudio-connect-databricks.png)"
  },
  {
    "objectID": "deployment/databricks-cluster.html#using-rstudio-package-manager-with-databricks",
    "href": "deployment/databricks-cluster.html#using-rstudio-package-manager-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Using RStudio Package Manager with Databricks",
    "text": "Using RStudio Package Manager with Databricks\nWhether RStudio Workbench is installed outside of the Databricks cluster (Recommended Option) or within the Databricks cluster (Alternative Option), you can install packages from repositories in RStudio Package Manager as long as HTTP/HTTPS network traffic is allowed from RStudio Workbench to RStudio Package Manager."
  },
  {
    "objectID": "deployment/index.html",
    "href": "deployment/index.html",
    "title": "Deployment",
    "section": "",
    "text": "YARN (Hadoop)\n\nUnderstanding Data Lakes\nSetting up an AWS EMR Cluster\nSetting up a Cloudera cluster in AWS\n\n\n\nStand Alone\n\nQubole cluster\nSetting up a Standalone Cluster in AWS EC2\nDatabricks cluster\n\nConnecting remotely\n\nWorking within the cluster\n\nConnecting via ODBC"
  },
  {
    "objectID": "deployment/qubole-cluster.html#overview",
    "href": "deployment/qubole-cluster.html#overview",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Overview",
    "text": "Overview\nQubole users can request access to RStudio Server Pro. This allows users to use sparklyr to interact directly with Spark from within the Qubole cluster."
  },
  {
    "objectID": "deployment/qubole-cluster.html#advantages-and-limitations",
    "href": "deployment/qubole-cluster.html#advantages-and-limitations",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Advantages and limitations",
    "text": "Advantages and limitations\nAdvantages:\n\nAbility for users to connect sparklyr directly to Spark within Qubole\nProvides a high-bandwidth connection between R and the Spark JVM processes because they are running on the same machine\nCan load data from the cluster directly into an R session since RStudio Workbench is installed within the Qubole cluster\nA unique, persistent home directory for each user\n\nLimitations:\n\nPersistent packages must be managed using Qubole Environments, not directly from within RStudio\nRStudio Workbench installed within a Qubole cluster will be limited to the compute resources and lifecycle of that particular Spark cluster\nNon-Spark jobs will use CPU and RAM resources within the Qubole cluster"
  },
  {
    "objectID": "deployment/qubole-cluster.html#access-rstudio-workbench",
    "href": "deployment/qubole-cluster.html#access-rstudio-workbench",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Access RStudio Workbench",
    "text": "Access RStudio Workbench\nRStudio Workbench can be accessed from the cluster resources menu:"
  },
  {
    "objectID": "deployment/qubole-cluster.html#use-sparklyr",
    "href": "deployment/qubole-cluster.html#use-sparklyr",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Use sparklyr",
    "text": "Use sparklyr\nUse the following R code to establish a connection from sparklyr to the Qubole cluster:\nlibrary(sparklyr)\nsc <- spark_connect(method = \"qubole\")"
  },
  {
    "objectID": "deployment/qubole-cluster.html#additional-information",
    "href": "deployment/qubole-cluster.html#additional-information",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Additional information",
    "text": "Additional information\nFor more information on using RStudio Workbench inside of Qubole, refer to the Qubole documentation."
  },
  {
    "objectID": "deployment/qubole-overview.html#overview",
    "href": "deployment/qubole-overview.html#overview",
    "title": "Using sparklyr with Qubole",
    "section": "Overview",
    "text": "Overview\nThis documentation demonstrates how to use sparklyr with Apache Spark in Qubole along with RStudio Server Pro and RStudio Connect."
  },
  {
    "objectID": "deployment/qubole-overview.html#best-practices-for-working-with-qubole",
    "href": "deployment/qubole-overview.html#best-practices-for-working-with-qubole",
    "title": "Using sparklyr with Qubole",
    "section": "Best practices for working with Qubole",
    "text": "Best practices for working with Qubole\n\nManage packages via Qubole Environments - Packages installed via install.packages() are not available on cluster restart. Packages managed through Qubole Environments are persistent.\nRestrict workloads to interactive analysis - Only perform workloads related to exploratory or interactive analysis with Spark, then write the results to a database, file system, or cloud storage for more efficient retrieval in apps, reports, and APIs.\nLoad and query results efficiently - Because of the nature of Spark computations and the associated overhead, Shiny apps that use Spark on the backend tend to have performance and runtime issues; consider reading the results from a database, file system, or cloud storage instead."
  },
  {
    "objectID": "deployment/qubole-overview.html#using-rstudio-workbench-with-qubole",
    "href": "deployment/qubole-overview.html#using-rstudio-workbench-with-qubole",
    "title": "Using sparklyr with Qubole",
    "section": "Using RStudio Workbench with Qubole",
    "text": "Using RStudio Workbench with Qubole\nThe Qubole platform includes RStudio Workbench. More details about how to request RStudio Workbench and access it from within a Qubole cluster are available from Qubole.\nTo steps for running RStudio Workbench inside Qubole go to the follow article: Qubole cluster"
  },
  {
    "objectID": "deployment/qubole-overview.html#using-rstudio-connect-with-qubole",
    "href": "deployment/qubole-overview.html#using-rstudio-connect-with-qubole",
    "title": "Using sparklyr with Qubole",
    "section": "Using RStudio Connect with Qubole",
    "text": "Using RStudio Connect with Qubole\nThe best configuration for working with Qubole and RStudio Connect is to install RStudio Connect outside of the Qubole cluster and connect to Qubole remotely. This is accomplished using the Qubole ODBC Driver."
  },
  {
    "objectID": "deployment/stand-alone-aws.html#overview",
    "href": "deployment/stand-alone-aws.html#overview",
    "title": "Spark Standalone Deployment in AWS",
    "section": "Overview",
    "text": "Overview\nThe plan is to launch 4 identical EC2 server instances. One server will be the Master node and the other 3 the worker nodes. In one of the worker nodes, we will install RStudio server.\nWhat makes a server the Master node is only the fact that it is running the master service, while the other machines are running the slave service and are pointed to that first master. This simple setup, allows us to install the same Spark components on all 4 servers and then just add RStudio to one of them.\nThe topology will look something like this:"
  },
  {
    "objectID": "deployment/stand-alone-aws.html#aws-ec-instances",
    "href": "deployment/stand-alone-aws.html#aws-ec-instances",
    "title": "Spark Standalone Deployment in AWS",
    "section": "AWS EC Instances",
    "text": "AWS EC Instances\nHere are the details of the EC2 instance, just deploy one at this point:\n\nType: t2.medium\nOS: Ubuntu 16.04 LTS\nDisk space: At least 20GB\nSecurity group: Open the following ports: 8080 (Spark UI), 4040 (Spark Worker UI), 8088 (sparklyr UI) and 8787 (RStudio). Also open All TCP ports for the machines inside the security group."
  },
  {
    "objectID": "deployment/stand-alone-aws.html#spark",
    "href": "deployment/stand-alone-aws.html#spark",
    "title": "Spark Standalone Deployment in AWS",
    "section": "Spark",
    "text": "Spark\nPerform the steps in this section on all of the servers that will be part of the cluster.\n\nInstall Java 8\n\nWe will add the Java 8 repository, install it and set it as default\n\n\nsudo apt-add-repository ppa:webupd8team/java\nsudo apt-get update\nsudo apt-get install oracle-java8-installer\nsudo apt-get install oracle-java8-set-default\nsudo apt-get update\nor alternatively, run\nsudo apt install openjdk-8-jdk\nto install Open JDK version 8.\n\n\nDownload Spark\n\nDownload and unpack a pre-compiled version of Spark. Here’s is the link to the official Spark download page\n\n\nwget http://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-hadoop2.7.tgz\ntar -xvzf spark-2.1.0-bin-hadoop2.7.tgz\ncd spark-2.1.0-bin-hadoop2.7\n\n\nCreate and launch AMI\n\nWe will create an image of the server. In Amazon, these are called AMIs, for information please see the User Guide.\nLaunch 3 instances of the AMI"
  },
  {
    "objectID": "deployment/stand-alone-aws.html#rstudio-server",
    "href": "deployment/stand-alone-aws.html#rstudio-server",
    "title": "Spark Standalone Deployment in AWS",
    "section": "RStudio Server",
    "text": "RStudio Server\nSelect one of the nodes to execute this section. Please check the RStudio download page for the latest version\n\nInstall R\n\nIn order to get the latest R core, we will need to update the source list in Ubuntu.\n\n\nsudo sh -c 'echo \"deb http://cran.rstudio.com/bin/linux/ubuntu xenial/\" >> /etc/apt/sources.list'\ngpg --keyserver keyserver.ubuntu.com --recv-key 0x517166190x51716619e084dab9\ngpg -a --export 0x517166190x51716619e084dab9 | sudo apt-key add -\nsudo apt-get update\n\nNow we can install R\n\n\nsudo apt-get install r-base\nsudo apt-get install gdebi-core\n\n\nInstall RStudio\n\nWe will download and install 1.044 of RStudio Server. To find the latest version, please visit the RStudio website. In order to get the enhanced integration with Spark, RStudio version 1.044 or later will be needed.\n\n\nwget https://download2.rstudio.org/rstudio-server-1.0.153-amd64.deb\nsudo gdebi rstudio-server-1.0.153-amd64.deb\n\n\nInstall dependencies\n\nRun the following commands\n\n\nsudo apt-get -y install libcurl4-gnutls-dev\nsudo apt-get -y install libssl-dev\nsudo apt-get -y install libxml2-dev\n\n\nAdd default user\n\nRun the following command to add a default user\n\n\nsudo adduser rstudio-user\n\n\nStart the Master node\n\nSelect one of the servers to become your Master node\nRun the command that starts the master service\n\n\nsudo spark-2.1.0-bin-hadoop2.7/sbin/start-master.sh\n\nClose the terminal connection (optional)\n\n\n\nStart Worker nodes\n\nStart the “slave” service. Important: Use dots not dashes as separators for the Spark Master node’s address\n\n\nsudo spark-2.1.0-bin-hadoop2.7/sbin/start-slave.sh spark://[Master node's IP address]:7077\nsudo spark-2.1.0-bin-hadoop2.7/sbin/start-slave.sh spark://ip-172-30-1-94.us-west-2.compute.internal:7077\n\nClose the terminal connection (optional)\n\n\n\nPre-load pacakges\n\nLog into RStudio (port 8787)\nUse ‘rstudio-user’\n\ninstall.packages(\"sparklyr\")\n\n\nConnect to the Spark Master\n\nNavigate to the Spark Master’s UI, typically on port 8080\n\n\n\nNote the Spark Master URL\nLogon to RStudio\nRun the following code\n\nlibrary(sparklyr)\n\nconf <- spark_config()\nconf$spark.executor.memory <- \"2GB\"\nconf$spark.memory.fraction <- 0.9\n\nsc <- spark_connect(master=\"[Spark Master URL]\",\n              version = \"2.1.0\",\n              config = conf,\n              spark_home = \"/home/ubuntu/spark-2.1.0-bin-hadoop2.7/\"\n              )"
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#set-up-the-cluster",
    "href": "deployment/yarn-cluster-emr.html#set-up-the-cluster",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Set up the cluster",
    "text": "Set up the cluster\nThis demonstration uses Amazon Web Services (AWS), but it could just as easily use Microsoft, Google, or any other provider. We will use Elastic Map Reduce (EMR) to easily set up a cluster with two core nodes and one master node. Nodes use virtual servers from the Elastic Compute Cloud (EC2). Note: There is no free tier for EMR, charges will apply.\nBefore beginning this setup we assume you have:\n\nFamiliarity with and access to an AWS account\nFamiliarity with basic linux commands\nSudo privileges in order to install software from the command line\n\n\n\nBuild an EMR cluster\nBefore beginning the EMR wizard setup, make sure you create the following in AWS:\n\nAn AWS key pair (.pem key) so you can SSH into the EC2 master node\nA security group that gives you access to port 22 on your IP and port 8787 from anywhere\n\n\n\n\nStep 1: Select software\nMake sure to select Hive and Spark as part of the install. Note that by choosing Spark, R will also be installed on the master node as part of the distribution.\n\n\n\n\nStep 2: Select hardware\nInstall 2 core nodes and one master node with m3.xlarge 80 GiB storage per node. You can easily increase the number of nodes later.\n\n\n\n\nStep 3: Select general cluster settings\nClick next on the general cluster settings.\n\n\n\n\nStep 4: Select security\nEnter your EC2 key pair and security group. Make sure the security group has ports 22 and 8787 open.\n\n\n\n\n\nConnect to EMR\nThe cluster page will give you details about your EMR cluster and instructions on connecting.\n\nConnect to the master node via SSH using your key pair. Once you connect you will see the EMR welcome.\n\n# Log in to master node\nssh -i ~/spark-demo.pem hadoop@ec2-52-10-102-11.us-west-2.compute.amazonaws.com\n\n\n\n\nInstall RStudio Server\nEMR uses Amazon Linux which is based on Centos. Update your master node and install dependencies that will be used by R packages.\n\n# Update\nsudo yum update\nsudo yum install libcurl-devel openssl-devel # used for devtools\n\nThe installation of RStudio Server is easy. Download the preview version of RStudio and install on the master node.\n\n# Install RStudio Server\nwget -P /tmp https://s3.amazonaws.com/rstudio-dailybuilds/rstudio-server-rhel-0.99.1266-x86_64.rpm\nsudo yum install --nogpgcheck /tmp/rstudio-server-rhel-0.99.1266-x86_64.rpm\n\n\n\nCreate a User\nCreate a user called rstudio-user that will perform the data analysis. Create a user directory for rstudio-user on HDFS with the hadoop fs command.\n\n# Make User\nsudo useradd -m rstudio-user\nsudo passwd rstudio-user\n\n# Create new directory in hdfs\nhadoop fs -mkdir /user/rstudio-user\nhadoop fs -chmod 777 /user/rstudio-user"
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#download-flights-data",
    "href": "deployment/yarn-cluster-emr.html#download-flights-data",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Download flights data",
    "text": "Download flights data\nThe flights data is a well known data source representing 123 million flights over 22 years. It consumes roughly 12 GiB of storage in uncompressed CSV format in yearly files.\n\nSwitch User\nFor data loading and analysis, make sure you are logged in as regular user.\n\n# create directories on hdfs for new user\nhadoop fs -mkdir /user/rstudio-user\nhadoop fs -chmod 777 /user/rstudio-user\n\n# switch user\nsu rstudio-user\n\n\n\nDownload data\nRun the following script to download data from the web onto your master node. Download the yearly flight data and the airlines lookup table.\n\n# Make download directory\nmkdir /tmp/flights\n\n# Download flight data by year\nfor i in {1987..2008}\n  do\n    echo \"$(date) $i Download\"\n    fnam=$i.csv.bz2\n    wget -O /tmp/flights/$fnam http://stat-computing.org/dataexpo/2009/$fnam\n    echo \"$(date) $i Unzip\"\n    bunzip2 /tmp/flights/$fnam\n  done\n\n# Download airline carrier data\nwget -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS\n\n# Download airports data\nwget -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\n\n\n\nDistribute into HDFS\nCopy data into HDFS using the hadoop fs command.\n\n# Copy flight data to HDFS\nhadoop fs -mkdir /user/rstudio-user/flights/\nhadoop fs -put /tmp/flights /user/rstudio-user/\n\n# Copy airline data to HDFS\nhadoop fs -mkdir /user/rstudio-user/airlines/\nhadoop fs -put /tmp/airlines.csv /user/rstudio-user/airlines\n\n# Copy airport data to HDFS\nhadoop fs -mkdir /user/rstudio-user/airports/\nhadoop fs -put /tmp/airports.csv /user/rstudio-user/airports"
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#create-hive-tables",
    "href": "deployment/yarn-cluster-emr.html#create-hive-tables",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Create Hive tables",
    "text": "Create Hive tables\nLaunch Hive from the command line.\n\n# Open Hive prompt\nhive\n\nCreate the metadata that will structure the flights table. Load data into the Hive table.\n# Create metadata for flights\nCREATE EXTERNAL TABLE IF NOT EXISTS flights\n(\nyear int,\nmonth int,\ndayofmonth int,\ndayofweek int,\ndeptime int,\ncrsdeptime int,\narrtime int, \ncrsarrtime int,\nuniquecarrier string,\nflightnum int,\ntailnum string, \nactualelapsedtime int,\ncrselapsedtime int,\nairtime string,\narrdelay int,\ndepdelay int, \norigin string,\ndest string,\ndistance int,\ntaxiin string,\ntaxiout string,\ncancelled int,\ncancellationcode string,\ndiverted int,\ncarrierdelay string,\nweatherdelay string,\nnasdelay string,\nsecuritydelay string,\nlateaircraftdelay string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLINES TERMINATED BY '\\n'\nTBLPROPERTIES(\"skip.header.line.count\"=\"1\");\n\n# Load data into table\nLOAD DATA INPATH '/user/rstudio-user/flights' INTO TABLE flights;\nCreate the metadata that will structure the airlines table. Load data into the Hive table.\n# Create metadata for airlines\nCREATE EXTERNAL TABLE IF NOT EXISTS airlines\n(\nCode string,\nDescription string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE\ntblproperties(\"skip.header.line.count\"=\"1\");\n\n# Load data into table\nLOAD DATA INPATH '/user/rstudio-user/airlines' INTO TABLE airlines;\nCreate the metadata that will structure the airports table. Load data into the Hive table.\n# Create metadata for airports\nCREATE EXTERNAL TABLE IF NOT EXISTS airports\n(\nid string,\nname string,\ncity string,\ncountry string,\nfaa string,\nicao string,\nlat double,\nlon double,\nalt int,\ntz_offset double,\ndst string,\ntz_name string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE;\n\n# Load data into table\nLOAD DATA INPATH '/user/rstudio-user/airports' INTO TABLE airports;"
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#connect-to-spark",
    "href": "deployment/yarn-cluster-emr.html#connect-to-spark",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Connect to Spark",
    "text": "Connect to Spark\nLog in to RStudio Server by pointing a browser at your master node IP:8787.\n\nSet the environment variable SPARK_HOME and then run spark_connect. After connecting you will be able to browse the Hive metadata in the RStudio Server Spark pane.\n\n# Connect to Spark\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nSys.setenv(SPARK_HOME=\"/usr/lib/spark\")\nconfig <- spark_config()\nsc <- spark_connect(master = \"yarn-client\", config = config, version = '1.6.2')\n\nOnce you are connected, you will see the Spark pane appear along with your hive tables.\n\nYou can inspect your tables by clicking on the data icon."
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#cache-the-tables-into-memory",
    "href": "deployment/yarn-cluster-emr.html#cache-the-tables-into-memory",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Cache the tables into memory",
    "text": "Cache the tables into memory\nUse tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame.\n\n# Cache flights Hive table into Spark\ntbl_cache(sc, 'flights')\nflights_tbl <- tbl(sc, 'flights')\n\n# Cache airlines Hive table into Spark\ntbl_cache(sc, 'airlines')\nairlines_tbl <- tbl(sc, 'airlines')\n\n# Cache airports Hive table into Spark\ntbl_cache(sc, 'airports')\nairports_tbl <- tbl(sc, 'airports')"
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#create-a-model-data-set",
    "href": "deployment/yarn-cluster-emr.html#create-a-model-data-set",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Create a model data set",
    "text": "Create a model data set\nFilter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight.\n\n# Filter records and create target variable 'gain'\nmodel_data <- flights_tbl %>%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>%\n  filter(depdelay > 15 & depdelay < 240) %>%\n  filter(arrdelay > -60 & arrdelay < 360) %>%\n  filter(year >= 2003 & year <= 2007) %>%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>%\n  mutate(gain = depdelay - arrdelay) %>%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain)\n\n# Summarize data by carrier\nmodel_data %>%\n  group_by(uniquecarrier) %>%\n  summarize(description = min(description), gain=mean(gain), \n            distance=mean(distance), depdelay=mean(depdelay)) %>%\n  select(description, gain, distance, depdelay) %>%\n  arrange(gain)\n\nSource:   query [?? x 4]\nDatabase: spark connection master=yarn-client app=sparklyr local=FALSE\n\n                    description       gain  distance depdelay\n                          <chr>      <dbl>     <dbl>    <dbl>\n1        ATA Airlines d/b/a ATA -3.3480120 1134.7084 56.06583\n2  ExpressJet Airlines Inc. (1) -3.0326180  519.7125 59.41659\n3                     Envoy Air -2.5434415  416.3716 53.12529\n4       Northwest Airlines Inc. -2.2030586  779.2342 48.52828\n5          Delta Air Lines Inc. -1.8248026  868.3997 50.77174\n6   AirTran Airways Corporation -1.4331555  641.8318 54.96702\n7    Continental Air Lines Inc. -0.9617003 1116.6668 57.00553\n8        American Airlines Inc. -0.8860262 1074.4388 55.45045\n9             Endeavor Air Inc. -0.6392733  467.1951 58.47395\n10              JetBlue Airways -0.3262134 1139.0443 54.06156\n# ... with more rows"
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#train-a-linear-model",
    "href": "deployment/yarn-cluster-emr.html#train-a-linear-model",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Train a linear model",
    "text": "Train a linear model\nPredict time gained or lost in flight as a function of distance, departure delay, and airline carrier.\n\n# Partition the data into training and validation sets\nmodel_partition <- model_data %>% \n  sdf_partition(train = 0.8, valid = 0.2, seed = 5555)\n\n# Fit a linear model\nml1 <- model_partition$train %>%\n  ml_linear_regression(gain ~ distance + depdelay + uniquecarrier)\n\n# Summarize the linear model\nsummary(ml1)\n\nDeviance Residuals: (approximate):\n     Min       1Q   Median       3Q      Max \n-305.422   -5.593    2.699    9.750  147.871 \n\nCoefficients:\n                    Estimate  Std. Error  t value  Pr(>|t|)    \n(Intercept)      -1.24342576  0.10248281 -12.1330 < 2.2e-16 ***\ndistance          0.00326600  0.00001670 195.5709 < 2.2e-16 ***\ndepdelay         -0.01466233  0.00020337 -72.0977 < 2.2e-16 ***\nuniquecarrier_AA -2.32650517  0.10522524 -22.1098 < 2.2e-16 ***\nuniquecarrier_AQ  2.98773637  0.28798507  10.3746 < 2.2e-16 ***\nuniquecarrier_AS  0.92054894  0.11298561   8.1475 4.441e-16 ***\nuniquecarrier_B6 -1.95784698  0.11728289 -16.6934 < 2.2e-16 ***\nuniquecarrier_CO -2.52618081  0.11006631 -22.9514 < 2.2e-16 ***\nuniquecarrier_DH  2.23287189  0.11608798  19.2343 < 2.2e-16 ***\nuniquecarrier_DL -2.68848119  0.10621977 -25.3106 < 2.2e-16 ***\nuniquecarrier_EV  1.93484736  0.10724290  18.0417 < 2.2e-16 ***\nuniquecarrier_F9 -0.89788137  0.14422281  -6.2257 4.796e-10 ***\nuniquecarrier_FL -1.46706706  0.11085354 -13.2343 < 2.2e-16 ***\nuniquecarrier_HA -0.14506644  0.25031456  -0.5795    0.5622    \nuniquecarrier_HP  2.09354855  0.12337515  16.9690 < 2.2e-16 ***\nuniquecarrier_MQ -1.88297535  0.10550507 -17.8473 < 2.2e-16 ***\nuniquecarrier_NW -2.79538927  0.10752182 -25.9983 < 2.2e-16 ***\nuniquecarrier_OH  0.83520117  0.11032997   7.5700 3.730e-14 ***\nuniquecarrier_OO  0.61993842  0.10679884   5.8047 6.447e-09 ***\nuniquecarrier_TZ -4.99830389  0.15912629 -31.4109 < 2.2e-16 ***\nuniquecarrier_UA -0.68294396  0.10638099  -6.4198 1.365e-10 ***\nuniquecarrier_US -0.61589284  0.10669583  -5.7724 7.815e-09 ***\nuniquecarrier_WN  3.86386059  0.10362275  37.2878 < 2.2e-16 ***\nuniquecarrier_XE -2.59658123  0.10775736 -24.0966 < 2.2e-16 ***\nuniquecarrier_YV  3.11113140  0.11659679  26.6828 < 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nR-Squared: 0.02385\nRoot Mean Squared Error: 17.74"
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#assess-model-performance",
    "href": "deployment/yarn-cluster-emr.html#assess-model-performance",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Assess model performance",
    "text": "Assess model performance\nCompare the model performance using the validation data.\n\n# Calculate average gains by predicted decile\nmodel_deciles <- lapply(model_partition, function(x) {\n  sdf_predict(ml1, x) %>%\n    mutate(decile = ntile(desc(prediction), 10)) %>%\n    group_by(decile) %>%\n    summarize(gain = mean(gain)) %>%\n    select(decile, gain) %>%\n    collect()\n})\n\n# Create a summary dataset for plotting\ndeciles <- rbind(\n  data.frame(data = 'train', model_deciles$train),\n  data.frame(data = 'valid', model_deciles$valid),\n  make.row.names = FALSE\n)\n\n# Plot average gains by predicted decile\ndeciles %>%\n  ggplot(aes(factor(decile), gain, fill = data)) +\n  geom_bar(stat = 'identity', position = 'dodge') +\n  labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes')"
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#visualize-predictions",
    "href": "deployment/yarn-cluster-emr.html#visualize-predictions",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Visualize predictions",
    "text": "Visualize predictions\nCompare actual gains to predicted gains for an out of time sample.\n\n# Select data from an out of time sample\ndata_2008 <- flights_tbl %>%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>%\n  filter(depdelay > 15 & depdelay < 240) %>%\n  filter(arrdelay > -60 & arrdelay < 360) %>%\n  filter(year == 2008) %>%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>%\n  mutate(gain = depdelay - arrdelay) %>%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest)\n\n# Summarize data by carrier\ncarrier <- sdf_predict(ml1, data_2008) %>%\n  group_by(description) %>%\n  summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>%\n  filter(freq > 10000) %>%\n  collect\n\n# Plot actual gains and predicted gains by airline carrier\nggplot(carrier, aes(gain, prediction)) + \n  geom_point(alpha = 0.75, color = 'red', shape = 3) +\n  geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') +\n  geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) +\n  labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted')\n\n\nSome carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights."
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#build-dashboard",
    "href": "deployment/yarn-cluster-emr.html#build-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Build dashboard",
    "text": "Build dashboard\nAggregate the scored data by origin, destination, and airline. Save the aggregated data.\n\n# Summarize by origin, destination, and carrier\nsummary_2008 <- sdf_predict(ml1, data_2008) %>%\n  rename(carrier = uniquecarrier, airline = description) %>%\n  group_by(origin, dest, carrier, airline) %>%\n  summarize(\n    flights = n(),\n    distance = mean(distance),\n    avg_dep_delay = mean(depdelay),\n    avg_arr_delay = mean(arrdelay),\n    avg_gain = mean(gain),\n    pred_gain = mean(prediction)\n    )\n\n# Collect and save objects\npred_data <- collect(summary_2008)\nairports <- collect(select(airports_tbl, name, faa, lat, lon))\nml1_summary <- capture.output(summary(ml1))\nsave(pred_data, airports, ml1_summary, file = 'flights_pred_2008.RData')"
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#publish-dashboard",
    "href": "deployment/yarn-cluster-emr.html#publish-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Publish dashboard",
    "text": "Publish dashboard\nUse the saved data to build an R Markdown flexdashboard. Publish the flexdashboard to Shiny Server, Shinyapps.io or RStudio Connect."
  },
  {
    "objectID": "get-started/index.html#install-the-package",
    "href": "get-started/index.html#install-the-package",
    "title": "Install",
    "section": "Install the package",
    "text": "Install the package\nYou can install the sparklyr package from CRAN as follows:\n\ninstall.packages(\"sparklyr\")"
  },
  {
    "objectID": "get-started/index.html#install-spark-locally",
    "href": "get-started/index.html#install-spark-locally",
    "title": "Install",
    "section": "Install Spark locally",
    "text": "Install Spark locally\n\n\n\n\n\n\nCaution\n\n\n\nThe steps in this section are only needed if you need to run Spark in your computer. If you already have a running Spark cluster that you will use to learn sparklyr, then skip this section.\n\n\nThis section is meant for developers new to sparklyr. You will need a running Spark environment to connect to. sparklyr can install Spark in your computer. The installed Spark environment is meant for learning and prototyping purposes. The installation will work on all the major Operating Systems that R works on, including Linux, MacOS, and Windows.\n\nlibrary(sparklyr)\n\nspark_install()\n\nPlease be aware that after installation, Spark is not running. The next section will explain how to start a single node Spark cluster in your machine."
  },
  {
    "objectID": "get-started/index.html#connect-to-spark",
    "href": "get-started/index.html#connect-to-spark",
    "title": "Install",
    "section": "Connect to Spark",
    "text": "Connect to Spark\nYou can use spark_connect() to connect to Spark clusters. The arguments passed to this functions depend on the type of Spark cluster you are connecting to. There are several different types of Spark clusters, such as YARN, Stand Alone and Kubernetes.\nspark_connect() is able to both start, and connect to, the single node Spark cluster in your machine. In order to do that, pass “local” as the argument for master:\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nThe sc variable now contains all of the connection information needed to interact with the cluster.\nTo learn how to connect to other types of Spark clusters, see the Deployment section of this site."
  },
  {
    "objectID": "get-started/index.html#disconnect-from-spark",
    "href": "get-started/index.html#disconnect-from-spark",
    "title": "Install",
    "section": "Disconnect from Spark",
    "text": "Disconnect from Spark\nFor “local” connection, spark_disconnect() will shut down the single node Spark environment in your machine, and tell R that the connection is no longer valid. For other types of Spark clusters, spark_disconnect() will only end the Spark session, it will not shut down the Spark cluster itself.\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "get-started/index.html#clusters",
    "href": "get-started/index.html#clusters",
    "title": "Install",
    "section": "Clusters",
    "text": "Clusters\nHere are some examples of how to use spark_connect() to connect to different types of Spark clusters:\nHadoop YARN:\nsc <- spark_connect(master = \"yarn\")\nMesos:\nsc <- spark_connect(master = \"mesos://host:port\")\nKubernetes:\nsc <- spark_connect(master = \"k8s://https://server\")\nApache Livy:\nsc <- spark_connect(master = \"http://server/livy\", method = \"livy\")\nStand Alone:\nsc <- spark_connect(master = \"spark://master-url:7077\")\nQubole: (for more info visit the Qubole page on this site)\nsc <- spark_connect(method = \"qubole\")\nDatabricks - Visit the Databricks page on this site to review the connection options"
  },
  {
    "objectID": "get-started/model-data.html#exercise",
    "href": "get-started/model-data.html#exercise",
    "title": "Model Data",
    "section": "Exercise",
    "text": "Exercise\nHere’s an example where we use ml_linear_regression() to fit a linear regression model. We’ll use the built-in mtcars dataset, and see if we can predict a car’s fuel consumption (mpg) based on its weight (wt), and the number of cylinders the engine contains (cyl). We’ll assume in each case that the relationship between mpg and each of our features is linear.\n\nInitialize the environment\nWe will start by creating a local Spark session and load the mtcars data frame to it.\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- copy_to(sc, mtcars, overwrite = TRUE)\n\n\n\nPrepare the data\nSpark provides data frame operations that makes it easier to prepare data for modeling. In this case, we will use the sdf_partition() command to divide the mtcars data into “training” and “test”.\n\npartitions <- mtcars_tbl %>%\n  select(mpg, wt, cyl) %>% \n  sdf_random_split(training = 0.5, test = 0.5, seed = 1099)\n\nNote that the newly created partitions variable does not contain data, it contains a pointer to where the data was split within Spark. That means that no data is downloaded to the R session.\n\n\nFit the model\nNext, we will fit a linear model to the training data set:\n\nfit <- partitions$training %>%\n  ml_linear_regression(mpg ~ .)\n\nfit\n\nFormula: mpg ~ .\n\nCoefficients:\n(Intercept)          wt         cyl \n  38.927395   -4.131014   -0.938832 \n\n\nFor linear regression models produced by Spark, we can use summary() to learn a bit more about the quality of our fit, and the statistical significance of each of our predictors.\n\nsummary(fit)\n\nDeviance Residuals:\n    Min      1Q  Median      3Q     Max \n-3.4891 -1.5262 -0.1481  0.8508  6.3162 \n\nCoefficients:\n(Intercept)          wt         cyl \n  38.927395   -4.131014   -0.938832 \n\nR-Squared: 0.8469\nRoot Mean Squared Error: 2.416\n\n\n\n\nUse the model\nWe can use ml_predict() to create a Spark data frame that contains the predictions against the testing data set.\n\npred <- ml_predict(fit, partitions$test)\n\nhead(pred)\n\n# Source: spark<?> [?? x 4]\n    mpg    wt   cyl prediction\n  <dbl> <dbl> <dbl>      <dbl>\n1  14.3  3.57     8      16.7 \n2  14.7  5.34     8       9.34\n3  15    3.57     8      16.7 \n4  15.2  3.44     8      17.2 \n5  15.2  3.78     8      15.8 \n6  15.5  3.52     8      16.9 \n\n\n\n\nFurther reading\nSpark machine learning supports a wide array of algorithms and feature transformations and as illustrated above it’s easy to chain these functions together with dplyr pipelines. To learn more see the Machine Learning article on this site. For a list of Spark ML models available through sparklyr visit Reference - ML"
  },
  {
    "objectID": "get-started/prepare-data.html#exercise",
    "href": "get-started/prepare-data.html#exercise",
    "title": "Prepare Data",
    "section": "Exercise",
    "text": "Exercise\nFor the exercise start a local session of Spark. We’ll start by copying a data set from R into the Spark cluster (note that you may need to install the nycflights13)\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nflights_tbl <- copy_to(sc, nycflights13::flights, \"spark_flights\")"
  },
  {
    "objectID": "get-started/prepare-data.html#using-dplyr",
    "href": "get-started/prepare-data.html#using-dplyr",
    "title": "Prepare Data",
    "section": "Using dplyr",
    "text": "Using dplyr\nWe can use familiar dplyr commands to prepare data inside Spark. The commands run inside Spark, so there are no unnecessary data transfers between R and Spark.\nIn this example, we can see how easy it is to summarize the flights data without having to know how to write Spark SQL:\n\ndelay <- flights_tbl %>%\n  group_by(tailnum) %>%\n  summarise(\n    count = n(), \n    dist = mean(distance, na.rm = TRUE), \n    delay = mean(arr_delay, na.rm = TRUE)\n    ) %>%\n  filter(count > 20, dist < 2000, !is.na(delay)) \n\ndelay\n\n# Source: spark<?> [?? x 4]\n   tailnum count  dist  delay\n   <chr>   <dbl> <dbl>  <dbl>\n 1 N24211    130 1330.  7.7  \n 2 N793JB    283 1529.  4.72 \n 3 N657JB    285 1286.  5.03 \n 4 N633AA     24 1587. -0.625\n 5 N9EAMQ    248  675.  9.24 \n 6 N3GKAA     77 1247.  4.97 \n 7 N997DL     63  868.  4.90 \n 8 N318NB    202  814. -1.12 \n 9 N651JB    261 1408.  7.58 \n10 N841UA     96 1208.  2.10 \n# … with more rows\n\n\nsparklyr and dplyr translate the R commands into Spark SQL for us. To see the resulting query use show_query():\n\ndplyr::show_query(delay)\n\n<SQL>\nSELECT *\nFROM (SELECT `tailnum`, COUNT(*) AS `count`, AVG(`distance`) AS `dist`, AVG(`arr_delay`) AS `delay`\nFROM `spark_flights`\nGROUP BY `tailnum`) `q01`\nWHERE ((`count` > 20.0) AND (`dist` < 2000.0) AND (NOT(((`delay`) IS NULL))))\n\n\nNotice that the delay variable does not contain data. It only contains the dplyr commands that are to run against the Spark connection.\nFor additional documentation on using dplyr with Spark see the Manipulating Data with dplyr article in this site"
  },
  {
    "objectID": "get-started/prepare-data.html#using-sql",
    "href": "get-started/prepare-data.html#using-sql",
    "title": "Prepare Data",
    "section": "Using SQL",
    "text": "Using SQL\nIt’s also possible to execute SQL queries directly against tables within a Spark cluster. The spark_connection() object implements a DBI interface for Spark, so you can use dbGetQuery() to execute SQL and return the result as an R data frame:\n\nlibrary(DBI)\n\ndbGetQuery(sc, \"SELECT carrier, sched_dep_time, dep_time, dep_delay FROM spark_flights LIMIT 5\")\n\n  carrier sched_dep_time dep_time dep_delay\n1      UA            515      517         2\n2      UA            529      533         4\n3      AA            540      542         2\n4      B6            545      544        -1\n5      DL            600      554        -6"
  },
  {
    "objectID": "get-started/prepare-data.html#using-feature-transformers",
    "href": "get-started/prepare-data.html#using-feature-transformers",
    "title": "Prepare Data",
    "section": "Using Feature Transformers",
    "text": "Using Feature Transformers\nBoth of the previous methods rely on SQL statements. Spark provides commands that make some data transformation more convenient, and without the use of SQL.\nFor example, the ft_binarizer() command simplifies the creation of a new column that indicates if the value of another column is above a certain threshold.\n\nflights_tbl %>% \n  ft_binarizer(\"dep_delay\", \"over_one\", threshold = 1) %>% \n  select(dep_delay, over_one) %>% \n  head(5)\n\n# Source: spark<?> [?? x 2]\n  dep_delay over_one\n      <dbl>    <dbl>\n1         2        1\n2         4        1\n3         2        1\n4        -1        0\n5        -6        0\n\n\nFind a full list of the Spark Feature Transformers available through sparklyr here: Reference - FT."
  },
  {
    "objectID": "get-started/prepare-data.html#disconnect-from-spark",
    "href": "get-started/prepare-data.html#disconnect-from-spark",
    "title": "Prepare Data",
    "section": "Disconnect from Spark",
    "text": "Disconnect from Spark\nLastly, cleanup your session by disconnecting Spark:\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "get-started/read-data.html#exercise",
    "href": "get-started/read-data.html#exercise",
    "title": "Read Data",
    "section": "Exercise",
    "text": "Exercise\nFor this exercise, we will start a “local” Spark session, and then transfer data from our R environment to the Spark session’s memory. To do that, we will use the copy_to() command:\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\ntbl_mtcars <- copy_to(sc, mtcars, \"spark_mtcars\")\n\nIf you are using the RStudio IDE, you will notice a new table in the Connections pane. The name of that table is spark_mtcars. That is the name of the data set inside the Spark memory. The tbl_mtcars variable does not contain any mtcars data, this variable contains the info that points to the location where the Spark session loaded the data to.\nCalling the tbl_mtcars variable in R will download the first 1,000 records and display them :\n\ntbl_mtcars\n\n# Source: spark<spark_mtcars> [?? x 11]\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# … with more rows\n\n\nNotice that at the top of the data print out, it is noted that records were downloaded from Spark: Source: spark….\nTo clean up the session, we will now stop the Spark session:\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "get-started/read-data.html#working-with-files",
    "href": "get-started/read-data.html#working-with-files",
    "title": "Read Data",
    "section": "Working with files",
    "text": "Working with files\nIn a formal Spark environment, it will be rare when we would have to upload data from R into Spark.\nUsing sparklyr, you can tell Spark to read and write data. Spark is able to interact with multiple types of file systems, such as HDFS, S3 and local. Additionally, Spark is able to read several file types such as CSV, Parquet, Delta and JSON. sparklyr provides functions that makes it easy to access these features. See the Spark Data section for a full list of available functions.\nThe following command will tell Spark to read a CSV file, and to also load it into Spark memory.\n\n# Do not run the next following command. It is for example purposes only.\nspark_read_csv(sc, name = \"test_table\",  path = \"/test/path/test_file.csv\")"
  },
  {
    "objectID": "guides/aws-s3.html#aws-access-keys",
    "href": "guides/aws-s3.html#aws-access-keys",
    "title": "Using Spark with AWS S3 buckets",
    "section": "AWS Access Keys",
    "text": "AWS Access Keys\nAWS Access Keys are needed to access S3 data. To learn how to setup a new keys, please review the AWS documentation: http://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html .We then pass the keys to R via Environment Variables:\nSys.setenv(AWS_ACCESS_KEY_ID=\"[Your access key]\")\nSys.setenv(AWS_SECRET_ACCESS_KEY=\"[Your secret access key]\")"
  },
  {
    "objectID": "guides/aws-s3.html#connecting-to-spark",
    "href": "guides/aws-s3.html#connecting-to-spark",
    "title": "Using Spark with AWS S3 buckets",
    "section": "Connecting to Spark",
    "text": "Connecting to Spark\nThere are four key settings needed to connect to Spark and use S3:\n\nA Hadoop-AWS package\nExecutor memory (key but not critical)\nThe master URL\nThe Spark Home\n\n\nHadoop-AWS package:\nA Spark connection can be enhanced by using packages, please note that these are not R packages. For example, there are packages that tells Spark how to read CSV files, Hadoop or Hadoop in AWS.\nIn order to read S3 buckets, our Spark connection will need a package called hadoop-aws. If needed, multiple packages can be used. We experimented with many combinations of packages, and determined that for reading data in S3 we only need one. The version we used, 3.3.1, refers to the latest Hadoop version, so as this article ages, please visit this site to make sure that you are using the latest version: Hadoop AWS Maven Repository\n\nlibrary(sparklyr)\n\nconf <- spark_config()\n\nconf$sparklyr.defaultPackages <- \"org.apache.hadoop:hadoop-aws:3.3.1\"\n\nsc <- spark_connect(maste = \"local\", config = conf)"
  },
  {
    "objectID": "guides/aws-s3.html#data-importwrangle-approach",
    "href": "guides/aws-s3.html#data-importwrangle-approach",
    "title": "Using Spark with AWS S3 buckets",
    "section": "Data Import/Wrangle approach",
    "text": "Data Import/Wrangle approach\nWe experimented with multiple approaches. Most of the factors for settling on a recommended approach were made based on the speed of each step. The premise is that we would rather wait longer during Data Import, if it meant that we can much faster Register and Cache our data subsets during Data Wrangling, especially since we would expect to end up with many subsets as we explore and model.The selected combination was the second slowest during the Import stage, but the fastest when caching a subset, by a lot.\nIn our original tests, it took 72 seconds to read and cache the 29 columns of the 41 million rows of data, the slowest was 77 seconds. But when it comes to registering and caching a considerably sizable subset of 3 columns and almost all of the 41 million records, this approach was 17X faster than the second fastest approach. It took 1/3 of a second to register and cache the subset, the second fastest was 5 seconds.\nTo implement this approach, we need to set three arguments in the spark_csv_read() step:\n\nmemory\ninfer_schema\ncolumns\n\nAgain, this is a recommended approach. The columns argument is needed only if infer_schema is set to FALSE. When memory is set to TRUE it makes Spark load the entire dataset into memory, and setting infer_schema to FALSE prevents Spark from trying to figure out what the schema of the files are. By trying different combinations the memory and infer_schema arguments you may be able to find an approach that may better fits your needs.\n\nReading the schema\nSurprisingly, another critical detail that can easily be overlooked is choosing the right s3 URI scheme. There are two options: s3n and s3a. In most examples and tutorials I found, there was no reason give of why or when to use which one. The article the finally clarified it was this one: https://wiki.apache.org/hadoop/AmazonS3\nThe gist of it is that s3a is the recommended one going forward, especially for Hadoop versions 2.7 and above. This means that if we copy from older examples that used Hadoop 2.6 we would more likely also used s3n thus making data import much, much slower."
  },
  {
    "objectID": "guides/aws-s3.html#data-import",
    "href": "guides/aws-s3.html#data-import",
    "title": "Using Spark with AWS S3 buckets",
    "section": "Data Import",
    "text": "Data Import\nAfter the long introduction in the previous section, there is only one point to add about the following code chunk. If there are any NA values in numeric fields, then define the column as character and then convert it on later subsets using dplyr. The data import will fail if it finds any NA values on numeric fields. This is a small trade off in this approach because the next fastest one does not have this issue but is 17X slower at caching subsets.\n\nflights <- spark_read_csv(sc, \"flights_spark\", \n                          path =  \"s3a://flights-data/full\", \n                          memory = TRUE, \n                          columns = list(\n                            Year = \"character\",\n                            Month = \"character\",\n                            DayofMonth = \"character\",\n                            DayOfWeek = \"character\",\n                            DepTime = \"character\",\n                            CRSDepTime = \"character\",\n                            ArrTime = \"character\",\n                            CRSArrTime = \"character\",\n                            UniqueCarrier = \"character\",\n                            FlightNum = \"character\",\n                            TailNum = \"character\",\n                            ActualElapsedTime = \"character\",\n                            CRSElapsedTime = \"character\",\n                            AirTime = \"character\",\n                            ArrDelay = \"character\",\n                            DepDelay = \"character\",\n                            Origin = \"character\",\n                            Dest = \"character\",\n                            Distance = \"character\",\n                            TaxiIn = \"character\",\n                            TaxiOut = \"character\",\n                            Cancelled = \"character\",\n                            CancellationCode = \"character\",\n                            Diverted = \"character\",\n                            CarrierDelay = \"character\",\n                            WeatherDelay = \"character\",\n                            NASDelay = \"character\",\n                            SecurityDelay = \"character\",\n                            LateAircraftDelay = \"character\"), \n                         infer_schema = FALSE)"
  },
  {
    "objectID": "guides/aws-s3.html#data-wrangle",
    "href": "guides/aws-s3.html#data-wrangle",
    "title": "Using Spark with AWS S3 buckets",
    "section": "Data Wrangle",
    "text": "Data Wrangle\nThere are a few points we need to highlight about the following simple dyplr code:\nBecause there were NAs in the original fields, we have to mutate them to a number. Try coercing any variable as integer instead of numeric, this will save a lot of space when cached to Spark memory. The sdf_register command can be piped at the end of the code. After running the code, a new table will appear in the RStudio IDE’s Spark tab\ntidy_flights <- tbl(sc, \"flights_spark\") %>%\n  mutate(ArrDelay = as.integer(ArrDelay),\n         DepDelay = as.integer(DepDelay),\n         Distance = as.integer(Distance)) %>%\n  filter(!is.na(ArrDelay)) %>%\n  select(DepDelay, ArrDelay, Distance) %>%\n  sdf_register(\"tidy_spark\")\nAfter we use tbl_cache() to load the tidy_spark table into Spark memory. We can see the new table in the Storage page of our Spark session.\ntbl_cache(sc, \"tidy_spark\")"
  },
  {
    "objectID": "guides/caching.html#introduction",
    "href": "guides/caching.html#introduction",
    "title": "Understanding Spark Caching",
    "section": "Introduction",
    "text": "Introduction\nSpark also supports pulling data sets into a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small dataset or when running an iterative algorithm like random forests. Since operations in Spark are lazy, caching can help force computation. sparklyr tools can be used to cache and un-cache DataFrames. The Spark UI will tell you which DataFrames and what percentages are in memory.\nBy using a reproducible example, we will review some of the main configuration settings, commands and command arguments that can be used that can help you get the best out of Spark’s memory management options."
  },
  {
    "objectID": "guides/caching.html#preparation",
    "href": "guides/caching.html#preparation",
    "title": "Understanding Spark Caching",
    "section": "Preparation",
    "text": "Preparation\n\nDownload Test Data\nBecause of their size, we will use trip data provided by the NYC Taxi & Limousine Commission. Each file represents a month’s worth of trips. We will download two files, the ones for January and February 2020.\n\nif(!file.exists(\"jan_2020.csv\")) {\n  download.file(\n    \"https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-01.csv\",\n    \"jan_2020.csv\",\n    mode = \"wb\"\n  )  \n}\n\nif(!file.exists(\"feb_2020.csv\")) {\n  download.file(\n    \"https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-02.csv\",\n    \"feb_2020.csv\",\n    mode = \"wb\"\n  )  \n}\n\n\n\nStart a Spark session\nA local deployment will be used for this example.\n\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n# Customize the connection configuration\nconf <- spark_config()\nconf$`sparklyr.shell.driver-memory` <- \"16G\"\n\n# Connect to Spark\nsc <- spark_connect(master = \"local\", config = conf)"
  },
  {
    "objectID": "guides/caching.html#the-memory-argument",
    "href": "guides/caching.html#the-memory-argument",
    "title": "Understanding Spark Caching",
    "section": "The Memory Argument",
    "text": "The Memory Argument\nIn the spark_read_… functions, the memory argument controls if the data will be loaded into memory as an RDD. Setting it to FALSE means that Spark will essentially map the file, but not make a copy of it in memory. This makes the spark_read_csv() command run faster, but the trade off is that any data transformation operations will take much longer.\n\nspark_read_csv(\n  sc, \n  \"taxi_jan_2020\", \n  \"jan_2020.csv\", \n  memory = FALSE\n  )\n\n# Source: spark<taxi_jan_2020> [?? x 18]\n   VendorID tpep_pickup_datetime tpep_dropoff_dat… passenger_count trip_distance\n      <int> <chr>                <chr>                       <int>         <dbl>\n 1        1 2020-01-01 00:28:15  2020-01-01 00:33…               1          1.2 \n 2        1 2020-01-01 00:35:39  2020-01-01 00:43…               1          1.2 \n 3        1 2020-01-01 00:47:41  2020-01-01 00:53…               1          0.6 \n 4        1 2020-01-01 00:55:23  2020-01-01 01:00…               1          0.8 \n 5        2 2020-01-01 00:01:58  2020-01-01 00:04…               1          0   \n 6        2 2020-01-01 00:09:44  2020-01-01 00:10…               1          0.03\n 7        2 2020-01-01 00:39:25  2020-01-01 00:39…               1          0   \n 8        2 2019-12-18 15:27:49  2019-12-18 15:28…               1          0   \n 9        2 2019-12-18 15:30:35  2019-12-18 15:31…               4          0   \n10        1 2020-01-01 00:29:01  2020-01-01 00:40…               2          0.7 \n# … with more rows, and 13 more variables: RatecodeID <int>,\n#   store_and_fwd_flag <chr>, PULocationID <int>, DOLocationID <int>,\n#   payment_type <int>, fare_amount <dbl>, extra <dbl>, mta_tax <dbl>,\n#   tip_amount <dbl>, tolls_amount <dbl>, improvement_surcharge <dbl>,\n#   total_amount <dbl>, congestion_surcharge <dbl>\n\n\nIn the RStudio IDE, the taxi_jan_2020 table now shows up in the Spark tab.\n\nTo access the Spark Web UI, click the Spark button in the RStudio Spark Tab. As expected, the Storage page shows no tables loaded into memory."
  },
  {
    "objectID": "guides/caching.html#loading-less-data-into-memory",
    "href": "guides/caching.html#loading-less-data-into-memory",
    "title": "Understanding Spark Caching",
    "section": "Loading Less Data into Memory",
    "text": "Loading Less Data into Memory\nUsing the pre-processing capabilities of Spark, the data will be transformed before being loaded into memory. In this section, we will continue to build on the example started in the previous section\n\nLazy Transform\nThe following dplyr script will not be immediately run, so the code is processed quickly. There are some check-ups made, but for the most part it is building a Spark SQL statement in the background.\n\ntrips_table <- tbl(sc,\"taxi_jan_2020\") %>%\n  filter(trip_distance > 20) %>% \n  select(VendorID, passenger_count, trip_distance)\n\n\n\nRegister in Spark\nsdf_register() will register the resulting Spark SQL in Spark. The results will show up as a table called trip_spark. But a table of the same name is still not loaded into memory in Spark.\n\nsdf_register(trips_table, \"trips_spark\")\n\n# Source: spark<trips_spark> [?? x 3]\n   VendorID passenger_count trip_distance\n      <int>           <int>         <dbl>\n 1        2               1          23.5\n 2        1               2          22.8\n 3        2               2          37.6\n 4        2               4          20.3\n 5        1               2          29.4\n 6        2               1          25.9\n 7        2               3          22.1\n 8        2               1          21.0\n 9        2               3          20.1\n10        2               1          32.5\n# … with more rows\n\n\n\n\n\nCache into Memory\nThe tbl_cache() command loads the results into an Spark RDD in memory, so any analysis from there on will not need to re-read and re-transform the original file. The resulting Spark RDD is smaller than the original file because the transformations created a smaller data set than the original file.\n\ntbl_cache(sc, \"trips_spark\")\n\n\n\n\nDriver Memory\nIn the Executors page of the Spark Web UI, we can see that the Storage Memory is at about half of the 16 gigabytes requested. This is mainly because of a Spark setting called spark.memory.fraction, which reserves by default 40% of the memory requested."
  },
  {
    "objectID": "guides/caching.html#process-on-the-fly",
    "href": "guides/caching.html#process-on-the-fly",
    "title": "Understanding Spark Caching",
    "section": "Process on the fly",
    "text": "Process on the fly\nThe plan for this exercise is to read the January file, combine it with the February file and summarize the data without bringing either file fully into memory.\n\nspark_read_csv(sc, \"taxi_feb_2020\" , \"feb_2020.csv\", memory = FALSE)\n\n# Source: spark<taxi_feb_2020> [?? x 18]\n   VendorID tpep_pickup_datetime tpep_dropoff_dat… passenger_count trip_distance\n      <int> <chr>                <chr>                       <int>         <dbl>\n 1        1 2020-02-01 00:17:35  2020-02-01 00:30…               1          2.6 \n 2        1 2020-02-01 00:32:47  2020-02-01 01:05…               1          4.8 \n 3        1 2020-02-01 00:31:44  2020-02-01 00:43…               1          3.2 \n 4        2 2020-02-01 00:07:35  2020-02-01 00:31…               1          4.38\n 5        2 2020-02-01 00:51:43  2020-02-01 01:01…               1          2.28\n 6        1 2020-02-01 00:15:49  2020-02-01 00:20…               2          1   \n 7        1 2020-02-01 00:25:31  2020-02-01 00:50…               2          3.4 \n 8        1 2020-02-01 00:11:15  2020-02-01 00:24…               1          2.1 \n 9        2 2020-02-01 00:58:26  2020-02-01 01:02…               1          0.8 \n10        2 2020-02-01 00:03:57  2020-02-01 00:48…               1          7.22\n# … with more rows, and 13 more variables: RatecodeID <int>,\n#   store_and_fwd_flag <chr>, PULocationID <int>, DOLocationID <int>,\n#   payment_type <int>, fare_amount <dbl>, extra <dbl>, mta_tax <dbl>,\n#   tip_amount <dbl>, tolls_amount <dbl>, improvement_surcharge <dbl>,\n#   total_amount <dbl>, congestion_surcharge <dbl>\n\n\n\nUnion and Transform\nThe union() command is akin to the dplyr::bind_rows() command. It will allow us to append the February file to the January file, and as with the previous transform, this script will be evaluated lazily.\n\npassenger_count <- tbl(sc, \"taxi_jan_2020\") %>%\n  union(tbl(sc, \"taxi_feb_2020\")) %>%\n  mutate(pickup_date = as.Date(tpep_pickup_datetime)) %>% \n  count(pickup_date)\n\n\n\nCollect into R\nWhen receiving a collect() command, Spark will execute the SQL statement and send the results back to R in a data frame. In this case, R only loads 51 observations into a data frame called passenger_count.\n\npassenger_count <- passenger_count %>%\n  collect()\n\n\n\nPlot in R\nNow the smaller data set can be plotted\n\npassenger_count %>% \n  filter(pickup_date >= \"2020-01-01\", pickup_date <= \"2020-02-28\") %>% \n  ggplot() +\n  geom_line(aes(pickup_date, n)) +\n  theme_minimal() +\n  labs(title = \"Daily Trip Volume\", \n       subtitle = \"NYC Yellow Cab - January and February 2020\",\n       y = \"Number of Trips\",\n       x = \"\"\n       )\n\n\n\n\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "guides/connections.html#local-mode",
    "href": "guides/connections.html#local-mode",
    "title": "Configuring Spark Connections",
    "section": "Local mode",
    "text": "Local mode\nLocal mode is an excellent way to learn and experiment with Spark. Local mode also provides a convenient development environment for analyses, reports, and applications that you plan to eventually deploy to a multi-node Spark cluster.\nTo work in local mode, you should first install a version of Spark for local use. You can do this using the spark_install() function, for example:\n\nRecommended properties\nThe following are the recommended Spark properties to set when connecting via R:\n\nsparklyr.cores.local - It defaults to using all of the available cores. Not a necessary property to set, unless there’s a reason to use less cores than available for a given Spark session.\nsparklyr.shell.driver-memory - The limit is the amount of RAM available in the computer minus what would be needed for OS operations.\nspark.memory.fraction - The default is set to 60% of the requested memory per executor. For more information, please see this Memory Management Overview page in the official Spark website.\n\n\n\nConnection example\nconf$`sparklyr.cores.local` <- 4\nconf$`sparklyr.shell.driver-memory` <- \"16G\"\nconf$spark.memory.fraction <- 0.9\n\nsc <- spark_connect(master = \"local\", \n                    version = \"2.1.0\",\n                    config = conf)\n\nExecutors page\nTo see how the requested configuration affected the Spark connection, go to the Executors page in the Spark Web UI available in http://localhost:4040/storage/"
  },
  {
    "objectID": "guides/connections.html#customizing-connections",
    "href": "guides/connections.html#customizing-connections",
    "title": "Configuring Spark Connections",
    "section": "Customizing connections",
    "text": "Customizing connections\nA connection to Spark can be customized by setting the values of certain Spark properties. In sparklyr, Spark properties can be set by using the config argument in the spark_connect() function.\nBy default, spark_connect() uses spark_config() as the default configuration. But that can be customized as shown in the example code below. Because of the unending number of possible combinations, spark_config() contains only a basic configuration, so it will be very likely that additional settings will be needed to properly connect to the cluster.\nconf <- spark_config()   # Load variable with spark_config()\n\nconf$spark.executor.memory <- \"16G\" # Use `$` to add or set values\n\nsc <- spark_connect(master = \"yarn-client\", \n                    config = conf)  # Pass the conf variable \n\nSpark definitions\nIt may be useful to provide some simple definitions for the Spark nomenclature:\n\nNode: A server\nWorker Node: A server that is part of the cluster and are available to run Spark jobs\nMaster Node: The server that coordinates the Worker nodes.\nExecutor: A sort of virtual machine inside a node. One Node can have multiple Executors.\nDriver Node: The Node that initiates the Spark session. Typically, this will be the server where sparklyr is located.\nDriver (Executor): The Driver Node will also show up in the Executor list.\n\n\n\nUseful concepts\n\nSpark configuration properties passed by R are just requests - In most cases, the cluster has the final say regarding the resources apportioned to a given Spark session.\nThe cluster overrides ‘silently’ - Many times, no errors are returned when more resources than allowed are requested, or if an attempt is made to change a setting fixed by the cluster."
  },
  {
    "objectID": "guides/connections.html#yarn",
    "href": "guides/connections.html#yarn",
    "title": "Configuring Spark Connections",
    "section": "YARN",
    "text": "YARN\n\nBackground\nUsing Spark and R inside a Hadoop based Data Lake is becoming a common practice at companies. Currently, there is no good way to manage user connections to the Spark service centrally. There are some caps and settings that can be applied, but in most cases there are configurations that the R user will need to customize.\nThe Running on YARN page in Spark’s official website is the best place to start for configuration settings reference, please bookmark it. Cluster administrators and users can benefit from this document. If Spark is new to the company, the YARN tunning article, courtesy of Cloudera, does a great job at explaining how the Spark/YARN architecture works.\n\n\nRecommended properties\nThe following are the recommended Spark properties to set when connecting via R:\n\nspark.executor.memory - The maximum possible is managed by the YARN cluster. See the Executor Memory Error\nspark.executor.cores - Number of cores assigned per Executor.\nspark.executor.instances - Number of executors to start. This property is acknowledged by the cluster if spark.dynamicAllocation.enabled is set to “false”.\nspark.dynamicAllocation.enabled - Overrides the mechanism that Spark provides to dynamically adjust resources. Disabling it provides more control over the number of the Executors that can be started, which in turn impact the amount of storage available for the session. For more information, please see the Dynamic Resource Allocation page in the official Spark website.\n\n\n\nClient mode\nUsing yarn-client as the value for the master argument in spark_connect() will make the server in which R is running to be the Spark’s session driver. Here is a sample connection:\nconf <- spark_config()\n\nconf$spark.executor.memory <- \"300M\"\nconf$spark.executor.cores <- 2\nconf$spark.executor.instances <- 3\nconf$spark.dynamicAllocation.enabled <- \"false\"\n\nsc <- spark_connect(master = \"yarn-client\", \n                    spark_home = \"/usr/lib/spark/\",\n                    version = \"1.6.0\",\n                    config = conf)\n\nExecutors page\nTo see how the requested configuration affected the Spark connection, go to the Executors page in the Spark Web UI. Typically, the Spark Web UI can be found using the exact same URL used for RStudio but on port 4040.\nNotice that 155.3MB per executor are assigned instead of the 300MB requested. This is because the spark.memory.fraction has been fixed by the cluster, plus, there is fixed amount of memory designated for overhead.\n\n\n\n\nCluster mode\nRunning in cluster mode means that YARN will choose where the driver of the Spark session will run. This means that the server where R is running may not necessarily be the driver for that session. Here is a good write-up explaining how running Spark applications work: Running Spark on YARN\nThe server will need to have copies of at least two files: yarn-site.xml and hive-site.xml. There may be other files needed based on your cluster’s individual setup.\nThis is an example of connecting to a Cloudera cluster:\nlibrary(sparklyr)\n\nSys.setenv(JAVA_HOME=\"/usr/lib/jvm/java-7-oracle-cloudera/\")\nSys.setenv(SPARK_HOME = '/opt/cloudera/parcels/CDH/lib/spark')\nSys.setenv(YARN_CONF_DIR = '/opt/cloudera/parcels/CDH/lib/spark/conf/yarn-conf')\n\nconf$spark.executor.memory <- \"300M\"\nconf$spark.executor.cores <- 2\nconf$spark.executor.instances <- 3\nconf$spark.dynamicAllocation.enabled <- \"false\"\nconf <- spark_config()\n\nsc <- spark_connect(master = \"yarn-cluster\", \n                    config = conf)\n\n\nExecutor memory error\nRequesting more memory or CPUs for Executors than allowed will return an error. This is one of the exceptions to the cluster’s ‘silent’ overrides. It will return a message similar to this:\n    Failed during initialize_connection: java.lang.IllegalArgumentException: Required executor memory (16384+1638 MB) is above the max threshold (8192 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'\nA cluster’s administrator is the only person who can make changes to the settings mentioned in the error. If the cluster is supported by a vendor, like Cloudera or Hortonworks, then the change can be made using the cluster’s web UI. Otherwise, changes to those settings are done directly in the yarn-default.xml file.\n\n\nKerberos\nThere are two options to access a “kerberized” data lake:\n\nUse kinit to get and cache the ticket. After kinit is installed and configured. After kinit is setup, it can used in R via a system() call prior to connecting to the cluster:\n\nsystem(\"echo '<password>' | kinit <username>\")\nFor more information visit this site: Apache - Authenticate with kinit\n\nA preferred option may be to use the out-of-the-box integration with Kerberos that the commercial version of RStudio Server offers."
  },
  {
    "objectID": "guides/connections.html#standalone-mode",
    "href": "guides/connections.html#standalone-mode",
    "title": "Configuring Spark Connections",
    "section": "Standalone mode",
    "text": "Standalone mode\n\nRecommended properties\nThe following are the recommended Spark properties to set when connecting via R:\nThe default behavior in Standalone mode is to create one executor per worker. So in a 3 worker node cluster, there will be 3 executors setup. The basic properties that can be set are:\n\nspark.executor.memory - The requested memory cannot exceed the actual RAM available.\nspark.memory.fraction - The default is set to 60% of the requested memory per executor. For more information, please see this Memory Management Overview page in the official Spark website.\nspark.executor.cores - The requested cores cannot be higher than the cores available in each worker.\n\n\nDynamic Allocation\nIf dynamic allocation is disabled, then Spark will attempt to assign all of the available cores evenly across the cluster. The property used is spark.dynamicAllocation.enabled.\nFor example, the Standalone cluster used for this article has 3 worker nodes. Each node has 14.7GB in RAM and 4 cores. This means that there are a total of 12 cores (3 workers with 4 cores) and 44.1GB in RAM (3 workers with 14.7GB in RAM each).\nIf the spark.executor.cores property is set to 2, and dynamic allocation is disabled, then Spark will spawn 6 executors. The spark.executor.memory property should be set to a level that when the value is multiplied by 6 (number of executors) it will not be over total available RAM. In this case, the value can be safely set to 7GB so that the total memory requested will be 42GB, which is under the available 44.1GB.\n\n\n\nConnection example\nconf <- spark_config()\nconf$spark.executor.memory <- \"7GB\"\nconf$spark.memory.fraction <- 0.9\nconf$spark.executor.cores <- 2\nconf$spark.dynamicAllocation.enabled <- \"false\"\n\nsc <- spark_connect(master=\"spark://master-url:7077\", \n              version = \"2.1.0\",\n              config = conf,\n              spark_home = \"/home/ubuntu/spark-2.1.0-bin-hadoop2.7/\")\n\nExecutors page\nTo see how the requested configuration affected the Spark connection, go to the Executors page in the Spark Web UI. Typically, the Spark Web UI can be found using the exact same URL used for RStudio but on port 4040:"
  },
  {
    "objectID": "guides/distributed-r.html#overview",
    "href": "guides/distributed-r.html#overview",
    "title": "Distributing R Computations",
    "section": "Overview",
    "text": "Overview\nsparklyr provides support to run arbitrary R code at scale within your Spark Cluster through spark_apply(). This is especially useful where there is a need to use functionality available only in R or R packages that is not available in Apache Spark nor Spark Packages.\nspark_apply() applies an R function to a Spark object (typically, a Spark DataFrame). Spark objects are partitioned so they can be distributed across a cluster. You can use spark_apply() with the default partitions or you can define your own partitions with the group_by() argument. Your R function must return another Spark DataFrame. spark_apply() will run your R function on each partition and output a single Spark DataFrame.\n\nApply an R function to a Spark Object\nLets run a simple example. We will apply the identify function, I(), over a list of numbers we created with the sdf_len() function.\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nsdf_len(sc, 5, repartition = 1) %>%\n  spark_apply(function(e) I(e))\n\n# Source: spark<?> [?? x 1]\n     id\n  <int>\n1     1\n2     2\n3     3\n4     4\n5     5\n\n\nYour R function should be designed to operate on an R data frame. The R function passed to spark_apply() expects a DataFrame and will return an object that can be cast as a DataFrame. We can use the class function to verify the class of the data.\n\nsdf_len(sc, 10, repartition = 1) %>%\n  spark_apply(function(e) class(e))\n\n# Source: spark<?> [?? x 1]\n  result    \n  <chr>     \n1 tbl_df    \n2 tbl       \n3 data.frame\n\n\nSpark will partition your data by hash or range so it can be distributed across a cluster. In the following example we create two partitions and count the number of rows in each partition. Then we print the first record in each partition.\n\ntrees_tbl <- sdf_copy_to(sc, trees, repartition = 2)\n\nspark_apply(\n  trees_tbl,\n  function(e) nrow(e), names = \"n\"\n  )\n\n# Source: spark<?> [?? x 1]\n      n\n  <int>\n1    15\n2    16\n\n\n\nspark_apply(trees_tbl, function(e) head(e, 1))\n\n# Source: spark<?> [?? x 3]\n  Girth Height Volume\n  <dbl>  <dbl>  <dbl>\n1   8.3     70   10.3\n2  12.9     74   22.2\n\n\nWe can apply any arbitrary function to the partitions in the Spark DataFrame. For instance, we can scale or jitter the columns. Notice that spark_apply() applies the R function to all partitions and returns a single DataFrame.\n\nspark_apply(trees_tbl, function(e) scale(e))\n\n# Source: spark<?> [?? x 3]\n     Girth Height  Volume\n     <dbl>  <dbl>   <dbl>\n 1 -2.05   -0.607 -1.69  \n 2 -1.79   -1.43  -1.69  \n 3 -1.62   -1.76  -1.71  \n 4 -0.134  -0.276 -0.339 \n 5  0.0407  1.21   0.191 \n 6  0.128   1.54   0.390 \n 7  0.302  -1.27  -0.515 \n 8  0.302   0.221  0.0589\n 9  0.389   1.05   1.03  \n10  0.477   0.221  0.434 \n# … with more rows\n\n\n\nspark_apply(trees_tbl, function(e) lapply(e, jitter))\n\n# Source: spark<?> [?? x 3]\n   Girth Height Volume\n   <dbl>  <dbl>  <dbl>\n 1  8.32   70.1   10.3\n 2  8.60   65.2   10.3\n 3  8.81   63.2   10.2\n 4 10.5    72.1   16.4\n 5 10.7    80.8   18.8\n 6 10.8    83.0   19.7\n 7 11.0    66.2   15.6\n 8 11.0    75.2   18.2\n 9 11.1    80.0   22.6\n10 11.2    74.8   19.9\n# … with more rows\n\n\nBy default spark_apply() derives the column names from the input Spark data frame. Use the names argument to rename or add new columns.\n\nspark_apply(\n  trees_tbl,\n  function(e) data.frame(2.54 * e$Girth, e), names = c(\"Girth(cm)\", colnames(trees))\n  )\n\n# Source: spark<?> [?? x 4]\n   `Girth(cm)` Girth Height Volume\n         <dbl> <dbl>  <dbl>  <dbl>\n 1        21.1   8.3     70   10.3\n 2        21.8   8.6     65   10.3\n 3        22.4   8.8     63   10.2\n 4        26.7  10.5     72   16.4\n 5        27.2  10.7     81   18.8\n 6        27.4  10.8     83   19.7\n 7        27.9  11       66   15.6\n 8        27.9  11       75   18.2\n 9        28.2  11.1     80   22.6\n10        28.4  11.2     75   19.9\n# … with more rows\n\n\n\n\nGroup By\nIn some cases you may want to apply your R function to specific groups in your data. For example, suppose you want to compute regression models against specific subgroups. To solve this, you can specify a group_by() argument. This example counts the number of rows in iris by species and then fits a simple linear model for each species.\n\niris_tbl <- sdf_copy_to(sc, iris)\n\nspark_apply(iris_tbl, nrow, group_by = \"Species\")\n\n# Source: spark<?> [?? x 2]\n  Species    result\n  <chr>       <int>\n1 versicolor     50\n2 virginica      50\n3 setosa         50\n\n\n\niris_tbl %>%\n  spark_apply(\n    function(e) summary(lm(Petal_Length ~ Petal_Width, e))$r.squared,\n    names = \"r.squared\",\n    group_by = \"Species\"\n    )\n\n# Source: spark<?> [?? x 2]\n  Species    r.squared\n  <chr>          <dbl>\n1 versicolor     0.619\n2 virginica      0.104\n3 setosa         0.110"
  },
  {
    "objectID": "guides/distributed-r.html#distributing-packages",
    "href": "guides/distributed-r.html#distributing-packages",
    "title": "Distributing R Computations",
    "section": "Distributing Packages",
    "text": "Distributing Packages\nWith spark_apply() you can use any R package inside Spark. For instance, you can use the broom package to create a tidy data frame from linear regression output.\n\nspark_apply(\n  iris_tbl,\n  function(e) broom::tidy(lm(Petal_Length ~ Petal_Width, e)),\n  names = c(\"term\", \"estimate\", \"std.error\", \"statistic\", \"p.value\"),\n  group_by = \"Species\"\n  )\n\n# Source: spark<?> [?? x 6]\n  Species    term        estimate std.error statistic  p.value\n  <chr>      <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 versicolor (Intercept)    1.78     0.284       6.28 9.48e- 8\n2 versicolor Petal_Width    1.87     0.212       8.83 1.27e-11\n3 virginica  (Intercept)    4.24     0.561       7.56 1.04e- 9\n4 virginica  Petal_Width    0.647    0.275       2.36 2.25e- 2\n5 setosa     (Intercept)    1.33     0.0600     22.1  7.68e-27\n6 setosa     Petal_Width    0.546    0.224       2.44 1.86e- 2\n\n\nTo use R packages inside Spark, your packages must be installed on the worker nodes. The first time you call spark_apply() all of the contents in your local .libPaths() will be copied into each Spark worker node via the SparkConf.addFile() function. Packages will only be copied once and will persist as long as the connection remains open. It’s not uncommon for R libraries to be several gigabytes in size, so be prepared for a one-time tax while the R packages are copied over to your Spark cluster. You can disable package distribution by setting packages = FALSE. Note: packages are not copied in local mode (master=\"local\") because the packages already exist on the system."
  },
  {
    "objectID": "guides/distributed-r.html#handling-errors",
    "href": "guides/distributed-r.html#handling-errors",
    "title": "Distributing R Computations",
    "section": "Handling Errors",
    "text": "Handling Errors\nIt can be more difficult to troubleshoot R issues in a cluster than in local mode. For instance, the following R code causes the distributed execution to fail and suggests you check the logs for details.\n\nspark_apply(iris_tbl, function(e) stop(\"Make this fail\"))\n\nIt is worth mentioning that different cluster providers and platforms expose worker logs in different ways. Specific documentation for your environment will point out how to retrieve these logs."
  },
  {
    "objectID": "guides/distributed-r.html#requirements",
    "href": "guides/distributed-r.html#requirements",
    "title": "Distributing R Computations",
    "section": "Requirements",
    "text": "Requirements\nThe R Runtime is expected to be pre-installed in the cluster for spark_apply() to function. Failure to install the cluster will trigger a Cannot run program, no such file or directory error while attempting to use spark_apply(). Contact your cluster administrator to consider making the R runtime available throughout the entire cluster.\nA Homogeneous Cluster is required since the driver node distributes, and potentially compiles, packages to the workers. For instance, the driver and workers must have the same processor architecture, system libraries, etc."
  },
  {
    "objectID": "guides/distributed-r.html#configuration",
    "href": "guides/distributed-r.html#configuration",
    "title": "Distributing R Computations",
    "section": "Configuration",
    "text": "Configuration\nThe following table describes relevant parameters while making use of spark_apply.\n\n\n\n\n\n\n\nValue\nDescription\n\n\n\n\nspark.r.command\nThe path to the R binary. Useful to select from multiple R versions.\n\n\nsparklyr.worker.gateway.address\nThe gateway address to use under each worker node. Defaults to sparklyr.gateway.address.\n\n\nsparklyr.worker.gateway.port\nThe gateway port to use under each worker node. Defaults to sparklyr.gateway.port.\n\n\n\nFor example, one could make use of an specific R version by running:\n\nconfig <- spark_config()\nconfig[[\"spark.r.command\"]] <- \"<path-to-r-version>\"\nsc <- spark_connect(master = \"local\", config = config)\n\nsdf_len(sc, 10) %>% spark_apply(function(e) e)"
  },
  {
    "objectID": "guides/distributed-r.html#limitations",
    "href": "guides/distributed-r.html#limitations",
    "title": "Distributing R Computations",
    "section": "Limitations",
    "text": "Limitations\n\nClosures\nClosures are serialized using serialize, which is described as “A simple low-level interface for serializing to connections.”. One of the current limitations of serialize is that it wont serialize objects being referenced outside of it’s environment. For instance, the following function will error out since the closures references external_value:\n\nexternal_value <- 1\nspark_apply(iris_tbl, function(e) e + external_value)\n\n\n\nLivy\nCurrently, Livy connections do not support distributing packages since the client machine where the libraries are pre-compiled might not have the same processor architecture, not operating systems that the cluster machines.\n\n\nComputing over Groups\nWhile performing computations over groups, spark_apply() will provide partitions over the selected column; however, this implies that each partition can fit into a worker node, if this is not the case an exception will be thrown. To perform operations over groups that exceed the resources of a single node, one can consider partitioning to smaller units or use dplyr::do() which is currently optimized for large partitions.\n\n\nPackage Installation\nSince packages are copied only once for the duration of the spark_connect() connection, installing additional packages is not supported while the connection is active. Therefore, if a new package needs to be installed, spark_disconnect() the connection, modify packages and reconnect."
  },
  {
    "objectID": "guides/dplyr.html#overview",
    "href": "guides/dplyr.html#overview",
    "title": "Manipulating Data with dplyr",
    "section": "Overview",
    "text": "Overview\ndplyr is an R package for working with structured data both in and outside of R. dplyr makes data manipulation for R users easy, consistent, and performant. With dplyr as an interface to manipulating Spark DataFrames, you can:\n\nSelect, filter, and aggregate data\nUse window functions (e.g. for sampling)\nPerform joins on DataFrames\nCollect data from Spark into R\n\nStatements in dplyr can be chained together using pipes defined by the magrittr R package. dplyr also supports non-standard evalution of its arguments. For more information on dplyr, see the introduction, a guide for connecting to databases, and a variety of vignettes.\n\nFlights Data\nThis guide will demonstrate some of the basic data manipulation verbs of dplyr by using data from the nycflights13 R package. This package contains data for all 336,776 flights departing New York City in 2013. It also includes useful metadata on airlines, airports, weather, and planes. The data comes from the US Bureau of Transportation Statistics, and is documented in ?nycflights13\nConnect to the cluster and copy the flights data using the copy_to() function. Caveat: The flight data in nycflights13 is convenient for dplyr demonstrations because it is small, but in practice large data should rarely be copied directly from R objects.\n\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nsc <- spark_connect(master=\"local\")\n\nflights_tbl <- copy_to(sc, nycflights13::flights, \"flights\")\n\nairlines_tbl <- copy_to(sc, nycflights13::airlines, \"airlines\")"
  },
  {
    "objectID": "guides/dplyr.html#dplyr-verbs",
    "href": "guides/dplyr.html#dplyr-verbs",
    "title": "Manipulating Data with dplyr",
    "section": "dplyr Verbs",
    "text": "dplyr Verbs\nVerbs are dplyr commands for manipulating data. When connected to a Spark DataFrame, dplyr translates the commands into Spark SQL statements. Remote data sources use exactly the same five verbs as local data sources. Here are the five verbs with their corresponding SQL commands:\n\nselect() ~ SELECT\nfilter() ~ WHERE\narrange() ~ ORDER\nsummarise() ~ aggregators: sum, min, sd, etc.\nmutate() ~ operators: +, *, log, etc.\n\n\nselect(flights_tbl, year:day, arr_delay, dep_delay)\n\n# Source: spark<?> [?? x 5]\n    year month   day arr_delay dep_delay\n   <int> <int> <int>     <dbl>     <dbl>\n 1  2013     1     1        11         2\n 2  2013     1     1        20         4\n 3  2013     1     1        33         2\n 4  2013     1     1       -18        -1\n 5  2013     1     1       -25        -6\n 6  2013     1     1        12        -4\n 7  2013     1     1        19        -5\n 8  2013     1     1       -14        -3\n 9  2013     1     1        -8        -3\n10  2013     1     1         8        -2\n# … with more rows\n\n\n\nfilter(flights_tbl, dep_delay > 1000)\n\n# Source: spark<?> [?? x 19]\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n1  2013     1     9      641            900      1301     1242           1530\n2  2013     1    10     1121           1635      1126     1239           1810\n3  2013     6    15     1432           1935      1137     1607           2120\n4  2013     7    22      845           1600      1005     1044           1815\n5  2013     9    20     1139           1845      1014     1457           2210\n# … with 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>\n\n\n\narrange(flights_tbl, desc(dep_delay))\n\n# Source:     spark<?> [?? x 19]\n# Ordered by: desc(dep_delay)\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     6    15     1432           1935      1137     1607           2120\n 3  2013     1    10     1121           1635      1126     1239           1810\n 4  2013     9    20     1139           1845      1014     1457           2210\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013     3    17     2321            810       911      135           1020\n 8  2013     6    27      959           1900       899     1236           2226\n 9  2013     7    22     2257            759       898      121           1026\n10  2013    12     5      756           1700       896     1058           2020\n# … with more rows, and 11 more variables: arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,\n#   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>\n\n\n\nsummarise(\n  flights_tbl, \n  mean_dep_delay = mean(dep_delay, na.rm = TRUE)\n  )\n\n# Source: spark<?> [?? x 1]\n  mean_dep_delay\n           <dbl>\n1           12.6\n\n\n\nmutate(flights_tbl, speed = distance / air_time * 60)\n\n# Source: spark<?> [?? x 20]\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# … with more rows, and 12 more variables: arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,\n#   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>, speed <dbl>"
  },
  {
    "objectID": "guides/dplyr.html#laziness",
    "href": "guides/dplyr.html#laziness",
    "title": "Manipulating Data with dplyr",
    "section": "Laziness",
    "text": "Laziness\nWhen working with databases, dplyr tries to be as lazy as possible:\n\nIt never pulls data into R unless you explicitly ask for it.\nIt delays doing any work until the last possible moment: it collects together everything you want to do and then sends it to the database in one step.\n\nFor example, take the following code:\n\nc1 <- filter(\n  flights_tbl, \n  day == 17, month == 5, carrier %in% c('UA', 'WN', 'AA', 'DL')\n  )\n\nc2 <- select(c1, year, month, day, carrier, dep_delay, air_time, distance)\n\nc3 <- mutate(c2, air_time_hours = air_time / 60)\n\nc4 <- arrange(c3, year, month, day, carrier)\n\nThis sequence of operations never actually touches the database. It’s not until you ask for the data (e.g. by printing c4) that dplyr requests the results from the database.\n\nc4\n\n# Source:     spark<?> [?? x 8]\n# Ordered by: year, month, day, carrier\n    year month   day carrier dep_delay air_time distance air_time_hours\n   <int> <int> <int> <chr>       <dbl>    <dbl>    <dbl>          <dbl>\n 1  2013     5    17 AA             -7      142     1089           2.37\n 2  2013     5    17 AA             -9      186     1389           3.1 \n 3  2013     5    17 AA             -6      143     1096           2.38\n 4  2013     5    17 AA             -7      119      733           1.98\n 5  2013     5    17 AA             -4      114      733           1.9 \n 6  2013     5    17 AA             -2      146     1085           2.43\n 7  2013     5    17 AA             -2      185     1372           3.08\n 8  2013     5    17 AA             -3      193     1598           3.22\n 9  2013     5    17 AA             -7      137      944           2.28\n10  2013     5    17 AA             -1      195     1389           3.25\n# … with more rows"
  },
  {
    "objectID": "guides/dplyr.html#piping",
    "href": "guides/dplyr.html#piping",
    "title": "Manipulating Data with dplyr",
    "section": "Piping",
    "text": "Piping\nYou can use magrittr pipes to write cleaner syntax. Using the same example from above, you can write a much cleaner version like this:\n\nc4 <- flights_tbl %>%\n  filter(month == 5, day == 17, carrier %in% c('UA', 'WN', 'AA', 'DL')) %>%\n  select(carrier, dep_delay, air_time, distance) %>%\n  mutate(air_time_hours = air_time / 60) %>% \n  arrange(carrier)"
  },
  {
    "objectID": "guides/dplyr.html#grouping",
    "href": "guides/dplyr.html#grouping",
    "title": "Manipulating Data with dplyr",
    "section": "Grouping",
    "text": "Grouping\nThe group_by() function corresponds to the GROUP BY statement in SQL.\n\nflights_tbl %>% \n  group_by(carrier) %>%\n  summarize(\n    count = n(), \n    mean_dep_delay = mean(dep_delay, na.rm = FALSE)\n    )\n\nWarning: Missing values are always removed in SQL.\nUse `mean(x, na.rm = TRUE)` to silence this warning\nThis warning is displayed only once per session.\n\n\n# Source: spark<?> [?? x 3]\n   carrier count mean_dep_delay\n   <chr>   <dbl>          <dbl>\n 1 WN      12275          17.7 \n 2 VX       5162          12.9 \n 3 YV        601          19.0 \n 4 DL      48110           9.26\n 5 OO         32          12.6 \n 6 B6      54635          13.0 \n 7 F9        685          20.2 \n 8 EV      54173          20.0 \n 9 US      20536           3.78\n10 UA      58665          12.1 \n# … with more rows"
  },
  {
    "objectID": "guides/dplyr.html#collecting-to-r",
    "href": "guides/dplyr.html#collecting-to-r",
    "title": "Manipulating Data with dplyr",
    "section": "Collecting to R",
    "text": "Collecting to R\nYou can copy data from Spark into R’s memory by using collect().\n\ncarrierhours <- collect(c4)\n\ncollect() executes the Spark query and returns the results to R for further analysis and visualization.\n\n# Test the significance of pairwise differences and plot the results\n\nwith(carrierhours, pairwise.t.test(air_time, carrier))\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  air_time and carrier \n\n   AA      DL      UA     \nDL 0.25057 -       -      \nUA 0.07957 0.00044 -      \nWN 0.07957 0.23488 0.00041\n\nP value adjustment method: holm \n\n\n\ncarrierhours %>% \n  ggplot() + \n  geom_boxplot(aes(carrier, air_time_hours))"
  },
  {
    "objectID": "guides/dplyr.html#sql-translation",
    "href": "guides/dplyr.html#sql-translation",
    "title": "Manipulating Data with dplyr",
    "section": "SQL Translation",
    "text": "SQL Translation\nIt’s relatively straightforward to translate R code to SQL (or indeed to any programming language) when doing simple mathematical operations of the form you normally use when filtering, mutating and summarizing. dplyr knows how to convert the following R functions to Spark SQL:\n# Basic math operators\n+, -, *, /, %%, ^\n  \n# Math functions\nabs, acos, asin, asinh, atan, atan2, ceiling, cos, cosh, exp, floor, log, \nlog10, round, sign, sin, sinh, sqrt, tan, tanh\n\n# Logical comparisons\n<, <=, !=, >=, >, ==, %in%\n\n# Boolean operations\n&, &&, |, ||, !\n\n# Character functions\npaste, tolower, toupper, nchar\n\n# Casting\nas.double, as.integer, as.logical, as.character, as.date\n\n# Basic aggregations\nmean, sum, min, max, sd, var, cor, cov, n\ndplyr supports Spark SQL window functions. Window functions are used in conjunction with mutate and filter to solve a wide range of problems. You can compare the dplyr syntax to the query it has generated by using dplyr::show_query().\n\n# Rank each flight within a daily\nranked <- flights_tbl %>%\n  group_by(year, month, day) %>%\n  select(dep_delay) %>% \n  mutate(rank = rank(desc(dep_delay)))\n\ndplyr::show_query(ranked)\n\n<SQL>\nSELECT `year`, `month`, `day`, `dep_delay`, RANK() OVER (PARTITION BY `year`, `month`, `day` ORDER BY `dep_delay` DESC) AS `rank`\nFROM `flights`\n\n\n\nranked \n\n# Source: spark<?> [?? x 5]\n# Groups: year, month, day\n    year month   day dep_delay  rank\n   <int> <int> <int>     <dbl> <int>\n 1  2013     1     1       853     1\n 2  2013     1     1       379     2\n 3  2013     1     1       290     3\n 4  2013     1     1       285     4\n 5  2013     1     1       260     5\n 6  2013     1     1       255     6\n 7  2013     1     1       216     7\n 8  2013     1     1       192     8\n 9  2013     1     1       157     9\n10  2013     1     1       155    10\n# … with more rows"
  },
  {
    "objectID": "guides/dplyr.html#peforming-joins",
    "href": "guides/dplyr.html#peforming-joins",
    "title": "Manipulating Data with dplyr",
    "section": "Peforming Joins",
    "text": "Peforming Joins\nIt’s rare that a data analysis involves only a single table of data. In practice, you’ll normally have many tables that contribute to an analysis, and you need flexible tools to combine them. In dplyr, there are three families of verbs that work with two tables at a time:\n\nMutating joins, which add new variables to one table from matching rows in another.\nFiltering joins, which filter observations from one table based on whether or not they match an observation in the other table.\nSet operations, which combine the observations in the data sets as if they were set elements.\n\nAll two-table verbs work similarly. The first two arguments are x and y, and provide the tables to combine. The output is always a new table with the same type as x.\n\nflights_tbl %>% \n  left_join(airlines_tbl, by = \"carrier\") %>% \n  select(name, flight, dep_time)\n\n# Source: spark<?> [?? x 3]\n   name           flight dep_time\n   <chr>           <int>    <int>\n 1 Virgin America    399      658\n 2 Virgin America     11      729\n 3 Virgin America    407      859\n 4 Virgin America    251      932\n 5 Virgin America     23     1031\n 6 Virgin America    409     1133\n 7 Virgin America     25     1203\n 8 Virgin America    411     1327\n 9 Virgin America     27     1627\n10 Virgin America    413     1655\n# … with more rows"
  },
  {
    "objectID": "guides/dplyr.html#sampling",
    "href": "guides/dplyr.html#sampling",
    "title": "Manipulating Data with dplyr",
    "section": "Sampling",
    "text": "Sampling\nYou can use sample_n() and sample_frac() to take a random sample of rows: use sample_n() for a fixed number and sample_frac() for a fixed fraction.\n\nsample_n(flights_tbl, 10) %>% \n  select(1:4)\n\n# Source: spark<?> [?? x 4]\n    year month   day dep_time\n   <int> <int> <int>    <int>\n 1  2013     8    20      800\n 2  2013     8    26     2046\n 3  2013     2    22     1950\n 4  2013     3    28      600\n 5  2013     7    10      902\n 6  2013    11    15     1318\n 7  2013     1    10      855\n 8  2013    11    25     1923\n 9  2013    11    27      621\n10  2013     7    22     1152\n\n\n\nsample_frac(flights_tbl, 0.01) %>% \n  count()\n\n# Source: spark<?> [?? x 1]\n      n\n  <dbl>\n1  3368"
  },
  {
    "objectID": "guides/dplyr.html#hive-functions",
    "href": "guides/dplyr.html#hive-functions",
    "title": "Manipulating Data with dplyr",
    "section": "Hive Functions",
    "text": "Hive Functions\nMany of Hive’s built-in functions (UDF) and built-in aggregate functions (UDAF) can be called inside dplyr’s mutate and summarize. The Languange Reference UDF page provides the list of available functions.\nThe following example uses the datediff and current_date Hive UDFs to figure the difference between the flight_date and the current system date:\n\nflights_tbl %>% \n  mutate(\n    flight_date = paste(year,month,day,sep=\"-\"),\n    days_since = datediff(current_date(), flight_date)\n    ) %>%\n  group_by(flight_date,days_since) %>%\n  count() %>%\n  arrange(-days_since)\n\n# Source:     spark<?> [?? x 3]\n# Groups:     flight_date\n# Ordered by: -days_since\n   flight_date days_since     n\n   <chr>            <int> <dbl>\n 1 2013-1-1          3331   842\n 2 2013-1-2          3330   943\n 3 2013-1-3          3329   914\n 4 2013-1-4          3328   915\n 5 2013-1-5          3327   720\n 6 2013-1-6          3326   832\n 7 2013-1-7          3325   933\n 8 2013-1-8          3324   899\n 9 2013-1-9          3323   902\n10 2013-1-10         3322   932\n# … with more rows\n\n\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "guides/extensions.html#introduction",
    "href": "guides/extensions.html#introduction",
    "title": "Creating Extensions for sparklyr",
    "section": "Introduction",
    "text": "Introduction\nThe sparklyr package provides a dplyr interface to Spark DataFrames as well as an R interface to Spark’s distributed machine learning pipelines. However, since Spark is a general-purpose cluster computing system there are many other R interfaces that could be built (e.g. interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).\nThe facilities used internally by sparklyr for its dplyr and machine learning interfaces are available to extension packages. This guide describes how you can use these tools to create your own custom R interfaces to Spark.\n\nExamples\nHere’s an example of an extension function that calls the text file line counting function available via the SparkContext:\n\nlibrary(sparklyr)\ncount_lines <- function(sc, file) {\n  spark_context(sc) %>% \n    invoke(\"textFile\", file, 1L) %>% \n    invoke(\"count\")\n}\n\nThe count_lines function takes a spark_connection (sc) argument which enables it to obtain a reference to the SparkContext object, and in turn call the textFile().count() method.\nYou can use this function with an existing sparklyr connection as follows:\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\ncount_lines(sc, \"hdfs://path/data.csv\")\n\nHere are links to some additional examples of extension packages:\n\n\n\nPackage\nDescription\n\n\n\n\nspark.sas7bdat\nRead in SAS data in parallel into Apache Spark.\n\n\nrsparkling\nExtension for using H2O machine learning algorithms against Spark Data Frames.\n\n\nsparkhello\nSimple example of including a custom JAR file within an extension package.\n\n\nrddlist\nImplements some methods of an R list as a Spark RDD (resilient distributed dataset).\n\n\nsparkwarc\nLoad WARC files into Apache Spark with sparklyr.\n\n\nsparkavro\nLoad Avro data into Spark with sparklyr. It is a wrapper of spark-avro\n\n\ncrassy\nConnect to Cassandra with sparklyr using the Spark-Cassandra-Connector.\n\n\nsparklygraphs\nR interface for GraphFrames which aims to provide the functionality of GraphX.\n\n\nsparklyr.nested\nExtension for working with nested data.\n\n\nsparklyudf\nSimple example registering an Scala UDF within an extension package.\n\n\nmleap\nR Interface to MLeap.\n\n\nsparkbq\nSparklyr extension package to connect to Google BigQuery.\n\n\nsparkgeo\nSparklyr extension package providing geospatial analytics capabilities.\n\n\nsparklytd\nSpaklyr plugin for td-spark to connect TD from R.\n\n\nsparkts\nExtensions for the spark-timeseries framework.\n\n\nsparkxgb\nR interface for XGBoost on Spark.\n\n\nsparktf\nR interface to Spark TensorFlow Connector.\n\n\ngeospark\nR interface to GeoSpark to perform spatial analysis in Spark.\n\n\nmmlspark\nMicrosoft Machine Learning for Apache Spark."
  },
  {
    "objectID": "guides/extensions.html#core-types",
    "href": "guides/extensions.html#core-types",
    "title": "Creating Extensions for sparklyr",
    "section": "Core Types",
    "text": "Core Types\nThree classes are defined for representing the fundamental types of the R to Java bridge:\n\n\n\nFunction\nDescription\n\n\n\n\nspark_connection\nConnection between R and the Spark shell process\n\n\nspark_jobj\nInstance of a remote Spark object\n\n\nspark_dataframe\nInstance of a remote Spark DataFrame object\n\n\n\nS3 methods are defined for each of these classes so they can be easily converted to or from objects that contain or wrap them. Note that for any given spark_jobj it’s possible to discover the underlying spark_connection."
  },
  {
    "objectID": "guides/extensions.html#calling-spark-from-r",
    "href": "guides/extensions.html#calling-spark-from-r",
    "title": "Creating Extensions for sparklyr",
    "section": "Calling Spark from R",
    "text": "Calling Spark from R\nThere are several functions available for calling the methods of Java objects and static methods of Java classes:\n\n\n\nFunction\nDescription\n\n\n\n\ninvoke\nCall a method on an object\n\n\ninvoke_new\nCreate a new object by invoking a constructor\n\n\ninvoke_static\nCall a static method on an object\n\n\n\nFor example, to create a new instance of the java.math.BigInteger class and then call the longValue() method on it you would use code like this:\n\nbillionBigInteger <- invoke_new(sc, \"java.math.BigInteger\", \"1000000000\")\nbillion <- invoke(billionBigInteger, \"longValue\")\n\nNote the sc argument: that’s the spark_connection object which is provided by the front-end package (e.g. sparklyr).\nThe previous example can be re-written to be more compact and clear using magrittr pipes:\n\nbillion <- sc %>% \n  invoke_new(\"java.math.BigInteger\", \"1000000000\") %>%\n    invoke(\"longValue\")\n\nThis syntax is similar to the method-chaining syntax often used with Scala code so is generally preferred.\nCalling a static method of a class is also straightforward. For example, to call the Math::hypot() static function you would use this code:\n\nhypot <- sc %>% \n  invoke_static(\"java.lang.Math\", \"hypot\", 10, 20)"
  },
  {
    "objectID": "guides/extensions.html#wrapper-functions",
    "href": "guides/extensions.html#wrapper-functions",
    "title": "Creating Extensions for sparklyr",
    "section": "Wrapper Functions",
    "text": "Wrapper Functions\nCreating an extension typically consists of writing R wrapper functions for a set of Spark services. In this section we’ll describe the typical form of these functions as well as how to handle special types like Spark DataFrames.\nHere’s the wrapper function for textFile().count() which we defined earlier:\n\ncount_lines <- function(sc, file) {\n  spark_context(sc) %>% \n    invoke(\"textFile\", file, 1L) %>% \n      invoke(\"count\")\n}\n\nThe count_lines function takes a spark_connection (sc) argument which enables it to obtain a reference to the SparkContext object, and in turn call the textFile().count() method.\nThe following functions are useful for implementing wrapper functions of various kinds:\n\n\n\nFunction\nDescription\n\n\n\n\nspark_connection\nGet the Spark connection associated with an object (S3)\n\n\nspark_jobj\nGet the Spark jobj associated with an object (S3)\n\n\nspark_dataframe\nGet the Spark DataFrame associated with an object (S3)\n\n\nspark_context\nGet the SparkContext for a spark_connection\n\n\nhive_context\nGet the HiveContext for a spark_connection\n\n\nspark_version\nGet the version of Spark (as a numeric_version) for a spark_connection\n\n\n\nThe use of these functions is illustrated in this simple example:\n\nanalyze <- function(x, features) {\n  \n  # normalize whatever we were passed (e.g. a dplyr tbl) into a DataFrame\n  df <- spark_dataframe(x)\n  \n  # get the underlying connection so we can create new objects\n  sc <- spark_connection(df)\n  \n  # create an object to do the analysis and call its `analyze` and `summary`\n  # methods (note that the df and features are passed to the analyze function)\n  summary <- sc %>%  \n    invoke_new(\"com.example.tools.Analyzer\") %>% \n      invoke(\"analyze\", df, features) %>% \n      invoke(\"summary\")\n\n  # return the results\n  summary\n}\n\nThe first argument is an object that can be accessed using the Spark DataFrame API (this might be an actual reference to a DataFrame or could rather be a dplyr tbl which has a DataFrame reference inside it).\nAfter using the spark_dataframe function to normalize the reference, we extract the underlying Spark connection associated with the data frame using the spark_connection function. Finally, we create a new Analyzer object, call it’s analyze method with the DataFrame and list of features, and then call the summary method on the results of the analysis.\nAccepting a spark_jobj or spark_dataframe as the first argument of a function makes it very easy to incorporate into magrittr pipelines so this pattern is highly recommended when possible."
  },
  {
    "objectID": "guides/extensions.html#dependencies",
    "href": "guides/extensions.html#dependencies",
    "title": "Creating Extensions for sparklyr",
    "section": "Dependencies",
    "text": "Dependencies\nWhen creating R packages which implement interfaces to Spark you may need to include additional dependencies. Your dependencies might be a set of Spark Packages or might be a custom JAR file. In either case, you’ll need a way to specify that these dependencies should be included during the initialization of a Spark session. A Spark dependency is defined using the spark_dependency function:\n\n\n\nFunction\nDescription\n\n\n\n\nspark_dependency\nDefine a Spark dependency consisting of JAR files and Spark packages\n\n\n\nYour extension package can specify it’s dependencies by implementing a function named spark_dependencies within the package (this function should not be publicly exported). For example, let’s say you were creating an extension package named sparkds that needs to include a custom JAR as well as the Redshift and Apache Avro packages:\n\nspark_dependencies <- function(spark_version, scala_version, ...) {\n  spark_dependency(\n    jars = c(\n      system.file(\n        sprintf(\"java/sparkds-%s-%s.jar\", spark_version, scala_version), \n        package = \"sparkds\"\n      )\n    ),\n    packages = c(\n      sprintf(\"com.databricks:spark-redshift_%s:0.6.0\", scala_version),\n      sprintf(\"com.databricks:spark-avro_%s:2.0.1\", scala_version)\n    )\n  )\n}\n\n.onLoad <- function(libname, pkgname) {\n  sparklyr::register_extension(pkgname)\n}\n\nThe spark_version argument is provided so that a package can support multiple Spark versions for it’s JARs. Note that the argument will include just the major and minor versions (e.g. 1.6 or 2.0) and will not include the patch level (as JARs built for a given major/minor version are expected to work for all patch levels).\nThe scala_version argument is provided so that a single package can support multiple Scala compiler versions for it’s JARs and packages (currently Scala 1.6 downloadable binaries are compiled with Scala 2.10 and Scala 2.0 downloadable binaries are compiled with Scala 2.11).\nThe ... argument is unused but nevertheless should be included to ensure compatibility if new arguments are added to spark_dependencies in the future.\nThe .onLoad function registers your extension package so that it’s spark_dependencies function will be automatically called when new connections to Spark are made via spark_connect:\n\nlibrary(sparklyr)\nlibrary(sparkds)\nsc <- spark_connect(master = \"local\")\n\n\nCompiling JARs\nThe sparklyr package includes a utility function (compile_package_jars) that will automatically compile a JAR file from your Scala source code for the required permutations of Spark and Scala compiler versions. To use the function just invoke it from the root directory of your R package as follows:\n\nsparklyr::compile_package_jars()\n\nNote that a prerequisite to calling compile_package_jars is the installation of the Scala 2.10 and 2.11 compilers to one of the following paths:\n\n/opt/scala\n/opt/local/scala\n/usr/local/scala\n~/scala (Windows-only)\n\nSee the sparkhello repository for a complete example of including a custom JAR within an extension package.\n\nCRAN\nWhen including a JAR file within an R package distributed on CRAN, you should follow the guidelines provided in Writing R Extensions:\n\nJava code is a special case: except for very small programs, .java files should be byte-compiled (to a .class file) and distributed as part of a .jar file: the conventional location for the .jar file(s) is inst/java. It is desirable (and required under an Open Source license) to make the Java source files available: this is best done in a top-level java directory in the package – the source files should not be installed."
  },
  {
    "objectID": "guides/extensions.html#data-types",
    "href": "guides/extensions.html#data-types",
    "title": "Creating Extensions for sparklyr",
    "section": "Data Types",
    "text": "Data Types\nThe ensure_* family of functions can be used to enforce specific data types that are passed to a Spark routine. For example, Spark routines that require an integer will not accept an R numeric element. Use these functions ensure certain parameters are scalar integers, or scalar doubles, and so on.\n\nensure_scalar_integer\nensure_scalar_double\nensure_scalar_boolean\nensure_scalar_character\n\nIn order to match the correct data types while calling Scala code from R, or retrieving results from Scala back to R, consider the following types mapping:\n\n\n\nFrom R\nScala\nTo R\n\n\n\n\nNULL\nvoid\nNULL\n\n\ninteger\nInt\ninteger\n\n\ncharacter\nString\ncharacter\n\n\nlogical\nBoolean\nlogical\n\n\ndouble\nDouble\ndouble\n\n\nnumeric\nDouble\ndouble\n\n\n\nFloat\ndouble\n\n\n\nDecimal\ndouble\n\n\n\nLong\ndouble\n\n\nraw\nArray[Byte]\nraw\n\n\nDate\nDate\nDate\n\n\nPOSIXlt\nTime\n\n\n\nPOSIXct\nTime\nPOSIXct\n\n\nlist\nArray[T]\nlist\n\n\nenvironment\nMap[String, T]\n\n\n\njobj\nObject\njobj"
  },
  {
    "objectID": "guides/extensions.html#compiling",
    "href": "guides/extensions.html#compiling",
    "title": "Creating Extensions for sparklyr",
    "section": "Compiling",
    "text": "Compiling\nMost Spark extensions won’t need to define their own compilation specification, and can instead rely on the default behavior of compile_package_jars. For users who would like to take more control over where the scalac compilers should be looked up, use the spark_compilation_spec fucnction. The Spark compilation specification is used when compiling Spark extension Java Archives, and defines which versions of Spark, as well as which versions of Scala, should be used for compilation."
  },
  {
    "objectID": "guides/h2o.html#overview",
    "href": "guides/h2o.html#overview",
    "title": "Sparkling Water (H2O) Machine Learning",
    "section": "Overview",
    "text": "Overview\nThe rsparkling extension package provides bindings to H2O’s distributed machine learning algorithms via sparklyr. In particular, rsparkling allows you to access the machine learning routines provided by the Sparkling Water Spark package.\nTogether with sparklyr’s dplyr interface, you can easily create and tune H2O machine learning workflows on Spark, orchestrated entirely within R.\nrsparkling provides a few simple conversion functions that allow the user to transfer data between Spark DataFrames and H2O Frames. Once the Spark DataFrames are available as H2O Frames, the h2o R interface can be used to train H2O machine learning algorithms on the data.\nA typical machine learning pipeline with rsparkling might be composed of the following stages. To fit a model, you might need to:\n\nPerform SQL queries through the sparklyr dplyr interface,\nUse the sdf_* and ft_* family of functions to generate new columns, or partition your data set,\nConvert your training, validation and/or test data frames into H2O Frames using the as_h2o_frame function,\nChoose an appropriate H2O machine learning algorithm to model your data,\nInspect the quality of your model fit, and use it to make predictions with new data."
  },
  {
    "objectID": "guides/h2o.html#installation",
    "href": "guides/h2o.html#installation",
    "title": "Sparkling Water (H2O) Machine Learning",
    "section": "Installation",
    "text": "Installation\nYou can install the rsparkling package from CRAN as follows:\ninstall.packages(\"rsparkling\")\nThen set the Sparkling Water version for rsparkling.:\noptions(rsparkling.sparklingwater.version = \"2.1.14\")\nFor Spark 2.0.x set rsparkling.sparklingwater.version to 2.0.3 instead, for Spark 1.6.2 use 1.6.8."
  },
  {
    "objectID": "guides/h2o.html#using-h2o",
    "href": "guides/h2o.html#using-h2o",
    "title": "Sparkling Water (H2O) Machine Learning",
    "section": "Using H2O",
    "text": "Using H2O\nNow let’s walk through a simple example to demonstrate the use of H2O’s machine learning algorithms within R. We’ll use h2o.glm to fit a linear regression model. Using the built-in mtcars dataset, we’ll try to predict a car’s fuel consumption (mpg) based on its weight (wt), and the number of cylinders the engine contains (cyl).\nFirst, we will initialize a local Spark connection, and copy the mtcars dataset into Spark.\nlibrary(rsparkling)\nlibrary(sparklyr)\nlibrary(h2o)\nlibrary(dplyr)\n\nsc <- spark_connect(\"local\", version = \"2.1.0\")\n\nmtcars_tbl <- copy_to(sc, mtcars, \"mtcars\")\nNow, let’s perform some simple transformations – we’ll\n\nRemove all cars with horsepower less than 100,\nProduce a column encoding whether a car has 8 cylinders or not,\nPartition the data into separate training and test data sets,\nFit a model to our training data set,\nEvaluate our predictive performance on our test dataset.\n\n# transform our data set, and then partition into 'training', 'test'\npartitions <- mtcars_tbl %>%\n  filter(hp >= 100) %>%\n  mutate(cyl8 = cyl == 8) %>%\n  sdf_partition(training = 0.5, test = 0.5, seed = 1099)\nNow, we convert our training and test sets into H2O Frames using rsparkling conversion functions. We have already split the data into training and test frames using dplyr.\ntraining <- as_h2o_frame(sc, partitions$training, strict_version_check = FALSE)\ntest <- as_h2o_frame(sc, partitions$test, strict_version_check = FALSE)\nAlternatively, we can use the h2o.splitFrame() function instead of sdf_partition() to partition the data within H2O instead of Spark (e.g. partitions <- h2o.splitFrame(as_h2o_frame(mtcars_tbl), 0.5))\n# fit a linear model to the training dataset\nglm_model <- h2o.glm(x = c(\"wt\", \"cyl\"), \n                     y = \"mpg\", \n                     training_frame = training,\n                     lambda_search = TRUE)\nFor linear regression models produced by H2O, we can use either print() or summary() to learn a bit more about the quality of our fit. The summary() method returns some extra information about scoring history and variable importance.\nglm_model\n## Model Details:\n## ==============\n## \n## H2ORegressionModel: glm\n## Model ID:  GLM_model_R_1510348062048_1 \n## GLM Model: summary\n##     family     link                               regularization\n## 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.05468 )\n##                                                                 lambda_search\n## 1 nlambda = 100, lambda.max = 5.4682, lambda.min = 0.05468, lambda.1se = -1.0\n##   number_of_predictors_total number_of_active_predictors\n## 1                          2                           2\n##   number_of_iterations                                training_frame\n## 1                  100 frame_rdd_32_929e407384e0082416acd4c9897144a0\n## \n## Coefficients: glm coefficients\n##       names coefficients standardized_coefficients\n## 1 Intercept    32.997281                 16.625000\n## 2       cyl    -0.906688                 -1.349195\n## 3        wt    -2.712562                 -2.282649\n## \n## H2ORegressionMetrics: glm\n## ** Reported on training data. **\n## \n## MSE:  2.03293\n## RMSE:  1.425808\n## MAE:  1.306314\n## RMSLE:  0.08238032\n## Mean Residual Deviance :  2.03293\n## R^2 :  0.8265696\n## Null Deviance :93.775\n## Null D.o.F. :7\n## Residual Deviance :16.26344\n## Residual D.o.F. :5\n## AIC :36.37884\nThe output suggests that our model is a fairly good fit, and that both a cars weight, as well as the number of cylinders in its engine, will be powerful predictors of its average fuel consumption. (The model suggests that, on average, heavier cars consume more fuel.)\nLet’s use our H2O model fit to predict the average fuel consumption on our test data set, and compare the predicted response with the true measured fuel consumption. We’ll build a simple ggplot2 plot that will allow us to inspect the quality of our predictions.\nlibrary(ggplot2)\n\n# compute predicted values on our test dataset\npred <- h2o.predict(glm_model, newdata = test)\n# convert from H2O Frame to Spark DataFrame\npredicted <- as_spark_dataframe(sc, pred, strict_version_check = FALSE)\n\n# extract the true 'mpg' values from our test dataset\nactual <- partitions$test %>%\n  select(mpg) %>%\n  collect() %>%\n  `[[`(\"mpg\")\n\n# produce a data.frame housing our predicted + actual 'mpg' values\ndata <- data.frame(\n  predicted = predicted,\n  actual    = actual\n)\n# a bug in data.frame does not set colnames properly; reset here \nnames(data) <- c(\"predicted\", \"actual\")\n\n# plot predicted vs. actual values\nggplot(data, aes(x = actual, y = predicted)) +\n  geom_abline(lty = \"dashed\", col = \"red\") +\n  geom_point() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  coord_fixed(ratio = 1) +\n  labs(\n    x = \"Actual Fuel Consumption\",\n    y = \"Predicted Fuel Consumption\",\n    title = \"Predicted vs. Actual Fuel Consumption\"\n  )\n\nAlthough simple, our model appears to do a fairly good job of predicting a car’s average fuel consumption.\nAs you can see, we can easily and effectively combine dplyr data transformation pipelines with the machine learning algorithms provided by H2O’s Sparkling Water."
  },
  {
    "objectID": "guides/h2o.html#algorithms",
    "href": "guides/h2o.html#algorithms",
    "title": "Sparkling Water (H2O) Machine Learning",
    "section": "Algorithms",
    "text": "Algorithms\nOnce the H2OContext is made available to Spark (as demonstrated below), all of the functions in the standard h2o R interface can be used with H2O Frames (converted from Spark DataFrames). Here is a table of the available algorithms:\n\n\n\nFunction\nDescription\n\n\n\n\nh2o.glm\nGeneralized Linear Model\n\n\nh2o.deeplearning\nMultilayer Perceptron\n\n\nh2o.randomForest\nRandom Forest\n\n\nh2o.gbm\nGradient Boosting Machine\n\n\nh2o.naiveBayes\nNaive-Bayes\n\n\nh2o.prcomp\nPrincipal Components Analysis\n\n\nh2o.svd\nSingular Value Decomposition\n\n\nh2o.glrm\nGeneralized Low Rank Model\n\n\nh2o.kmeans\nK-Means Clustering\n\n\nh2o.anomaly\nAnomaly Detection via Deep Learning Autoencoder\n\n\n\nAdditionally, the h2oEnsemble R package can be used to generate Super Learner ensembles of H2O algorithms:\n\n\n\nFunction\nDescription\n\n\n\n\nh2o.ensemble\nSuper Learner / Stacking\n\n\nh2o.stack\nSuper Learner / Stacking"
  },
  {
    "objectID": "guides/h2o.html#transformers",
    "href": "guides/h2o.html#transformers",
    "title": "Sparkling Water (H2O) Machine Learning",
    "section": "Transformers",
    "text": "Transformers\nA model is often fit not on a dataset as-is, but instead on some transformation of that dataset. Spark provides feature transformers, facilitating many common transformations of data within a Spark DataFrame, and sparklyr exposes these within the ft_* family of functions. Transformers can be used on Spark DataFrames, and the final training set can be sent to the H2O cluster for machine learning.\n\n\n\n\n\n\n\n\nFunction\n\n\nDescription\n\n\n\n\n\n\nft_binarizer\n\n\nThreshold numerical features to binary (0/1) feature\n\n\n\n\nft_bucketizer\n\n\nBucketizer transforms a column of continuous features to a column of feature buckets\n\n\n\n\nft_discrete_cosine_transform\n\n\nTransforms a length NN real-valued sequence in the time domain into another length NN real-valued sequence in the frequency domain\n\n\n\n\nft_elementwise_product\n\n\nMultiplies each input vector by a provided weight vector, using element-wise multiplication.\n\n\n\n\nft_index_to_string\n\n\nMaps a column of label indices back to a column containing the original labels as strings\n\n\n\n\nft_quantile_discretizer\n\n\nTakes a column with continuous features and outputs a column with binned categorical features\n\n\n\n\nft_sql_transformer\n\n\nImplements the transformations which are defined by a SQL statement\n\n\n\n\nft_string_indexer\n\n\nEncodes a string column of labels to a column of label indices\n\n\n\n\nft_vector_assembler\n\n\nCombines a given list of columns into a single vector column"
  },
  {
    "objectID": "guides/h2o.html#examples",
    "href": "guides/h2o.html#examples",
    "title": "Sparkling Water (H2O) Machine Learning",
    "section": "Examples",
    "text": "Examples\nWe will use the iris data set to examine a handful of learning algorithms and transformers. The iris data set measures attributes for 150 flowers in 3 different species of iris.\niris_tbl <- copy_to(sc, iris, \"iris\", overwrite = TRUE)\niris_tbl\n## # Source:   table<iris> [?? x 5]\n## # Database: spark_connection\n##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl>   <chr>\n##  1          5.1         3.5          1.4         0.2  setosa\n##  2          4.9         3.0          1.4         0.2  setosa\n##  3          4.7         3.2          1.3         0.2  setosa\n##  4          4.6         3.1          1.5         0.2  setosa\n##  5          5.0         3.6          1.4         0.2  setosa\n##  6          5.4         3.9          1.7         0.4  setosa\n##  7          4.6         3.4          1.4         0.3  setosa\n##  8          5.0         3.4          1.5         0.2  setosa\n##  9          4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## # ... with more rows\nConvert to an H2O Frame:\niris_hf <- as_h2o_frame(sc, iris_tbl, strict_version_check = FALSE)\n\nK-Means Clustering\nUse H2O’s K-means clustering to partition a dataset into groups. K-means clustering partitions points into k groups, such that the sum of squares from points to the assigned cluster centers is minimized.\nkmeans_model <- h2o.kmeans(training_frame = iris_hf, \n                           x = 3:4,\n                           k = 3,\n                           seed = 1)\nTo look at particular metrics of the K-means model, we can use h2o.centroid_stats() and h2o.centers() or simply print out all the model metrics using print(kmeans_model).\n# print the cluster centers\nh2o.centers(kmeans_model)\n##   petal_length petal_width\n## 1     1.462000     0.24600\n## 2     5.566667     2.05625\n## 3     4.296154     1.32500\n# print the centroid statistics\nh2o.centroid_stats(kmeans_model)\n## Centroid Statistics: \n##   centroid     size within_cluster_sum_of_squares\n## 1        1 50.00000                       1.41087\n## 2        2 48.00000                       9.29317\n## 3        3 52.00000                       7.20274\n\n\nPCA\nUse H2O’s Principal Components Analysis (PCA) to perform dimensionality reduction. PCA is a statistical method to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate in turn has the largest variance possible.\npca_model <- h2o.prcomp(training_frame = iris_hf,\n                        x = 1:4,\n                        k = 4,\n                        seed = 1)\n## Warning in doTryCatch(return(expr), name, parentenv, handler): _train:\n## Dataset used may contain fewer number of rows due to removal of rows with\n## NA/missing values. If this is not desirable, set impute_missing argument in\n## pca call to TRUE/True/true/... depending on the client language.\npca_model\n## Model Details:\n## ==============\n## \n## H2ODimReductionModel: pca\n## Model ID:  PCA_model_R_1510348062048_3 \n## Importance of components: \n##                             pc1      pc2      pc3      pc4\n## Standard deviation     7.861342 1.455041 0.283531 0.154411\n## Proportion of Variance 0.965303 0.033069 0.001256 0.000372\n## Cumulative Proportion  0.965303 0.998372 0.999628 1.000000\n## \n## \n## H2ODimReductionMetrics: pca\n## \n## No model metrics available for PCA\n\n\nRandom Forest\nUse H2O’s Random Forest to perform regression or classification on a dataset. We will continue to use the iris dataset as an example for this problem.\nAs usual, we define the response and predictor variables using the x and y arguments. Since we’d like to do a classification, we need to ensure that the response column is encoded as a factor (enum) column.\ny <- \"Species\"\nx <- setdiff(names(iris_hf), y)\niris_hf[,y] <- as.factor(iris_hf[,y])\nWe can split the iris_hf H2O Frame into a train and test set (the split defaults to 75/25 train/test).\nsplits <- h2o.splitFrame(iris_hf, seed = 1)\nThen we can train a Random Forest model:\nrf_model <- h2o.randomForest(x = x, \n                             y = y,\n                             training_frame = splits[[1]],\n                             validation_frame = splits[[2]],\n                             nbins = 32,\n                             max_depth = 5,\n                             ntrees = 20,\n                             seed = 1)\nSince we passed a validation frame, the validation metrics will be calculated. We can retrieve individual metrics using functions such as h2o.mse(rf_model, valid = TRUE). The confusion matrix can be printed using the following:\nh2o.confusionMatrix(rf_model, valid = TRUE)\n## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n##            setosa versicolor virginica  Error     Rate\n## setosa          7          0         0 0.0000 =  0 / 7\n## versicolor      0         13         0 0.0000 = 0 / 13\n## virginica       0          1        10 0.0909 = 1 / 11\n## Totals          7         14        10 0.0323 = 1 / 31\nTo view the variable importance computed from an H2O model, you can use either the h2o.varimp() or h2o.varimp_plot() functions:\nh2o.varimp_plot(rf_model)\n\n\n\nGradient Boosting Machine\nThe Gradient Boosting Machine (GBM) is one of H2O’s most popular algorithms, as it works well on many types of data. We will continue to use the iris dataset as an example for this problem.\nUsing the same dataset and x and y from above, we can train a GBM:\ngbm_model <- h2o.gbm(x = x, \n                     y = y,\n                     training_frame = splits[[1]],\n                     validation_frame = splits[[2]],                     \n                     ntrees = 20,\n                     max_depth = 3,\n                     learn_rate = 0.01,\n                     col_sample_rate = 0.7,\n                     seed = 1)\nSince this is a multi-class problem, we may be interested in inspecting the confusion matrix on a hold-out set. Since we passed along a validatin_frame at train time, the validation metrics are already computed and we just need to retreive them from the model object.\nh2o.confusionMatrix(gbm_model, valid = TRUE)\n## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n##            setosa versicolor virginica  Error     Rate\n## setosa          7          0         0 0.0000 =  0 / 7\n## versicolor      0         13         0 0.0000 = 0 / 13\n## virginica       0          1        10 0.0909 = 1 / 11\n## Totals          7         14        10 0.0323 = 1 / 31\n\n\nDeep Learning\nUse H2O’s Deep Learning to perform regression or classification on a dataset, extact non-linear features generated by the deep neural network, and/or detect anomalies using a deep learning model with auto-encoding.\nIn this example, we will use the prostate dataset available within the h2o package:\npath <- system.file(\"extdata\", \"prostate.csv\", package = \"h2o\")\nprostate_df <- spark_read_csv(sc, \"prostate\", path)\nhead(prostate_df)\n## # Source:   lazy query [?? x 9]\n## # Database: spark_connection\n##      ID CAPSULE   AGE  RACE DPROS DCAPS   PSA   VOL GLEASON\n##   <int>   <int> <int> <int> <int> <int> <dbl> <dbl>   <int>\n## 1     1       0    65     1     2     1   1.4   0.0       6\n## 2     2       0    72     1     3     2   6.7   0.0       7\n## 3     3       0    70     1     1     2   4.9   0.0       6\n## 4     4       0    76     2     2     1  51.2  20.0       7\n## 5     5       0    69     1     1     1  12.3  55.9       6\n## 6     6       1    71     1     3     2   3.3   0.0       8\nOnce we’ve done whatever data manipulation is required to run our model we’ll get a reference to it as an h2o frame then split it into training and test sets using the h2o.splitFrame function:\nprostate_hf <- as_h2o_frame(sc, prostate_df, strict_version_check = FALSE)\nsplits <- h2o.splitFrame(prostate_hf, seed = 1)\nNext we define the response and predictor columns.\ny <- \"VOL\"\n#remove response and ID cols\nx <- setdiff(names(prostate_hf), c(\"ID\", y))\nNow we can train a deep neural net.\ndl_fit <- h2o.deeplearning(x = x, y = y,\n                           training_frame = splits[[1]],\n                           epochs = 15,\n                           activation = \"Rectifier\",\n                           hidden = c(10, 5, 10),\n                           input_dropout_ratio = 0.7)\nEvaluate performance on a test set:\nh2o.performance(dl_fit, newdata = splits[[2]])\n## H2ORegressionMetrics: deeplearning\n## \n## MSE:  253.7022\n## RMSE:  15.92803\n## MAE:  12.90077\n## RMSLE:  1.885052\n## Mean Residual Deviance :  253.7022\nNote that the above metrics are not reproducible when H2O’s Deep Learning is run on multiple cores, however, the metrics should be fairly stable across repeat runs.\n\n\nGrid Search\nH2O’s grid search capabilities currently supports traditional (Cartesian) grid search and random grid search. Grid search in R provides the following capabilities:\n\nH2OGrid class: Represents the results of the grid search\nh2o.getGrid(<grid_id>, sort_by, decreasing): Display the specified grid\nh2o.grid: Start a new grid search parameterized by\n\nmodel builder name (e.g., algorithm = \"gbm\")\nmodel parameters (e.g., ntrees = 100)\nhyper_parameters: attribute for passing a list of hyper parameters (e.g., list(ntrees=c(1,100), learn_rate=c(0.1,0.001)))\nsearch_criteria: optional attribute for specifying more a advanced search strategy\n\n\n\nCartesian Grid Search\nBy default, h2o.grid() will train a Cartesian grid search – meaning, all possible models in the specified grid. In this example, we will re-use the prostate data as an example dataset for a regression problem.\nsplits <- h2o.splitFrame(prostate_hf, seed = 1)\n\ny <- \"VOL\"\n#remove response and ID cols\nx <- setdiff(names(prostate_hf), c(\"ID\", y))\nAfter prepping the data, we define a grid and execute the grid search.\n# GBM hyperparamters\ngbm_params1 <- list(learn_rate = c(0.01, 0.1),\n                    max_depth = c(3, 5, 9),\n                    sample_rate = c(0.8, 1.0),\n                    col_sample_rate = c(0.2, 0.5, 1.0))\n\n# Train and validate a grid of GBMs\ngbm_grid1 <- h2o.grid(\"gbm\", x = x, y = y,\n                      grid_id = \"gbm_grid1\",\n                      training_frame = splits[[1]],\n                      validation_frame = splits[[1]],\n                      ntrees = 100,\n                      seed = 1,\n                      hyper_params = gbm_params1)\n\n# Get the grid results, sorted by validation MSE\ngbm_gridperf1 <- h2o.getGrid(grid_id = \"gbm_grid1\", \n                             sort_by = \"mse\", \n                             decreasing = FALSE)\ngbm_gridperf1\n## H2O Grid Details\n## ================\n## \n## Grid ID: gbm_grid1 \n## Used hyper parameters: \n##   -  col_sample_rate \n##   -  learn_rate \n##   -  max_depth \n##   -  sample_rate \n## Number of models: 36 \n## Number of failed models: 0 \n## \n## Hyper-Parameter Search Summary: ordered by increasing mse\n##   col_sample_rate learn_rate max_depth sample_rate          model_ids\n## 1             1.0        0.1         9         1.0 gbm_grid1_model_35\n## 2             0.5        0.1         9         1.0 gbm_grid1_model_34\n## 3             1.0        0.1         9         0.8 gbm_grid1_model_17\n## 4             0.5        0.1         9         0.8 gbm_grid1_model_16\n## 5             1.0        0.1         5         0.8 gbm_grid1_model_11\n##                  mse\n## 1  88.10947523138782\n## 2  102.3118989994892\n## 3 102.78632321923726\n## 4  126.4217260351778\n## 5  149.6066650109763\n## \n## ---\n##    col_sample_rate learn_rate max_depth sample_rate          model_ids\n## 31             0.5       0.01         3         0.8  gbm_grid1_model_1\n## 32             0.2       0.01         5         1.0 gbm_grid1_model_24\n## 33             0.5       0.01         3         1.0 gbm_grid1_model_19\n## 34             0.2       0.01         5         0.8  gbm_grid1_model_6\n## 35             0.2       0.01         3         1.0 gbm_grid1_model_18\n## 36             0.2       0.01         3         0.8  gbm_grid1_model_0\n##                   mse\n## 31  324.8117304723162\n## 32 325.10992525687294\n## 33 325.27898443785045\n## 34 329.36983845305735\n## 35 338.54411936919456\n## 36  339.7744828617712\n\n\nRandom Grid Search\nH2O’s Random Grid Search samples from the given parameter space until a set of constraints is met. The user can specify the total number of desired models using (e.g. max_models = 40), the amount of time (e.g. max_runtime_secs = 1000), or tell the grid to stop after performance stops improving by a specified amount. Random Grid Search is a practical way to arrive at a good model without too much effort.\nThe example below is set to run fairly quickly – increase max_runtime_secs or max_models to cover more of the hyperparameter space in your grid search. Also, you can expand the hyperparameter space of each of the algorithms by modifying the definition of hyper_param below.\n# GBM hyperparamters\ngbm_params2 <- list(learn_rate = seq(0.01, 0.1, 0.01),\n                    max_depth = seq(2, 10, 1),\n                    sample_rate = seq(0.5, 1.0, 0.1),\n                    col_sample_rate = seq(0.1, 1.0, 0.1))\nsearch_criteria2 <- list(strategy = \"RandomDiscrete\", \n                         max_models = 50)\n\n# Train and validate a grid of GBMs\ngbm_grid2 <- h2o.grid(\"gbm\", x = x, y = y,\n                      grid_id = \"gbm_grid2\",\n                      training_frame = splits[[1]],\n                      validation_frame = splits[[2]],\n                      ntrees = 100,\n                      seed = 1,\n                      hyper_params = gbm_params2,\n                      search_criteria = search_criteria2)\n\n# Get the grid results, sorted by validation MSE\ngbm_gridperf2 <- h2o.getGrid(grid_id = \"gbm_grid2\", \n                             sort_by = \"mse\", \n                             decreasing = FALSE)\nTo get the best model, as measured by validation MSE, we simply grab the first row of the gbm_gridperf2@summary_table object, since this table is already sorted such that the lowest MSE model is on top.\ngbm_gridperf2@summary_table[1,]\n## Hyper-Parameter Search Summary: ordered by increasing mse\n##   col_sample_rate learn_rate max_depth sample_rate          model_ids\n## 1             0.8       0.01         2         0.7 gbm_grid2_model_35\n##                  mse\n## 1 244.61196951586288\nIn the examples above, we generated two different grids, specified by grid_id. The first grid was called grid_id = \"gbm_grid1\" and the second was called grid_id = \"gbm_grid2\". However, if we are using the same dataset & algorithm in two grid searches, it probably makes more sense just to add the results of the second grid search to the first. If you want to add models to an existing grid, rather than create a new one, you simply re-use the same grid_id."
  },
  {
    "objectID": "guides/h2o.html#exporting-models",
    "href": "guides/h2o.html#exporting-models",
    "title": "Sparkling Water (H2O) Machine Learning",
    "section": "Exporting Models",
    "text": "Exporting Models\nThere are two ways of exporting models from H2O – saving models as a binary file, or saving models as pure Java code.\n\nBinary Models\nThe more traditional method is to save a binary model file to disk using the h2o.saveModel() function. To load the models using h2o.loadModel(), the same version of H2O that generated the models is required. This method is commonly used when H2O is being used in a non-production setting.\nA binary model can be saved as follows:\nh2o.saveModel(my_model, path = \"/Users/me/h2omodels\")\n\n\nJava (POJO) Models\nOne of the most valuable features of H2O is it’s ability to export models as pure Java code, or rather, a “Plain Old Java Object” (POJO). You can learn more about H2O POJO models in this POJO quickstart guide. The POJO method is used most commonly when a model is deployed in a production setting. POJO models are ideal for when you need very fast prediction response times, and minimal requirements – the POJO is a standalone Java class with no dependencies on the full H2O stack.\nTo generate the POJO for your model, use the following command:\nh2o.download_pojo(my_model, path = \"/Users/me/h2omodels\")\nFinally, disconnect with:\nspark_disconnect_all()\n## [1] 1\nYou can learn more about how to take H2O models to production in the productionizing H2O models section of the H2O docs."
  },
  {
    "objectID": "guides/h2o.html#additional-resources",
    "href": "guides/h2o.html#additional-resources",
    "title": "Sparkling Water (H2O) Machine Learning",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nMain documentation site for Sparkling Water (and all H2O software projects)\nH2O.ai website\n\nIf you are new to H2O for machine learning, we recommend you start with the Intro to H2O Tutorial, followed by the H2O Grid Search & Model Selection Tutorial. There are a number of other H2O R tutorials and demos available, as well as the H2O World 2015 Training Gitbook, and the Machine Learning with R and H2O Booklet (pdf)."
  },
  {
    "objectID": "guides/index.html",
    "href": "guides/index.html",
    "title": "Guides",
    "section": "",
    "text": "Interacting with Spark\n\nUsing dplyr commands\n\nUnderstanding Spark Caching\nSpark connection options\nAccess AWS S3 Buckets\n\n\n\nModeling and Machine Learning\n\nSpark ML Overview\nML Pipelines\nUsing H2O in sparklyr\n\n\n\n\n\n\n\nNon-rectangular Data\n\nStream Data\nText Mining\n\n\n\nAdvanced\n\nRun R code in Spark\nCreate sparklyr extensions"
  },
  {
    "objectID": "guides/mlib.html#overview",
    "href": "guides/mlib.html#overview",
    "title": "Spark Machine Learning Library (MLlib)",
    "section": "Overview",
    "text": "Overview\nsparklyr provides bindings to Spark’s distributed machine learning library. In particular, sparklyr allows you to access the machine learning routines provided by the spark.ml package. Together with sparklyr’s dplyr interface, you can easily create and tune machine learning workflows on Spark, orchestrated entirely within R.\nsparklyr provides three families of functions that you can use with Spark machine learning:\n\nMachine learning algorithms for analyzing data (ml_*)\nFeature transformers for manipulating individual features (ft_*)\nFunctions for manipulating Spark DataFrames (sdf_*)\n\nAn analytic workflow with sparklyr might be composed of the following stages. For an example see Example Workflow.\n\nPerform SQL queries through the sparklyr dplyr interface\nUse the sdf_* and ft_* family of functions to generate new columns, or partition your data set\nChoose an appropriate machine learning algorithm from the ml_* family of functions to model your data\nInspect the quality of your model fit, and use it to make predictions with new data.\nCollect the results for visualization and further analysis in R"
  },
  {
    "objectID": "guides/mlib.html#algorithms",
    "href": "guides/mlib.html#algorithms",
    "title": "Spark Machine Learning Library (MLlib)",
    "section": "Algorithms",
    "text": "Algorithms\nSpark’s machine learning library can be accessed from sparklyr through the ml_* set of functions. Visit the sparklyr reference page to see the complete list of available algorithms: Reference - Spark Machine Learning\n\nFormulas\nThe ml_* functions take the arguments response and features. But features can also be a formula with main effects (it currently does not accept interaction terms). The intercept term can be omitted by using -1.\nThe following two statements are equivalent:\nml_linear_regression(z ~ -1 + x + y)\nml_linear_regression(intercept = FALSE, response = \"z\", features = c(\"x\", \"y\"))\n\n\nOptions\nThe Spark model output can be modified with the ml_options argument in the ml_* functions. The ml_options is an experts only interface for tweaking the model output. For example, model.transform can be used to mutate the Spark model object before the fit is performed."
  },
  {
    "objectID": "guides/mlib.html#transformers",
    "href": "guides/mlib.html#transformers",
    "title": "Spark Machine Learning Library (MLlib)",
    "section": "Transformers",
    "text": "Transformers\nA model is often fit not on a data set as-is, but instead on some transformation of that data set. Spark provides feature transformers, facilitating many common transformations of data within a Spark DataFrame, and sparklyr exposes these within the ft_* family of functions. These routines generally take one or more input columns, and generate a new output column formed as a transformation of those columns. Visit the sparklyr reference page to see the complete list of available transformers: Reference - Feature Transformers"
  },
  {
    "objectID": "guides/mlib.html#examples",
    "href": "guides/mlib.html#examples",
    "title": "Spark Machine Learning Library (MLlib)",
    "section": "Examples",
    "text": "Examples\nWe will use the iris data set to examine a handful of learning algorithms and transformers. The iris data set measures attributes for 150 flowers in 3 different species of iris.\n\nlibrary(sparklyr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\")\n\niris_tbl <- copy_to(sc, iris, \"iris\", overwrite = TRUE)\n\niris_tbl\n\n# Source: spark<iris> [?? x 5]\n   Sepal_Length Sepal_Width Petal_Length Petal_Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <chr>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with more rows\n\n\n\nK-Means Clustering\nUse Spark’s K-means clustering to partition a dataset into groups. K-means clustering partitions points into k groups, such that the sum of squares from points to the assigned cluster centers is minimized.\n\nkmeans_model <- iris_tbl %>%\n  ml_kmeans(k = 3, features = c(\"Petal_Length\", \"Petal_Width\"))\n\nkmeans_model\n\nK-means clustering with 3 clusters\n\nCluster centers:\n  Petal_Length Petal_Width\n1     5.626087    2.047826\n2     1.462000    0.246000\n3     4.292593    1.359259\n\nWithin Set Sum of Squared Errors =  not computed.\n\n\nRun and collect predictions into R:\n\npredicted <- ml_predict(kmeans_model, iris_tbl) %>%\n  collect()\n\ntable(predicted$Species, predicted$prediction)\n\n            \n              0  1  2\n  setosa      0 50  0\n  versicolor  2  0 48\n  virginica  44  0  6\n\n\nUse the collected data to plot the results:\n\npredicted %>%\n  ggplot(aes(Petal_Length, Petal_Width)) +\n  geom_point(aes(Petal_Width, Petal_Length, col = factor(prediction + 1)),\n    size = 2, alpha = 0.5\n  ) +\n  geom_point(\n    data = kmeans_model$centers, aes(Petal_Width, Petal_Length),\n    col = scales::muted(c(\"red\", \"green\", \"blue\")),\n    pch = \"x\", size = 12\n  ) +\n  scale_color_discrete(\n    name = \"Predicted Cluster\",\n    labels = paste(\"Cluster\", 1:3)\n  ) +\n  labs(\n    x = \"Petal Length\",\n    y = \"Petal Width\",\n    title = \"K-Means Clustering\",\n    subtitle = \"Use Spark.ML to predict cluster membership with the iris dataset.\"\n  )\n\n\n\n\n\n\nLinear Regression\nUse Spark’s linear regression to model the linear relationship between a response variable and one or more explanatory variables.\n\nlm_model <- iris_tbl %>%\n  ml_linear_regression(Petal_Length ~ Petal_Width)\n\nExtract the slope and the intercept into discrete R variables. We will use them to plot:\n\nspark_slope <- coef(lm_model)[[\"Petal_Width\"]]\nspark_intercept <- coef(lm_model)[[\"(Intercept)\"]]\n\n\niris_tbl %>%\n  select(Petal_Width, Petal_Length) %>%\n  collect() %>%\n  ggplot(aes(Petal_Length, Petal_Width)) +\n  geom_point(aes(Petal_Width, Petal_Length), size = 2, alpha = 0.5) +\n  geom_abline(aes(\n    slope = spark_slope,\n    intercept = spark_intercept\n  ),\n  color = \"red\"\n  ) +\n  labs(\n    x = \"Petal Width\",\n    y = \"Petal Length\",\n    title = \"Linear Regression: Petal Length ~ Petal Width\",\n    subtitle = \"Use Spark.ML linear regression to predict petal length as a function of petal width.\"\n  )\n\n\n\n\n\n\nLogistic Regression\nUse Spark’s logistic regression to perform logistic regression, modeling a binary outcome as a function of one or more explanatory variables.\n\nglm_model <- iris_tbl %>% \n  mutate(is_setosa = ifelse(Species == \"setosa\", 1, 0)) %>% \n  select_if(is.numeric) %>% \n  ml_logistic_regression(is_setosa ~.)\n\nsummary(glm_model)\n\nCoefficients:\n (Intercept) Sepal_Length  Sepal_Width Petal_Length  Petal_Width \n -0.02904898  -7.23312634  28.56334798  -9.02580864 -20.62238442 \n\n\n\nml_predict(glm_model, iris_tbl) %>% \n  count(Species, prediction) \n\n# Source: spark<?> [?? x 3]\n# Groups: Species\n  Species    prediction     n\n  <chr>           <dbl> <dbl>\n1 virginica           0    50\n2 versicolor          0    50\n3 setosa              1    50\n\n\n\n\nPCA\nUse Spark’s Principal Components Analysis (PCA) to perform dimensionality reduction. PCA is a statistical method to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate in turn has the largest variance possible.\n\npca_model <- tbl(sc, \"iris\") %>%\n  select(-Species) %>%\n  ml_pca()\n\npca_model\n\nExplained variance:\n\n        PC1         PC2         PC3         PC4 \n0.924618723 0.053066483 0.017102610 0.005212184 \n\nRotation:\n                     PC1         PC2         PC3        PC4\nSepal_Length -0.36138659 -0.65658877  0.58202985  0.3154872\nSepal_Width   0.08452251 -0.73016143 -0.59791083 -0.3197231\nPetal_Length -0.85667061  0.17337266 -0.07623608 -0.4798390\nPetal_Width  -0.35828920  0.07548102 -0.54583143  0.7536574\n\n\n\n\nRandom Forest\nUse Spark’s Random Forest to perform regression or multiclass classification.\n\nrf_model <- iris_tbl %>%\n  ml_random_forest(\n    Species ~ Petal_Length + Petal_Width, type = \"classification\"\n    )\n\nUse ml_predict() to use the apply the new model back to the data.\n\nrf_predict <- ml_predict(rf_model, iris_tbl) \n\nglimpse(rf_predict)\n\nRows: ??\n\n\nWarning in sdf_collect_static(object, impl, ...): NAs introduced by coercion to\ninteger range\n\n\nColumns: 14\nDatabase: spark_connection\n$ Sepal_Length           <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.…\n$ Sepal_Width            <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.…\n$ Petal_Length           <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.…\n$ Petal_Width            <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.…\n$ Species                <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…\n$ features               <list> <1.4, 0.2>, <1.4, 0.2>, <1.3, 0.2>, <1.5, 0.2>…\n$ label                  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ rawPrediction          <list> <20, 0, 0>, <20, 0, 0>, <20, 0, 0>, <20, 0, 0>…\n$ probability            <list> <1, 0, 0>, <1, 0, 0>, <1, 0, 0>, <1, 0, 0>, <1…\n$ prediction             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ predicted_label        <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa…\n$ probability_setosa     <dbl> 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.…\n$ probability_versicolor <dbl> 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.…\n$ probability_virginica  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\nTo get an idea of the model effectiveness, use count() to compare species against the prediction. ml_predict() created a variable called predicted_label. That variable contains the string value of the prediction:\n\nrf_predict %>% \n  count(Species, predicted_label) \n\n# Source: spark<?> [?? x 3]\n# Groups: Species\n  Species    predicted_label     n\n  <chr>      <chr>           <dbl>\n1 setosa     setosa             50\n2 versicolor virginica           1\n3 virginica  virginica          50\n4 versicolor versicolor         49\n\n\n\n\nFT String Indexing\nUse ft_string_indexer() and ft_index_to_string() to convert a character column into a numeric column and back again.\n\nft_string2idx <- iris_tbl %>%\n  ft_string_indexer(\"Species\", \"Species_idx\") %>%\n  ft_index_to_string(\"Species_idx\", \"Species_remap\") %>% \n  select(Species, Species_remap, Species_idx)\n\nTo see the value assigned to each value in Species, we can pull the aggregates of all the species, re-mapped species and index combinations:\n\nft_string2idx %>% \n  group_by_all() %>% \n  summarise(count = n(), .groups = \"keep\")\n\n# Source: spark<?> [?? x 4]\n# Groups: Species, Species_remap, Species_idx\n  Species    Species_remap Species_idx count\n  <chr>      <chr>               <dbl> <dbl>\n1 setosa     setosa                  0    50\n2 versicolor versicolor              1    50\n3 virginica  virginica               2    50\n\n\n\n\nSDF Partitioning\nSplit a Spark DataFrame into “training” and “test” datasets.\n\npartitions <- iris_tbl %>%\n  sdf_random_split(training = 0.75, test = 0.25, seed = 1099)\n\nThe partitions variable is now a list with two elements called training and test. It does not contain any data. It is just a pointer to where Spark has separated the data, so nothing is downloaded into R. Use partitions$training to access the data the Spark has separated for that purpose.\n\nfit <- partitions$training %>%\n  ml_linear_regression(Petal_Length ~ Petal_Width)\n\nUse ml_predict() to then calculate the mse of the “test” data:\n\nml_predict(fit, partitions$test) %>%\n  mutate(resid = Petal_Length - prediction) %>%\n  summarize(mse = mean(resid ^ 2, na.rm = TRUE)) \n\n# Source: spark<?> [?? x 1]\n    mse\n  <dbl>\n1 0.212\n\n\n\n\nDisconnect from Spark\nLastly, cleanup your session by disconnecting Spark:\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "guides/pipelines.html#introduction-to-ml-pipelines",
    "href": "guides/pipelines.html#introduction-to-ml-pipelines",
    "title": "Spark ML Pipelines",
    "section": "Introduction to ML Pipelines",
    "text": "Introduction to ML Pipelines\nThe official Apache Spark site contains a more complete overview of ML Pipelines. This article will focus in introducing the basic concepts and steps to work with ML Pipelines via sparklyr.\nThere are two important stages in building an ML Pipeline. The first one is creating a Pipeline. A good way to look at it, or call it, is as an “empty” pipeline. This step just builds the steps that the data will go through. This is the somewhat equivalent of doing this in R:\n\nlibrary(dplyr)\n\nr_pipeline <-  . %>% mutate(cyl = paste0(\"c\", cyl)) %>% lm(am ~ cyl + mpg, data = .)\nr_pipeline\n\nFunctional sequence with the following components:\n\n 1. mutate(., cyl = paste0(\"c\", cyl))\n 2. lm(am ~ cyl + mpg, data = .)\n\nUse 'functions' to extract the individual functions. \n\n\nThe r_pipeline object has all the steps needed to transform and fit the model, but it has not yet transformed any data. The second step, is to pass data through the pipeline, which in turn will output a fitted model. That is called a PipelineModel. The PipelineModel can then be used to produce predictions.\n\nr_model <- r_pipeline(mtcars)\nr_model\n\n\nCall:\nlm(formula = am ~ cyl + mpg, data = .)\n\nCoefficients:\n(Intercept)        cylc6        cylc8          mpg  \n   -0.54388      0.03124     -0.03313      0.04767  \n\n\n\nTaking advantage of Pipelines and PipelineModels\nThe two stage ML Pipeline approach produces two final data products:\n\nA PipelineModel that can be added to the daily Spark jobs which will produce new predictions for the incoming data, and again, with no R dependencies.\nA Pipeline that can be easily re-fitted on a regular interval, say every month. All that is needed is to pass a new sample to obtain the new coefficients."
  },
  {
    "objectID": "guides/pipelines.html#pipeline",
    "href": "guides/pipelines.html#pipeline",
    "title": "Spark ML Pipelines",
    "section": "Pipeline",
    "text": "Pipeline\nAn additional goal of this article is that the reader can follow along, so the data, transformations and Spark connection in this example will be kept as easy to reproduce as possible.\n\nlibrary(nycflights13)\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\")\n\nspark_flights <- copy_to(sc, flights)\n\n\nFeature Transformers\nPipelines make heavy use of Feature Transformers. If new to Spark, and sparklyr, it would be good to review what these transformers do. These functions use the Spark API directly to transform the data, and may be faster at making the data manipulations that a dplyr (SQL) transformation.\nIn sparklyr the ft functions are essentially are wrappers to original Spark feature transformer.\n\n\nft_dplyr_transformer\nThis example will start with dplyr transformations, which are ultimately SQL transformations, loaded into the df variable.\nIn sparklyr, there is one feature transformer that is not available in Spark, ft_dplyr_transformer(). The goal of this function is to convert the dplyr code to a SQL Feature Transformer that can then be used in a Pipeline.\n\ndf <- spark_flights %>%\n  filter(!is.na(dep_delay)) %>%\n  mutate(\n    month = paste0(\"m\", month),\n    day = paste0(\"d\", day)\n  ) %>%\n  select(dep_delay, sched_dep_time, month, day, distance) \n\nThis is the resulting pipeline stage produced from the dplyr code:\n\nft_dplyr_transformer(sc, df)\n\nSQLTransformer (Transformer)\n<dplyr_transformer__8ba1e82c_e9b0_4914_b571_2a71c8e1f4ea> \n (Parameters -- Column Names)\n\n\nUse the ml_param() function to extract the “statement” attribute. That attribute contains the finalized SQL statement. Notice that the flights table name has been replace with __THIS__. This allows the pipeline to accept different table names as its source, making the pipeline very modular.\n\nft_dplyr_transformer(sc, df) %>%\n  ml_param(\"statement\")\n\n[1] \"SELECT `dep_delay`, `sched_dep_time`, CONCAT(\\\"m\\\", `month`) AS `month`, CONCAT(\\\"d\\\", `day`) AS `day`, `distance`\\nFROM `__THIS__`\\nWHERE (NOT(((`dep_delay`) IS NULL)))\"\n\n\n\n\nCreating the Pipeline\nThe following step will create a 5 stage pipeline:\n\nSQL transformer - Resulting from the ft_dplyr_transformer() transformation\nBinarizer - To determine if the flight should be considered delay. The eventual outcome variable.\nBucketizer - To split the day into specific hour buckets\nR Formula - To define the model’s formula\nLogistic Model\n\n\nflights_pipeline <- ml_pipeline(sc) %>%\n  ft_dplyr_transformer(\n    tbl = df\n    ) %>%\n  ft_binarizer(\n    input_col = \"dep_delay\",\n    output_col = \"delayed\",\n    threshold = 15\n  ) %>%\n  ft_bucketizer(\n    input_col = \"sched_dep_time\",\n    output_col = \"hours\",\n    splits = c(400, 800, 1200, 1600, 2000, 2400)\n  )  %>%\n  ft_r_formula(delayed ~ month + day + hours + distance) %>% \n  ml_logistic_regression()\n\nAnother nice feature for ML Pipelines in sparklyr, is the print-out. It makes it really easy to how each stage is setup:\n\nflights_pipeline\n\nPipeline (Estimator) with 5 stages\n<pipeline__e4745ba2_5bf8_4555_88d9_4aa66aa7b6ba> \n  Stages \n  |--1 SQLTransformer (Transformer)\n  |    <dplyr_transformer__e0e59acc_d897_4a93_9760_474ebc786add> \n  |     (Parameters -- Column Names)\n  |--2 Binarizer (Transformer)\n  |    <binarizer__5b3946fe_1209_4d5d_805c_ee7c79bea1f2> \n  |     (Parameters -- Column Names)\n  |      input_col: dep_delay\n  |      output_col: delayed\n  |--3 Bucketizer (Transformer)\n  |    <bucketizer__84db4fe8_4994_4ad3_9e04_a6bbb70e5cec> \n  |     (Parameters -- Column Names)\n  |      input_col: sched_dep_time\n  |      output_col: hours\n  |--4 RFormula (Estimator)\n  |    <r_formula__b6e8b330_5824_4719_9417_d14bd05de34f> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |     (Parameters)\n  |      force_index_label: FALSE\n  |      formula: delayed ~ month + day + hours + distance\n  |      handle_invalid: error\n  |      stringIndexerOrderType: frequencyDesc\n  |--5 LogisticRegression (Estimator)\n  |    <logistic_regression__3991e32e_fa8c_46f8_b337_1ea749ffb337> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |      prediction_col: prediction\n  |      probability_col: probability\n  |      raw_prediction_col: rawPrediction\n  |     (Parameters)\n  |      aggregation_depth: 2\n  |      elastic_net_param: 0\n  |      family: auto\n  |      fit_intercept: TRUE\n  |      max_iter: 100\n  |      maxBlockSizeInMB: 0\n  |      reg_param: 0\n  |      standardization: TRUE\n  |      threshold: 0.5\n  |      tol: 1e-06\n\n\nNotice that there are no coefficients defined yet. That’s because no data has been actually processed. Even though df uses spark_flights(), recall that the final SQL transformer makes that name, so there’s no data to process yet."
  },
  {
    "objectID": "guides/pipelines.html#pipelinemodel",
    "href": "guides/pipelines.html#pipelinemodel",
    "title": "Spark ML Pipelines",
    "section": "PipelineModel",
    "text": "PipelineModel\nA quick partition of the data is created for this exercise.\n\npartitioned_flights <- sdf_random_split(\n  spark_flights,\n  training = 0.01,\n  testing = 0.01,\n  rest = 0.98\n)\n\nThe ml_fit() function produces the PipelineModel. The training partition of the partitioned_flights data is used to train the model:\n\nfitted_pipeline <- ml_fit(\n  flights_pipeline,\n  partitioned_flights$training\n)\nfitted_pipeline\n\nPipelineModel (Transformer) with 5 stages\n<pipeline__e4745ba2_5bf8_4555_88d9_4aa66aa7b6ba> \n  Stages \n  |--1 SQLTransformer (Transformer)\n  |    <dplyr_transformer__e0e59acc_d897_4a93_9760_474ebc786add> \n  |     (Parameters -- Column Names)\n  |--2 Binarizer (Transformer)\n  |    <binarizer__5b3946fe_1209_4d5d_805c_ee7c79bea1f2> \n  |     (Parameters -- Column Names)\n  |      input_col: dep_delay\n  |      output_col: delayed\n  |--3 Bucketizer (Transformer)\n  |    <bucketizer__84db4fe8_4994_4ad3_9e04_a6bbb70e5cec> \n  |     (Parameters -- Column Names)\n  |      input_col: sched_dep_time\n  |      output_col: hours\n  |--4 RFormulaModel (Transformer)\n  |    <r_formula__b6e8b330_5824_4719_9417_d14bd05de34f> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |     (Transformer Info)\n  |      formula:  chr \"delayed ~ month + day + hours + distance\" \n  |--5 LogisticRegressionModel (Transformer)\n  |    <logistic_regression__3991e32e_fa8c_46f8_b337_1ea749ffb337> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |      prediction_col: prediction\n  |      probability_col: probability\n  |      raw_prediction_col: rawPrediction\n  |     (Transformer Info)\n  |      coefficient_matrix:  num [1, 1:43] 0.47246 0.8963 0.23965 0.00185 -0.03954 ... \n  |      coefficients:  num [1:43] 0.47246 0.8963 0.23965 0.00185 -0.03954 ... \n  |      intercept:  num -2.8 \n  |      intercept_vector:  num -2.8 \n  |      num_classes:  int 2 \n  |      num_features:  int 43 \n  |      threshold:  num 0.5 \n  |      thresholds:  num [1:2] 0.5 0.5 \n\n\nNotice that the print-out for the fitted pipeline now displays the model’s coefficients.\nThe ml_transform() function can be used to run predictions, in other words it is used instead of predict() or sdf_predict().\n\npredictions <- ml_transform(\n  fitted_pipeline,\n  partitioned_flights$testing\n)\n\npredictions %>%\n  group_by(delayed, prediction) %>%\n  tally()\n\n# Source: spark<?> [?? x 3]\n# Groups: delayed\n  delayed prediction     n\n    <dbl>      <dbl> <dbl>\n1       0          1    35\n2       0          0  2569\n3       1          0   650\n4       1          1    56"
  },
  {
    "objectID": "guides/pipelines.html#save-the-pipelines-to-disk",
    "href": "guides/pipelines.html#save-the-pipelines-to-disk",
    "title": "Spark ML Pipelines",
    "section": "Save the pipelines to disk",
    "text": "Save the pipelines to disk\nThe ml_save() command can be used to save the Pipeline and PipelineModel to disk. The resulting output is a folder with the selected name, which contains all of the necessary Scala scripts:\n\nml_save(\n  flights_pipeline,\n  \"flights_pipeline\",\n  overwrite = TRUE\n)\n\nModel successfully saved.\n\nml_save(\n  fitted_pipeline,\n  \"flights_model\",\n  overwrite = TRUE\n)\n\nModel successfully saved."
  },
  {
    "objectID": "guides/pipelines.html#use-an-existing-pipelinemodel",
    "href": "guides/pipelines.html#use-an-existing-pipelinemodel",
    "title": "Spark ML Pipelines",
    "section": "Use an existing PipelineModel",
    "text": "Use an existing PipelineModel\nThe ml_load() command can be used to re-load Pipelines and PipelineModels. The saved ML Pipeline files can only be loaded into an open Spark session.\n\nreloaded_model <- ml_load(sc, \"flights_model\")\n\nA simple query can be used as the table that will be used to make the new predictions. This of course, does not have to done in R, at this time the “flights_model” can be loaded into an independent Spark session outside of R.\n\nnew_df <- spark_flights %>%\n  filter(\n    month == 7,\n    day == 5\n  )\n\nml_transform(reloaded_model, new_df)\n\n# Source: spark<?> [?? x 12]\n   dep_delay sched_dep_time month day   distance delayed hours features   label\n       <dbl>          <int> <chr> <chr>    <dbl>   <dbl> <dbl> <list>     <dbl>\n 1        39           2359 m7    d5        1617       1     4 <dbl [43]>     1\n 2       141           2245 m7    d5        2475       1     4 <dbl [43]>     1\n 3         0            500 m7    d5         529       0     0 <dbl [43]>     0\n 4        -5            536 m7    d5        1400       0     0 <dbl [43]>     0\n 5        -2            540 m7    d5        1089       0     0 <dbl [43]>     0\n 6        -7            545 m7    d5        1416       0     0 <dbl [43]>     0\n 7        -3            545 m7    d5        1576       0     0 <dbl [43]>     0\n 8        -7            600 m7    d5        1076       0     0 <dbl [43]>     0\n 9        -7            600 m7    d5          96       0     0 <dbl [43]>     0\n10        -6            600 m7    d5         937       0     0 <dbl [43]>     0\n# … with more rows, and 3 more variables: rawPrediction <list>,\n#   probability <list>, prediction <dbl>"
  },
  {
    "objectID": "guides/pipelines.html#re-fit-an-existing-pipeline",
    "href": "guides/pipelines.html#re-fit-an-existing-pipeline",
    "title": "Spark ML Pipelines",
    "section": "Re-fit an existing Pipeline",
    "text": "Re-fit an existing Pipeline\nFirst, reload the pipeline into an open Spark session:\n\nreloaded_pipeline <- ml_load(sc, \"flights_pipeline\")\n\nUse ml_fit() again to pass new data, in this case, sample_frac() is used instead of sdf_partition() to provide the new data. The idea being that the re-fitting would happen at a later date than when the model was initially fitted.\n\nnew_model <-  ml_fit(reloaded_pipeline, sample_frac(spark_flights, 0.01))\n\nnew_model\n\nPipelineModel (Transformer) with 5 stages\n<pipeline__e4745ba2_5bf8_4555_88d9_4aa66aa7b6ba> \n  Stages \n  |--1 SQLTransformer (Transformer)\n  |    <dplyr_transformer__e0e59acc_d897_4a93_9760_474ebc786add> \n  |     (Parameters -- Column Names)\n  |--2 Binarizer (Transformer)\n  |    <binarizer__5b3946fe_1209_4d5d_805c_ee7c79bea1f2> \n  |     (Parameters -- Column Names)\n  |      input_col: dep_delay\n  |      output_col: delayed\n  |--3 Bucketizer (Transformer)\n  |    <bucketizer__84db4fe8_4994_4ad3_9e04_a6bbb70e5cec> \n  |     (Parameters -- Column Names)\n  |      input_col: sched_dep_time\n  |      output_col: hours\n  |--4 RFormulaModel (Transformer)\n  |    <r_formula__b6e8b330_5824_4719_9417_d14bd05de34f> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |     (Transformer Info)\n  |      formula:  chr \"delayed ~ month + day + hours + distance\" \n  |--5 LogisticRegressionModel (Transformer)\n  |    <logistic_regression__3991e32e_fa8c_46f8_b337_1ea749ffb337> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |      prediction_col: prediction\n  |      probability_col: probability\n  |      raw_prediction_col: rawPrediction\n  |     (Transformer Info)\n  |      coefficient_matrix:  num [1, 1:43] 0.111 -0.798 -0.626 -0.355 -0.19 ... \n  |      coefficients:  num [1:43] 0.111 -0.798 -0.626 -0.355 -0.19 ... \n  |      intercept:  num -2.08 \n  |      intercept_vector:  num -2.08 \n  |      num_classes:  int 2 \n  |      num_features:  int 43 \n  |      threshold:  num 0.5 \n  |      thresholds:  num [1:2] 0.5 0.5 \n\n\nThe new model can be saved using ml_save(). A new name is used in this case, but the same name as the existing PipelineModel to replace it.\n\nml_save(new_model, \"new_flights_model\", overwrite = TRUE)\n\nModel successfully saved.\n\n\nFinally, this example is complete by closing the Spark session.\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "guides/streaming.html#the-sparklyr-interface",
    "href": "guides/streaming.html#the-sparklyr-interface",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "The sparklyr interface",
    "text": "The sparklyr interface\nAs stated in the Spark’s official site, Spark Streaming makes it easy to build scalable fault-tolerant streaming applications. Because is part of the Spark API, it is possible to re-use query code that queries the current state of the stream, as well as joining the streaming data with historical data. Please see Spark’s official documentation for a deeper look into Spark Streaming.\nThe sparklyr interface provides the following:\n\nAbility to run dplyr, SQL, spark_apply(), and PipelineModels against a stream\nRead in multiple formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc\nWrite stream results to Spark memory and the following file formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc\nAn out-of-the box graph visualization to monitor the stream\nA new reactiveSpark() function, that allows Shiny apps to poll the contents of the stream create Shiny apps that are able to read the contents of the stream"
  },
  {
    "objectID": "guides/streaming.html#interacting-with-a-stream",
    "href": "guides/streaming.html#interacting-with-a-stream",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Interacting with a stream",
    "text": "Interacting with a stream\nA good way of looking at the way how Spark streams update is as a three stage operation:\n\nInput - Spark reads the data inside a given folder. The folder is expected to contain multiple data files, with new files being created containing the most current stream data.\nProcessing - Spark applies the desired operations on top of the data. These operations could be data manipulations (dplyr, SQL), data transformations (sdf operations, PipelineModel predictions), or native R manipulations (spark_apply()).\nOutput - The results of processing the input files are saved in a different folder.\n\nIn the same way all of the read and write operations in sparklyr for Spark Standalone, or in sparklyr’s local mode, the input and output folders are actual OS file system folders. For Hadoop clusters, these will be folder locations inside the HDFS."
  },
  {
    "objectID": "guides/streaming.html#example-1---inputoutput",
    "href": "guides/streaming.html#example-1---inputoutput",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 1 - Input/Output",
    "text": "Example 1 - Input/Output\nThe first intro example is a small script that can be used with a local master. The result should be to see the stream_view() app showing live the number of records processed for each iteration of test data being sent to the stream.\n\nlibrary(future)\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\nstream_generate_test(iterations = 1)\nread_folder <- stream_read_csv(sc, \"source\") \nwrite_output <- stream_write_csv(read_folder, \"source-out\")\ninvisible(future(stream_generate_test(interval = 0.5)))\n\nstream_view(write_output)\n\n\n\nstream_stop(write_output)\nspark_disconnect(sc)\n\n\nCode breakdown\n\nOpen the Spark connection\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\n\nOptional step. This resets the input and output folders. It makes it easier to run the code multiple times in a clean manner.\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\nProduces a single test file inside the “source” folder. This allows the “read” function to infer CSV file definition.\n\nstream_generate_test(iterations = 1)\nlist.files(\"source\")\n\n[1] \"stream_1.csv\"\nPoints the stream reader to the folder where the streaming files will be placed. Since it is primed with a single CSV file, it will use as the expected layout of subsequent files. By default, stream_read_csv() creates a single integer variable data frame.\n\nread_folder <- stream_read_csv(sc, \"source\")\n\nThe output writer is what starts the streaming job. It will start monitoring the input folder, and then write the new results in the “source-out” folder. So as new records stream in, new files will be created in the “source-out” folder. Since there are no operations on the incoming data at this time, the output files will have the same exact raw data as the input files. The only difference is that the files and sub folders within “source-out” will be structured how Spark structures data folders.\n\nwrite_output <- stream_write_csv(read_folder, \"source-out\")\nlist.files(\"source-out\")\n\n[1] \"_spark_metadata\"                                     \"checkpoint\"\n[3] \"part-00000-1f29719a-2314-40e1-b93d-a647a3d57154-c000.csv\"\nThe test generation function will run 100 files every 0.2 seconds. To run the tests “out-of-sync” with the current R session, the future package is used.\n\nlibrary(future)\n\ninvisible(\n  future(\n    stream_generate_test(interval = 0.2, iterations = 100)\n    )\n  )\n\nThe stream_view() function can be used before the 50 tests are complete because of the use of the future package. It will monitor the status of the job that write_output is pointing to and provide information on the amount of data coming into the “source” folder and going out into the “source-out” folder.\n\nstream_view(write_output)\n\nThe monitor will continue to run even after the tests are complete. To end the experiment, stop the Shiny app and then use the following to stop the stream and close the Spark session.\n\nstream_stop(write_output)\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "guides/streaming.html#example-2---processing",
    "href": "guides/streaming.html#example-2---processing",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 2 - Processing",
    "text": "Example 2 - Processing\nThe second example builds on the first. It adds a processing step that manipulates the input data before saving it to the output folder. In this case, a new binary field is added indicating if the value from x is over 400 or not. This time, while run the second code chunk in this example a few times during the stream tests to see the aggregated values change.\n\nlibrary(future)\nlibrary(sparklyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nsc <- spark_connect(master = \"local\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\nstream_generate_test(iterations = 1)\nread_folder <- stream_read_csv(sc, \"source\") \n\nprocess_stream <- read_folder %>%\n  mutate(x = as.double(x)) %>%\n  ft_binarizer(\n    input_col = \"x\",\n    output_col = \"over\",\n    threshold = 400\n  )\n\nwrite_output <- stream_write_csv(process_stream, \"source-out\")\ninvisible(future(stream_generate_test(interval = 0.2, iterations = 100)))\n\nRun this code a few times during the experiment:\n\nspark_read_csv(sc, \"stream\", \"source-out\", memory = FALSE) %>%\n  group_by(over) %>%\n  tally()\n\nThe results would look similar to this. The n totals will increase as the experiment progresses.\n# Source:   lazy query [?? x 2]\n# Database: spark_connection\n   over     n\n  <dbl> <dbl>\n1     0 40215\n2     1 60006\nClean up after the experiment\n\nstream_stop(write_output)\nspark_disconnect(sc)\n\n\nCode breakdown\n\nThe processing starts with the read_folder variable that contains the input stream. It coerces the integer field x, into a type double. This is because the next function, ft_binarizer() does not accept integers. The binarizer determines if x is over 400 or not. This is a good illustration of how dplyr can help simplify the manipulation needed during the processing stage.\n\nprocess_stream <- read_folder %>%\n  mutate(x = as.double(x)) %>%\n  ft_binarizer(\n    input_col = \"x\",\n    output_col = \"over\",\n    threshold = 400\n  )\n\nThe output now needs to write-out the processed data instead of the raw input data. Swap read_folder with process_stream.\n\nwrite_output <- stream_write_csv(process_stream, \"source-out\")\n\nThe “source-out” folder can be treated as a if it was a single table within Spark. Using spark_read_csv(), the data can be mapped, but not brought into memory (memory = FALSE). This allows the current results to be further analyzed using regular dplyr commands.\n\nspark_read_csv(sc, \"stream\", \"source-out\", memory = FALSE) %>%\n  group_by(over) %>%\n  tally()"
  },
  {
    "objectID": "guides/streaming.html#example-3---aggregate-in-process-and-output-to-memory",
    "href": "guides/streaming.html#example-3---aggregate-in-process-and-output-to-memory",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 3 - Aggregate in process and output to memory",
    "text": "Example 3 - Aggregate in process and output to memory\nAnother option is to save the results of the processing into a in-memory Spark table. Unless intentionally saving it to disk, the table and its data will only exist while the Spark session is active.\nThe biggest advantage of using Spark memory as the target, is that it will allow for aggregation to happen during processing. This is an advantage because aggregation is not allowed for any file output, expect Kafka, on the input/process stage.\nUsing example 2 as the base, this example code will perform some aggregations to the current stream input and save only those summarized results into Spark memory:\n\nlibrary(future)\nlibrary(sparklyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nsc <- spark_connect(master = \"local\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\n\nstream_generate_test(iterations = 1)\nread_folder <- stream_read_csv(sc, \"source\") \n\nprocess_stream <- read_folder %>%\n  stream_watermark() %>%\n  group_by(timestamp) %>%\n  summarise(\n    max_x = max(x, na.rm = TRUE),\n    min_x = min(x, na.rm = TRUE),\n    count = n()\n  )\n\nwrite_output <- stream_write_memory(process_stream, name = \"stream\")\n\ninvisible(future(stream_generate_test()))\n\nRun this command a different times while the experiment is running:\n\ntbl(sc, \"stream\") \n\nClean up after the experiment\n\nstream_stop(write_output)\n\nspark_disconnect(sc)\n\n\nCode breakdown\n\nThe stream_watermark() functions add a new timestamp variable that is then used in the group_by() command. This is required by Spark Stream to accept summarized results as output of the stream. The second step is to simply decide what kinds of aggregations we need to perform. In this case, a simply max, min and count are performed.\n\nprocess_stream <- read_folder %>%\n  stream_watermark() %>%\n  group_by(timestamp) %>%\n  summarise(\n    max_x = max(x, na.rm = TRUE),\n    min_x = min(x, na.rm = TRUE),\n    count = n()\n  )\n\nThe spark_write_memory() function is used to write the output to Spark memory. The results will appear as a table of the Spark session with the name assigned in the name argument, in this case the name selected is: “stream”.\n\nwrite_output <- stream_write_memory(process_stream, name = \"stream\")\n\nTo query the current data in the “stream” table can be queried by using the dplyr tbl() command.\n\ntbl(sc, \"stream\")"
  },
  {
    "objectID": "guides/streaming.html#example-4---shiny-integration",
    "href": "guides/streaming.html#example-4---shiny-integration",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 4 - Shiny integration",
    "text": "Example 4 - Shiny integration\nsparklyr provides a new Shiny function called reactiveSpark(). It can take a Spark data frame, in this case the one created as a result of the stream processing, and then creates a Spark memory stream table, the same way a table is created in example 3.\n\nlibrary(future)\nlibrary(sparklyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(ggplot2)\n\nsc <- spark_connect(master = \"local\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\nstream_generate_test(iterations = 1)\nread_folder <- stream_read_csv(sc, \"source\") \n\nprocess_stream <- read_folder %>%\n  stream_watermark() %>%\n  group_by(timestamp) %>%\n  summarise(\n    max_x = max(x, na.rm = TRUE),\n    min_x = min(x, na.rm = TRUE),\n    count = n()\n  )\n\ninvisible(future(stream_generate_test(interval = 0.2, iterations = 100)))\n\nlibrary(shiny)\nui <- function(){\n  tableOutput(\"table\")\n}\nserver <- function(input, output, session){\n  \n  ps <- reactiveSpark(process_stream)\n  \n  output$table <- renderTable({\n    ps() %>%\n      mutate(timestamp = as.character(timestamp)) \n    })\n}\nrunGadget(ui, server)\n\n\n\nCode breakdown\n\nNotice that there is no stream_write_... command. The reason is that reactiveSpark() function contains the stream_write_memory() function.\nThis very basic Shiny app simply displays the output of a table in the ui section\n\nlibrary(shiny)\n\nui <- function(){\n  tableOutput(\"table\")\n}\n\nIn the server section, the reactiveSpark() function will update every time there’s a change to the stream and return a data frame. The results are saved to a variable called ps() in this script. Treat the ps() variable as a regular table that can be piped from, as shown in the example. In this case, the timestamp variable is converted to string for to make it easier to read.\n\nserver <- function(input, output, session){\n\n  ps <- reactiveSpark(process_stream)\n\n  output$table <- renderTable({\n    ps() %>%\n      mutate(timestamp = as.character(timestamp)) \n  })\n}\n\nUse runGadget() to display the Shiny app in the Viewer pane. This is optional, the app can be run using normal Shiny run functions.\n\nrunGadget(ui, server)"
  },
  {
    "objectID": "guides/streaming.html#example-5---ml-pipeline-model",
    "href": "guides/streaming.html#example-5---ml-pipeline-model",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 5 - ML Pipeline Model",
    "text": "Example 5 - ML Pipeline Model\nThis example uses a fitted Pipeline Model to process the input, and saves the predictions to the output. This approach would be used to apply Machine Learning on top of streaming data.\n\nlibrary(sparklyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nsc <- spark_connect(master = \"local\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\ndf <- data.frame(x = rep(1:1000), y = rep(2:1001))\n\nstream_generate_test(df = df, iteration = 1)\n\nmodel_sample <- spark_read_csv(sc, \"sample\", \"source\")\n\npipeline <- sc %>%\n  ml_pipeline() %>%\n  ft_r_formula(x ~ y) %>%\n  ml_linear_regression()\n\nfitted_pipeline <- ml_fit(pipeline, model_sample)\n\nml_stream <- stream_read_csv(\n    sc = sc, \n    path = \"source\", \n    columns = c(x = \"integer\", y = \"integer\")\n  )  %>%\n  ml_transform(fitted_pipeline, .)  %>%\n  select(- features) %>%\n  stream_write_csv(\"source-out\")\n\nstream_generate_test(df = df, interval = 0.5)\n\n\nspark_read_csv(sc, \"stream\", \"source-out\", memory = FALSE) \n\n### Source: spark<stream> [?? x 4]\n##       x     y label prediction\n## * <int> <int> <dbl>      <dbl>\n## 1   276   277   276       276.\n## 2   277   278   277       277.\n## 3   278   279   278       278.\n## 4   279   280   279       279.\n## 5   280   281   280       280.\n## 6   281   282   281       281.\n## 7   282   283   282       282.\n## 8   283   284   283       283.\n## 9   284   285   284       284.\n##10   285   286   285       285.\n### ... with more rows\n\nstream_stop(ml_stream)\nspark_disconnect(sc)\n\n\nCode Breakdown\n\nCreates and fits a pipeline\n\ndf <- data.frame(x = rep(1:1000), y = rep(2:1001))\nstream_generate_test(df = df, iteration = 1)\nmodel_sample <- spark_read_csv(sc, \"sample\", \"source\")\n\npipeline <- sc %>%\n  ml_pipeline() %>%\n  ft_r_formula(x ~ y) %>%\n  ml_linear_regression()\n\nfitted_pipeline <- ml_fit(pipeline, model_sample)\n\nThis example pipelines the input, process and output in a single code segment. The ml_transform() function is used to create the predictions. Because the CSV format does not support list type fields, the features column is removed before the results are sent to the output.\n\nml_stream <- stream_read_csv(\n    sc = sc, \n    path = \"source\", \n    columns = c(x = \"integer\", y = \"integer\")\n  )  %>%\n  ml_transform(fitted_pipeline, .)  %>%\n  select(- features) %>%\n  stream_write_csv(\"source-out\")\n\n\n\nstream_stop(write_output)\nspark_disconnect(sc)"
  },
  {
    "objectID": "guides/textmining.html#data-import",
    "href": "guides/textmining.html#data-import",
    "title": "Text mining with Spark & sparklyr",
    "section": "Data Import",
    "text": "Data Import\n\nConnect to Spark\nAn additional goal of this article is to encourage the reader to try it out, so a simple Spark local mode session is used.\n\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\")\n\n\n\nspark_read_text()\nThe spark_read_text() is a new function which works like readLines() but for sparklyr. It comes in handy when non-structured data, such as lines in a book, is what is available for analysis.\n\n# Imports Mark Twain's file\n\ntwain_path <- paste0(\"file:///\", here::here(), \"/mark_twain.txt\")\ntwain <-  spark_read_text(sc, \"twain\", twain_path)\n\n\n# Imports Sir Arthur Conan Doyle's file\ndoyle_path <- paste0(\"file:///\", here::here(), \"/arthur_doyle.txt\")\ndoyle <-  spark_read_text(sc, \"doyle\", doyle_path)"
  },
  {
    "objectID": "guides/textmining.html#data-transformation",
    "href": "guides/textmining.html#data-transformation",
    "title": "Text mining with Spark & sparklyr",
    "section": "Data transformation",
    "text": "Data transformation\nThe objective is to end up with a tidy table inside Spark with one row per word used. The steps will be:\n\nThe needed data transformations apply to the data from both authors. The data sets will be appended to one another\nPunctuation will be removed\nThe words inside each line will be separated, or tokenized\nFor a cleaner analysis, stop words will be removed\nTo tidy the data, each word in a line will become its own row\nThe results will be saved to Spark memory\n\n\nsdf_bind_rows()\n\nsdf_bind_rows() appends the doyle Spark Dataframe to the twain Spark Dataframe. This function can be used in lieu of a dplyr::bind_rows() wrapper function. For this exercise, the column author is added to differentiate between the two bodies of work.\n\n\nall_words <- doyle %>%\n  mutate(author = \"doyle\") %>%\n  sdf_bind_rows({\n    twain %>%\n      mutate(author = \"twain\")\n  }) %>%\n  filter(nchar(line) > 0)\n\n\n\nregexp_replace()\n\nThe Hive UDF, regexp_replace, is used as a sort of gsub() that works inside Spark. In this case it is used to remove punctuation. The usual [:punct:] regular expression did not work well during development, so a custom list is provided. For more information, see the Hive Functions section in the dplyr page.\n\n\nall_words <- all_words %>%\n  mutate(line = regexp_replace(line, \"[_\\\"\\'():;,.!?\\\\-]\", \" \"))\n\n\n\nft_tokenizer()\n\nft_tokenizer() uses the Spark API to separate each word. It creates a new list column with the results.\n\n\nall_words <- all_words %>%\n    ft_tokenizer(\n      input_col = \"line\",\n      output_col = \"word_list\"\n      )\n\nhead(all_words, 4)\n\n# Source: spark<?> [?? x 3]\n  line                          author word_list \n  <chr>                         <chr>  <list>    \n1 cover                         doyle  <list [1]>\n2 The Return of Sherlock Holmes doyle  <list [5]>\n3 by Sir Arthur Conan Doyle     doyle  <list [5]>\n4 Contents                      doyle  <list [1]>\n\n\n\n\nft_stop_words_remover()\n\nft_stop_words_remover() is a new function that, as its name suggests, takes care of removing stop words from the previous transformation. It expects a list column, so it is important to sequence it correctly after a ft_tokenizer() command. In the sample results, notice that the new wo_stop_words column contains less items than word_list.\n\n\nall_words <- all_words %>%\n  ft_stop_words_remover(\n    input_col = \"word_list\",\n    output_col = \"wo_stop_words\"\n    )\n\nhead(all_words, 4)\n\n# Source: spark<?> [?? x 4]\n  line                          author word_list  wo_stop_words\n  <chr>                         <chr>  <list>     <list>       \n1 cover                         doyle  <list [1]> <list [1]>   \n2 The Return of Sherlock Holmes doyle  <list [5]> <list [3]>   \n3 by Sir Arthur Conan Doyle     doyle  <list [5]> <list [4]>   \n4 Contents                      doyle  <list [1]> <list [1]>   \n\n\n\n\nexplode()\n\nThe Hive UDF explode performs the job of unnesting the tokens into their own row. Some further filtering and field selection is done to reduce the size of the dataset.\n\n\nall_words <- all_words %>%\n  mutate(word = explode(wo_stop_words)) %>%\n  select(word, author) %>%\n  filter(nchar(word) > 2)\n\nhead(all_words, 4)\n\n# Source: spark<?> [?? x 2]\n  word     author\n  <chr>    <chr> \n1 cover    doyle \n2 return   doyle \n3 sherlock doyle \n4 holmes   doyle \n\n\n\n\ncompute()\n\ncompute() will operate this transformation and cache the results in Spark memory. It is a good idea to pass a name to compute() to make it easier to identify it inside the Spark environment. In this case the name will be all_words\n\n\nall_words <- all_words %>%\n  compute(\"all_words\")\n\n\n\nFull code\nThis is what the code would look like on an actual analysis:\n\nall_words <- doyle %>%\n  mutate(author = \"doyle\") %>%\n  sdf_bind_rows({\n    twain %>%\n      mutate(author = \"twain\")\n  }) %>%\n  filter(nchar(line) > 0) %>%\n  mutate(line = regexp_replace(line, \"[_\\\"\\'():;,.!?\\\\-]\", \" \")) %>%\n  ft_tokenizer(\n    input_col = \"line\",\n    output_col = \"word_list\"\n  ) %>%\n  ft_stop_words_remover(\n    input_col = \"word_list\",\n    output_col = \"wo_stop_words\"\n  ) %>%\n  mutate(word = explode(wo_stop_words)) %>%\n  select(word, author) %>%\n  filter(nchar(word) > 2) %>%\n  compute(\"all_words\")"
  },
  {
    "objectID": "guides/textmining.html#data-analysis",
    "href": "guides/textmining.html#data-analysis",
    "title": "Text mining with Spark & sparklyr",
    "section": "Data Analysis",
    "text": "Data Analysis\n\nWords used the most\n\nword_count <- all_words %>%\n  count(author, word) %>% \n  ungroup()\n\nword_count\n\n# Source: spark<?> [?? x 3]\n   author word              n\n   <chr>  <chr>         <dbl>\n 1 doyle  empty           398\n 2 doyle  students        109\n 3 doyle  golden          303\n 4 doyle  abbey           164\n 5 doyle  grange           18\n 6 doyle  year            866\n 7 doyle  world          1520\n 8 doyle  circumstances   284\n 9 doyle  particulars      49\n10 doyle  crime           357\n# … with more rows\n\n\n\n\nWords used by Doyle and not Twain\n\ndoyle_unique <- filter(word_count, author == \"doyle\") %>%\n  anti_join(\n    filter(word_count, author == \"twain\"), \n    by = \"word\"\n    ) %>%\n  compute(\"doyle_unique\")\n\ndoyle_unique %>% \n  arrange(-n)\n\n# Source:     spark<?> [?? x 3]\n# Ordered by: -n\n   author word          n\n   <chr>  <chr>     <dbl>\n 1 doyle  nigel       972\n 2 doyle  alleyne     500\n 3 doyle  ezra        421\n 4 doyle  maude       337\n 5 doyle  aylward     336\n 6 doyle  lestrade    311\n 7 doyle  catinat     301\n 8 doyle  sharkey     281\n 9 doyle  summerlee   248\n10 doyle  congo       211\n# … with more rows\n\n\n\ndoyle_unique %>%\n  arrange(-n) %>%\n  head(100) %>%\n  collect() %>%\n  with(wordcloud::wordcloud(\n    word,\n    n,\n    colors = c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#56B4E9\")\n  ))\n\n\n\n\n\n\nTwain and Sherlock\nThe word cloud highlighted something interesting. The word “lestrade” is listed as one of the words used by Doyle but not Twain. Lestrade is the last name of a major character in the Sherlock Holmes books. It makes sense that the word “sherlock” appears considerably more times than “lestrade” in Doyle’s books, so why is Sherlock not in the word cloud? Did Mark Twain use the word “sherlock” in his writings?\n\nall_words %>%\n  filter(\n    author == \"twain\",\n    word == \"sherlock\"\n    ) %>%\n  count()\n\n# Source: spark<?> [?? x 1]\n      n\n  <dbl>\n1    16\n\n\nThe all_words table contains 16 instances of the word sherlock in the words used by Twain in his works. The instr Hive UDF is used to extract the lines that contain that word in the twain table. This Hive function works can be used instead of base::grep() or stringr::str_detect(). To account for any word capitalization, the lower command will be used in mutate() to make all words in the full text lower cap.\n\n\ninstr() & lower()\nMost of these lines are in a short story by Mark Twain called A Double Barrelled Detective Story. As per the Wikipedia page about this story, this is a satire by Twain on the mystery novel genre, published in 1902.\n\ntwain %>%\n  mutate(line = lower(line)) %>%\n  filter(instr(line, \"sherlock\") > 0) %>%\n  pull(line)\n\n [1] \"late sherlock holmes, and yet discernible by a member of a race charged\" \n [2] \"sherlock holmes.\"                                                        \n [3] \"“uncle sherlock! the mean luck of it!--that he should come just\"         \n [4] \"another trouble presented itself. “uncle sherlock 'll be wanting to talk\"\n [5] \"flint buckner's cabin in the frosty gloom. they were sherlock holmes and\"\n [6] \"“uncle sherlock's got some work to do, gentlemen, that 'll keep him till\"\n [7] \"“by george, he's just a duke, boys! three cheers for sherlock holmes,\"   \n [8] \"he brought sherlock holmes to the billiard-room, which was jammed with\"  \n [9] \"of interest was there--sherlock holmes. the miners stood silent and\"     \n[10] \"the room; the chair was on it; sherlock holmes, stately, imposing,\"      \n[11] \"“you have hunted me around the world, sherlock holmes, yet god is my\"    \n[12] \"“if it's only sherlock holmes that's troubling you, you needn't worry\"   \n[13] \"they sighed; then one said: “we must bring sherlock holmes. he can be\"   \n[14] \"i had small desire that sherlock holmes should hang for my deeds, as you\"\n[15] \"“my name is sherlock holmes, and i have not been doing anything.”\"       \n[16] \"late sherlock holmes, and yet discernible by a member of a race charged\" \n\n\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "guides/textmining.html#appendix",
    "href": "guides/textmining.html#appendix",
    "title": "Text mining with Spark & sparklyr",
    "section": "Appendix",
    "text": "Appendix\n\ngutenbergr package\nThis is an example of how the data for this article was pulled from the Gutenberg site:\nlibrary(gutenbergr)\n\ngutenberg_works()  %>%\n  filter(author == \"Twain, Mark\") %>%\n  pull(gutenberg_id) %>%\n  gutenberg_download(mirror = \"http://mirrors.xmission.com/gutenberg/\") %>%\n  pull(text) %>%\n  writeLines(\"mark_twain.txt\")"
  },
  {
    "objectID": "guides/troubleshooting.html#help-with-code-debugging",
    "href": "guides/troubleshooting.html#help-with-code-debugging",
    "title": "Troubleshooting",
    "section": "Help with code debugging",
    "text": "Help with code debugging\nFor general programming questions with sparklyr, please ask on Stack Overflow."
  },
  {
    "objectID": "guides/troubleshooting.html#code-does-not-work-after-upgrading-to-the-latest-sparklyr-version",
    "href": "guides/troubleshooting.html#code-does-not-work-after-upgrading-to-the-latest-sparklyr-version",
    "title": "Troubleshooting",
    "section": "Code does not work after upgrading to the latest sparklyr version",
    "text": "Code does not work after upgrading to the latest sparklyr version\nPlease refer to the NEWS section of the sparklyr package to find out if any of the updates listed may have changed the way your code needs to work.\nIf it seems that current version of the package has a bug, or the new functionality does not perform as stated, please refer to the sparklyr ISSUES page. If no existing issue matches to what your problem is, please open a new issue."
  },
  {
    "objectID": "guides/troubleshooting.html#not-able-to-connect-or-the-jobs-take-a-long-time-when-working-with-a-data-lake",
    "href": "guides/troubleshooting.html#not-able-to-connect-or-the-jobs-take-a-long-time-when-working-with-a-data-lake",
    "title": "Troubleshooting",
    "section": "Not able to connect, or the jobs take a long time when working with a Data Lake",
    "text": "Not able to connect, or the jobs take a long time when working with a Data Lake\nThe Configuration connections contains an overview and recommendations for requesting resources form the cluster.\nThe articles in the Guides section provide best-practice information about specific operations that may match to the intent of your code.\nTo verify your infrastructure, please review the Deployment Examples section."
  },
  {
    "objectID": "packages/index.html#on-cran",
    "href": "packages/index.html#on-cran",
    "title": "Packages",
    "section": "On CRAN",
    "text": "On CRAN\n\n\n\nPackage\nDescription\n\n\n\n\nmleap\nA sparklyr extension that provides an interface to MLeap, which allows us to take Spark pipelines to production.\n\n\ngraphframes\nEnables graph analysis in Spark via GraphFrames"
  },
  {
    "objectID": "packages/index.html#on-github",
    "href": "packages/index.html#on-github",
    "title": "Packages",
    "section": "On Github",
    "text": "On Github\n\n\n\nPackage\nDescription\n\n\n\n\nsparktf\nAllows writing of Spark DataFrame’s to TFRecord, the recommended format for persisting data to be used in training with TensorFlow\n\n\nsparkxgb\nA sparklyr extension that provides an interface to XGBoost on Spark."
  },
  {
    "objectID": "packages/graphframes/latest/index.html#installation",
    "href": "packages/graphframes/latest/index.html#installation",
    "title": "sparklyr",
    "section": "Installation",
    "text": "Installation\nFor those already using sparklyr simply run:\ninstall.packages(\"graphframes\")\n# or, for the development version,\n# devtools::install_github(\"rstudio/graphframes\")\nOtherwise, install first sparklyr from CRAN using:\ninstall.packages(\"sparklyr\")\nThe examples make use of the highschool dataset from the ggplot package."
  },
  {
    "objectID": "packages/graphframes/latest/index.html#getting-started",
    "href": "packages/graphframes/latest/index.html#getting-started",
    "title": "sparklyr",
    "section": "Getting Started",
    "text": "Getting Started\nWe will calculate PageRank over the built-in “friends” dataset as follows.\nlibrary(graphframes)\nlibrary(sparklyr)\nlibrary(dplyr)\n\n# connect to spark using sparklyr\nsc <- spark_connect(master = \"local\", version = \"2.3.0\")\n\n# obtain the example graph\ng <- gf_friends(sc)\n\n# compute PageRank\nresults <- gf_pagerank(g, tol = 0.01, reset_probability = 0.15)\nresults\n## GraphFrame\n## Vertices:\n##   $ id       <chr> \"f\", \"b\", \"g\", \"a\", \"d\", \"c\", \"e\"\n##   $ name     <chr> \"Fanny\", \"Bob\", \"Gabby\", \"Alice\", \"David\", \"Charlie\",...\n##   $ age      <int> 36, 36, 60, 34, 29, 30, 32\n##   $ pagerank <dbl> 0.3283607, 2.6555078, 0.1799821, 0.4491063, 0.3283607...\n## Edges:\n##   $ src          <chr> \"b\", \"c\", \"d\", \"e\", \"a\", \"a\", \"e\", \"f\"\n##   $ dst          <chr> \"c\", \"b\", \"a\", \"f\", \"e\", \"b\", \"d\", \"c\"\n##   $ relationship <chr> \"follow\", \"follow\", \"friend\", \"follow\", \"friend\",...\n##   $ weight       <dbl> 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0\nWe can then visualize the results by collecting the results to R:\nlibrary(tidygraph)\nlibrary(ggraph)\n\nvertices <- results %>%\n  gf_vertices() %>%\n  collect()\n\nedges <- results %>%\n  gf_edges() %>%\n  collect()\n\nedges %>%\n  as_tbl_graph() %>%\n  activate(nodes) %>%\n  left_join(vertices, by = c(name = \"id\")) %>%\n  ggraph(layout = \"nicely\") +\n  geom_node_label(aes(label = name.y, color = pagerank)) +\n  geom_edge_link(\n    aes(\n      alpha = weight,\n      start_cap = label_rect(node1.name.y),\n      end_cap = label_rect(node2.name.y)\n    ),\n    arrow = arrow(length = unit(4, \"mm\"))\n  ) +\n  theme_graph(fg_text_colour = 'white')"
  },
  {
    "objectID": "packages/graphframes/latest/index.html#further-reading",
    "href": "packages/graphframes/latest/index.html#further-reading",
    "title": "sparklyr",
    "section": "Further Reading",
    "text": "Further Reading\nAppart from calculating PageRank using gf_pagerank, many other functions are available, including:\n\ngf_bfs(): Breadth-first search (BFS).\ngf_connected_components(): Connected components.\ngf_shortest_paths(): Shortest paths algorithm.\ngf_scc(): Strongly connected components.\ngf_triangle_count(): Computes the number of triangles passing through each vertex and others.\ngf_degrees(): Degrees of vertices\n\nFor instance, one can calculate the degrees of vertices using gf_degrees as follows:\ngf_friends(sc) %>% gf_degrees()\n## # Source: spark<?> [?? x 2]\n##   id    degree\n## * <chr>  <int>\n## 1 f          2\n## 2 b          3\n## 3 a          3\n## 4 c          3\n## 5 e          3\n## 6 d          2\nFinally, we disconnect from Spark:\nspark_disconnect(sc)"
  },
  {
    "objectID": "packages/graphframes/latest/news.html",
    "href": "packages/graphframes/latest/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "graphframes 0.1.2\n\nUpdated dependency to graphframes 0.6.0, with support for Spark 2.3."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#description",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nBreadth-first search (BFS)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#usage",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_bfs(x, from_expr, to_expr, max_path_length = 10, edge_filter = NULL,\n  ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). from_expr | Spark SQL expression specifying valid starting vertices for the BFS. to_expr | Spark SQL expression specifying valid target vertices for the BFS. max_path_length | Limit on the length of paths. edge_filter | Spark SQL expression specifying edges which may be used in the search. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#examples",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_bfs(g, from_expr = \"name = 'Esther'\", to_expr = \"age < 32\")"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_cache.html#description",
    "href": "packages/graphframes/latest/reference/gf_cache.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCache the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_cache.html#usage",
    "href": "packages/graphframes/latest/reference/gf_cache.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_cache(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_cache.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_cache.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#description",
    "href": "packages/graphframes/latest/reference/gf_chain.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReturns a chain graph of the given size with Long ID type. The vertex IDs are 0, 1, …, n-1, and the edges are (0, 1), (1, 2), …., (n-2, n-1)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#usage",
    "href": "packages/graphframes/latest/reference/gf_chain.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_chain(sc, n)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_chain.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSize of the graph to return."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#examples",
    "href": "packages/graphframes/latest/reference/gf_chain.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_chain(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#description",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nComputes the connected component membership of each vertex and returns a DataFrame of vertex information with each vertex assigned a component ID."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#usage",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_connected_components(x, broadcast_threshold = 1000000L,\n  algorithm = c(\"graphframes\", \"graphx\"), checkpoint_interval = 2L,\n  ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). broadcast_threshold | Broadcast threshold in propagating component assignments. algorithm | One of ‘graphframes’ or ‘graphx’. checkpoint_interval | Checkpoint interval in terms of number of iterations. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#examples",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n# checkpoint directory is required for gf_connected_components()\nspark_set_checkpoint_dir(sc, tempdir())\ng <- gf_friends(sc)\ngf_connected_components(g)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_degrees.html#description",
    "href": "packages/graphframes/latest/reference/gf_degrees.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nDegrees of vertices"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_degrees.html#usage",
    "href": "packages/graphframes/latest/reference/gf_degrees.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_degrees.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_degrees.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edge_columns.html#description",
    "href": "packages/graphframes/latest/reference/gf_edge_columns.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nEdges column names"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edge_columns.html#usage",
    "href": "packages/graphframes/latest/reference/gf_edge_columns.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_edge_columns(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edge_columns.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_edge_columns.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edges.html#description",
    "href": "packages/graphframes/latest/reference/gf_edges.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nExtract edges DataFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edges.html#usage",
    "href": "packages/graphframes/latest/reference/gf_edges.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_edges(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edges.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_edges.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#description",
    "href": "packages/graphframes/latest/reference/gf_find.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nMotif finding uses a simple Domain-Specific Language (DSL) for expressing structural queries. For example, gf_find(g, “(a)-[e]->(b); (b)-[e2]->(a)”) will search for pairs of vertices a,b connected by edges in both directions. It will return a DataFrame of all such structures in the graph, with columns for each of the named elements (vertices or edges) in the motif. In this case, the returned columns will be in order of the pattern: “a, e, b, e2.”"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#usage",
    "href": "packages/graphframes/latest/reference/gf_find.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_find(x, pattern)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_find.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). pattern | pattern specifying a motif to search for"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#examples",
    "href": "packages/graphframes/latest/reference/gf_find.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_friends(sc) %>%\n  gf_find(\"(a)-[e]->(b); (b)-[e2]->(a)\")"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#description",
    "href": "packages/graphframes/latest/reference/gf_friends.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGraph of friends in a social network."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#usage",
    "href": "packages/graphframes/latest/reference/gf_friends.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_friends(sc)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_friends.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#examples",
    "href": "packages/graphframes/latest/reference/gf_friends.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\ngf_friends(sc)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#description",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCreate a new GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#usage",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_graphframe(vertices = NULL, edges)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nvertices\nA tbl_spark representing vertices.\n\n\nedges\nA tbl_psark representing edges."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#examples",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.3.0\")\nv_tbl <- sdf_copy_to(\n  sc, data.frame(id = 1:3, name = LETTERS[1:3])\n)\ne_tbl <- sdf_copy_to(\n  sc, data.frame(src = c(1, 2, 2), dst = c(2, 1, 3),\n                 action = c(\"love\", \"hate\", \"follow\"))\n)\ngf_graphframe(v_tbl, e_tbl)\ngf_graphframe(edges = e_tbl)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#description",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerate a grid Ising model with random parameters"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#usage",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_grid_ising_model(sc, n, v_std = 1, e_std = 1)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nLength of one side of the grid. The grid will be of size n x n.\n\n\nv_std\nStandard deviation of normal distribution used to generate vertex factors “a”. Default of 1.0.\n\n\ne_std\nStandard deviation of normal distribution used to generate edge factors “b”. Default of 1.0."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#details",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nThis method generates a grid Ising model with random parameters. Ising models are probabilistic graphical models over binary variables xi. Each binary variable xi corresponds to one vertex, and it may take values -1 or +1. The probability distribution P(X) (over all xi) is parameterized by vertex factors ai and edge factors bij:\nP(X) = (1/Z) * exp[ i a_i x_i + {ij} b_{ij} x_i x_j ]"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#value",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nGraphFrame. Vertices have columns “id” and “a”. Edges have columns “src”, “dst”, and “b”. Edges are directed, but they should be treated as undirected in any algorithms run on this model. Vertex IDs are of the form “i,j”. E.g., vertex “1,3” is in the second row and fourth column of the grid."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#examples",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_grid_ising_model(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_in_degrees.html#description",
    "href": "packages/graphframes/latest/reference/gf_in_degrees.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nIn-degrees of vertices"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_in_degrees.html#usage",
    "href": "packages/graphframes/latest/reference/gf_in_degrees.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_in_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_in_degrees.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_in_degrees.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#description",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRun static Label Propagation for detecting communities in networks. Each node in the network is initially assigned to its own community. At every iteration, nodes send their community affiliation to all neighbors and update their state to the mode community affiliation of incoming messages. LPA is a standard community detection algorithm for graphs. It is very inexpensive computationally, although (1) convergence is not guaranteed and (2) one can end up with trivial solutions (all nodes are identified into a single community)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#usage",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_lpa(x, max_iter, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). max_iter | Maximum number of iterations. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#examples",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_lpa(g, max_iter = 5)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_out_degrees.html#description",
    "href": "packages/graphframes/latest/reference/gf_out_degrees.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nOut-degrees of vertices"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_out_degrees.html#usage",
    "href": "packages/graphframes/latest/reference/gf_out_degrees.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_out_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_out_degrees.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_out_degrees.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#description",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPageRank"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#usage",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_pagerank(x, tol = NULL, reset_probability = 0.15, max_iter = NULL,\n  source_id = NULL, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). tol | Tolerance. reset_probability | Reset probability. max_iter | Maximum number of iterations. source_id | (Optional) Source vertex for a personalized pagerank. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#examples",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_pagerank(g, reset_probability = 0.15, tol = 0.01)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_persist.html#description",
    "href": "packages/graphframes/latest/reference/gf_persist.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPersist the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_persist.html#usage",
    "href": "packages/graphframes/latest/reference/gf_persist.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_persist(x, storage_level = \"MEMORY_AND_DISK\")"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_persist.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_persist.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). storage_level | The storage level to be used. Please view the http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistenceSpark Documentation for information on what storage levels are accepted."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_register.html#description",
    "href": "packages/graphframes/latest/reference/gf_register.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRegister a GraphFrame object"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_register.html#usage",
    "href": "packages/graphframes/latest/reference/gf_register.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_register(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_register.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_register.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#description",
    "href": "packages/graphframes/latest/reference/gf_scc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCompute the strongly connected component (SCC) of each vertex and return a DataFrame with each vertex assigned to the SCC containing that vertex."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#usage",
    "href": "packages/graphframes/latest/reference/gf_scc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_scc(x, max_iter, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_scc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). max_iter | Maximum number of iterations. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#examples",
    "href": "packages/graphframes/latest/reference/gf_scc.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_scc(g, max_iter = 10)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#description",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nComputes shortest paths from every vertex to the given set of landmark vertices. Note that this takes edge direction into account."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#usage",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_shortest_paths(x, landmarks, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). landmarks | IDs of landmark vertices. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#examples",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_shortest_paths(g, landmarks = c(\"a\", \"d\"))"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#description",
    "href": "packages/graphframes/latest/reference/gf_star.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReturns a star graph with Long ID type, consisting of a central element indexed 0 (the root) and the n other leaf vertices 1, 2, …, n."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#usage",
    "href": "packages/graphframes/latest/reference/gf_star.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_star(sc, n)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_star.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nThe number of leaves."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#examples",
    "href": "packages/graphframes/latest/reference/gf_star.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_star(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#description",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis algorithm ignores edge direction; i.e., all edges are treated as undirected. In a multigraph, duplicate edges will be counted only once."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#usage",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_triangle_count(x, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#examples",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_triangle_count(g)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triplets.html#description",
    "href": "packages/graphframes/latest/reference/gf_triplets.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nTriplets of graph"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triplets.html#usage",
    "href": "packages/graphframes/latest/reference/gf_triplets.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_triplets(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triplets.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_triplets.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#description",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nTwo densely connected blobs (vertices 0->n-1 and n->2n-1) connected by a single edge (0->n)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#usage",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_two_blobs(sc, blob_size)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nblob_size\nThe size of each blob."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#examples",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_two_blobs(sc, 3)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_unpersist.html#description",
    "href": "packages/graphframes/latest/reference/gf_unpersist.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nUnpersist the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_unpersist.html#usage",
    "href": "packages/graphframes/latest/reference/gf_unpersist.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_unpersist(x, blocking = FALSE)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_unpersist.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_unpersist.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). blocking | whether to block until all blocks are deleted"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertex_columns.html#description",
    "href": "packages/graphframes/latest/reference/gf_vertex_columns.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nVertices column names"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertex_columns.html#usage",
    "href": "packages/graphframes/latest/reference/gf_vertex_columns.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_vertex_columns(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertex_columns.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_vertex_columns.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertices.html#description",
    "href": "packages/graphframes/latest/reference/gf_vertices.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nExtract vertices DataFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertices.html#usage",
    "href": "packages/graphframes/latest/reference/gf_vertices.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_vertices(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertices.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_vertices.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/index.html",
    "href": "packages/graphframes/latest/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ngf_bfs()\nBreadth-first search (BFS)\n\n\ngf_cache()\nCache the GraphFrame\n\n\ngf_chain()\nChain graph\n\n\ngf_connected_components()\nConnected components\n\n\ngf_degrees()\nDegrees of vertices\n\n\ngf_edge_columns()\nEdges column names\n\n\ngf_edges()\nExtract edges DataFrame\n\n\ngf_find()\nMotif finding: Searching the graph for structural patterns\n\n\ngf_friends()\nGraph of friends in a social network.\n\n\ngf_graphframe()\nCreate a new GraphFrame\n\n\ngf_grid_ising_model()\nGenerate a grid Ising model with random parameters\n\n\ngf_in_degrees()\nIn-degrees of vertices\n\n\ngf_lpa()\nLabel propagation algorithm (LPA)\n\n\ngf_out_degrees()\nOut-degrees of vertices\n\n\ngf_pagerank()\nPageRank\n\n\ngf_persist()\nPersist the GraphFrame\n\n\ngf_register()\nRegister a GraphFrame object\n\n\ngf_scc()\nStrongly connected components\n\n\ngf_shortest_paths()\nShortest paths\n\n\ngf_star()\nGenerate a star graph\n\n\ngf_triangle_count()\nComputes the number of triangles passing through each vertex.\n\n\ngf_triplets()\nTriplets of graph\n\n\ngf_two_blobs()\nGenerate two blobs\n\n\ngf_unpersist()\nUnpersist the GraphFrame\n\n\ngf_vertex_columns()\nVertices column names\n\n\ngf_vertices()\nExtract vertices DataFrame\n\n\nspark_graphframe() spark_graphframe()\nRetrieve a GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/spark_graphframe.html#description",
    "href": "packages/graphframes/latest/reference/spark_graphframe.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieve a GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/spark_graphframe.html#usage",
    "href": "packages/graphframes/latest/reference/spark_graphframe.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_graphframe(x, ...)\n\nspark_graphframe(x, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/spark_graphframe.html#arguments",
    "href": "packages/graphframes/latest/reference/spark_graphframe.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). … | additional arguments, not used"
  },
  {
    "objectID": "packages/mleap/0.1.0/index.html#getting-started",
    "href": "packages/mleap/0.1.0/index.html#getting-started",
    "title": "sparklyr",
    "section": "Getting started",
    "text": "Getting started\nmleap can be installed from CRAN via\ninstall.packages(\"mleap\")\nor, for the latest development version from GitHub, using\ndevtools::install_github(\"rstudio/mleap\")\nOnce mleap has been installed, we can install the external dependencies using\nlibrary(mleap)\ninstall_maven()\n# Alternatively, if you already have Maven installed, you can \n#  set options(maven.home = \"path/to/maven\")\ninstall_mleap()\nWe can now export Spark ML pipelines from sparklyr.\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\n\n# Create a pipeline and fit it\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\n\n# A transformed data frame with the appropriate schema is required\n#   for exporting the pipeline model\ntransformed_tbl <- ml_transform(pipeline_model, mtcars_tbl)\n\n# Export model\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, transformed_tbl, model_path)\n\n# Disconnect from Spark\nspark_disconnect(sc)\nAt this point, we can share mtcars_model.zip with our deployment/implementation engineers, and they would be able to embed the model in another application. See the MLeap docs for details.\nWe also provide R functions for testing that the saved models behave as expected. Here we load the previously saved model:\nmodel <- mleap_load_bundle(model_path)\nmodel\n## MLeap Transformer\n## <fcf4647e-31ee-4d8e-b620-05b28c23a4c0> \n##   Name: pipeline_930b6090bbf9 \n##   Format: json \n##   MLeap Version: 0.10.0-SNAPSHOT\nWe can retrieve the schema associated with the model:\nmleap_model_schema(model)\n## # A tibble: 6 x 4\n##   name       type   nullable dimension\n##   <chr>      <chr>  <lgl>    <chr>    \n## 1 qsec       double TRUE     <NA>     \n## 2 hp         double FALSE    <NA>     \n## 3 wt         double TRUE     <NA>     \n## 4 big_hp     double FALSE    <NA>     \n## 5 features   double TRUE     (3)      \n## 6 prediction double FALSE    <NA>\nThen, we create a new data frame to be scored, and make predictions using our model:\nnewdata <- tibble::tribble(\n  ~qsec, ~hp, ~wt,\n  16.2,  101, 2.68,\n  18.1,  99,  3.08\n)\n\n# Transform the data frame\ntransformed_df <- mleap_transform(model, newdata)\ndplyr::glimpse(transformed_df)\n## Observations: 2\n## Variables: 6\n## $ qsec       <dbl> 16.2, 18.1\n## $ hp         <dbl> 101, 99\n## $ wt         <dbl> 2.68, 3.08\n## $ big_hp     <dbl> 1, 0\n## $ features   <list> [[[1, 2.68, 16.2], [3]], [[0, 3.08, 18.1], [3]]]\n## $ prediction <dbl> 21.06529, 22.36667"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/index.html",
    "href": "packages/mleap/0.1.0/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ninstall_maven()\nInstall Maven\n\n\ninstall_mleap()\nInstall MLeap runtime\n\n\nml_write_bundle()\nExport a Spark pipeline for serving\n\n\nmleap_installed_versions()\nFind existing MLeap installations\n\n\nmleap_load_bundle()\nLoads an MLeap bundle\n\n\nmleap_model_schema()\nMLeap model schema\n\n\nmleap_transform()\nTransform data using an MLeap model"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/install_maven.html#description",
    "href": "packages/mleap/0.1.0/reference/install_maven.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis function installs Apache Maven."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/install_maven.html#usage",
    "href": "packages/mleap/0.1.0/reference/install_maven.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninstall_maven(dir = NULL, version = NULL)"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/install_maven.html#arguments",
    "href": "packages/mleap/0.1.0/reference/install_maven.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\ndir\n(Optional) Directory to install maven in.\n\n\n\nDefaults to maven/ under user’s home directory. version | Version of Maven to install, defaults to the latest version tested with this package."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/install_maven.html#examples",
    "href": "packages/mleap/0.1.0/reference/install_maven.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ninstall_maven()"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/install_mleap.html#description",
    "href": "packages/mleap/0.1.0/reference/install_mleap.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nInstall MLeap runtime"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/install_mleap.html#usage",
    "href": "packages/mleap/0.1.0/reference/install_mleap.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninstall_mleap(dir = NULL, version = NULL, use_temp_cache = TRUE)"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/install_mleap.html#arguments",
    "href": "packages/mleap/0.1.0/reference/install_mleap.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\ndir\n(Optional) Directory to save the jars\n\n\nversion\nVersion of MLeap to install, defaults to the latest version tested with this package.\n\n\nuse_temp_cache\nWhether to use a temporary Maven cache directory for downloading.\n\n\n\nSetting this to TRUE prevents Maven from creating a persistent .m2/ directory. Defaults to TRUE."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/install_mleap.html#examples",
    "href": "packages/mleap/0.1.0/reference/install_mleap.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ninstall_mleap()"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/ml_write_bundle.html#description",
    "href": "packages/mleap/0.1.0/reference/ml_write_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis functions serializes a Spark pipeline model into an MLeap bundle."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/ml_write_bundle.html#usage",
    "href": "packages/mleap/0.1.0/reference/ml_write_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_write_bundle(x, dataset, path, overwrite = FALSE)"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/ml_write_bundle.html#arguments",
    "href": "packages/mleap/0.1.0/reference/ml_write_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark pipeline model object.\n\n\ndataset\nA Spark DataFrame with the schema of the transformed DataFrame.\n\n\npath\nWhere to save the bundle.\n\n\noverwrite\nWhether to overwrite an existing file, defaults to FALSE."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/ml_write_bundle.html#examples",
    "href": "packages/mleap/0.1.0/reference/ml_write_bundle.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, \n                ml_transform(pipeline_model, mtcars_tbl),\n                model_path,\n                overwrite = TRUE)"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_installed_versions.html#description",
    "href": "packages/mleap/0.1.0/reference/mleap_installed_versions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFind existing MLeap installations"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_installed_versions.html#usage",
    "href": "packages/mleap/0.1.0/reference/mleap_installed_versions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_installed_versions()"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_installed_versions.html#value",
    "href": "packages/mleap/0.1.0/reference/mleap_installed_versions.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA data frame of MLeap Runtime installation versions and their locations."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_load_bundle.html#description",
    "href": "packages/mleap/0.1.0/reference/mleap_load_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nLoads an MLeap bundle"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_load_bundle.html#usage",
    "href": "packages/mleap/0.1.0/reference/mleap_load_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_load_bundle(path)"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_load_bundle.html#arguments",
    "href": "packages/mleap/0.1.0/reference/mleap_load_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\npath\nPath to the exported bundle zip file."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_load_bundle.html#value",
    "href": "packages/mleap/0.1.0/reference/mleap_load_bundle.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_model_schema.html#description",
    "href": "packages/mleap/0.1.0/reference/mleap_model_schema.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReturns the schema of an MLeap transformer."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_model_schema.html#usage",
    "href": "packages/mleap/0.1.0/reference/mleap_model_schema.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_model_schema(x)"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_model_schema.html#arguments",
    "href": "packages/mleap/0.1.0/reference/mleap_model_schema.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_model_schema.html#value",
    "href": "packages/mleap/0.1.0/reference/mleap_model_schema.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA data frame of the model schema."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_transform.html#description",
    "href": "packages/mleap/0.1.0/reference/mleap_transform.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis functions transforms an R data frame using an MLeap model."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_transform.html#usage",
    "href": "packages/mleap/0.1.0/reference/mleap_transform.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_transform(model, data)"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_transform.html#arguments",
    "href": "packages/mleap/0.1.0/reference/mleap_transform.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nmodel\nAn MLeap model object, obtained by mleap_load_bundle().\n\n\ndata\nAn R data frame."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_transform.html#value",
    "href": "packages/mleap/0.1.0/reference/mleap_transform.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA transformed data frame."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_transform.html#see-also",
    "href": "packages/mleap/0.1.0/reference/mleap_transform.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\n[mleap_load_bundle()]"
  },
  {
    "objectID": "packages/mleap/0.1.3/index.html#getting-started",
    "href": "packages/mleap/0.1.3/index.html#getting-started",
    "title": "sparklyr",
    "section": "Getting started",
    "text": "Getting started\nmleap can be installed from CRAN via\ninstall.packages(\"mleap\")\nor, for the latest development version from GitHub, using\ndevtools::install_github(\"rstudio/mleap\")\nOnce mleap has been installed, we can install the external dependencies using\nlibrary(mleap)\ninstall_maven()\n# Alternatively, if you already have Maven installed, you can \n#  set options(maven.home = \"path/to/maven\")\ninstall_mleap()\nWe can now export Spark ML pipelines from sparklyr.\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.3.0\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\n\n# Create a pipeline and fit it\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\n\n# A transformed data frame with the appropriate schema is required\n#   for exporting the pipeline model\ntransformed_tbl <- ml_transform(pipeline_model, mtcars_tbl)\n\n# Export model\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, transformed_tbl, model_path)\n\n# Disconnect from Spark\nspark_disconnect(sc)\nAt this point, we can share mtcars_model.zip with our deployment/implementation engineers, and they would be able to embed the model in another application. See the MLeap docs for details.\nWe also provide R functions for testing that the saved models behave as expected. Here we load the previously saved model:\nmodel <- mleap_load_bundle(model_path)\nmodel\n## MLeap Transformer\n## <97ff1e90-5c3e-40fc-99dd-1919276e76be> \n##   Name: pipeline_1b49362281ef \n##   Format: json \n##   MLeap Version: 0.12.0\nWe can retrieve the schema associated with the model:\nmleap_model_schema(model)\n## # A tibble: 6 x 4\n##   name       type   nullable dimension\n##   <chr>      <chr>  <lgl>    <chr>    \n## 1 qsec       double TRUE     <NA>     \n## 2 hp         double FALSE    <NA>     \n## 3 wt         double TRUE     <NA>     \n## 4 big_hp     double FALSE    <NA>     \n## 5 features   double TRUE     (3)      \n## 6 prediction double FALSE    <NA>\nThen, we create a new data frame to be scored, and make predictions using our model:\nnewdata <- tibble::tribble(\n  ~qsec, ~hp, ~wt,\n  16.2,  101, 2.68,\n  18.1,  99,  3.08\n)\n\n# Transform the data frame\ntransformed_df <- mleap_transform(model, newdata)\ndplyr::glimpse(transformed_df)\n## Observations: 2\n## Variables: 6\n## $ qsec       <dbl> 16.2, 18.1\n## $ hp         <dbl> 101, 99\n## $ wt         <dbl> 2.68, 3.08\n## $ big_hp     <dbl> 1, 0\n## $ features   <list> [[[1, 2.68, 16.2], [3]], [[0, 3.08, 18.1], [3]]]\n## $ prediction <dbl> 21.00084, 20.56445"
  },
  {
    "objectID": "packages/mleap/0.1.3/news.html",
    "href": "packages/mleap/0.1.3/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "mleap 0.1.3\n\nSupport MLeap 0.12.0.\n\n\n\nmleap 0.1.2\n\nSupport MLeap 0.10.1 and Spark 2.3.0.\n\n\n\nmleap 0.1.1\n\nAllow heterogenous inputs (numeric and character) in mleap_transform() (#16)."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/index.html",
    "href": "packages/mleap/0.1.3/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ninstall_maven()\nInstall Maven\n\n\ninstall_mleap()\nInstall MLeap runtime\n\n\nml_write_bundle()\nExport a Spark pipeline for serving\n\n\nmleap_installed_versions()\nFind existing MLeap installations\n\n\nmleap_load_bundle()\nLoads an MLeap bundle\n\n\nmleap_model_schema()\nMLeap model schema\n\n\nmleap_transform()\nTransform data using an MLeap model"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/install_maven.html#description",
    "href": "packages/mleap/0.1.3/reference/install_maven.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis function installs Apache Maven."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/install_maven.html#usage",
    "href": "packages/mleap/0.1.3/reference/install_maven.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninstall_maven(dir = NULL, version = NULL)"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/install_maven.html#arguments",
    "href": "packages/mleap/0.1.3/reference/install_maven.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\ndir\n(Optional) Directory to install maven in.\n\n\n\nDefaults to maven/ under user’s home directory. version | Version of Maven to install, defaults to the latest version tested with this package."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/install_maven.html#examples",
    "href": "packages/mleap/0.1.3/reference/install_maven.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ninstall_maven()"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/install_mleap.html#description",
    "href": "packages/mleap/0.1.3/reference/install_mleap.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nInstall MLeap runtime"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/install_mleap.html#usage",
    "href": "packages/mleap/0.1.3/reference/install_mleap.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninstall_mleap(dir = NULL, version = NULL, use_temp_cache = TRUE)"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/install_mleap.html#arguments",
    "href": "packages/mleap/0.1.3/reference/install_mleap.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\ndir\n(Optional) Directory to save the jars\n\n\nversion\nVersion of MLeap to install, defaults to the latest version tested with this package.\n\n\nuse_temp_cache\nWhether to use a temporary Maven cache directory for downloading.\n\n\n\nSetting this to TRUE prevents Maven from creating a persistent .m2/ directory. Defaults to TRUE."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/install_mleap.html#examples",
    "href": "packages/mleap/0.1.3/reference/install_mleap.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ninstall_mleap()"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/ml_write_bundle.html#description",
    "href": "packages/mleap/0.1.3/reference/ml_write_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis functions serializes a Spark pipeline model into an MLeap bundle."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/ml_write_bundle.html#usage",
    "href": "packages/mleap/0.1.3/reference/ml_write_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_write_bundle(x, dataset, path, overwrite = FALSE)"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/ml_write_bundle.html#arguments",
    "href": "packages/mleap/0.1.3/reference/ml_write_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark pipeline model object.\n\n\ndataset\nA Spark DataFrame with the schema of the transformed DataFrame.\n\n\npath\nWhere to save the bundle.\n\n\noverwrite\nWhether to overwrite an existing file, defaults to FALSE."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/ml_write_bundle.html#examples",
    "href": "packages/mleap/0.1.3/reference/ml_write_bundle.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, \n                ml_transform(pipeline_model, mtcars_tbl),\n                model_path,\n                overwrite = TRUE)"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_installed_versions.html#description",
    "href": "packages/mleap/0.1.3/reference/mleap_installed_versions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFind existing MLeap installations"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_installed_versions.html#usage",
    "href": "packages/mleap/0.1.3/reference/mleap_installed_versions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_installed_versions()"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_installed_versions.html#value",
    "href": "packages/mleap/0.1.3/reference/mleap_installed_versions.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA data frame of MLeap Runtime installation versions and their locations."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_load_bundle.html#description",
    "href": "packages/mleap/0.1.3/reference/mleap_load_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nLoads an MLeap bundle"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_load_bundle.html#usage",
    "href": "packages/mleap/0.1.3/reference/mleap_load_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_load_bundle(path)"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_load_bundle.html#arguments",
    "href": "packages/mleap/0.1.3/reference/mleap_load_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\npath\nPath to the exported bundle zip file."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_load_bundle.html#value",
    "href": "packages/mleap/0.1.3/reference/mleap_load_bundle.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_model_schema.html#description",
    "href": "packages/mleap/0.1.3/reference/mleap_model_schema.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReturns the schema of an MLeap transformer."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_model_schema.html#usage",
    "href": "packages/mleap/0.1.3/reference/mleap_model_schema.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_model_schema(x)"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_model_schema.html#arguments",
    "href": "packages/mleap/0.1.3/reference/mleap_model_schema.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_model_schema.html#value",
    "href": "packages/mleap/0.1.3/reference/mleap_model_schema.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA data frame of the model schema."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_transform.html#description",
    "href": "packages/mleap/0.1.3/reference/mleap_transform.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis functions transforms an R data frame using an MLeap model."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_transform.html#usage",
    "href": "packages/mleap/0.1.3/reference/mleap_transform.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_transform(model, data)"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_transform.html#arguments",
    "href": "packages/mleap/0.1.3/reference/mleap_transform.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nmodel\nAn MLeap model object, obtained by mleap_load_bundle().\n\n\ndata\nAn R data frame."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_transform.html#value",
    "href": "packages/mleap/0.1.3/reference/mleap_transform.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA transformed data frame."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_transform.html#see-also",
    "href": "packages/mleap/0.1.3/reference/mleap_transform.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\n[mleap_load_bundle()]"
  },
  {
    "objectID": "packages/mleap/dev/index.html#getting-started",
    "href": "packages/mleap/dev/index.html#getting-started",
    "title": "sparklyr",
    "section": "Getting started",
    "text": "Getting started\nmleap can be installed from CRAN via\ninstall.packages(\"mleap\")\nor, for the latest development version from GitHub, using\ndevtools::install_github(\"rstudio/mleap\")\nOnce mleap has been installed, we can install the external dependencies using\nlibrary(mleap)\ninstall_maven()\n# Alternatively, if you already have Maven installed, you can \n#  set options(maven.home = \"path/to/maven\")\ninstall_mleap()\nWe can now export Spark ML pipelines from sparklyr.\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.4.0\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\n\n# Create a pipeline and fit it\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\n\n# Export model\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, sample_input = mtcars_tbl, path = model_path)\n\n# Disconnect from Spark\nspark_disconnect(sc)\n## NULL\nAt this point, we can share mtcars_model.zip with our deployment/implementation engineers, and they would be able to embed the model in another application. See the MLeap docs for details.\nWe also provide R functions for testing that the saved models behave as expected. Here we load the previously saved model:\nmodel <- mleap_load_bundle(model_path)\nmodel\n## MLeap Transformer\n## <7e2f61ed-154b-4c9e-9926-85fa326d69ef> \n##   Name: pipeline_c1754f374a53 \n##   Format: json \n##   MLeap Version: 0.14.0\nWe can retrieve the schema associated with the model:\nmleap_model_schema(model)\n## # A tibble: 6 x 5\n##   name       type   nullable dimension io    \n##   <chr>      <chr>  <lgl>    <chr>     <chr> \n## 1 qsec       double TRUE     <NA>      input \n## 2 hp         double FALSE    <NA>      input \n## 3 wt         double TRUE     <NA>      input \n## 4 big_hp     double FALSE    <NA>      output\n## 5 features   double TRUE     (3)       output\n## 6 prediction double FALSE    <NA>      output\nThen, we create a new data frame to be scored, and make predictions using our model:\nnewdata <- tibble::tribble(\n  ~qsec, ~hp, ~wt,\n  16.2,  101, 2.68,\n  18.1,  99,  3.08\n)\n\n# Transform the data frame\ntransformed_df <- mleap_transform(model, newdata)\ndplyr::glimpse(transformed_df)\n## Observations: 2\n## Variables: 6\n## $ qsec       <dbl> 16.2, 18.1\n## $ hp         <dbl> 101, 99\n## $ wt         <dbl> 2.68, 3.08\n## $ big_hp     <dbl> 1, 0\n## $ features   <list> [[[1, 2.68, 16.2], [3]], [[0, 3.08, 18.1], [3]]]\n## $ prediction <dbl> 21.00084, 20.56445"
  },
  {
    "objectID": "packages/mleap/dev/news.html",
    "href": "packages/mleap/dev/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "mleap 1.0.0\n\nBreaking change: ml_write_bundle() now takes the sample_input instead of dataset.\nSupport for Spark 2.4 and MLeap 0.14.\nInteger inputs are now supported (#36).\n\n\n\nmleap 0.1.3\n\nSupport MLeap 0.12.0.\n\n\n\nmleap 0.1.2\n\nSupport MLeap 0.10.1 and Spark 2.3.0.\n\n\n\nmleap 0.1.1\n\nAllow heterogenous inputs (numeric and character) in mleap_transform() (#16)."
  },
  {
    "objectID": "packages/mleap/dev/reference/index.html",
    "href": "packages/mleap/dev/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ninstall_maven()\nInstall Maven\n\n\ninstall_mleap()\nInstall MLeap runtime\n\n\nml_write_bundle()\nExport a Spark pipeline for serving\n\n\nmleap_installed_versions()\nFind existing MLeap installations\n\n\nmleap_load_bundle()\nLoads an MLeap bundle\n\n\nmleap_model_schema()\nMLeap model schema\n\n\nmleap_transform()\nTransform data using an MLeap model"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#description",
    "href": "packages/mleap/dev/reference/install_maven.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis function installs Apache Maven."
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#usage",
    "href": "packages/mleap/dev/reference/install_maven.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninstall_maven(dir = NULL, version = NULL)"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#arguments",
    "href": "packages/mleap/dev/reference/install_maven.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\ndir\n(Optional) Directory to install maven in.\n\n\n\nDefaults to maven/ under user’s home directory. version | Version of Maven to install, defaults to the latest version tested with this package."
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#examples",
    "href": "packages/mleap/dev/reference/install_maven.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ninstall_maven()"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#description",
    "href": "packages/mleap/dev/reference/install_mleap.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nInstall MLeap runtime"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#usage",
    "href": "packages/mleap/dev/reference/install_mleap.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninstall_mleap(dir = NULL, version = NULL, use_temp_cache = TRUE)"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#arguments",
    "href": "packages/mleap/dev/reference/install_mleap.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\ndir\n(Optional) Directory to save the jars\n\n\nversion\nVersion of MLeap to install, defaults to the latest version tested with this package.\n\n\nuse_temp_cache\nWhether to use a temporary Maven cache directory for downloading.\n\n\n\nSetting this to TRUE prevents Maven from creating a persistent .m2/ directory. Defaults to TRUE."
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#examples",
    "href": "packages/mleap/dev/reference/install_mleap.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ninstall_mleap()"
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#description",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis functions serializes a Spark pipeline model into an MLeap bundle."
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#usage",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_write_bundle(x, sample_input, path, overwrite = FALSE)"
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#arguments",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark pipeline model object.\n\n\nsample_input\nA sample input Spark DataFrame with the expected schema.\n\n\npath\nWhere to save the bundle.\n\n\noverwrite\nWhether to overwrite an existing file, defaults to FALSE."
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#examples",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, \n                mtcars_tbl,\n                model_path,\n                overwrite = TRUE)"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_installed_versions.html#description",
    "href": "packages/mleap/dev/reference/mleap_installed_versions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFind existing MLeap installations"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_installed_versions.html#usage",
    "href": "packages/mleap/dev/reference/mleap_installed_versions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_installed_versions()"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_installed_versions.html#value",
    "href": "packages/mleap/dev/reference/mleap_installed_versions.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA data frame of MLeap Runtime installation versions and their locations."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#description",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nLoads an MLeap bundle"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#usage",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_load_bundle(path)"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#arguments",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\npath\nPath to the exported bundle zip file."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#value",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#description",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReturns the schema of an MLeap transformer."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#usage",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_model_schema(x)"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#arguments",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#value",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA data frame of the model schema."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#description",
    "href": "packages/mleap/dev/reference/mleap_transform.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis functions transforms an R data frame using an MLeap model."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#usage",
    "href": "packages/mleap/dev/reference/mleap_transform.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_transform(model, data)"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#arguments",
    "href": "packages/mleap/dev/reference/mleap_transform.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nmodel\nAn MLeap model object, obtained by mleap_load_bundle().\n\n\ndata\nAn R data frame."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#value",
    "href": "packages/mleap/dev/reference/mleap_transform.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA transformed data frame."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#see-also",
    "href": "packages/mleap/dev/reference/mleap_transform.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\n[mleap_load_bundle()]"
  },
  {
    "objectID": "packages/mleap/latest/index.html#getting-started",
    "href": "packages/mleap/latest/index.html#getting-started",
    "title": "sparklyr",
    "section": "Getting started",
    "text": "Getting started\nmleap can be installed from CRAN via\ninstall.packages(\"mleap\")\nor, for the latest development version from GitHub, using\ndevtools::install_github(\"rstudio/mleap\")\nOnce mleap has been installed, we can install the external dependencies using\nlibrary(mleap)\ninstall_maven()\n# Alternatively, if you already have Maven installed, you can \n#  set options(maven.home = \"path/to/maven\")\ninstall_mleap()\nWe can now export Spark ML pipelines from sparklyr.\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.4.0\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\n\n# Create a pipeline and fit it\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\n\n# Export model\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, sample_input = mtcars_tbl, path = model_path)\n\n# Disconnect from Spark\nspark_disconnect(sc)\n## NULL\nAt this point, we can share mtcars_model.zip with our deployment/implementation engineers, and they would be able to embed the model in another application. See the MLeap docs for details.\nWe also provide R functions for testing that the saved models behave as expected. Here we load the previously saved model:\nmodel <- mleap_load_bundle(model_path)\nmodel\n## MLeap Transformer\n## <7e2f61ed-154b-4c9e-9926-85fa326d69ef> \n##   Name: pipeline_c1754f374a53 \n##   Format: json \n##   MLeap Version: 0.14.0\nWe can retrieve the schema associated with the model:\nmleap_model_schema(model)\n## # A tibble: 6 x 5\n##   name       type   nullable dimension io    \n##   <chr>      <chr>  <lgl>    <chr>     <chr> \n## 1 qsec       double TRUE     <NA>      input \n## 2 hp         double FALSE    <NA>      input \n## 3 wt         double TRUE     <NA>      input \n## 4 big_hp     double FALSE    <NA>      output\n## 5 features   double TRUE     (3)       output\n## 6 prediction double FALSE    <NA>      output\nThen, we create a new data frame to be scored, and make predictions using our model:\nnewdata <- tibble::tribble(\n  ~qsec, ~hp, ~wt,\n  16.2,  101, 2.68,\n  18.1,  99,  3.08\n)\n\n# Transform the data frame\ntransformed_df <- mleap_transform(model, newdata)\ndplyr::glimpse(transformed_df)\n## Observations: 2\n## Variables: 6\n## $ qsec       <dbl> 16.2, 18.1\n## $ hp         <dbl> 101, 99\n## $ wt         <dbl> 2.68, 3.08\n## $ big_hp     <dbl> 1, 0\n## $ features   <list> [[[1, 2.68, 16.2], [3]], [[0, 3.08, 18.1], [3]]]\n## $ prediction <dbl> 21.00084, 20.56445"
  },
  {
    "objectID": "packages/mleap/latest/news.html",
    "href": "packages/mleap/latest/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "mleap 1.0.0\n\nBreaking change: ml_write_bundle() now takes the sample_input instead of dataset.\nSupport for Spark 2.4 and MLeap 0.14.\nInteger inputs are now supported (#36).\n\n\n\nmleap 0.1.3\n\nSupport MLeap 0.12.0.\n\n\n\nmleap 0.1.2\n\nSupport MLeap 0.10.1 and Spark 2.3.0.\n\n\n\nmleap 0.1.1\n\nAllow heterogenous inputs (numeric and character) in mleap_transform() (#16)."
  },
  {
    "objectID": "packages/mleap/latest/reference/index.html",
    "href": "packages/mleap/latest/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ninstall_maven()\nInstall Maven\n\n\ninstall_mleap()\nInstall MLeap runtime\n\n\nml_write_bundle()\nExport a Spark pipeline for serving\n\n\nmleap_installed_versions()\nFind existing MLeap installations\n\n\nmleap_load_bundle()\nLoads an MLeap bundle\n\n\nmleap_model_schema()\nMLeap model schema\n\n\nmleap_transform()\nTransform data using an MLeap model"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#description",
    "href": "packages/mleap/latest/reference/install_maven.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis function installs Apache Maven."
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#usage",
    "href": "packages/mleap/latest/reference/install_maven.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninstall_maven(dir = NULL, version = NULL)"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#arguments",
    "href": "packages/mleap/latest/reference/install_maven.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\ndir\n(Optional) Directory to install maven in.\n\n\n\nDefaults to maven/ under user’s home directory. version | Version of Maven to install, defaults to the latest version tested with this package."
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#examples",
    "href": "packages/mleap/latest/reference/install_maven.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ninstall_maven()"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#description",
    "href": "packages/mleap/latest/reference/install_mleap.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nInstall MLeap runtime"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#usage",
    "href": "packages/mleap/latest/reference/install_mleap.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninstall_mleap(dir = NULL, version = NULL, use_temp_cache = TRUE)"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#arguments",
    "href": "packages/mleap/latest/reference/install_mleap.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\ndir\n(Optional) Directory to save the jars\n\n\nversion\nVersion of MLeap to install, defaults to the latest version tested with this package.\n\n\nuse_temp_cache\nWhether to use a temporary Maven cache directory for downloading.\n\n\n\nSetting this to TRUE prevents Maven from creating a persistent .m2/ directory. Defaults to TRUE."
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#examples",
    "href": "packages/mleap/latest/reference/install_mleap.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ninstall_mleap()"
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#description",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis functions serializes a Spark pipeline model into an MLeap bundle."
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#usage",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_write_bundle(x, sample_input, path, overwrite = FALSE)"
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#arguments",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark pipeline model object.\n\n\nsample_input\nA sample input Spark DataFrame with the expected schema.\n\n\npath\nWhere to save the bundle.\n\n\noverwrite\nWhether to overwrite an existing file, defaults to FALSE."
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#examples",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, \n                mtcars_tbl,\n                model_path,\n                overwrite = TRUE)"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_installed_versions.html#description",
    "href": "packages/mleap/latest/reference/mleap_installed_versions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFind existing MLeap installations"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_installed_versions.html#usage",
    "href": "packages/mleap/latest/reference/mleap_installed_versions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_installed_versions()"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_installed_versions.html#value",
    "href": "packages/mleap/latest/reference/mleap_installed_versions.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA data frame of MLeap Runtime installation versions and their locations."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#description",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nLoads an MLeap bundle"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#usage",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_load_bundle(path)"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#arguments",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\npath\nPath to the exported bundle zip file."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#value",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#description",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReturns the schema of an MLeap transformer."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#usage",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_model_schema(x)"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#arguments",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#value",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA data frame of the model schema."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#description",
    "href": "packages/mleap/latest/reference/mleap_transform.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis functions transforms an R data frame using an MLeap model."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#usage",
    "href": "packages/mleap/latest/reference/mleap_transform.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_transform(model, data)"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#arguments",
    "href": "packages/mleap/latest/reference/mleap_transform.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nmodel\nAn MLeap model object, obtained by mleap_load_bundle().\n\n\ndata\nAn R data frame."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#value",
    "href": "packages/mleap/latest/reference/mleap_transform.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA transformed data frame."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#see-also",
    "href": "packages/mleap/latest/reference/mleap_transform.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\n[mleap_load_bundle()]"
  },
  {
    "objectID": "packages/sparklyr/latest/news.html#distributed-r-7",
    "href": "packages/sparklyr/latest/news.html#distributed-r-7",
    "title": "sparklyr",
    "section": "Distributed R",
    "text": "Distributed R\n\nThe memory parameter in spark_apply() now defaults to FALSE when the name parameter is not specified."
  },
  {
    "objectID": "packages/sparklyr/latest/news.html#other",
    "href": "packages/sparklyr/latest/news.html#other",
    "title": "sparklyr",
    "section": "Other",
    "text": "Other\n\nRemoved dreprecated sdf_mutate().\nRemove exported ensure_ functions which were deprecated.\nFixed missing Hive tables not rendering under some Spark distributions (#1823).\nRemove dependency on broom.\nFixed re-entrancy job progress issues when running RStudio 1.2.\nTables with periods supported by setting sparklyr.dplyr.period.splits to FALSE.\nsdf_len(), sdf_along() and sdf_seq() default to 32 bit integers but allow support for 64 bits through bits parameter.\nSupport for detecting Spark version using spark-submit."
  },
  {
    "objectID": "packages/sparklyr/latest/news.html#batches-1",
    "href": "packages/sparklyr/latest/news.html#batches-1",
    "title": "sparklyr",
    "section": "Batches",
    "text": "Batches\n\nAdded support for spark_submit() to assist submitting non-interactive Spark jobs.\n\n\nSpark ML\n\n(Breaking change) The formula API for ML classification algorithms no longer indexes numeric labels, to avoid the confusion of 0 being mapped to \"1\" and vice versa. This means that if the largest numeric label is N, Spark will fit a N+1-class classification model, regardless of how many distinct labels there are in the provided training set (#1591).\nFix retrieval of coefficients in ml_logistic_regression() (@shabbybanks, #1596).\n(Breaking change) For model objects, lazy val and def attributes have been converted to closures, so they are not evaluated at object instantiation (#1453).\nInput and output column names are no longer required to construct pipeline objects to be consistent with Spark (#1513).\nVector attributes of pipeline stages are now printed correctly (#1618).\nDeprecate various aliases favoring method names in Spark.\n\nml_binary_classification_eval()\nml_classification_eval()\nml_multilayer_perceptron()\nml_survival_regression()\nml_als_factorization()\n\nDeprecate incompatible signatures for sdf_transform() and ml_transform() families of methods; the former should take a tbl_spark as the first argument while the latter should take a model object as the first argument.\nInput and output column names are no longer required to construct pipeline objects to be consistent with Spark (#1513).\n\n\n\nData\n\nImplemented support for DBI::db_explain() (#1623).\nFixed for timestamp fields when using copy_to() (#1312, @yutannihilation).\nAdded support to read and write ORC files using spark_read_orc() and spark_write_orc() (#1548).\n\n\n\nLivy\n\nFixed must share the same src error for sdf_broadcast() and other functions when using Livy connections.\nAdded support for logging sparklyr server events and logging sparklyr invokes as comments in the Livy UI.\nAdded support to open the Livy UI from the connections viewer while using RStudio.\nImprove performance in Livy for long execution queries, fixed livy.session.command.timeout and support for livy.session.command.interval to control max polling while waiting for command response (#1538).\nFixed Livy version with MapR distributions.\nRemoved install column from livy_available_versions().\n\n\n\nDistributed R\n\nAdded name parameter to spark_apply() to optionally name resulting table.\nFix to spark_apply() to retain column types when NAs are present (#1665).\nspark_apply() now supports rlang anonymous functions. For example, sdf_len(sc, 3) %>% spark_apply(~.x+1).\nBreaking Change: spark_apply() no longer defaults to the input column names when the columns parameter is nos specified.\nSupport for reading column names from the R data frame returned by spark_apply().\nFix to support retrieving empty data frames in grouped spark_apply() operations (#1505).\nAdded support for sparklyr.apply.packages to configure default behavior for spark_apply() parameters (#1530).\nAdded support for spark.r.libpaths to configure package library in spark_apply() (#1530).\n\n\n\nConnections\n\nDefault to Spark 2.3.1 for installation and local connections (#1680).\nml_load() no longer keeps extraneous table views which was cluttering up the RStudio Connections pane (@randomgambit, #1549).\nAvoid preparing windows environment in non-local connections.\n\n\n\nExtensions\n\nThe ensure_* family of functions is deprecated in favor of forge which doesn’t use NSE and provides more informative errors messages for debugging (#1514).\nSupport for sparklyr.invoke.trace and sparklyr.invoke.trace.callstack configuration options to trace all invoke() calls.\nSupport to invoke methods with char types using single character strings (@lawremi, #1395).\n\n\n\nSerialization\n\nFixed collection of Date types to support correct local JVM timezone to UTC ().\n\n\n\nDocumentation\n\nMany new examples for ft_binarizer(), ft_bucketizer(), ft_min_max_scaler, ft_max_abs_scaler(), ft_standard_scaler(), ml_kmeans(), ml_pca(), ml_bisecting_kmeans(), ml_gaussian_mixture(), ml_naive_bayes(), ml_decision_tree(), ml_random_forest(), ml_multilayer_perceptron_classifier(), ml_linear_regression(), ml_logistic_regression(), ml_gradient_boosted_trees(), ml_generalized_linear_regression(), ml_cross_validator(), ml_evaluator(), ml_clustering_evaluator(), ml_corr(), ml_chisquare_test() and sdf_pivot() (@samuelmacedo83).\n\n\n\nBroom\n\nImplemented tidy(), augment(), and glance() for ml_aft_survival_regression(), ml_isotonic_regression(), ml_naive_bayes(), ml_logistic_regression(), ml_decision_tree(), ml_random_forest(), ml_gradient_boosted_trees(), ml_bisecting_kmeans(), ml_kmeans()and ml_gaussian_mixture() models (@samuelmacedo83)\n\n\n\nConfiguration\n\nDeprecated configuration option sparklyr.dplyr.compute.nocache.\nAdded spark_config_settings() to list all sparklyr configuration settings and describe them, cleaned all settings and grouped by area while maintaining support for previous settings.\nStatic SQL configuration properties are now respected for Spark 2.3, and spark.sql.catalogImplementation defaults to hive to maintain Hive support (#1496, #415).\nspark_config() values can now also be specified as options().\nSupport for functions as values in entries to spark_config() to enable advanced configuration workflows."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/arrow_enabled_object.html#description",
    "href": "packages/sparklyr/latest/reference/arrow_enabled_object.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nIf the given R object is not serializable by arrow due to some known limitations of arrow, then return FALSE, otherwise return TRUE"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/arrow_enabled_object.html#usage",
    "href": "packages/sparklyr/latest/reference/arrow_enabled_object.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\narrow_enabled_object(object)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/arrow_enabled_object.html#arguments",
    "href": "packages/sparklyr/latest/reference/arrow_enabled_object.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nobject\nThe object to be serialized"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/arrow_enabled_object.html#examples",
    "href": "packages/sparklyr/latest/reference/arrow_enabled_object.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\ndf <- tibble::tibble(x = seq(5))\narrow_enabled_object(df)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/checkpoint_directory.html#description",
    "href": "packages/sparklyr/latest/reference/checkpoint_directory.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSet/Get Spark checkpoint directory"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/checkpoint_directory.html#usage",
    "href": "packages/sparklyr/latest/reference/checkpoint_directory.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_set_checkpoint_dir(sc, dir)\n\nspark_get_checkpoint_dir(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/checkpoint_directory.html#arguments",
    "href": "packages/sparklyr/latest/reference/checkpoint_directory.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\ndir\ncheckpoint directory, must be HDFS path of running on cluster"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect.html#description",
    "href": "packages/sparklyr/latest/reference/collect.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee collect for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect_from_rds.html#description",
    "href": "packages/sparklyr/latest/reference/collect_from_rds.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nDeserialize Spark data that is serialized using spark_write_rds() into a R dataframe."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect_from_rds.html#usage",
    "href": "packages/sparklyr/latest/reference/collect_from_rds.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ncollect_from_rds(path)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect_from_rds.html#arguments",
    "href": "packages/sparklyr/latest/reference/collect_from_rds.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\npath\nPath to a local RDS file that is produced by spark_write_rds()\n\n\n\n(RDS files stored in HDFS will need to be downloaded to local filesystem first (e.g., by running hadoop fs -copyToLocal ... or similar)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/collect_from_rds.html#see-also",
    "href": "packages/sparklyr/latest/reference/collect_from_rds.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/compile_package_jars.html#description",
    "href": "packages/sparklyr/latest/reference/compile_package_jars.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCompile the scala source files contained within an package into a Java Archive (jar) file that can be loaded and used within a Spark environment."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/compile_package_jars.html#usage",
    "href": "packages/sparklyr/latest/reference/compile_package_jars.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ncompile_package_jars(..., spec = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/compile_package_jars.html#arguments",
    "href": "packages/sparklyr/latest/reference/compile_package_jars.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\n…\nOptional compilation specifications, as generated by\n\n\n\nspark_compilation_spec. When no arguments are passed, spark_default_compilation_spec is used instead. spec | An optional list of compilation specifications. When set, this option takes precedence over arguments passed to ...."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_config.html#description",
    "href": "packages/sparklyr/latest/reference/connection_config.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead configuration values for a connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_config.html#usage",
    "href": "packages/sparklyr/latest/reference/connection_config.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nconnection_config(sc, prefix, not_prefix = list())"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_config.html#arguments",
    "href": "packages/sparklyr/latest/reference/connection_config.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nspark_connection\n\n\nprefix\nPrefix to read parameters for\n\n\n\n(e.g. spark.context., spark.sql., etc.) not_prefix | Prefix to not include."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_config.html#value",
    "href": "packages/sparklyr/latest/reference/connection_config.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nNamed list of config parameters (note that if a prefix was specified then the names will not include the prefix)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_is_open.html#description",
    "href": "packages/sparklyr/latest/reference/connection_is_open.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCheck whether the connection is open"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_is_open.html#usage",
    "href": "packages/sparklyr/latest/reference/connection_is_open.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nconnection_is_open(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_is_open.html#arguments",
    "href": "packages/sparklyr/latest/reference/connection_is_open.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nspark_connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_spark_shinyapp.html#description",
    "href": "packages/sparklyr/latest/reference/connection_spark_shinyapp.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nA Shiny app that can be used to construct a spark_connect statement"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/connection_spark_shinyapp.html#usage",
    "href": "packages/sparklyr/latest/reference/connection_spark_shinyapp.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nconnection_spark_shinyapp()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.html#description",
    "href": "packages/sparklyr/latest/reference/copy_to.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee copy_to for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#description",
    "href": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCopy an R data.frame to Spark, and return a reference to the generated Spark DataFrame as a tbl_spark. The returned object will act as a dplyr-compatible interface to the underlying Spark table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#usage",
    "href": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ncopy_tospark_connection( dest, df, name = spark_table_name(substitute(df)), overwrite = FALSE, memory = TRUE, repartition = 0L, … )"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#arguments",
    "href": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\ndest\nA spark_connection.\n\n\ndf\nAn data.frame.\n\n\nname\nThe name to assign to the copied table in Spark.\n\n\noverwrite\nBoolean; overwrite a pre-existing table with the name name\n\n\n\nif one already exists? memory | Boolean; should the table be cached into memory? repartition | The number of partitions to use when distributing the table across the Spark cluster. The default (0) can be used to avoid partitioning. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#value",
    "href": "packages/sparklyr/latest/reference/copy_to.spark_connection.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA tbl_spark, representing a dplyr-compatible interface to a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/dbisparkresult-class.html#description",
    "href": "packages/sparklyr/latest/reference/dbisparkresult-class.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nDBI Spark Result."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/distinct.html#description",
    "href": "packages/sparklyr/latest/reference/distinct.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee distinct for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/download_scalac.html#description",
    "href": "packages/sparklyr/latest/reference/download_scalac.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\ncompile_package_jars requires several versions of the scala compiler to work, this is to match Spark scala versions. To help setup your environment, this function will download the required compilers under the default search path."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/download_scalac.html#usage",
    "href": "packages/sparklyr/latest/reference/download_scalac.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ndownload_scalac(dest_path = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/download_scalac.html#arguments",
    "href": "packages/sparklyr/latest/reference/download_scalac.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\ndest_path\nThe destination path where scalac will be\n\n\n\ndownloaded to."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/download_scalac.html#details",
    "href": "packages/sparklyr/latest/reference/download_scalac.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nSee find_scalac for a list of paths searched and used by this function to install the required compilers."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/dplyr_hof.html#description",
    "href": "packages/sparklyr/latest/reference/dplyr_hof.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThese methods implement dplyr grammars for Apache Spark higher order functions"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ensure.html#description",
    "href": "packages/sparklyr/latest/reference/ensure.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThese routines are useful when preparing to pass objects to a Spark routine, as it is often necessary to ensure certain parameters are scalar integers, or scalar doubles, and so on."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ensure.html#arguments",
    "href": "packages/sparklyr/latest/reference/ensure.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nobject\nAn object.\n\n\nallow.na\nAre NA values permitted for this object?\n\n\nallow.null\nAre NULL values permitted for this object?\n\n\ndefault\nIf object is NULL, what value should\n\n\n\nbe used in its place? If default is specified, allow.null is ignored (and assumed to be TRUE)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/fill.html#description",
    "href": "packages/sparklyr/latest/reference/fill.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee fill for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/filter.html#description",
    "href": "packages/sparklyr/latest/reference/filter.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee filter for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/find_scalac.html#description",
    "href": "packages/sparklyr/latest/reference/find_scalac.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFind the scalac compiler for a particular version of scala, by scanning some common directories containing scala installations."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/find_scalac.html#usage",
    "href": "packages/sparklyr/latest/reference/find_scalac.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nfind_scalac(version, locations = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/find_scalac.html#arguments",
    "href": "packages/sparklyr/latest/reference/find_scalac.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nversion\nThe scala version to search for. Versions\n\n\n\nof the form major.minor will be matched against the scalac installation with version major.minor.patch; if multiple compilers are discovered the most recent one will be used. locations | Additional locations to scan. By default, the directories /opt/scala and /usr/local/scala will be scanned."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nApply thresholding to a column, such that values less than or equal to the threshold are assigned the value 0.0, and values greater than the threshold are assigned the value 1.0. Column output is numeric for compatibility with other modeling functions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_binarizer(\n  x,\n  input_col,\n  output_col,\n  threshold = 0,\n  uid = random_string(\"binarizer_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nthreshold\nThreshold used to binarize continuous features.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#examples",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\niris_tbl %>%\n  ft_binarizer(\n    input_col = \"Sepal_Length\",\n    output_col = \"Sepal_Length_bin\",\n    threshold = 5\n  ) %>%\n  select(Sepal_Length, Sepal_Length_bin, Species)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_binarizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_binarizer.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSimilar to ’s cut function, this transforms a numeric column into a discretized column, with breaks specified through the splits parameter."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_bucketizer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  splits = NULL,\n  input_cols = NULL,\n  output_cols = NULL,\n  splits_array = NULL,\n  handle_invalid = \"error\",\n  uid = random_string(\"bucketizer_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nsplits\nA numeric vector of cutpoints, indicating the bucket boundaries.\n\n\ninput_cols\nNames of input columns.\n\n\noutput_cols\nNames of output columns.\n\n\nsplits_array\nParameter for specifying multiple splits parameters. Each\n\n\n\nelement in this array can be used to map continuous features into buckets. handle_invalid | (Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error” uid | A character string used to uniquely identify the feature transformer. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#examples",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\niris_tbl %>%\n  ft_bucketizer(\n    input_col = \"Sepal_Length\",\n    output_col = \"Sepal_Length_bucket\",\n    splits = c(0, 4.5, 5, 8)\n  ) %>%\n  select(Sepal_Length, Sepal_Length_bucket, Species)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_bucketizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_bucketizer.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_chisq_selector.html#description",
    "href": "packages/sparklyr/latest/reference/ft_chisq_selector.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nChi-Squared feature selection, which selects categorical features to use for predicting a categorical label"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_chisq_selector.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_chisq_selector.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_chisq_selector(\n  x,\n  features_col = \"features\",\n  output_col = NULL,\n  label_col = \"label\",\n  selector_type = \"numTopFeatures\",\n  fdr = 0.05,\n  fpr = 0.05,\n  fwe = 0.05,\n  num_top_features = 50,\n  percentile = 0.1,\n  uid = random_string(\"chisq_selector_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_chisq_selector.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_chisq_selector.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\noutput_col\nThe name of the output column.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nselector_type\n(Spark 2.1.0+) The selector type of the ChisqSelector. Supported options: “numTopFeatures” (default), “percentile”, “fpr”, “fdr”, “fwe”.\n\n\nfdr\n(Spark 2.2.0+) The upper bound of the expected false discovery rate. Only applicable when selector_type = “fdr”. Default value is 0.05.\n\n\nfpr\n(Spark 2.1.0+) The highest p-value for features to be kept. Only applicable when selector_type= “fpr”. Default value is 0.05.\n\n\nfwe\n(Spark 2.2.0+) The upper bound of the expected family-wise error rate. Only applicable when selector_type = “fwe”. Default value is 0.05.\n\n\nnum_top_features\nNumber of features that selector will select, ordered by ascending p-value. If the number of features is less than num_top_features, then this will select all features. Only applicable when selector_type = “numTopFeatures”. The default value of num_top_features is 50.\n\n\npercentile\n(Spark 2.1.0+) Percentile of features that selector will select, ordered by statistics value descending. Only applicable when selector_type = “percentile”. Default value is 0.1.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_chisq_selector.html#details",
    "href": "packages/sparklyr/latest/reference/ft_chisq_selector.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_chisq_selector.html#value",
    "href": "packages/sparklyr/latest/reference/ft_chisq_selector.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_chisq_selector.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_chisq_selector.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nExtracts a vocabulary from document collections."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_count_vectorizer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  binary = FALSE,\n  min_df = 1,\n  min_tf = 1,\n  vocab_size = 2^18,\n  uid = random_string(\"count_vectorizer_\"),\n  ...\n)\n\nml_vocabulary(model)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nbinary\nBinary toggle to control the output vector values.\n\n\n\nIf TRUE, all nonzero counts (after min_tf filter applied) are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. Default: FALSE min_df | Specifies the minimum number of different documents a term must appear in to be included in the vocabulary. If this is an integer greater than or equal to 1, this specifies the number of documents the term must appear in; if this is a double in [0,1), then this specifies the fraction of documents. Default: 1. min_tf | Filter to ignore rare words in a document. For each document, terms with frequency/count less than the given threshold are ignored. If this is an integer greater than or equal to 1, then this specifies a count (of times the term must appear in the document); if this is a double in [0,1), then this specifies a fraction (out of the document’s token count). Default: 1. vocab_size | Build a vocabulary that only considers the top vocab_size terms ordered by term frequency across the corpus. Default: 2^18. uid | A character string used to uniquely identify the feature transformer. … | Optional arguments; currently unused. model | A ml_count_vectorizer_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#details",
    "href": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark\n\nml_vocabulary() returns a vector of vocabulary built."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_count_vectorizer.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#description",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nA feature transformer that takes the 1D discrete cosine transform of a real vector. No zero padding is performed on the input vector. It returns a real vector of the same length representing the DCT. The return vector is scaled such that the transform matrix is unitary (aka scaled DCT-II)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_dct(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  inverse = FALSE,\n  uid = random_string(\"dct_\"),\n  ...\n)\n\nft_discrete_cosine_transform(\n  x,\n  input_col,\n  output_col,\n  inverse = FALSE,\n  uid = random_string(\"dct_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\ninverse\nIndicates whether to perform the inverse DCT (TRUE) or forward DCT (FALSE).\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#details",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nft_discrete_cosine_transform() is an alias for ft_dct for backwards compatibility."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#value",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_dct.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_dct.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_elementwise_product.html#description",
    "href": "packages/sparklyr/latest/reference/ft_elementwise_product.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nOutputs the Hadamard product (i.e., the element-wise product) of each input vector with a provided “weight” vector. In other words, it scales each column of the dataset by a scalar multiplier."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_elementwise_product.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_elementwise_product.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_elementwise_product(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  scaling_vec = NULL,\n  uid = random_string(\"elementwise_product_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_elementwise_product.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_elementwise_product.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nscaling_vec\nthe vector to multiply with input vectors\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_elementwise_product.html#value",
    "href": "packages/sparklyr/latest/reference/ft_elementwise_product.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_elementwise_product.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_elementwise_product.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_feature_hasher.html#description",
    "href": "packages/sparklyr/latest/reference/ft_feature_hasher.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFeature Transformation – FeatureHasher (Transformer)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_feature_hasher.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_feature_hasher.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_feature_hasher(\n  x,\n  input_cols = NULL,\n  output_col = NULL,\n  num_features = 2^18,\n  categorical_cols = NULL,\n  uid = random_string(\"feature_hasher_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_feature_hasher.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_feature_hasher.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_cols\nNames of input columns.\n\n\noutput_col\nName of output column.\n\n\nnum_features\nNumber of features. Defaults to 2^18.\n\n\ncategorical_cols\nNumeric columns to treat as categorical features.\n\n\n\nBy default only string and boolean columns are treated as categorical, so this param can be used to explicitly specify the numerical columns to treat as categorical. uid | A character string used to uniquely identify the feature transformer. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_feature_hasher.html#details",
    "href": "packages/sparklyr/latest/reference/ft_feature_hasher.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nFeature hashing projects a set of categorical or numerical features into a feature vector of specified dimension (typically substantially smaller than that of the original feature space). This is done using the hashing trick https://en.wikipedia.org/wiki/Feature_hashing to map features to indices in the feature vector.\nThe FeatureHasher transformer operates on multiple columns. Each column may contain either numeric or categorical features. Behavior and handling of column data types is as follows: -Numeric columns: For numeric features, the hash value of the column name is used to map the feature value to its index in the feature vector. By default, numeric features are not treated as categorical (even when they are integers). To treat them as categorical, specify the relevant columns in categoricalCols. -String columns: For categorical features, the hash value of the string “column_name=value” is used to map to the vector index, with an indicator value of 1.0. Thus, categorical features are “one-hot” encoded (similarly to using OneHotEncoder with drop_last=FALSE). -Boolean columns: Boolean values are treated in the same way as string columns. That is, boolean features are represented as “column_name=true” or “column_name=false”, with an indicator value of 1.0.\nNull (missing) values are ignored (implicitly zero in the resulting feature vector).\nThe hash function used here is also the MurmurHash 3 used in HashingTF. Since a simple modulo on the hashed value is used to determine the vector index, it is advisable to use a power of two as the num_features parameter; otherwise the features will not be mapped evenly to the vector indices."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_feature_hasher.html#value",
    "href": "packages/sparklyr/latest/reference/ft_feature_hasher.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_feature_hasher.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_feature_hasher.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html#description",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nMaps a sequence of terms to their term frequencies using the hashing trick."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_hashing_tf(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  binary = FALSE,\n  num_features = 2^18,\n  uid = random_string(\"hashing_tf_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nbinary\nBinary toggle to control term frequency counts.\n\n\n\nIf true, all non-zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. (default = FALSE) num_features | Number of features. Should be greater than 0. (default = 2^18) uid | A character string used to uniquely identify the feature transformer. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html#value",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_hashing_tf.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_hashing_tf.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#description",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCompute the Inverse Document Frequency (IDF) given a collection of documents."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_idf(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  min_doc_freq = 0,\n  uid = random_string(\"idf_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nmin_doc_freq\nThe minimum number of documents in which a term should appear. Default: 0\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#details",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#value",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_idf.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_idf.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nImputation estimator for completing missing values, either using the mean or the median of the columns in which the missing values are located. The input columns should be of numeric type. This function requires Spark 2.2.0+."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_imputer(\n  x,\n  input_cols = NULL,\n  output_cols = NULL,\n  missing_value = NULL,\n  strategy = \"mean\",\n  uid = random_string(\"imputer_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_cols\nThe names of the input columns\n\n\noutput_cols\nThe names of the output columns.\n\n\nmissing_value\nThe placeholder for the missing values. All occurrences of\n\n\n\nmissing_value will be imputed. Note that null values are always treated as missing. strategy | The imputation strategy. Currently only “mean” and “median” are supported. If “mean”, then replace missing values using the mean value of the feature. If “median”, then replace missing values using the approximate median value of the feature. Default: mean uid | A character string used to uniquely identify the feature transformer. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#details",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_imputer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_imputer.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html#description",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nA Transformer that maps a column of indices back to a new column of corresponding string values. The index-string mapping is either from the ML attributes of the input column, or from user-supplied labels (which take precedence over ML attributes). This function is the inverse of ft_string_indexer."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_index_to_string(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  labels = NULL,\n  uid = random_string(\"index_to_string_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nlabels\nOptional param for array of labels specifying index-string mapping.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html#value",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_index_to_string.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_index_to_string.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nft_string_indexer\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_interaction.html#description",
    "href": "packages/sparklyr/latest/reference/ft_interaction.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nImplements the feature interaction transform. This transformer takes in Double and Vector type columns and outputs a flattened vector of their feature interactions. To handle interaction, we first one-hot encode any nominal features. Then, a vector of the feature cross-products is produced."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_interaction.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_interaction.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_interaction(\n  x,\n  input_cols = NULL,\n  output_col = NULL,\n  uid = random_string(\"interaction_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_interaction.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_interaction.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_cols\nThe names of the input columns\n\n\noutput_col\nThe name of the output column.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_interaction.html#value",
    "href": "packages/sparklyr/latest/reference/ft_interaction.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_interaction.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_interaction.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh.html#description",
    "href": "packages/sparklyr/latest/reference/ft_lsh.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nLocality Sensitive Hashing functions for Euclidean distance (Bucketed Random Projection) and Jaccard distance (MinHash)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_lsh.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_bucketed_random_projection_lsh(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  bucket_length = NULL,\n  num_hash_tables = 1,\n  seed = NULL,\n  uid = random_string(\"bucketed_random_projection_lsh_\"),\n  ...\n)\n\nft_minhash_lsh(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  num_hash_tables = 1L,\n  seed = NULL,\n  uid = random_string(\"minhash_lsh_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_lsh.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nbucket_length\nThe length of each hash bucket, a larger bucket lowers the\n\n\n\nfalse negative rate. The number of buckets will be (max L2 norm of input vectors) / bucketLength. num_hash_tables | Number of hash tables used in LSH OR-amplification. LSH OR-amplification can be used to reduce the false negative rate. Higher values for this param lead to a reduced false negative rate, at the expense of added computational complexity. seed | A random seed. Set this value if you need your results to be reproducible across repeated calls. uid | A character string used to uniquely identify the feature transformer. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh.html#details",
    "href": "packages/sparklyr/latest/reference/ft_lsh.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh.html#value",
    "href": "packages/sparklyr/latest/reference/ft_lsh.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_lsh.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nft_lsh_utils\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh_utils.html#description",
    "href": "packages/sparklyr/latest/reference/ft_lsh_utils.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nUtility functions for LSH models"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh_utils.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_lsh_utils.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_approx_nearest_neighbors(\n  model,\n  dataset,\n  key,\n  num_nearest_neighbors,\n  dist_col = \"distCol\"\n)\n\nml_approx_similarity_join(\n  model,\n  dataset_a,\n  dataset_b,\n  threshold,\n  dist_col = \"distCol\"\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_lsh_utils.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_lsh_utils.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nmodel\nA fitted LSH model, returned by either ft_minhash_lsh()\n\n\n\nor ft_bucketed_random_projection_lsh(). dataset | The dataset to search for nearest neighbors of the key. key | Feature vector representing the item to search for. num_nearest_neighbors | The maximum number of nearest neighbors. dist_col | Output column for storing the distance between each result row and the key. dataset_a | One of the datasets to join. dataset_b | Another dataset to join. threshold | The threshold for the distance of row pairs."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#description",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRescale each feature individually to range [-1, 1] by dividing through the largest maximum absolute value in each feature. It does not shift/center the data, and thus does not destroy any sparsity."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_max_abs_scaler(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  uid = random_string(\"max_abs_scaler_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#details",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#value",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#examples",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\nfeatures <- c(\"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\")\n\niris_tbl %>%\n  ft_vector_assembler(\n    input_col = features,\n    output_col = \"features_temp\"\n  ) %>%\n  ft_max_abs_scaler(\n    input_col = \"features_temp\",\n    output_col = \"features\"\n  )"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_max_abs_scaler.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#description",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRescale each feature individually to a common range [min, max] linearly using column summary statistics, which is also known as min-max normalization or Rescaling"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_min_max_scaler(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  min = 0,\n  max = 1,\n  uid = random_string(\"min_max_scaler_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nmin\nLower bound after transformation, shared by all features Default: 0.0\n\n\nmax\nUpper bound after transformation, shared by all features Default: 1.0\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#details",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#value",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#examples",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\nfeatures <- c(\"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\")\n\niris_tbl %>%\n  ft_vector_assembler(\n    input_col = features,\n    output_col = \"features_temp\"\n  ) %>%\n  ft_min_max_scaler(\n    input_col = \"features_temp\",\n    output_col = \"features\"\n  )"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_min_max_scaler.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_ngram.html#description",
    "href": "packages/sparklyr/latest/reference/ft_ngram.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nA feature transformer that converts the input array of strings into an array of n-grams. Null values in the input array are ignored. It returns an array of n-grams where each n-gram is represented by a space-separated string of words."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_ngram.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_ngram.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_ngram(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  n = 2,\n  uid = random_string(\"ngram_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_ngram.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_ngram.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nn\nMinimum n-gram length, greater than or equal to 1. Default: 2, bigram features\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_ngram.html#details",
    "href": "packages/sparklyr/latest/reference/ft_ngram.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nWhen the input is empty, an empty array is returned. When the input array length is less than n (number of elements per n-gram), no n-grams are returned."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_ngram.html#value",
    "href": "packages/sparklyr/latest/reference/ft_ngram.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_ngram.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_ngram.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nNormalize a vector to have unit norm using the given p-norm."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_normalizer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  p = 2,\n  uid = random_string(\"normalizer_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\np\nNormalization in L^p space. Must be >= 1. Defaults to 2.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_normalizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_normalizer.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#description",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nOne-hot encoding maps a column of label indices to a column of binary vectors, with at most a single one-value. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features. Typically, used with ft_string_indexer() to index a column first."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_one_hot_encoder(\n  x,\n  input_cols = NULL,\n  output_cols = NULL,\n  handle_invalid = NULL,\n  drop_last = TRUE,\n  uid = random_string(\"one_hot_encoder_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_cols\nThe name of the input columns.\n\n\noutput_cols\nThe name of the output columns.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are\n\n\n\n‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error” drop_last | Whether to drop the last category. Defaults to TRUE. uid | A character string used to uniquely identify the feature transformer. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#value",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#description",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nA one-hot encoder that maps a column of category indices to a column of binary vectors, with at most a single one-value per row that indicates the input category index. For example with 5 categories, an input value of 2.0 would map to an output vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included by default (configurable via dropLast), because it makes the vector entries sum up to one, and hence linearly dependent. So an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0]."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_one_hot_encoder_estimator(\n  x,\n  input_cols = NULL,\n  output_cols = NULL,\n  handle_invalid = \"error\",\n  drop_last = TRUE,\n  uid = random_string(\"one_hot_encoder_estimator_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_cols\nNames of input columns.\n\n\noutput_cols\nNames of output columns.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are\n\n\n\n‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error” drop_last | Whether to drop the last category. Defaults to TRUE. uid | A character string used to uniquely identify the feature transformer. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#details",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#value",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_one_hot_encoder_estimator.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_pca.html#description",
    "href": "packages/sparklyr/latest/reference/ft_pca.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPCA trains a model to project vectors to a lower dimensional space of the top k principal components."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_pca.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_pca.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_pca(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  k = NULL,\n  uid = random_string(\"pca_\"),\n  ...\n)\n\nml_pca(x, features = tbl_vars(x), k = length(features), pc_prefix = \"PC\", ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_pca.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_pca.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nk\nThe number of principal components\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused.\n\n\nfeatures\nThe columns to use in the principal components\n\n\n\nanalysis. Defaults to all columns in x. pc_prefix | Length-one character vector used to prepend names of components."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_pca.html#details",
    "href": "packages/sparklyr/latest/reference/ft_pca.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark.\nml_pca() is a wrapper around ft_pca() that returns a ml_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_pca.html#value",
    "href": "packages/sparklyr/latest/reference/ft_pca.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_pca.html#examples",
    "href": "packages/sparklyr/latest/reference/ft_pca.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\niris_tbl %>%\n  select(-Species) %>%\n  ml_pca(k = 2)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_pca.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_pca.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#description",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPerform feature expansion in a polynomial space. E.g. take a 2-variable feature vector as an example: (x, y), if we want to expand it with degree 2, then we get (x, x * x, y, x * y, y * y)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_polynomial_expansion(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  degree = 2,\n  uid = random_string(\"polynomial_expansion_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\ndegree\nThe polynomial degree to expand, which should be greater\n\n\n\nthan equal to 1. A value of 1 means no expansion. Default: 2 uid | A character string used to uniquely identify the feature transformer. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#value",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_polynomial_expansion.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nft_quantile_discretizer takes a column with continuous features and outputs a column with binned categorical features. The number of bins can be set using the num_buckets parameter. It is possible that the number of buckets used will be smaller than this value, for example, if there are too few distinct values of the input to create enough distinct quantiles."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_quantile_discretizer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  num_buckets = 2,\n  input_cols = NULL,\n  output_cols = NULL,\n  num_buckets_array = NULL,\n  handle_invalid = \"error\",\n  relative_error = 0.001,\n  uid = random_string(\"quantile_discretizer_\"),\n  weight_column = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nnum_buckets\nNumber of buckets (quantiles, or categories) into which data\n\n\n\npoints are grouped. Must be greater than or equal to 2. input_cols | Names of input columns. output_cols | Names of output columns. num_buckets_array | Array of number of buckets (quantiles, or categories) into which data points are grouped. Each value must be greater than or equal to 2. handle_invalid | (Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error” relative_error | (Spark 2.0.0+) Relative error (see documentation for org.apache.spark.sql.DataFrameStatFunctions.approxQuantile https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctionshere for description). Must be in the range [0, 1]. default: 0.001 uid | A character string used to uniquely identify the feature transformer. weight_column | If not NULL, then a generalized version of the Greenwald-Khanna algorithm will be run to compute weighted percentiles, with each input having a relative weight specified by the corresponding value in weight_column. The weights can be considered as relative frequencies of sample inputs. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#details",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nNaN handling: null and NaN values will be ignored from the column during QuantileDiscretizer fitting. This will produce a Bucketizer model for making predictions. During the transformation, Bucketizer will raise an error when it finds NaN values in the dataset, but the user can also choose to either keep or remove NaN values within the dataset by setting handle_invalid If the user chooses to keep NaN values, they will be handled specially and placed into their own bucket, for example, if 4 buckets are used, then non-NaN data will be put into buckets[0-3], but NaNs will be counted in a special bucket[4].\nAlgorithm: The bin ranges are chosen using an approximate algorithm (see the documentation for org.apache.spark.sql.DataFrameStatFunctions.approxQuantile https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctionshere for a detailed description). The precision of the approximation can be controlled with the relative_error parameter. The lower and upper bin bounds will be -Infinity and +Infinity, covering all real values.\nNote that the result may be different every time you run it, since the sample strategy behind it is non-deterministic.\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_quantile_discretizer.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nft_bucketizer\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_r_formula.html#description",
    "href": "packages/sparklyr/latest/reference/ft_r_formula.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nImplements the transforms required for fitting a dataset against an R model formula. Currently we support a limited subset of the R operators, including ~, ., :, +, and -. Also see the R formula docs here: http://stat.ethz.ch/R-manual/R-patched/library/stats/html/formula.html"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_r_formula.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_r_formula.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_r_formula(\n  x,\n  formula = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  force_index_label = FALSE,\n  uid = random_string(\"r_formula_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_r_formula.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_r_formula.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nR formula as a character string or a formula. Formula objects are\n\n\n\nconverted to character strings directly and the environment is not captured. features_col | Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula. label_col | Label column name. The column should be a numeric column. Usually this column is output by ft_r_formula. force_index_label | (Spark 2.1.0+) Force to index label whether it is numeric or string type. Usually we index label only when it is string type. If the formula was used by classification algorithms, we can force to index label even it is numeric type by setting this param with true. Default: FALSE. uid | A character string used to uniquely identify the feature transformer. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_r_formula.html#details",
    "href": "packages/sparklyr/latest/reference/ft_r_formula.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nThe basic operators in the formula are:\n\n~ separate target and terms\n\nconcat terms, “+ 0” means removing intercept\n\n\nremove a term, “- 1” means removing intercept\n\n: interaction (multiplication for numeric values, or binarized categorical values)\n. all columns except target\n\nSuppose a and b are double columns, we use the following simple examples to illustrate the effect of RFormula:\n\ny ~ a + b means model y ~ w0 + w1 * a + w2 * b where w0 is the intercept and w1, w2 are coefficients.\ny ~ a + b + a:b - 1 means model y ~ w1 * a + w2 * b + w3 * a * b where w1, w2, w3 are coefficients.\n\nRFormula produces a vector column of features and a double or string column of label. Like when formulas are used in R for linear regression, string input columns will be one-hot encoded, and numeric columns will be cast to doubles. If the label column is of type string, it will be first transformed to double with StringIndexer. If the label column does not exist in the DataFrame, the output label column will be created from the specified response variable in the formula.\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_r_formula.html#value",
    "href": "packages/sparklyr/latest/reference/ft_r_formula.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_r_formula.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_r_formula.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nA regex based tokenizer that extracts tokens either by using the provided regex pattern to split the text (default) or repeatedly matching the regex (if gaps is false). Optional parameters also allow filtering tokens using a minimal length. It returns an array of strings that can be empty."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_regex_tokenizer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  gaps = TRUE,\n  min_token_length = 1,\n  pattern = \"\\\\s+\",\n  to_lower_case = TRUE,\n  uid = random_string(\"regex_tokenizer_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\ngaps\nIndicates whether regex splits on gaps (TRUE) or matches tokens (FALSE).\n\n\nmin_token_length\nMinimum token length, greater than or equal to 0.\n\n\npattern\nThe regular expression pattern to be used.\n\n\nto_lower_case\nIndicates whether to convert all characters to lowercase before tokenizing.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_regex_tokenizer.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_robust_scaler.html#description",
    "href": "packages/sparklyr/latest/reference/ft_robust_scaler.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRobustScaler removes the median and scales the data according to the quantile range. The quantile range is by default IQR (Interquartile Range, quantile range between the 1st quartile = 25th quantile and the 3rd quartile = 75th quantile) but can be configured. Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and quantile range are then stored to be used on later data using the transform method. Note that missing values are ignored in the computation of medians and ranges."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_robust_scaler.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_robust_scaler.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_robust_scaler(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  lower = 0.25,\n  upper = 0.75,\n  with_centering = TRUE,\n  with_scaling = TRUE,\n  relative_error = 0.001,\n  uid = random_string(\"ft_robust_scaler_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_robust_scaler.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_robust_scaler.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nlower\nLower quantile to calculate quantile range.\n\n\nupper\nUpper quantile to calculate quantile range.\n\n\nwith_centering\nWhether to center data with median.\n\n\nwith_scaling\nWhether to scale the data to quantile range.\n\n\nrelative_error\nThe target relative error for quantile computation.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_robust_scaler.html#details",
    "href": "packages/sparklyr/latest/reference/ft_robust_scaler.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_robust_scaler.html#value",
    "href": "packages/sparklyr/latest/reference/ft_robust_scaler.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_robust_scaler.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_robust_scaler.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#description",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nStandardizes features by removing the mean and scaling to unit variance using column summary statistics on the samples in the training set. The “unit std” is computed using the corrected sample standard deviation, which is computed as the square root of the unbiased sample variance."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_standard_scaler(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  with_mean = FALSE,\n  with_std = TRUE,\n  uid = random_string(\"standard_scaler_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nwith_mean\nWhether to center the data with mean before scaling. It will\n\n\n\nbuild a dense output, so take care when applying to sparse input. Default: FALSE with_std | Whether to scale the data to unit standard deviation. Default: TRUE uid | A character string used to uniquely identify the feature transformer. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#details",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#value",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#examples",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\nfeatures <- c(\"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\")\n\niris_tbl %>%\n  ft_vector_assembler(\n    input_col = features,\n    output_col = \"features_temp\"\n  ) %>%\n  ft_standard_scaler(\n    input_col = \"features_temp\",\n    output_col = \"features\",\n    with_mean = TRUE\n  )"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_standard_scaler.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_standard_scaler.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#description",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nA feature transformer that filters out stop words from input."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_stop_words_remover(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  case_sensitive = FALSE,\n  stop_words = ml_default_stop_words(spark_connection(x), \"english\"),\n  uid = random_string(\"stop_words_remover_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\ncase_sensitive\nWhether to do a case sensitive comparison over the stop words.\n\n\nstop_words\nThe words to be filtered out.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#value",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_stop_words_remover.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nml_default_stop_words\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nA label indexer that maps a string column of labels to an ML column of label indices. If the input column is numeric, we cast it to string and index the string values. The indices are in [0, numLabels), ordered by label frequencies. So the most frequent label gets index 0. This function is the inverse of ft_index_to_string."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_string_indexer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  handle_invalid = \"error\",\n  string_order_type = \"frequencyDesc\",\n  uid = random_string(\"string_indexer_\"),\n  ...\n)\n\nml_labels(model)\n\nft_string_indexer_model(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  labels,\n  handle_invalid = \"error\",\n  uid = random_string(\"string_indexer_model_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are\n\n\n\n‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error” string_order_type | (Spark 2.3+)How to order labels of string column. The first label after ordering is assigned an index of 0. Options are \"frequencyDesc\", \"frequencyAsc\", \"alphabetDesc\", and \"alphabetAsc\". Defaults to \"frequencyDesc\". uid | A character string used to uniquely identify the feature transformer. … | Optional arguments; currently unused. model | A fitted StringIndexer model returned by ft_string_indexer() labels | Vector of labels, corresponding to indices to be assigned."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#details",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark\n\nml_labels() returns a vector of labels, corresponding to indices to be assigned."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_string_indexer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_string_indexer.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nft_index_to_string\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nA tokenizer that converts the input string to lowercase and then splits it by white spaces."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_tokenizer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  uid = random_string(\"tokenizer_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_tokenizer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_tokenizer.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html#description",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCombine multiple vectors into a single row-vector; that is, where each row element of the newly generated column is a vector formed by concatenating each row element from the specified input columns."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_vector_assembler(\n  x,\n  input_cols = NULL,\n  output_col = NULL,\n  uid = random_string(\"vector_assembler_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_cols\nThe names of the input columns\n\n\noutput_col\nThe name of the output column.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html#value",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_assembler.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_vector_assembler.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nIndexing categorical feature columns in a dataset of Vector."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_vector_indexer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  handle_invalid = \"error\",\n  max_categories = 20,\n  uid = random_string(\"vector_indexer_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are\n\n\n\n‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error” max_categories | Threshold for the number of values a categorical feature can take. If a feature is found to have > max_categories values, then it is declared continuous. Must be greater than or equal to 2. Defaults to 20. uid | A character string used to uniquely identify the feature transformer. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#details",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_indexer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_vector_indexer.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html#description",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nTakes a feature vector and outputs a new feature vector with a subarray of the original features."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_vector_slicer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  indices = NULL,\n  uid = random_string(\"vector_slicer_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nindices\nAn vector of indices to select features from a vector column.\n\n\n\nNote that the indices are 0-based. uid | A character string used to uniquely identify the feature transformer. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html#value",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_vector_slicer.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_vector_slicer.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#description",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nWord2Vec transforms a word into a code for further natural language processing or machine learning process."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#usage",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_word2vec(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  vector_size = 100,\n  min_count = 5,\n  max_sentence_length = 1000,\n  num_partitions = 1,\n  step_size = 0.025,\n  max_iter = 1,\n  seed = NULL,\n  uid = random_string(\"word2vec_\"),\n  ...\n)\n\nml_find_synonyms(model, word, num)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#arguments",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nvector_size\nThe dimension of the code that you want to transform from words. Default: 100\n\n\nmin_count\nThe minimum number of times a token must appear to be included in\n\n\n\nthe word2vec model’s vocabulary. Default: 5 max_sentence_length | (Spark 2.0.0+) Sets the maximum length (in words) of each sentence in the input data. Any sentence longer than this threshold will be divided into chunks of up to max_sentence_length size. Default: 1000 num_partitions | Number of partitions for sentences of words. Default: 1 step_size | Param for Step size to be used for each iteration of optimization (> 0). max_iter | The maximum number of iterations to use. seed | A random seed. Set this value if you need your results to be reproducible across repeated calls. uid | A character string used to uniquely identify the feature transformer. … | Optional arguments; currently unused. model | A fitted Word2Vec model, returned by ft_word2vec(). word | A word, as a length-one character vector. num | Number of words closest in similarity to the given word to find."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#details",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nIn the case where x is a tbl_spark, the estimator fits against x to obtain a transformer, which is then immediately used to transform x, returning a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#value",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark\n\nml_find_synonyms() returns a DataFrame of synonyms and cosine similarities"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ft_word2vec.html#see-also",
    "href": "packages/sparklyr/latest/reference/ft_word2vec.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_sql_transformer(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/full_join.html#description",
    "href": "packages/sparklyr/latest/reference/full_join.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee full_join for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/generic_call_interface.html#description",
    "href": "packages/sparklyr/latest/reference/generic_call_interface.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGeneric Call Interface"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/generic_call_interface.html#arguments",
    "href": "packages/sparklyr/latest/reference/generic_call_interface.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nspark_connection\n\n\nstatic\nIs this a static method call (including a constructor). If so\n\n\n\nthen the object parameter should be the name of a class (otherwise it should be a spark_jobj instance). object | Object instance or name of class (for static) method | Name of method … | Call parameters"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#description",
    "href": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieve the Spark connection’s SQL catalog implementation property"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#usage",
    "href": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nget_spark_sql_catalog_implementation(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#arguments",
    "href": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nspark_connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#value",
    "href": "packages/sparklyr/latest/reference/get_spark_sql_catalog_implementation.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nspark.sql.catalogImplementation property from the connection’s runtime configuration"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#description",
    "href": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nInfix operator that allows a lambda expression to be composed in R and be translated to Spark SQL equivalent using ’ dbplyr::translate_sql functionalities"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#usage",
    "href": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nparams %->% ..."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#arguments",
    "href": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nparams\nParameter(s) of the lambda expression, can be either a single\n\n\n\nparameter or a comma separated listed of parameters in the form of .(param1, param2, ... ) (see examples) … | Body of the lambda expression, must be within parentheses"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#details",
    "href": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nNotice when composing a lambda expression in R, the body of the lambda expression must always be surrounded with parentheses, otherwise a parsing error will occur."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#examples",
    "href": "packages/sparklyr/latest/reference/grapes-greater-than-grapes.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\na %->% (mean(a) + 1) # translates to <SQL> `a` -> (AVG(`a`) OVER () + 1.0)\n\n.(a, b) %->% (a < 1 && b > 1) # translates to <SQL> `a`,`b` -> (`a` < 1.0 AND `b` > 1.0)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hive_context_config.html#description",
    "href": "packages/sparklyr/latest/reference/hive_context_config.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieves the runtime configuration interface for Hive."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hive_context_config.html#usage",
    "href": "packages/sparklyr/latest/reference/hive_context_config.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nhive_context_config(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hive_context_config.html#arguments",
    "href": "packages/sparklyr/latest/reference/hive_context_config.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_aggregate.html#description",
    "href": "packages/sparklyr/latest/reference/hof_aggregate.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nApply an element-wise aggregation function to an array column (this is essentially a dplyr wrapper for the aggregate(array<T>, A, function<A, T, A>[, function<A, R>]): R built-in Spark SQL functions)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_aggregate.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_aggregate.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nhof_aggregate(\n  x,\n  start,\n  merge,\n  finish = NULL,\n  expr = NULL,\n  dest_col = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_aggregate.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_aggregate.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nThe Spark data frame to run aggregation on\n\n\nstart\nThe starting value of the aggregation\n\n\nmerge\nThe aggregation function\n\n\nfinish\nOptional param specifying a transformation to apply on the final value of the aggregation\n\n\nexpr\nThe array being aggregated, could be any SQL expression evaluating to an array\n\n\n\n(default: the last column of the Spark data frame) dest_col | Column to store the aggregated result (default: expr) … | Additional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_aggregate.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_aggregate.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\n# concatenates all numbers of each array in `array_column` and add parentheses\n# around the resulting string\ncopy_to(sc, tibble::tibble(array_column = list(1:5, 21:25))) %>%\n  hof_aggregate(\n    start = \"\",\n    merge = ~ CONCAT(.y, .x),\n    finish = ~ CONCAT(\"(\", .x, \")\")\n  )"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_array_sort.html#description",
    "href": "packages/sparklyr/latest/reference/hof_array_sort.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nApplies a custom comparator function to sort an array (this is essentially a dplyr wrapper to the array_sort(expr, func) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_array_sort.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_array_sort.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nhof_array_sort(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_array_sort.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_array_sort.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\nfunc\nThe comparator function to apply (it should take 2 array elements as arguments\n\n\n\nand return an integer, with a return value of -1 indicating the first element is less than the second, 0 indicating equality, or 1 indicating the first element is greater than the second) expr | The array being sorted, could be any SQL expression evaluating to an array (default: the last column of the Spark data frame) dest_col | Column to store the sorted result (default: expr) … | Additional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_array_sort.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_array_sort.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"3.0.0\")\ncopy_to(\n  sc,\n  tibble::tibble(\n    # x contains 2 arrays each having elements in ascending order\n    x = list(1:5, 6:10)\n  )\n) %>%\n  # now each array from x gets sorted in descending order\n  hof_array_sort(~ as.integer(sign(.y - .x)))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_exists.html#description",
    "href": "packages/sparklyr/latest/reference/hof_exists.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nDetermines whether an element satisfying the given predicate exists in each array from an array column (this is essentially a dplyr wrapper for the exists(array<T>, function<T, Boolean>): Boolean built-in Spark SQL function)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_exists.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_exists.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nhof_exists(x, pred, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_exists.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_exists.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nThe Spark data frame to search\n\n\npred\nA boolean predicate\n\n\nexpr\nThe array being searched (could be any SQL expression evaluating to an array)\n\n\ndest_col\nColumn to store the search result\n\n\n…\nAdditional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_filter.html#description",
    "href": "packages/sparklyr/latest/reference/hof_filter.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nApply an element-wise filtering function to an array column (this is essentially a dplyr wrapper for the filter(array<T>, function<T, Boolean>): array<T> built-in Spark SQL functions)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_filter.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_filter.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nhof_filter(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_filter.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_filter.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nThe Spark data frame to filter\n\n\nfunc\nThe filtering function\n\n\nexpr\nThe array being filtered, could be any SQL expression evaluating to an array\n\n\n\n(default: the last column of the Spark data frame) dest_col | Column to store the filtered result (default: expr) … | Additional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_filter.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_filter.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\n# only keep odd elements in each array in `array_column`\ncopy_to(sc, tibble::tibble(array_column = list(1:5, 21:25))) %>%\n  hof_filter(~ .x %% 2 == 1)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_forall.html#description",
    "href": "packages/sparklyr/latest/reference/hof_forall.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nChecks whether the predicate specified holds for all elements in an array (this is essentially a dplyr wrapper to the forall(expr, pred) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_forall.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_forall.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nhof_forall(x, pred, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_forall.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_forall.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\npred\nThe predicate to test (it should take an array element as argument and\n\n\n\nreturn a boolean value) expr | The array being tested, could be any SQL expression evaluating to an array (default: the last column of the Spark data frame) dest_col | Column to store the boolean result (default: expr) … | Additional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_forall.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_forall.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nsc <- spark_connect(master = \"local\", version = \"3.0.0\")\ndf <- tibble::tibble(\n  x = list(c(1, 2, 3, 4, 5), c(6, 7, 8, 9, 10)),\n  y = list(c(1, 4, 2, 8, 5), c(7, 1, 4, 2, 8)),\n)\nsdf <- sdf_copy_to(sc, df, overwrite = TRUE)\n\nall_positive_tbl <- sdf %>%\n  hof_forall(pred = ~ .x > 0, expr = y, dest_col = all_positive) %>%\n  dplyr::select(all_positive)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_filter.html#description",
    "href": "packages/sparklyr/latest/reference/hof_map_filter.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFilters entries in a map using the function specified (this is essentially a dplyr wrapper to the map_filter(expr, func) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_filter.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_map_filter.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nhof_map_filter(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_filter.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_map_filter.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\nfunc\nThe filter function to apply (it should take (key, value) as arguments\n\n\n\nand return a boolean value, with FALSE indicating the key-value pair should be discarded and TRUE otherwise) expr | The map being filtered, could be any SQL expression evaluating to a map (default: the last column of the Spark data frame) dest_col | Column to store the filtered result (default: expr) … | Additional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_filter.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_map_filter.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"3.0.0\")\nsdf <- sdf_len(sc, 1) %>% dplyr::mutate(m = map(1, 0, 2, 2, 3, -1))\nfiltered_sdf <- sdf %>% hof_map_filter(~ .x > .y)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_zip_with.html#description",
    "href": "packages/sparklyr/latest/reference/hof_map_zip_with.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nMerges two maps into a single map by applying the function specified to pairs of values with the same key (this is essentially a dplyr wrapper to the map_zip_with(map1, map2, func) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_zip_with.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_map_zip_with.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nhof_map_zip_with(x, func, dest_col = NULL, map1 = NULL, map2 = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_zip_with.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_map_zip_with.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\nfunc\nThe function to apply (it should take (key, value1, value2) as arguments,\n\n\n\nwhere (key, value1) is a key-value pair present in map1, (key, value2) is a key-value pair present in map2, and return a transformed value associated with key in the resulting map dest_col | Column to store the query result (default: the last column of the Spark data frame) map1 | The first map being merged, could be any SQL expression evaluating to a map (default: the first column of the Spark data frame) map2 | The second map being merged, could be any SQL expression evaluating to a map (default: the second column of the Spark data frame) … | Additional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_map_zip_with.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_map_zip_with.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"3.0.0\")\n\n# create a Spark dataframe with 2 columns of type MAP<STRING, INT>\ntwo_maps_tbl <- sdf_copy_to(\n  sc,\n  tibble::tibble(\n    m1 = c(\"{\\\"1\\\":2,\\\"3\\\":4,\\\"5\\\":6}\", \"{\\\"2\\\":1,\\\"4\\\":3,\\\"6\\\":5}\"),\n    m2 = c(\"{\\\"1\\\":1,\\\"3\\\":3,\\\"5\\\":5}\", \"{\\\"2\\\":2,\\\"4\\\":4,\\\"6\\\":6}\")\n  ),\n  overwrite = TRUE\n) %>%\n  dplyr::mutate(m1 = from_json(m1, \"MAP<STRING, INT>\"),\n                m2 = from_json(m2, \"MAP<STRING, INT>\"))\n\n# create a 3rd column containing MAP<STRING, INT> values derived from the\n# first 2 columns\n\ntransformed_two_maps_tbl <- two_maps_tbl %>%\n  hof_map_zip_with(\n    func = .(k, v1, v2) %->% (CONCAT(k, \"_\", v1, \"_\", v2)),\n    dest_col = m3\n  )"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform.html#description",
    "href": "packages/sparklyr/latest/reference/hof_transform.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nApply an element-wise transformation function to an array column (this is essentially a dplyr wrapper for the transform(array<T>, function<T, U>): array<U> and the transform(array<T>, function<T, Int, U>): array<U> built-in Spark SQL functions)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_transform.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nhof_transform(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_transform.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nThe Spark data frame to transform\n\n\nfunc\nThe transformation to apply\n\n\nexpr\nThe array being transformed, could be any SQL expression evaluating to an array\n\n\n\n(default: the last column of the Spark data frame) dest_col | Column to store the transformed result (default: expr) … | Additional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_transform.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\n# applies the (x -> x * x) transformation to elements of all arrays\ncopy_to(sc, tibble::tibble(arr = list(1:5, 21:25))) %>%\n  hof_transform(~ .x * .x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_keys.html#description",
    "href": "packages/sparklyr/latest/reference/hof_transform_keys.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nApplies the transformation function specified to all keys of a map (this is essentially a dplyr wrapper to the transform_keys(expr, func) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_keys.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_transform_keys.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nhof_transform_keys(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_keys.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_transform_keys.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\nfunc\nThe transformation function to apply (it should take (key, value) as\n\n\n\narguments and return a transformed key) expr | The map being transformed, could be any SQL expression evaluating to a map (default: the last column of the Spark data frame) dest_col | Column to store the transformed result (default: expr) … | Additional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_keys.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_transform_keys.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"3.0.0\")\nsdf <- sdf_len(sc, 1) %>% dplyr::mutate(m = map(\"a\", 0L, \"b\", 2L, \"c\", -1L))\ntransformed_sdf <- sdf %>% hof_transform_keys(~ CONCAT(.x, \" == \", .y))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_values.html#description",
    "href": "packages/sparklyr/latest/reference/hof_transform_values.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nApplies the transformation function specified to all values of a map (this is essentially a dplyr wrapper to the transform_values(expr, func) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_values.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_transform_values.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nhof_transform_values(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_values.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_transform_values.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\nfunc\nThe transformation function to apply (it should take (key, value) as\n\n\n\narguments and return a transformed value) expr | The map being transformed, could be any SQL expression evaluating to a map (default: the last column of the Spark data frame) dest_col | Column to store the transformed result (default: expr) … | Additional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_transform_values.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_transform_values.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"3.0.0\")\nsdf <- sdf_len(sc, 1) %>% dplyr::mutate(m = map(\"a\", 0L, \"b\", 2L, \"c\", -1L))\ntransformed_sdf <- sdf %>% hof_transform_values(~ CONCAT(.x, \" == \", .y))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_zip_with.html#description",
    "href": "packages/sparklyr/latest/reference/hof_zip_with.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nApplies an element-wise function to combine elements from 2 array columns (this is essentially a dplyr wrapper for the zip_with(array<T>, array<U>, function<T, U, R>): array<R> built-in function in Spark SQL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_zip_with.html#usage",
    "href": "packages/sparklyr/latest/reference/hof_zip_with.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nhof_zip_with(x, func, dest_col = NULL, left = NULL, right = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_zip_with.html#arguments",
    "href": "packages/sparklyr/latest/reference/hof_zip_with.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nThe Spark data frame to process\n\n\nfunc\nElement-wise combining function to be applied\n\n\ndest_col\nColumn to store the query result\n\n\n\n(default: the last column of the Spark data frame) left | Any expression evaluating to an array (default: the first column of the Spark data frame) right | Any expression evaluating to an array (default: the second column of the Spark data frame) … | Additional params to dplyr::mutate"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/hof_zip_with.html#examples",
    "href": "packages/sparklyr/latest/reference/hof_zip_with.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\n# compute element-wise products of 2 arrays from each row of `left` and `right`\n# and store the resuling array in `res`\ncopy_to(\n  sc,\n  tibble::tibble(\n    left = list(1:5, 21:25),\n    right = list(6:10, 16:20),\n    res = c(0, 0)\n  )\n) %>%\n  hof_zip_with(~ .x * .y)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-operations",
    "href": "packages/sparklyr/latest/reference/index.html#spark-operations",
    "title": "sparklyr",
    "section": "Spark Operations",
    "text": "Spark Operations\n\n\n\nFunction(s)\nDescription\n\n\n\n\nget_spark_sql_catalog_implementation()\nRetrieve the Spark connection’s SQL catalog implementation property\n\n\nspark_config()\nRead Spark Configuration\n\n\nspark_connect() spark_connection_is_open() spark_disconnect() spark_disconnect_all() spark_submit()\nManage Spark Connections\n\n\nspark_install() spark_uninstall() spark_install_dir() spark_install_tar() spark_installed_versions() spark_available_versions()\nDownload and install various versions of Spark\n\n\nspark_log()\nView Entries in the Spark Log\n\n\nspark_web()\nOpen the Spark web interface\n\n\nconnection_is_open()\nCheck whether the connection is open\n\n\nconnection_spark_shinyapp()\nA Shiny app that can be used to construct a spark_connect statement\n\n\nspark_session_config()\nRuntime configuration interface for the Spark Session\n\n\nspark_set_checkpoint_dir() spark_get_checkpoint_dir()\nSet/Get Spark checkpoint directory\n\n\nspark_table_name()\nGenerate a Table Name from Expression\n\n\nspark_version_from_home()\nGet the Spark Version Associated with a Spark Installation\n\n\nspark_versions()\nRetrieves a dataframe available Spark versions that van be installed.\n\n\nspark_config_kubernetes()\nKubernetes Configuration\n\n\nspark_config_settings()\nRetrieve Available Settings\n\n\nspark_connection_find()\nFind Spark Connection\n\n\nspark_dependency_fallback()\nFallback to Spark Dependency\n\n\nspark_extension()\nCreate Spark Extension\n\n\nspark_load_table()\nReads from a Spark Table into a Spark DataFrame.\n\n\nlist_sparklyr_jars()\nlist all sparklyr-*.jar files that have been built\n\n\nspark_config_packages()\nCreates Spark Configuration\n\n\nspark_connection()\nRetrieve the Spark Connection Associated with an R Object\n\n\nspark_adaptive_query_execution()\nRetrieves or sets status of Spark AQE\n\n\nspark_advisory_shuffle_partition_size()\nRetrieves or sets advisory size of the shuffle partition\n\n\nspark_auto_broadcast_join_threshold()\nRetrieves or sets the auto broadcast join threshold\n\n\nspark_coalesce_initial_num_partitions()\nRetrieves or sets initial number of shuffle partitions before coalescing\n\n\nspark_coalesce_min_num_partitions()\nRetrieves or sets the minimum number of shuffle partitions after coalescing\n\n\nspark_coalesce_shuffle_partitions()\nRetrieves or sets whether coalescing contiguous shuffle partitions is enabled"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-data",
    "href": "packages/sparklyr/latest/reference/index.html#spark-data",
    "title": "sparklyr",
    "section": "Spark Data",
    "text": "Spark Data\n\n\n\nFunction(s)\nDescription\n\n\n\n\nspark_read()\nRead file(s) into a Spark DataFrame using a custom reader\n\n\nspark_read_avro()\nRead Apache Avro data into a Spark DataFrame.\n\n\nspark_read_binary()\nRead binary data into a Spark DataFrame.\n\n\nspark_read_csv()\nRead a CSV file into a Spark DataFrame\n\n\nspark_read_delta()\nRead from Delta Lake into a Spark DataFrame.\n\n\nspark_read_image()\nRead image data into a Spark DataFrame.\n\n\nspark_read_jdbc()\nRead from JDBC connection into a Spark DataFrame.\n\n\nspark_read_json()\nRead a JSON file into a Spark DataFrame\n\n\nspark_read_libsvm()\nRead libsvm file into a Spark DataFrame.\n\n\nspark_read_parquet()\nRead a Parquet file into a Spark DataFrame\n\n\nspark_read_source()\nRead from a generic source into a Spark DataFrame.\n\n\nspark_read_table()\nReads from a Spark Table into a Spark DataFrame.\n\n\nspark_read_orc()\nRead a ORC file into a Spark DataFrame\n\n\nspark_read_text()\nRead a Text file into a Spark DataFrame\n\n\nspark_save_table()\nSaves a Spark DataFrame as a Spark table\n\n\nspark_write()\nWrite Spark DataFrame to file using a custom writer\n\n\nspark_write_avro()\nSerialize a Spark DataFrame into Apache Avro format\n\n\nspark_write_orc()\nWrite a Spark DataFrame to a ORC file\n\n\nspark_write_text()\nWrite a Spark DataFrame to a Text file\n\n\nspark_write_csv()\nWrite a Spark DataFrame to a CSV\n\n\nspark_write_delta()\nWrites a Spark DataFrame into Delta Lake\n\n\nspark_write_jdbc()\nWrites a Spark DataFrame into a JDBC table\n\n\nspark_write_json()\nWrite a Spark DataFrame to a JSON file\n\n\nspark_write_parquet()\nWrite a Spark DataFrame to a Parquet file\n\n\nspark_write_source()\nWrites a Spark DataFrame into a generic source\n\n\nspark_write_table()\nWrites a Spark DataFrame into a Spark table\n\n\nspark_write_rds()\nWrite Spark DataFrame to RDS files\n\n\ncollect_from_rds()\nCollect Spark data serialized in RDS format into R"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-tables",
    "href": "packages/sparklyr/latest/reference/index.html#spark-tables",
    "title": "sparklyr",
    "section": "Spark Tables",
    "text": "Spark Tables\n\n\n\nFunction(s)\nDescription\n\n\n\n\nsrc_databases()\nShow database list\n\n\ntbl_cache()\nCache a Spark Table\n\n\ntbl_change_db()\nUse specific database\n\n\ntbl_uncache()\nUncache a Spark Table"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-dataframes",
    "href": "packages/sparklyr/latest/reference/index.html#spark-dataframes",
    "title": "sparklyr",
    "section": "Spark DataFrames",
    "text": "Spark DataFrames\n\n\n\nFunction(s)\nDescription\n\n\n\n\n[(<tbl_spark>)\nSubsetting operator for Spark dataframe\n\n\ncopy_to(<spark_connection>)\nCopy an R Data Frame to Spark\n\n\nsdf_along()\nCreate DataFrame for along Object\n\n\nsdf_bind_rows() sdf_bind_cols()\nBind multiple Spark DataFrames by row and column\n\n\nsdf_broadcast()\nBroadcast hint\n\n\nsdf_checkpoint()\nCheckpoint a Spark DataFrame\n\n\nsdf_coalesce()\nCoalesces a Spark DataFrame\n\n\nsdf_copy_to() sdf_import()\nCopy an Object into Spark\n\n\nsdf_distinct()\nInvoke distinct on a Spark DataFrame\n\n\nsdf_drop_duplicates()\nRemove duplicates from a Spark DataFrame\n\n\nsdf_expand_grid()\nCreate a Spark dataframe containing all combinations of inputs\n\n\nsdf_from_avro()\nConvert column(s) from avro format\n\n\nsdf_len()\nCreate DataFrame for Length\n\n\nsdf_num_partitions()\nGets number of partitions of a Spark DataFrame\n\n\nsdf_random_split() sdf_partition()\nPartition a Spark Dataframe\n\n\nsdf_partition_sizes()\nCompute the number of records within each partition of a Spark DataFrame\n\n\nsdf_pivot()\nPivot a Spark DataFrame\n\n\nsdf_predict() sdf_transform() sdf_fit() sdf_fit_and_transform()\nSpark ML – Transform, fit, and predict methods (sdf_ interface)\n\n\nsdf_rbeta()\nGenerate random samples from a Beta distribution\n\n\nsdf_rbinom()\nGenerate random samples from a binomial distribution\n\n\nsdf_rcauchy()\nGenerate random samples from a Cauchy distribution\n\n\nsdf_rchisq()\nGenerate random samples from a chi-squared distribution\n\n\nsdf_rexp()\nGenerate random samples from an exponential distribution\n\n\nsdf_rgamma()\nGenerate random samples from a Gamma distribution\n\n\nsdf_rgeom()\nGenerate random samples from a geometric distribution\n\n\nsdf_rhyper()\nGenerate random samples from a hypergeometric distribution\n\n\nsdf_rlnorm()\nGenerate random samples from a log normal distribution\n\n\nsdf_rnorm()\nGenerate random samples from the standard normal distribution\n\n\nsdf_rpois()\nGenerate random samples from a Poisson distribution\n\n\nsdf_rt()\nGenerate random samples from a t-distribution\n\n\nsdf_runif()\nGenerate random samples from the uniform distribution U(0, 1).\n\n\nsdf_rweibull()\nGenerate random samples from a Weibull distribution.\n\n\nsdf_read_column()\nRead a Column from a Spark DataFrame\n\n\nsdf_register()\nRegister a Spark DataFrame\n\n\nsdf_repartition()\nRepartition a Spark DataFrame\n\n\nsdf_residuals()\nModel Residuals\n\n\nsdf_sample()\nRandomly Sample Rows from a Spark DataFrame\n\n\nsdf_separate_column()\nSeparate a Vector Column into Scalar Columns\n\n\nsdf_seq()\nCreate DataFrame for Range\n\n\nsdf_sort()\nSort a Spark DataFrame\n\n\nsdf_to_avro()\nConvert column(s) to avro format\n\n\nsdf_with_unique_id()\nAdd a Unique ID Column to a Spark DataFrame\n\n\nsdf_collect()\nCollect a Spark DataFrame into R.\n\n\nsdf_crosstab()\nCross Tabulation\n\n\nsdf_debug_string()\nDebug Info for Spark DataFrame\n\n\nsdf_describe()\nCompute summary statistics for columns of a data frame\n\n\nsdf_dim() sdf_nrow() sdf_ncol()\nSupport for Dimension Operations\n\n\nsdf_is_streaming()\nSpark DataFrame is Streaming\n\n\nsdf_last_index()\nReturns the last index of a Spark DataFrame\n\n\nsdf_save_table() sdf_load_table() sdf_save_parquet() sdf_load_parquet()\nSave / Load a Spark DataFrame\n\n\nsdf_persist()\nPersist a Spark DataFrame\n\n\nsdf_project()\nProject features onto principal components\n\n\nsdf_quantile()\nCompute (Approximate) Quantiles with a Spark DataFrame\n\n\nsdf_schema()\nRead the Schema of a Spark DataFrame\n\n\nsdf_sql()\nSpark DataFrame from SQL\n\n\nsdf_unnest_longer()\nUnnest longer\n\n\nsdf_unnest_wider()\nUnnest wider\n\n\nsdf_with_sequential_id()\nAdd a Sequential ID Column to a Spark DataFrame\n\n\ninner_join(<tbl_spark>) left_join(<tbl_spark>) right_join(<tbl_spark>) full_join(<tbl_spark>)\nJoin Spark tbls.\n\n\nhof_aggregate()\nApply Aggregate Function to Array Column\n\n\nhof_array_sort()\nSorts array using a custom comparator\n\n\nhof_exists()\nDetermine Whether Some Element Exists in an Array Column\n\n\nhof_filter()\nFilter Array Column\n\n\nhof_forall()\nChecks whether all elements in an array satisfy a predicate\n\n\nhof_map_filter()\nFilters a map\n\n\nhof_map_zip_with()\nMerges two maps into one\n\n\nhof_transform()\nTransform Array Column\n\n\nhof_transform_keys()\nTransforms keys of a map\n\n\nhof_transform_values()\nTransforms values of a map\n\n\nhof_zip_with()\nCombines 2 Array Columns\n\n\nsdf_weighted_sample()\nPerform Weighted Random Sampling on a Spark DataFrame\n\n\ntransform_sdf()\ntransform a subset of column(s) in a Spark Dataframe"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-machine-learning",
    "href": "packages/sparklyr/latest/reference/index.html#spark-machine-learning",
    "title": "sparklyr",
    "section": "Spark Machine Learning",
    "text": "Spark Machine Learning\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_decision_tree_classifier() ml_decision_tree() ml_decision_tree_regressor()\nSpark ML – Decision Trees\n\n\nml_generalized_linear_regression()\nSpark ML – Generalized Linear Regression\n\n\nml_gbt_classifier() ml_gradient_boosted_trees() ml_gbt_regressor()\nSpark ML – Gradient Boosted Trees\n\n\nml_kmeans() ml_compute_cost() ml_compute_silhouette_measure()\nSpark ML – K-Means Clustering\n\n\nml_lda() ml_describe_topics() ml_log_likelihood() ml_log_perplexity() ml_topics_matrix()\nSpark ML – Latent Dirichlet Allocation\n\n\nml_linear_regression()\nSpark ML – Linear Regression\n\n\nml_logistic_regression()\nSpark ML – Logistic Regression\n\n\nml_model_data()\nExtracts data associated with a Spark ML model\n\n\nml_multilayer_perceptron_classifier() ml_multilayer_perceptron()\nSpark ML – Multilayer Perceptron\n\n\nml_naive_bayes()\nSpark ML – Naive-Bayes\n\n\nml_one_vs_rest()\nSpark ML – OneVsRest\n\n\nft_pca() ml_pca()\nFeature Transformation – PCA (Estimator)\n\n\nml_prefixspan() ml_freq_seq_patterns()\nFrequent Pattern Mining – PrefixSpan\n\n\nml_random_forest_classifier() ml_random_forest() ml_random_forest_regressor()\nSpark ML – Random Forest\n\n\nml_aft_survival_regression() ml_survival_regression()\nSpark ML – Survival Regression\n\n\nml_add_stage()\nAdd a Stage to a Pipeline\n\n\nml_als() ml_recommend()\nSpark ML – ALS\n\n\nml_approx_nearest_neighbors() ml_approx_similarity_join()\nUtility functions for LSH models\n\n\nml_fpgrowth() ml_association_rules() ml_freq_itemsets()\nFrequent Pattern Mining – FPGrowth\n\n\nml_binary_classification_evaluator() ml_binary_classification_eval() ml_multiclass_classification_evaluator() ml_classification_eval() ml_regression_evaluator()\nSpark ML - Evaluators\n\n\nml_bisecting_kmeans()\nSpark ML – Bisecting K-Means Clustering\n\n\nml_call_constructor()\nWrap a Spark ML JVM object\n\n\nml_chisquare_test()\nChi-square hypothesis testing for categorical data.\n\n\nml_clustering_evaluator()\nSpark ML - Clustering Evaluator\n\n\nml_supervised_pipeline() ml_clustering_pipeline() ml_construct_model_supervised() ml_construct_model_clustering() new_ml_model_prediction() new_ml_model() new_ml_model_classification() new_ml_model_regression() new_ml_model_clustering()\nConstructors for ml_model Objects\n\n\nml_corr()\nCompute correlation matrix\n\n\nml_sub_models() ml_validation_metrics() ml_cross_validator() ml_train_validation_split()\nSpark ML – Tuning\n\n\nml_default_stop_words()\nDefault stop words\n\n\nml_evaluate()\nEvaluate the Model on a Validation Set\n\n\nml_feature_importances() ml_tree_feature_importance()\nSpark ML - Feature Importance for Tree Models\n\n\nft_word2vec() ml_find_synonyms()\nFeature Transformation – Word2Vec (Estimator)\n\n\nis_ml_transformer() is_ml_estimator() ml_fit() ml_transform() ml_fit_and_transform() ml_predict()\nSpark ML – Transform, fit, and predict methods (ml_ interface)\n\n\nml_gaussian_mixture()\nSpark ML – Gaussian Mixture clustering.\n\n\nml_is_set() ml_param_map() ml_param() ml_params()\nSpark ML – ML Params\n\n\nml_isotonic_regression()\nSpark ML – Isotonic Regression\n\n\nft_string_indexer() ml_labels() ft_string_indexer_model()\nFeature Transformation – StringIndexer (Estimator)\n\n\nml_linear_svc()\nSpark ML – LinearSVC\n\n\nml_save() ml_load()\nSpark ML – Model Persistence\n\n\nml_pipeline()\nSpark ML – Pipelines\n\n\nml_power_iteration()\nSpark ML – Power Iteration Clustering\n\n\nml_stage() ml_stages()\nSpark ML – Pipeline stage extraction\n\n\nml_standardize_formula()\nStandardize Formula Input for ml_model\n\n\nml_summary()\nSpark ML – Extraction of summary metrics\n\n\nml_uid()\nSpark ML – UID\n\n\nft_count_vectorizer() ml_vocabulary()\nFeature Transformation – CountVectorizer (Estimator)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-feature-transformers",
    "href": "packages/sparklyr/latest/reference/index.html#spark-feature-transformers",
    "title": "sparklyr",
    "section": "Spark Feature Transformers",
    "text": "Spark Feature Transformers\n\n\n\nFunction(s)\nDescription\n\n\n\n\nft_binarizer()\nFeature Transformation – Binarizer (Transformer)\n\n\nft_bucketizer()\nFeature Transformation – Bucketizer (Transformer)\n\n\nft_count_vectorizer() ml_vocabulary()\nFeature Transformation – CountVectorizer (Estimator)\n\n\nft_dct() ft_discrete_cosine_transform()\nFeature Transformation – Discrete Cosine Transform (DCT) (Transformer)\n\n\nft_elementwise_product()\nFeature Transformation – ElementwiseProduct (Transformer)\n\n\nft_index_to_string()\nFeature Transformation – IndexToString (Transformer)\n\n\nft_one_hot_encoder()\nFeature Transformation – OneHotEncoder (Transformer)\n\n\nft_quantile_discretizer()\nFeature Transformation – QuantileDiscretizer (Estimator)\n\n\nft_sql_transformer() ft_dplyr_transformer()\nFeature Transformation – SQLTransformer\n\n\nft_string_indexer() ml_labels() ft_string_indexer_model()\nFeature Transformation – StringIndexer (Estimator)\n\n\nft_vector_assembler()\nFeature Transformation – VectorAssembler (Transformer)\n\n\nft_tokenizer()\nFeature Transformation – Tokenizer (Transformer)\n\n\nft_regex_tokenizer()\nFeature Transformation – RegexTokenizer (Transformer)\n\n\nft_bucketed_random_projection_lsh() ft_minhash_lsh()\nFeature Transformation – LSH (Estimator)\n\n\nft_chisq_selector()\nFeature Transformation – ChiSqSelector (Estimator)\n\n\nft_feature_hasher()\nFeature Transformation – FeatureHasher (Transformer)\n\n\nft_hashing_tf()\nFeature Transformation – HashingTF (Transformer)\n\n\nft_idf()\nFeature Transformation – IDF (Estimator)\n\n\nft_imputer()\nFeature Transformation – Imputer (Estimator)\n\n\nft_interaction()\nFeature Transformation – Interaction (Transformer)\n\n\nft_max_abs_scaler()\nFeature Transformation – MaxAbsScaler (Estimator)\n\n\nft_min_max_scaler()\nFeature Transformation – MinMaxScaler (Estimator)\n\n\nft_ngram()\nFeature Transformation – NGram (Transformer)\n\n\nft_normalizer()\nFeature Transformation – Normalizer (Transformer)\n\n\nft_one_hot_encoder_estimator()\nFeature Transformation – OneHotEncoderEstimator (Estimator)\n\n\nft_pca() ml_pca()\nFeature Transformation – PCA (Estimator)\n\n\nft_polynomial_expansion()\nFeature Transformation – PolynomialExpansion (Transformer)\n\n\nft_r_formula()\nFeature Transformation – RFormula (Estimator)\n\n\nft_standard_scaler()\nFeature Transformation – StandardScaler (Estimator)\n\n\nft_stop_words_remover()\nFeature Transformation – StopWordsRemover (Transformer)\n\n\nft_vector_indexer()\nFeature Transformation – VectorIndexer (Estimator)\n\n\nft_vector_slicer()\nFeature Transformation – VectorSlicer (Transformer)\n\n\nft_word2vec() ml_find_synonyms()\nFeature Transformation – Word2Vec (Estimator)\n\n\nft_robust_scaler()\nFeature Transformation – RobustScaler (Estimator)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#spark-machine-learning-utilities",
    "href": "packages/sparklyr/latest/reference/index.html#spark-machine-learning-utilities",
    "title": "sparklyr",
    "section": "Spark Machine Learning Utilities",
    "text": "Spark Machine Learning Utilities\n\n\n\nFunction(s)\nDescription\n\n\n\n\nml_binary_classification_evaluator() ml_binary_classification_eval() ml_multiclass_classification_evaluator() ml_classification_eval() ml_regression_evaluator()\nSpark ML - Evaluators\n\n\nml_feature_importances() ml_tree_feature_importance()\nSpark ML - Feature Importance for Tree Models\n\n\ntidy(<ml_model_als>) augment(<ml_model_als>) glance(<ml_model_als>)\nTidying methods for Spark ML ALS\n\n\ntidy(<ml_model_generalized_linear_regression>) tidy(<ml_model_linear_regression>) augment(<ml_model_generalized_linear_regression>) augment(<ml_model_linear_regression>) glance(<ml_model_generalized_linear_regression>) glance(<ml_model_linear_regression>)\nTidying methods for Spark ML linear models\n\n\ntidy(<ml_model_isotonic_regression>) augment(<ml_model_isotonic_regression>) glance(<ml_model_isotonic_regression>)\nTidying methods for Spark ML Isotonic Regression\n\n\ntidy(<ml_model_lda>) augment(<ml_model_lda>) glance(<ml_model_lda>)\nTidying methods for Spark ML LDA models\n\n\ntidy(<ml_model_linear_svc>) augment(<ml_model_linear_svc>) glance(<ml_model_linear_svc>)\nTidying methods for Spark ML linear svc\n\n\ntidy(<ml_model_logistic_regression>) augment(<ml_model_logistic_regression>) glance(<ml_model_logistic_regression>)\nTidying methods for Spark ML Logistic Regression\n\n\ntidy(<ml_model_multilayer_perceptron_classification>) augment(<ml_model_multilayer_perceptron_classification>) glance(<ml_model_multilayer_perceptron_classification>)\nTidying methods for Spark ML MLP\n\n\ntidy(<ml_model_naive_bayes>) augment(<ml_model_naive_bayes>) glance(<ml_model_naive_bayes>)\nTidying methods for Spark ML Naive Bayes\n\n\ntidy(<ml_model_pca>) augment(<ml_model_pca>) glance(<ml_model_pca>)\nTidying methods for Spark ML Principal Component Analysis\n\n\ntidy(<ml_model_aft_survival_regression>) augment(<ml_model_aft_survival_regression>) glance(<ml_model_aft_survival_regression>)\nTidying methods for Spark ML Survival Regression\n\n\ntidy(<ml_model_decision_tree_classification>) tidy(<ml_model_decision_tree_regression>) augment(<ml_model_decision_tree_classification>) augment(<ml_model_decision_tree_regression>) glance(<ml_model_decision_tree_classification>) glance(<ml_model_decision_tree_regression>) tidy(<ml_model_random_forest_classification>) tidy(<ml_model_random_forest_regression>) augment(<ml_model_random_forest_classification>) augment(<ml_model_random_forest_regression>) glance(<ml_model_random_forest_classification>) glance(<ml_model_random_forest_regression>) tidy(<ml_model_gbt_classification>) tidy(<ml_model_gbt_regression>) augment(<ml_model_gbt_classification>) augment(<ml_model_gbt_regression>) glance(<ml_model_gbt_classification>) glance(<ml_model_gbt_regression>)\nTidying methods for Spark ML tree models\n\n\ntidy(<ml_model_kmeans>) augment(<ml_model_kmeans>) glance(<ml_model_kmeans>) tidy(<ml_model_bisecting_kmeans>) augment(<ml_model_bisecting_kmeans>) glance(<ml_model_bisecting_kmeans>) tidy(<ml_model_gaussian_mixture>) augment(<ml_model_gaussian_mixture>) glance(<ml_model_gaussian_mixture>)\nTidying methods for Spark ML unsupervised models"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#extensions",
    "href": "packages/sparklyr/latest/reference/index.html#extensions",
    "title": "sparklyr",
    "section": "Extensions",
    "text": "Extensions\n\n\n\nFunction(s)\nDescription\n\n\n\n\ncompile_package_jars()\nCompile Scala sources into a Java Archive (jar)\n\n\nconnection_config()\nRead configuration values for a connection\n\n\ndownload_scalac()\nDownloads default Scala Compilers\n\n\nfind_scalac()\nDiscover the Scala Compiler\n\n\nspark_context() java_context() hive_context() spark_session()\nAccess the Spark API\n\n\nhive_context_config()\nRuntime configuration interface for Hive\n\n\ninvoke() invoke_static() invoke_new()\nInvoke a Method on a JVM Object\n\n\nj_invoke() j_invoke_static() j_invoke_new()\nInvoke a Java function.\n\n\njarray()\nInstantiate a Java array with a specific element type.\n\n\njfloat()\nInstantiate a Java float type.\n\n\njfloat_array()\nInstantiate an Array[Float].\n\n\nregister_extension() registered_extensions()\nRegister a Package that Implements a Spark Extension\n\n\nspark_compilation_spec()\nDefine a Spark Compilation Specification\n\n\nspark_default_compilation_spec()\nDefault Compilation Specification for Spark Extensions\n\n\nspark_context_config()\nRuntime configuration interface for the Spark Context.\n\n\nspark_dataframe()\nRetrieve a Spark DataFrame\n\n\nspark_dependency()\nDefine a Spark dependency\n\n\nspark_home_set()\nSet the SPARK_HOME environment variable\n\n\nspark_jobj()\nRetrieve a Spark JVM Object Reference\n\n\nspark_version()\nGet the Spark Version Associated with a Spark Connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#distributed-computing",
    "href": "packages/sparklyr/latest/reference/index.html#distributed-computing",
    "title": "sparklyr",
    "section": "Distributed Computing",
    "text": "Distributed Computing\n\n\n\nFunction(s)\nDescription\n\n\n\n\nspark_apply()\nApply an R Function in Spark\n\n\nspark_apply_bundle()\nCreate Bundle for Spark Apply\n\n\nspark_apply_log()\nLog Writer for Spark Apply\n\n\nregisterDoSpark()\nRegister a Parallel Backend"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#livy",
    "href": "packages/sparklyr/latest/reference/index.html#livy",
    "title": "sparklyr",
    "section": "Livy",
    "text": "Livy\n\n\n\nFunction(s)\nDescription\n\n\n\n\nlivy_install() livy_available_versions() livy_install_dir() livy_installed_versions() livy_home_dir()\nInstall Livy\n\n\nlivy_config()\nCreate a Spark Configuration for Livy\n\n\nlivy_service_start() livy_service_stop()\nStart Livy"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/index.html#streaming",
    "href": "packages/sparklyr/latest/reference/index.html#streaming",
    "title": "sparklyr",
    "section": "Streaming",
    "text": "Streaming\n\n\n\nFunction(s)\nDescription\n\n\n\n\nstream_find()\nFind Stream\n\n\nstream_generate_test()\nGenerate Test Stream\n\n\nstream_id()\nSpark Stream’s Identifier\n\n\nstream_lag()\nApply lag function to columns of a Spark Streaming DataFrame\n\n\nstream_name()\nSpark Stream’s Name\n\n\nstream_read_csv()\nRead CSV Stream\n\n\nstream_read_json()\nRead JSON Stream\n\n\nstream_read_delta()\nRead Delta Stream\n\n\nstream_read_kafka()\nRead Kafka Stream\n\n\nstream_read_orc()\nRead ORC Stream\n\n\nstream_read_parquet()\nRead Parquet Stream\n\n\nstream_read_socket()\nRead Socket Stream\n\n\nstream_read_text()\nRead Text Stream\n\n\nstream_render()\nRender Stream\n\n\nstream_stats()\nStream Statistics\n\n\nstream_stop()\nStops a Spark Stream\n\n\nstream_trigger_continuous()\nSpark Stream Continuous Trigger\n\n\nstream_trigger_interval()\nSpark Stream Interval Trigger\n\n\nstream_view()\nView Stream\n\n\nstream_watermark()\nWatermark Stream\n\n\nstream_write_console()\nWrite Console Stream\n\n\nstream_write_csv()\nWrite CSV Stream\n\n\nstream_write_delta()\nWrite Delta Stream\n\n\nstream_write_json()\nWrite JSON Stream\n\n\nstream_write_kafka()\nWrite Kafka Stream\n\n\nstream_write_memory()\nWrite Memory Stream\n\n\nstream_write_orc()\nWrite a ORC Stream\n\n\nstream_write_parquet()\nWrite Parquet Stream\n\n\nstream_write_text()\nWrite Text Stream\n\n\nreactiveSpark()\nReactive spark reader"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/inner_join.html#description",
    "href": "packages/sparklyr/latest/reference/inner_join.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee inner_join for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke.html#description",
    "href": "packages/sparklyr/latest/reference/invoke.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nInvoke methods on Java object references. These functions provide a mechanism for invoking various Java object methods directly from ."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke.html#usage",
    "href": "packages/sparklyr/latest/reference/invoke.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninvoke(jobj, method, ...)\n\ninvoke_static(sc, class, method, ...)\n\ninvoke_new(sc, class, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke.html#arguments",
    "href": "packages/sparklyr/latest/reference/invoke.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\njobj\nAn object acting as a Java object reference (typically, a spark_jobj).\n\n\nmethod\nThe name of the method to be invoked.\n\n\n…\nOptional arguments, currently unused.\n\n\nsc\nA spark_connection.\n\n\nclass\nThe name of the Java class whose methods should be invoked."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke.html#details",
    "href": "packages/sparklyr/latest/reference/invoke.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nUse each of these functions in the following scenarios:\nlll invoke Execute a method on a Java object reference (typically, a spark_jobj). invoke_static Execute a static method associated with a Java class. invoke_new Invoke a constructor associated with a Java class."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke.html#examples",
    "href": "packages/sparklyr/latest/reference/invoke.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"spark://HOST:PORT\")\nspark_context(sc) %>%\n  invoke(\"textFile\", \"file.csv\", 1L) %>%\n  invoke(\"count\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke_method.html#description",
    "href": "packages/sparklyr/latest/reference/invoke_method.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGeneric Call Interface"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke_method.html#usage",
    "href": "packages/sparklyr/latest/reference/invoke_method.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninvoke_method(sc, static, object, method, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/invoke_method.html#arguments",
    "href": "packages/sparklyr/latest/reference/invoke_method.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nspark_connection\n\n\nstatic\nIs this a static method call (including a constructor). If so\n\n\n\nthen the object parameter should be the name of a class (otherwise it should be a spark_jobj instance). object | Object instance or name of class (for static) method | Name of method … | Call parameters"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke.html#description",
    "href": "packages/sparklyr/latest/reference/j_invoke.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nInvoke a Java function and force return value of the call to be retrieved as a Java object reference."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke.html#usage",
    "href": "packages/sparklyr/latest/reference/j_invoke.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nj_invoke(jobj, method, ...)\n\nj_invoke_static(sc, class, method, ...)\n\nj_invoke_new(sc, class, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke.html#arguments",
    "href": "packages/sparklyr/latest/reference/j_invoke.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\njobj\nAn object acting as a Java object reference (typically, a spark_jobj).\n\n\nmethod\nThe name of the method to be invoked.\n\n\n…\nOptional arguments, currently unused.\n\n\nsc\nA spark_connection.\n\n\nclass\nThe name of the Java class whose methods should be invoked."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke_method.html#description",
    "href": "packages/sparklyr/latest/reference/j_invoke_method.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCall a Java method and retrieve the return value through a JVM object reference."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke_method.html#usage",
    "href": "packages/sparklyr/latest/reference/j_invoke_method.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nj_invoke_method(sc, static, object, method, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/j_invoke_method.html#arguments",
    "href": "packages/sparklyr/latest/reference/j_invoke_method.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nspark_connection\n\n\nstatic\nIs this a static method call (including a constructor). If so\n\n\n\nthen the object parameter should be the name of a class (otherwise it should be a spark_jobj instance). object | Object instance or name of class (for static) method | Name of method … | Call parameters"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jarray.html#description",
    "href": "packages/sparklyr/latest/reference/jarray.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGiven a list of Java object references, instantiate an Array[T] containing the same list of references, where T is a non-primitive type that is more specific than java.lang.Object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jarray.html#usage",
    "href": "packages/sparklyr/latest/reference/jarray.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\njarray(sc, x, element_type)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jarray.html#arguments",
    "href": "packages/sparklyr/latest/reference/jarray.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nx\nA list of Java object references.\n\n\nelement_type\nA valid Java class name representing the generic type\n\n\n\nparameter of the Java array to be instantiated. Each element of x must refer to a Java object that is assignable to element_type."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jarray.html#examples",
    "href": "packages/sparklyr/latest/reference/jarray.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\nsc <- spark_connect(master = \"spark://HOST:PORT\")\n\nstring_arr <- jarray(sc, letters, element_type = \"java.lang.String\")\n# string_arr is now a reference to an array of type String[]"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat.html#description",
    "href": "packages/sparklyr/latest/reference/jfloat.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nInstantiate a java.lang.Float object with the value specified. NOTE: this method is useful when one has to invoke a Java/Scala method requiring a float (instead of double) type for at least one of its parameters."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat.html#usage",
    "href": "packages/sparklyr/latest/reference/jfloat.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\njfloat(sc, x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat.html#arguments",
    "href": "packages/sparklyr/latest/reference/jfloat.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nx\nA numeric value in R."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat.html#examples",
    "href": "packages/sparklyr/latest/reference/jfloat.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\nsc <- spark_connect(master = \"spark://HOST:PORT\")\n\njflt <- jfloat(sc, 1.23e-8)\n# jflt is now a reference to a java.lang.Float object"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat_array.html#description",
    "href": "packages/sparklyr/latest/reference/jfloat_array.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nInstantiate an Array[Float] object with the value specified. NOTE: this method is useful when one has to invoke a Java/Scala method requiring an Array[Float] as one of its parameters."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat_array.html#usage",
    "href": "packages/sparklyr/latest/reference/jfloat_array.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\njfloat_array(sc, x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat_array.html#arguments",
    "href": "packages/sparklyr/latest/reference/jfloat_array.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nx\nA numeric vector in R."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jfloat_array.html#examples",
    "href": "packages/sparklyr/latest/reference/jfloat_array.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\nsc <- spark_connect(master = \"spark://HOST:PORT\")\n\njflt_arr <- jfloat_array(sc, c(-1.23e-8, 0, -1.23e-8))\n# jflt_arr is now a reference an array of java.lang.Float"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_class.html#description",
    "href": "packages/sparklyr/latest/reference/jobj_class.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nExtract the classes that a Java object inherits from. This is the jobj equivalent of class()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_class.html#usage",
    "href": "packages/sparklyr/latest/reference/jobj_class.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\njobj_class(jobj, simple_name = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_class.html#arguments",
    "href": "packages/sparklyr/latest/reference/jobj_class.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\njobj\nA spark_jobj\n\n\nsimple_name\nWhether to return simple names, defaults to TRUE"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_set_param.html#description",
    "href": "packages/sparklyr/latest/reference/jobj_set_param.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSets a parameter value for a pipeline stage object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_set_param.html#usage",
    "href": "packages/sparklyr/latest/reference/jobj_set_param.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\njobj_set_param(jobj, setter, value, min_version = NULL, default = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/jobj_set_param.html#arguments",
    "href": "packages/sparklyr/latest/reference/jobj_set_param.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\njobj\nA pipeline stage jobj.\n\n\nsetter\nThe name of the setter method as a string.\n\n\nvalue\nThe value to be set.\n\n\nmin_version\nThe minimum required Spark version for this parameter to be valid.\n\n\ndefault\nThe default value of the parameter, to be used together with min_version.\n\n\n\nAn error is thrown if the user’s Spark version is older than min_version and value differs from default."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/join.tbl_spark.html#description",
    "href": "packages/sparklyr/latest/reference/join.tbl_spark.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThese functions are wrappers around their dplyr equivalents that set Spark SQL-compliant values for the suffix argument by replacing dots (.) with underscores (_). See [join] for a description of the general purpose of the functions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/join.tbl_spark.html#usage",
    "href": "packages/sparklyr/latest/reference/join.tbl_spark.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninner_jointbl_spark( x, y, by = NULL, copy = FALSE, suffix = c(“_x”, “_y”), auto_index = FALSE, …, sql_on = NULL )\nleft_jointbl_spark( x, y, by = NULL, copy = FALSE, suffix = c(“_x”, “_y”), auto_index = FALSE, …, sql_on = NULL )\nright_jointbl_spark( x, y, by = NULL, copy = FALSE, suffix = c(“_x”, “_y”), auto_index = FALSE, …, sql_on = NULL )\nfull_jointbl_spark( x, y, by = NULL, copy = FALSE, suffix = c(“_x”, “_y”), auto_index = FALSE, …, sql_on = NULL )"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/join.tbl_spark.html#arguments",
    "href": "packages/sparklyr/latest/reference/join.tbl_spark.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA pair of lazy data frames backed by database queries.\n\n\ny\nA pair of lazy data frames backed by database queries.\n\n\nby\nA character vector of variables to join by.\n\n\n\nIf NULL, the default, *_join() will perform a natural join, using all variables in common across x and y. A message lists the variables so that you can check they’re correct; suppress the message by supplying by explicitly.\nTo join by different variables on x and y, use a named vector. For example, by = c(\"a\" = \"b\") will match x$a to y$b.\nTo join by multiple variables, use a vector with length > 1. For example, by = c(\"a\", \"b\") will match x$a to y$a and x$b to y$b. Use a named vector to match different variables in x and y. For example, by = c(\"a\" = \"b\", \"c\" = \"d\") will match x$a to y$b and x$c to y$d.\nTo perform a cross-join, generating all combinations of x and y, use by = character(). copy | If x and y are not from the same data source, and copy is TRUE, then y will be copied into a temporary table in same database as x. *_join() will automatically run ANALYZE on the created table in the hope that this will make you queries as efficient as possible by giving more data to the query planner.\nThis allows you to join tables across srcs, but it’s potentially expensive operation so you must opt into it. suffix | If there are non-joined duplicate variables in x and y, these suffixes will be added to the output to disambiguate them. Should be a character vector of length 2. auto_index | if copy is TRUE, automatically create indices for the variables in by. This may speed up the join if there are matching indexes in x. … | Other parameters passed onto methods. sql_on | A custom join predicate as an SQL expression. Usually joins use column equality, but you can perform more complex queries by supply sql_on which should be a SQL expression that uses LHS and RHS aliases to refer to the left-hand side or right-hand side of the join respectively."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/left_join.html#description",
    "href": "packages/sparklyr/latest/reference/left_join.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee left_join for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/list_sparklyr_jars.html#description",
    "href": "packages/sparklyr/latest/reference/list_sparklyr_jars.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nlist all sparklyr-*.jar files that have been built"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/list_sparklyr_jars.html#usage",
    "href": "packages/sparklyr/latest/reference/list_sparklyr_jars.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nlist_sparklyr_jars()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_config.html#description",
    "href": "packages/sparklyr/latest/reference/livy_config.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCreate a Spark Configuration for Livy"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_config.html#usage",
    "href": "packages/sparklyr/latest/reference/livy_config.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nlivy_config(\n  config = spark_config(),\n  username = NULL,\n  password = NULL,\n  negotiate = FALSE,\n  custom_headers = list(`X-Requested-By` = \"sparklyr\"),\n  proxy = NULL,\n  curl_opts = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_config.html#arguments",
    "href": "packages/sparklyr/latest/reference/livy_config.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nconfig\nOptional base configuration\n\n\nusername\nThe username to use in the Authorization header\n\n\npassword\nThe password to use in the Authorization header\n\n\nnegotiate\nWhether to use gssnegotiate method or not\n\n\ncustom_headers\nList of custom headers to append to http requests. Defaults to list(\"X-Requested-By\" = \"sparklyr\").\n\n\nproxy\nEither NULL or a proxy specified by httr::use_proxy(). Defaults to NULL.\n\n\ncurl_opts\nList of CURL options (e.g., verbose, connecttimeout, dns_cache_timeout, etc, see httr::httr_options() for a\n\n\n\nlist of valid options) – NOTE: these configurations are for libcurl only and separate from HTTP headers or Livy session parameters. … | additional Livy session parameters"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_config.html#details",
    "href": "packages/sparklyr/latest/reference/livy_config.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nExtends a Spark spark_config() configuration with settings for Livy. For instance, username and password define the basic authentication settings for a Livy session.\nThe default value of \"custom_headers\" is set to list(\"X-Requested-By\" = \"sparklyr\") in order to facilitate connection to Livy servers with CSRF protection enabled.\nAdditional parameters for Livy sessions are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that queue is supported only by version 0.4.0 of Livy or newer. If you are using the older one, specify queue via config (e.g. config = spark_config(spark.yarn.queue = \"my_queue\"))."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_config.html#value",
    "href": "packages/sparklyr/latest/reference/livy_config.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nNamed list with configuration data"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_install.html#description",
    "href": "packages/sparklyr/latest/reference/livy_install.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nAutomatically download and install http://livy.io/livy. livy provides a REST API to Spark.\nFind the LIVY_HOME directory for a given version of Livy that was previously installed using livy_install."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_install.html#usage",
    "href": "packages/sparklyr/latest/reference/livy_install.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nlivy_install(version = \"0.6.0\", spark_home = NULL, spark_version = NULL)\n\nlivy_available_versions()\n\nlivy_install_dir()\n\nlivy_installed_versions()\n\nlivy_home_dir(version = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_install.html#arguments",
    "href": "packages/sparklyr/latest/reference/livy_install.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nversion\nVersion of Livy\n\n\nspark_home\nThe path to a Spark installation. The downloaded and\n\n\n\ninstalled version of livy will then be associated with this Spark installation. When unset (NULL), the value is inferred based on the value of spark_version supplied. spark_version | The version of Spark to use. When unset (NULL), the value is inferred based on the value of livy_version supplied. A version of Spark known to be compatible with the requested version of livy is chosen when possible."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_install.html#value",
    "href": "packages/sparklyr/latest/reference/livy_install.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nPath to LIVY_HOME (or NULL if the specified version was not found)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_service.html#description",
    "href": "packages/sparklyr/latest/reference/livy_service.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nStarts the livy service.\nStops the running instances of the livy service."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_service.html#usage",
    "href": "packages/sparklyr/latest/reference/livy_service.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nlivy_service_start(\n  version = NULL,\n  spark_version = NULL,\n  stdout = \"\",\n  stderr = \"\",\n  ...\n)\n\nlivy_service_stop()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/livy_service.html#arguments",
    "href": "packages/sparklyr/latest/reference/livy_service.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nversion\nThe version of livy to use.\n\n\nspark_version\nThe version of spark to connect to.\n\n\nstdout, stderr\nwhere output to ‘stdout’ or ‘stderr’ should\n\n\n\nbe sent. Same options as system2. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-constructors.html#description",
    "href": "packages/sparklyr/latest/reference/ml-constructors.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFunctions for developers writing extensions for Spark ML."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-constructors.html#usage",
    "href": "packages/sparklyr/latest/reference/ml-constructors.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nnew_ml_transformer(jobj, ..., class = character())\n\nnew_ml_prediction_model(jobj, ..., class = character())\n\nnew_ml_classification_model(jobj, ..., class = character())\n\nnew_ml_probabilistic_classification_model(jobj, ..., class = character())\n\nnew_ml_clustering_model(jobj, ..., class = character())\n\nnew_ml_estimator(jobj, ..., class = character())\n\nnew_ml_predictor(jobj, ..., class = character())\n\nnew_ml_classifier(jobj, ..., class = character())\n\nnew_ml_probabilistic_classifier(jobj, ..., class = character())"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-constructors.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml-constructors.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\njobj\nPointer to the pipeline stage object.\n\n\n…\n(Optional) additional attributes of the object.\n\n\nclass\nName of class."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-model-constructors.html#description",
    "href": "packages/sparklyr/latest/reference/ml-model-constructors.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFunctions for developers writing extensions for Spark ML. These functions are constructors for ml_model objects that are returned when using the formula interface."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-model-constructors.html#usage",
    "href": "packages/sparklyr/latest/reference/ml-model-constructors.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_supervised_pipeline(predictor, dataset, formula, features_col, label_col)\n\nml_clustering_pipeline(predictor, dataset, formula, features_col)\n\nml_construct_model_supervised(\n  constructor,\n  predictor,\n  formula,\n  dataset,\n  features_col,\n  label_col,\n  ...\n)\n\nml_construct_model_clustering(\n  constructor,\n  predictor,\n  formula,\n  dataset,\n  features_col,\n  ...\n)\n\nnew_ml_model_prediction(\n  pipeline_model,\n  formula,\n  dataset,\n  label_col,\n  features_col,\n  ...,\n  class = character()\n)\n\nnew_ml_model(pipeline_model, formula, dataset, ..., class = character())\n\nnew_ml_model_classification(\n  pipeline_model,\n  formula,\n  dataset,\n  label_col,\n  features_col,\n  predicted_label_col,\n  ...,\n  class = character()\n)\n\nnew_ml_model_regression(\n  pipeline_model,\n  formula,\n  dataset,\n  label_col,\n  features_col,\n  ...,\n  class = character()\n)\n\nnew_ml_model_clustering(\n  pipeline_model,\n  formula,\n  dataset,\n  features_col,\n  ...,\n  class = character()\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-model-constructors.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml-model-constructors.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\npredictor\nThe pipeline stage corresponding to the ML algorithm.\n\n\ndataset\nThe training dataset.\n\n\nformula\nThe formula used for data preprocessing\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nconstructor\nThe constructor function for the ml_model.\n\n\npipeline_model\nThe pipeline model object returned by ml_supervised_pipeline().\n\n\nclass\nName of the subclass."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-params.html#description",
    "href": "packages/sparklyr/latest/reference/ml-params.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nHelper methods for working with parameters for ML objects."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-params.html#usage",
    "href": "packages/sparklyr/latest/reference/ml-params.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_is_set(x, param, ...)\n\nml_param_map(x, ...)\n\nml_param(x, param, allow_null = FALSE, ...)\n\nml_params(x, params = NULL, allow_null = FALSE, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-params.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml-params.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark ML object, either a pipeline stage or an evaluator.\n\n\nparam\nThe parameter to extract or set.\n\n\n…\nOptional arguments; currently unused.\n\n\nallow_null\nWhether to allow NULL results when extracting parameters. If FALSE, an error will be thrown if the specified parameter is not found. Defaults to FALSE.\n\n\nparams\nA vector of parameters to extract."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-persistence.html#description",
    "href": "packages/sparklyr/latest/reference/ml-persistence.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSave/load Spark ML objects"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-persistence.html#usage",
    "href": "packages/sparklyr/latest/reference/ml-persistence.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_save(x, path, overwrite = FALSE, …)\nml_saveml_model( x, path, overwrite = FALSE, type = c(“pipeline_model”, “pipeline”), … )\nml_load(sc, path)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-persistence.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml-persistence.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA ML object, which could be a ml_pipeline_stage or a ml_model\n\n\npath\nThe path where the object is to be serialized/deserialized.\n\n\noverwrite\nWhether to overwrite the existing path, defaults to FALSE.\n\n\n…\nOptional arguments; currently unused.\n\n\ntype\nWhether to save the pipeline model or the pipeline.\n\n\nsc\nA Spark connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-persistence.html#value",
    "href": "packages/sparklyr/latest/reference/ml-persistence.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nml_save() serializes a Spark object into a format that can be read back into sparklyr or by the Scala or PySpark APIs. When called on ml_model objects, i.e. those that were created via the tbl_spark - formula signature, the associated pipeline model is serialized. In other words, the saved model contains both the data processing (RFormulaModel) stage and the machine learning stage.\nml_load() reads a saved Spark object into sparklyr. It calls the correct Scala load method based on parsing the saved metadata. Note that a PipelineModel object saved from a sparklyr ml_model via ml_save() will be read back in as an ml_pipeline_model, rather than the ml_model object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html#description",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nMethods for transformation, fit, and prediction. These are mirrors of the corresponding sdf-transform-methods."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html#usage",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nis_ml_transformer(x)\nis_ml_estimator(x)\nml_fit(x, dataset, …)\nml_transform(x, dataset, …)\nml_fit_and_transform(x, dataset, …)\nml_predict(x, dataset, …)\nml_predictml_model_classification(x, dataset, probability_prefix = “probability_”, …)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA ml_estimator, ml_transformer (or a list thereof), or ml_model object.\n\n\ndataset\nA tbl_spark.\n\n\n…\nOptional arguments; currently unused.\n\n\nprobability_prefix\nString used to prepend the class probability output columns."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html#details",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nThese methods are"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-transform-methods.html#value",
    "href": "packages/sparklyr/latest/reference/ml-transform-methods.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nWhen x is an estimator, ml_fit() returns a transformer whereas ml_fit_and_transform() returns a transformed dataset. When x is a transformer, ml_transform() and ml_predict() return a transformed dataset. When ml_predict() is called on a ml_model object, additional columns (e.g. probabilities in case of classification models) are appended to the transformed output for the user’s convenience."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#description",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPerform hyper-parameter tuning using either K-fold cross validation or train-validation split."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#usage",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_sub_models(model)\n\nml_validation_metrics(model)\n\nml_cross_validator(\n  x,\n  estimator = NULL,\n  estimator_param_maps = NULL,\n  evaluator = NULL,\n  num_folds = 3,\n  collect_sub_models = FALSE,\n  parallelism = 1,\n  seed = NULL,\n  uid = random_string(\"cross_validator_\"),\n  ...\n)\n\nml_train_validation_split(\n  x,\n  estimator = NULL,\n  estimator_param_maps = NULL,\n  evaluator = NULL,\n  train_ratio = 0.75,\n  collect_sub_models = FALSE,\n  parallelism = 1,\n  seed = NULL,\n  uid = random_string(\"train_validation_split_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nmodel\nA cross validation or train-validation-split model.\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nestimator\nA ml_estimator object.\n\n\nestimator_param_maps\nA named list of stages and hyper-parameter sets to tune. See details.\n\n\nevaluator\nA ml_evaluator object, see ml_evaluator.\n\n\nnum_folds\nNumber of folds for cross validation. Must be >= 2. Default: 3\n\n\ncollect_sub_models\nWhether to collect a list of sub-models trained during tuning.\n\n\n\nIf set to FALSE, then only the single best sub-model will be available after fitting. If set to true, then all sub-models will be available. Warning: For large models, collecting all sub-models can cause OOMs on the Spark driver. parallelism | The number of threads to use when running parallel algorithms. Default is 1 for serial execution. seed | A random seed. Set this value if you need your results to be reproducible across repeated calls. uid | A character string used to uniquely identify the ML estimator. … | Optional arguments; currently unused. train_ratio | Ratio between train and validation data. Must be between 0 and 1. Default: 0.75"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#details",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nml_cross_validator() performs k-fold cross validation while ml_train_validation_split() performs tuning on one pair of train and validation datasets."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#value",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_cross_validator or ml_traing_validation_split object.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the tuning estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a tuning estimator is constructed then immediately fit with the input tbl_spark, returning a ml_cross_validation_model or a ml_train_validation_split_model object.\n\nFor cross validation, ml_sub_models() returns a nested list of models, where the first layer represents fold indices and the second layer represents param maps. For train-validation split, ml_sub_models() returns a list of models, corresponding to the order of the estimator param maps.\nml_validation_metrics() returns a data frame of performance metrics and hyperparameter combinations."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml-tuning.html#examples",
    "href": "packages/sparklyr/latest/reference/ml-tuning.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\n# Create a pipeline\npipeline <- ml_pipeline(sc) %>%\n  ft_r_formula(Species ~ .) %>%\n  ml_random_forest_classifier()\n\n# Specify hyperparameter grid\ngrid <- list(\n  random_forest = list(\n    num_trees = c(5, 10),\n    max_depth = c(5, 10),\n    impurity = c(\"entropy\", \"gini\")\n  )\n)\n\n# Create the cross validator object\ncv <- ml_cross_validator(\n  sc,\n  estimator = pipeline, estimator_param_maps = grid,\n  evaluator = ml_multiclass_classification_evaluator(sc),\n  num_folds = 3,\n  parallelism = 4\n)\n\n# Train the models\ncv_model <- ml_fit(cv, iris_tbl)\n\n# Print the metrics\nml_validation_metrics(cv_model)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_add_stage.html#description",
    "href": "packages/sparklyr/latest/reference/ml_add_stage.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nAdds a stage to a pipeline."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_add_stage.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_add_stage.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_add_stage(x, stage)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_add_stage.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_add_stage.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA pipeline or a pipeline stage.\n\n\nstage\nA pipeline stage."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#description",
    "href": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFit a parametric survival regression model named accelerated failure time (AFT) model (see https://en.wikipedia.org/wiki/Accelerated_failure_time_modelAccelerated failure time model (Wikipedia)) based on the Weibull distribution of the survival time."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_aft_survival_regression(\n  x,\n  formula = NULL,\n  censor_col = \"censor\",\n  quantile_probabilities = c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99),\n  fit_intercept = TRUE,\n  max_iter = 100L,\n  tol = 1e-06,\n  aggregation_depth = 2,\n  quantiles_col = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"aft_survival_regression_\"),\n  ...\n)\n\nml_survival_regression(\n  x,\n  formula = NULL,\n  censor_col = \"censor\",\n  quantile_probabilities = c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99),\n  fit_intercept = TRUE,\n  max_iter = 100L,\n  tol = 1e-06,\n  aggregation_depth = 2,\n  quantiles_col = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"aft_survival_regression_\"),\n  response = NULL,\n  features = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\ncensor_col\nCensor column name. The value of this column could be 0 or 1. If the value is 1, it means the event has occurred i.e. uncensored; otherwise censored.\n\n\nquantile_probabilities\nQuantile probabilities array. Values of the quantile probabilities array should be in the range (0, 1) and the array should be non-empty.\n\n\nfit_intercept\nBoolean; should the model be fit with an intercept term?\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\naggregation_depth\n(Spark 2.1.0+) Suggested depth for treeAggregate (>= 2).\n\n\nquantiles_col\nQuantiles column name. This column will output quantiles of corresponding quantileProbabilities if it is set.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details.\n\n\nresponse\n(Deprecated) The name of the response column (as a length-one character vector.)\n\n\nfeatures\n(Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#details",
    "href": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows.\nml_survival_regression() is an alias for ml_aft_survival_regression() for backwards compatibility."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#value",
    "href": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(survival)\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\novarian_tbl <- sdf_copy_to(sc, ovarian, name = \"ovarian_tbl\", overwrite = TRUE)\n\npartitions <- ovarian_tbl %>%\n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\novarian_training <- partitions$training\novarian_test <- partitions$test\n\nsur_reg <- ovarian_training %>%\n  ml_aft_survival_regression(futime ~ ecog_ps + rx + age + resid_ds, censor_col = \"fustat\")\n\npred <- ml_predict(sur_reg, ovarian_test)\npred"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_aft_survival_regression.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#description",
    "href": "packages/sparklyr/latest/reference/ml_als.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPerform recommendation using Alternating Least Squares (ALS) matrix factorization."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_als.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_als(\n  x,\n  formula = NULL,\n  rating_col = \"rating\",\n  user_col = \"user\",\n  item_col = \"item\",\n  rank = 10,\n  reg_param = 0.1,\n  implicit_prefs = FALSE,\n  alpha = 1,\n  nonnegative = FALSE,\n  max_iter = 10,\n  num_user_blocks = 10,\n  num_item_blocks = 10,\n  checkpoint_interval = 10,\n  cold_start_strategy = \"nan\",\n  intermediate_storage_level = \"MEMORY_AND_DISK\",\n  final_storage_level = \"MEMORY_AND_DISK\",\n  uid = random_string(\"als_\"),\n  ...\n)\n\nml_recommend(model, type = c(\"items\", \"users\"), n = 1)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_als.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula.\n\n\n\nThis is used to transform the input dataframe before fitting, see ft_r_formula for details. The ALS model requires a specific formula format, please use rating_col ~ user_col + item_col. rating_col | Column name for ratings. Default: “rating” user_col | Column name for user ids. Ids must be integers. Other numeric types are supported for this column, but will be cast to integers as long as they fall within the integer value range. Default: “user” item_col | Column name for item ids. Ids must be integers. Other numeric types are supported for this column, but will be cast to integers as long as they fall within the integer value range. Default: “item” rank | Rank of the matrix factorization (positive). Default: 10 reg_param | Regularization parameter. implicit_prefs | Whether to use implicit preference. Default: FALSE. alpha | Alpha parameter in the implicit preference formulation (nonnegative). nonnegative | Whether to apply nonnegativity constraints. Default: FALSE. max_iter | Maximum number of iterations. num_user_blocks | Number of user blocks (positive). Default: 10 num_item_blocks | Number of item blocks (positive). Default: 10 checkpoint_interval | Set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10. cold_start_strategy | (Spark 2.2.0+) Strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: - “nan”: predicted value for unknown ids will be NaN. - “drop”: rows in the input DataFrame containing unknown ids will be dropped from the output DataFrame containing predictions. Default: “nan”. intermediate_storage_level | (Spark 2.0.0+) StorageLevel for intermediate datasets. Pass in a string representation of StorageLevel. Cannot be “NONE”. Default: “MEMORY_AND_DISK”. final_storage_level | (Spark 2.0.0+) StorageLevel for ALS model factors. Pass in a string representation of StorageLevel. Default: “MEMORY_AND_DISK”. uid | A character string used to uniquely identify the ML estimator. … | Optional arguments; currently unused. model | An ALS model object type | What to recommend, one of items or users n | Maximum number of recommendations to return"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#details",
    "href": "packages/sparklyr/latest/reference/ml_als.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nml_recommend() returns the top n users/items recommended for each item/user, for all items/users. The output has been transformed (exploded and separated) from the default Spark outputs to be more user friendly."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#value",
    "href": "packages/sparklyr/latest/reference/ml_als.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nALS attempts to estimate the ratings matrix R as the product of two lower-rank matrices, X and Y, i.e. X * Yt = R. Typically these approximations are called ‘factor’ matrices. The general approach is iterative. During each iteration, one of the factor matrices is held constant, while the other is solved for using least squares. The newly-solved factor matrix is then held constant while solving for the other factor matrix.\nThis is a blocked implementation of the ALS factorization algorithm that groups the two sets of factors (referred to as “users” and “products”) into blocks and reduces communication by only sending one copy of each user vector to each product block on each iteration, and only for the product blocks that need that user’s feature vector. This is achieved by pre-computing some information about the ratings matrix to determine the “out-links” of each user (which blocks of products it will contribute to) and “in-link” information for each product (which of the feature vectors it receives from each user block it will depend on). This allows us to send only an array of feature vectors between each user block and product block, and have the product block find the users’ ratings and update the products based on these messages.\nFor implicit preference data, the algorithm used is based on “Collaborative Filtering for Implicit Feedback Datasets”, available at 10.1109/ICDM.2008.22tools:::Rd_expr_doi(\"10.1109/ICDM.2008.22\"), adapted for the blocked approach used here.\nEssentially instead of finding the low-rank approximations to the rating matrix R, this finds the approximations for a preference matrix P where the elements of P are 1 if r is greater than 0 and 0 if r is less than or equal to 0. The ratings then act as ‘confidence’ values related to strength of indicated user preferences rather than explicit ratings given to items.\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_als recommender object, which is an Estimator.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the recommender appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a recommender estimator is constructed then immediately fit with the input tbl_spark, returning a recommendation model, i.e. ml_als_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_als.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\n\nmovies <- data.frame(\n  user   = c(1, 2, 0, 1, 2, 0),\n  item   = c(1, 1, 1, 2, 2, 0),\n  rating = c(3, 1, 2, 4, 5, 4)\n)\nmovies_tbl <- sdf_copy_to(sc, movies)\n\nmodel <- ml_als(movies_tbl, rating ~ user + item)\n\nml_predict(model, movies_tbl)\n\nml_recommend(model, type = \"item\", 1)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_als_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_als_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ntidyml_model_als(x, …)\naugmentml_model_als(x, newdata = NULL, …)\nglanceml_model_als(x, …)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_als_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_als_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#description",
    "href": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nA bisecting k-means algorithm based on the paper “A comparison of document clustering techniques” by Steinbach, Karypis, and Kumar, with modification to fit Spark. The algorithm starts from a single cluster that contains all points. Iteratively it finds divisible clusters on the bottom level and bisects each of them using k-means, until there are k leaf clusters in total or no leaf clusters are divisible. The bisecting steps of clusters on the same level are grouped together to increase parallelism. If bisecting all divisible clusters on the bottom level would result more than k leaf clusters, larger clusters get higher priority."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_bisecting_kmeans(\n  x,\n  formula = NULL,\n  k = 4,\n  max_iter = 20,\n  seed = NULL,\n  min_divisible_cluster_size = 1,\n  features_col = \"features\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"bisecting_bisecting_kmeans_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nk\nThe number of clusters to create\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nseed\nA random seed. Set this value if you need your results to be\n\n\n\nreproducible across repeated calls. min_divisible_cluster_size | The minimum number of points (if greater than or equal to 1.0) or the minimum proportion of points (if less than 1.0) of a divisible cluster (default: 1.0). features_col | Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula. prediction_col | Prediction column name. uid | A character string used to uniquely identify the ML estimator. … | Optional arguments, see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#value",
    "href": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the clustering estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, an estimator is constructed then immediately fit with the input tbl_spark, returning a clustering model.\ntbl_spark, with formula or features specified: When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the estimator. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model. This signature does not apply to ml_lda()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\niris_tbl %>%\n  select(-Species) %>%\n  ml_bisecting_kmeans(k = 4, Species ~ .)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_bisecting_kmeans.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-clustering.html for more information on the set of clustering algorithms.\nOther ml clustering algorithms: ml_gaussian_mixture(), ml_kmeans(), ml_lda()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_call_constructor.html#description",
    "href": "packages/sparklyr/latest/reference/ml_call_constructor.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nIdentifies the associated sparklyr ML constructor for the JVM object by inspecting its class and performing a lookup. The lookup table is specified by the sparkml/class_mapping.json files of sparklyr and the loaded extensions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_call_constructor.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_call_constructor.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_call_constructor(jobj)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_call_constructor.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_call_constructor.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\njobj\nThe jobj for the pipeline stage."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_chisquare_test.html#description",
    "href": "packages/sparklyr/latest/reference/ml_chisquare_test.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nConduct Pearson’s independence test for every feature against the label. For each feature, the (feature, label) pairs are converted into a contingency matrix for which the Chi-squared statistic is computed. All label and feature values must be categorical."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_chisquare_test.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_chisquare_test.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_chisquare_test(x, features, label)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_chisquare_test.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_chisquare_test.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA tbl_spark.\n\n\nfeatures\nThe name(s) of the feature columns. This can also be the name\n\n\n\nof a single vector column created using ft_vector_assembler(). label | The name of the label column."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_chisquare_test.html#value",
    "href": "packages/sparklyr/latest/reference/ml_chisquare_test.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA data frame with one row for each (feature, label) pair with p-values, degrees of freedom, and test statistics."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_chisquare_test.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_chisquare_test.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\nfeatures <- c(\"Petal_Width\", \"Petal_Length\", \"Sepal_Length\", \"Sepal_Width\")\n\nml_chisquare_test(iris_tbl, features = features, label = \"Species\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#description",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nEvaluator for clustering results. The metric computes the Silhouette measure using the squared Euclidean distance. The Silhouette is a measure for the validation of the consistency within clusters. It ranges between 1 and -1, where a value close to 1 means that the points in a cluster are close to the other points in the same cluster and far from the points of the other clusters."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_clustering_evaluator(\n  x,\n  features_col = \"features\",\n  prediction_col = \"prediction\",\n  metric_name = \"silhouette\",\n  uid = random_string(\"clustering_evaluator_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection object or a tbl_spark containing label and prediction columns. The latter should be the output of sdf_predict.\n\n\nfeatures_col\nName of features column.\n\n\nprediction_col\nName of the prediction column.\n\n\nmetric_name\nThe performance metric. Currently supports “silhouette”.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#value",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe calculated performance metric"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_clustering_evaluator.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\npartitions <- iris_tbl %>%\n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\niris_training <- partitions$training\niris_test <- partitions$test\n\nformula <- Species ~ .\n\n# Train the models\nkmeans_model <- ml_kmeans(iris_training, formula = formula)\nb_kmeans_model <- ml_bisecting_kmeans(iris_training, formula = formula)\ngmm_model <- ml_gaussian_mixture(iris_training, formula = formula)\n\n# Predict\npred_kmeans <- ml_predict(kmeans_model, iris_test)\npred_b_kmeans <- ml_predict(b_kmeans_model, iris_test)\npred_gmm <- ml_predict(gmm_model, iris_test)\n\n# Evaluate\nml_clustering_evaluator(pred_kmeans)\nml_clustering_evaluator(pred_b_kmeans)\nml_clustering_evaluator(pred_gmm)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html#description",
    "href": "packages/sparklyr/latest/reference/ml_corr.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCompute correlation matrix"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_corr.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_corr(x, columns = NULL, method = c(\"pearson\", \"spearman\"))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_corr.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA tbl_spark.\n\n\ncolumns\nThe names of the columns to calculate correlations of. If only one\n\n\n\ncolumn is specified, it must be a vector column (for example, assembled using ft_vector_assember()). method | The method to use, either \"pearson\" or \"spearman\"."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html#value",
    "href": "packages/sparklyr/latest/reference/ml_corr.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA correlation matrix organized as a data frame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_corr.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_corr.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\nfeatures <- c(\"Petal_Width\", \"Petal_Length\", \"Sepal_Length\", \"Sepal_Width\")\n\nml_corr(iris_tbl, columns = features, method = \"pearson\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#description",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPerform classification and regression using decision trees."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_decision_tree_classifier(\n  x,\n  formula = NULL,\n  max_depth = 5,\n  max_bins = 32,\n  min_instances_per_node = 1,\n  min_info_gain = 0,\n  impurity = \"gini\",\n  seed = NULL,\n  thresholds = NULL,\n  cache_node_ids = FALSE,\n  checkpoint_interval = 10,\n  max_memory_in_mb = 256,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"decision_tree_classifier_\"),\n  ...\n)\n\nml_decision_tree(\n  x,\n  formula = NULL,\n  type = c(\"auto\", \"regression\", \"classification\"),\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  variance_col = NULL,\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  checkpoint_interval = 10L,\n  impurity = \"auto\",\n  max_bins = 32L,\n  max_depth = 5L,\n  min_info_gain = 0,\n  min_instances_per_node = 1L,\n  seed = NULL,\n  thresholds = NULL,\n  cache_node_ids = FALSE,\n  max_memory_in_mb = 256L,\n  uid = random_string(\"decision_tree_\"),\n  response = NULL,\n  features = NULL,\n  ...\n)\n\nml_decision_tree_regressor(\n  x,\n  formula = NULL,\n  max_depth = 5,\n  max_bins = 32,\n  min_instances_per_node = 1,\n  min_info_gain = 0,\n  impurity = \"variance\",\n  seed = NULL,\n  cache_node_ids = FALSE,\n  checkpoint_interval = 10,\n  max_memory_in_mb = 256,\n  variance_col = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"decision_tree_regressor_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nmax_depth\nMaximum depth of the tree (>= 0); that is, the maximum\n\n\n\nnumber of nodes separating any leaves from the root of the tree. max_bins | The maximum number of bins used for discretizing continuous features and for choosing how to split on features at each node. More bins give higher granularity. min_instances_per_node | Minimum number of instances each child must have after split. min_info_gain | Minimum information gain for a split to be considered at a tree node. Should be >= 0, defaults to 0. impurity | Criterion used for information gain calculation. Supported: “entropy” and “gini” (default) for classification and “variance” (default) for regression. For ml_decision_tree, setting \"auto\" will default to the appropriate criterion based on model type. seed | Seed for random numbers. thresholds | Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold. cache_node_ids | If FALSE, the algorithm will pass trees to executors to match instances with nodes. If TRUE, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Defaults to FALSE. checkpoint_interval | Set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10. max_memory_in_mb | Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. Defaults to 256. features_col | Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula. label_col | Label column name. The column should be a numeric column. Usually this column is output by ft_r_formula. prediction_col | Prediction column name. probability_col | Column name for predicted class conditional probabilities. raw_prediction_col | Raw prediction (a.k.a. confidence) column name. uid | A character string used to uniquely identify the ML estimator. … | Optional arguments; see Details. type | The type of model to fit. \"regression\" treats the response as a continuous variable, while \"classification\" treats the response as a categorical variable. When \"auto\" is used, the model type is inferred based on the response variable type – if it is a numeric type, then regression is used; classification otherwise. variance_col | (Optional) Column name for the biased sample variance of prediction. response | (Deprecated) The name of the response column (as a length-one character vector.) features | (Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#details",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows.\nml_decision_tree is a wrapper around ml_decision_tree_regressor.tbl_spark and ml_decision_tree_classifier.tbl_spark and calls the appropriate method based on model type."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#value",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\npartitions <- iris_tbl %>%\n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\niris_training <- partitions$training\niris_test <- partitions$test\n\ndt_model <- iris_training %>%\n  ml_decision_tree(Species ~ .)\n\npred <- ml_predict(dt_model, iris_test)\n\nml_multiclass_classification_evaluator(pred)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_decision_tree.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_decision_tree.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#description",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nLoads the default stop words for the given language."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_default_stop_words(\n  sc,\n  language = c(\"english\", \"danish\", \"dutch\", \"finnish\", \"french\", \"german\",\n    \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"russian\", \"spanish\", \"swedish\",\n    \"turkish\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection\n\n\nlanguage\nA character string.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#details",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nSupported languages: danish, dutch, english, finnish, french, german, hungarian, italian, norwegian, portuguese, russian, spanish, swedish, turkish. Defaults to English. See https://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/ for more details"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#value",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA list of stop words."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_default_stop_words.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_default_stop_words.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nft_stop_words_remover"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluate.html#description",
    "href": "packages/sparklyr/latest/reference/ml_evaluate.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCompute performance metrics."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluate.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_evaluate.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_evaluate(x, dataset)\nml_evaluateml_model_logistic_regression(x, dataset)\nml_evaluateml_logistic_regression_model(x, dataset)\nml_evaluateml_model_linear_regression(x, dataset)\nml_evaluateml_linear_regression_model(x, dataset)\nml_evaluateml_model_generalized_linear_regression(x, dataset)\nml_evaluateml_generalized_linear_regression_model(x, dataset)\nml_evaluateml_model_clustering(x, dataset)\nml_evaluateml_model_classification(x, dataset)\nml_evaluateml_evaluator(x, dataset)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluate.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_evaluate.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn ML model object or an evaluator object.\n\n\ndataset\nThe dataset to be validate the model on."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluate.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_evaluate.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\nml_gaussian_mixture(iris_tbl, Species ~ .) %>%\n  ml_evaluate(iris_tbl)\n\nml_kmeans(iris_tbl, Species ~ .) %>%\n  ml_evaluate(iris_tbl)\n\nml_bisecting_kmeans(iris_tbl, Species ~ .) %>%\n  ml_evaluate(iris_tbl)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#description",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nA set of functions to calculate performance metrics for prediction models. Also see the Spark ML Documentation https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.evaluation.packagehttps://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.evaluation.package"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_binary_classification_evaluator(\n  x,\n  label_col = \"label\",\n  raw_prediction_col = \"rawPrediction\",\n  metric_name = \"areaUnderROC\",\n  uid = random_string(\"binary_classification_evaluator_\"),\n  ...\n)\n\nml_binary_classification_eval(\n  x,\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  metric_name = \"areaUnderROC\"\n)\n\nml_multiclass_classification_evaluator(\n  x,\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  metric_name = \"f1\",\n  uid = random_string(\"multiclass_classification_evaluator_\"),\n  ...\n)\n\nml_classification_eval(\n  x,\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  metric_name = \"f1\"\n)\n\nml_regression_evaluator(\n  x,\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  metric_name = \"rmse\",\n  uid = random_string(\"regression_evaluator_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection object or a tbl_spark containing label and prediction columns. The latter should be the output of sdf_predict.\n\n\nlabel_col\nName of column string specifying which column contains the true labels or values.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nmetric_name\nThe performance metric. See details.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; currently unused.\n\n\nprediction_col\nName of the column that contains the predicted\n\n\n\nlabel or value NOT the scored probability. Column should be of type Double."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#details",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nThe following metrics are supported\n\nBinary Classification: areaUnderROC (default) or areaUnderPR (not available in Spark 2.X.)\nMulticlass Classification: f1 (default), precision, recall, weightedPrecision, weightedRecall or accuracy; for Spark 2.X: f1 (default), weightedPrecision, weightedRecall or accuracy.\nRegression: rmse (root mean squared error, default), mse (mean squared error), r2, or mae (mean absolute error.)\n\nml_binary_classification_eval() is an alias for ml_binary_classification_evaluator() for backwards compatibility.\nml_classification_eval() is an alias for ml_multiclass_classification_evaluator() for backwards compatibility."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#value",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe calculated performance metric"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_evaluator.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_evaluator.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, name = \"mtcars_tbl\", overwrite = TRUE)\n\npartitions <- mtcars_tbl %>%\n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\nmtcars_training <- partitions$training\nmtcars_test <- partitions$test\n\n# for multiclass classification\nrf_model <- mtcars_training %>%\n  ml_random_forest(cyl ~ ., type = \"classification\")\n\npred <- ml_predict(rf_model, mtcars_test)\n\nml_multiclass_classification_evaluator(pred)\n\n# for regression\nrf_model <- mtcars_training %>%\n  ml_random_forest(cyl ~ ., type = \"regression\")\n\npred <- ml_predict(rf_model, mtcars_test)\n\nml_regression_evaluator(pred, label_col = \"cyl\")\n\n# for binary classification\nrf_model <- mtcars_training %>%\n  ml_random_forest(am ~ gear + carb, type = \"classification\")\n\npred <- ml_predict(rf_model, mtcars_test)\n\nml_binary_classification_evaluator(pred)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_feature_importances.html#description",
    "href": "packages/sparklyr/latest/reference/ml_feature_importances.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSpark ML - Feature Importance for Tree Models"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_feature_importances.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_feature_importances.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_feature_importances(model, ...)\n\nml_tree_feature_importance(model, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_feature_importances.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_feature_importances.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nmodel\nA decision tree-based model.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_feature_importances.html#value",
    "href": "packages/sparklyr/latest/reference/ml_feature_importances.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nFor ml_model, a sorted data frame with feature labels and their relative importance. For ml_prediction_model, a vector of relative importances."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_fpgrowth.html#description",
    "href": "packages/sparklyr/latest/reference/ml_fpgrowth.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nA parallel FP-growth algorithm to mine frequent itemsets."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_fpgrowth.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_fpgrowth.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_fpgrowth(\n  x,\n  items_col = \"items\",\n  min_confidence = 0.8,\n  min_support = 0.3,\n  prediction_col = \"prediction\",\n  uid = random_string(\"fpgrowth_\"),\n  ...\n)\n\nml_association_rules(model)\n\nml_freq_itemsets(model)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_fpgrowth.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_fpgrowth.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nitems_col\nItems column name. Default: “items”\n\n\nmin_confidence\nMinimal confidence for generating Association Rule.\n\n\n\nmin_confidence will not affect the mining for frequent itemsets, but will affect the association rules generation. Default: 0.8 min_support | Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears more than (min_support * size-of-the-dataset) times will be output in the frequent itemsets. Default: 0.3 prediction_col | Prediction column name. uid | A character string used to uniquely identify the ML estimator. … | Optional arguments; currently unused. model | A fitted FPGrowth model returned by ml_fpgrowth()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#description",
    "href": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis class performs expectation maximization for multivariate Gaussian Mixture Models (GMMs). A GMM represents a composite distribution of independent Gaussian distributions with associated “mixing” weights specifying each’s contribution to the composite. Given a set of sample points, this class will maximize the log-likelihood for a mixture of k Gaussians, iterating until the log-likelihood changes by less than tol, or until it has reached the max number of iterations. While this process is generally guaranteed to converge, it is not guaranteed to find a global optimum."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_gaussian_mixture(\n  x,\n  formula = NULL,\n  k = 2,\n  max_iter = 100,\n  tol = 0.01,\n  seed = NULL,\n  features_col = \"features\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  uid = random_string(\"gaussian_mixture_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nk\nThe number of clusters to create\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nseed\nA random seed. Set this value if you need your results to be\n\n\n\nreproducible across repeated calls. features_col | Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula. prediction_col | Prediction column name. probability_col | Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. uid | A character string used to uniquely identify the ML estimator. … | Optional arguments, see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#value",
    "href": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the clustering estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, an estimator is constructed then immediately fit with the input tbl_spark, returning a clustering model.\ntbl_spark, with formula or features specified: When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the estimator. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model. This signature does not apply to ml_lda()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\ngmm_model <- ml_gaussian_mixture(iris_tbl, Species ~ .)\npred <- sdf_predict(iris_tbl, gmm_model)\nml_clustering_evaluator(pred)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_gaussian_mixture.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-clustering.html for more information on the set of clustering algorithms.\nOther ml clustering algorithms: ml_bisecting_kmeans(), ml_kmeans(), ml_lda()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#description",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPerform regression using Generalized Linear Model (GLM)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_generalized_linear_regression(\n  x,\n  formula = NULL,\n  family = \"gaussian\",\n  link = NULL,\n  fit_intercept = TRUE,\n  offset_col = NULL,\n  link_power = NULL,\n  link_prediction_col = NULL,\n  reg_param = 0,\n  max_iter = 25,\n  weight_col = NULL,\n  solver = \"irls\",\n  tol = 1e-06,\n  variance_power = 0,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"generalized_linear_regression_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nfamily\nName of family which is a description of the error distribution to be used in the model. Supported options: “gaussian”, “binomial”, “poisson”, “gamma” and “tweedie”. Default is “gaussian”.\n\n\nlink\nName of link function which provides the relationship between the linear predictor and the mean of the distribution function. See for supported link functions.\n\n\nfit_intercept\nBoolean; should the model be fit with an intercept term?\n\n\noffset_col\nOffset column name. If this is not set, we treat all instance offsets as 0.0. The feature specified as offset has a constant coefficient of 1.0.\n\n\nlink_power\nIndex in the power link function. Only applicable to the Tweedie family. Note that link power 0, 1, -1 or 0.5 corresponds to the Log, Identity, Inverse or Sqrt link, respectively. When not set, this value defaults to 1 - variancePower, which matches the R “statmod” package.\n\n\nlink_prediction_col\nLink prediction (linear predictor) column name. Default is not set, which means we do not output link prediction.\n\n\nreg_param\nRegularization parameter (aka lambda)\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nweight_col\nThe name of the column to use as weights for the model fit.\n\n\nsolver\nSolver algorithm for optimization.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nvariance_power\nPower in the variance function of the Tweedie distribution which provides the relationship between the variance and mean of the distribution. Only applicable to the Tweedie family. (see https://en.wikipedia.org/wiki/Tweedie_distributionTweedie Distribution (Wikipedia)) Supported values: 0 and [1, Inf). Note that variance power 0, 1, or 2 corresponds to the Gaussian, Poisson or Gamma family, respectively.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#details",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows.\nValid link functions for each family is listed below. The first link function of each family is the default one.\n\ngaussian: “identity”, “log”, “inverse”\nbinomial: “logit”, “probit”, “loglog”\npoisson: “log”, “identity”, “sqrt”\ngamma: “inverse”, “identity”, “log”\ntweedie: power link function specified through link_power. The default link power in the tweedie family is 1 - variance_power."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#value",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, name = \"mtcars_tbl\", overwrite = TRUE)\n\npartitions <- mtcars_tbl %>%\n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\nmtcars_training <- partitions$training\nmtcars_test <- partitions$test\n\n# Specify the grid\nfamily <- c(\"gaussian\", \"gamma\", \"poisson\")\nlink <- c(\"identity\", \"log\")\nfamily_link <- expand.grid(family = family, link = link, stringsAsFactors = FALSE)\nfamily_link <- data.frame(family_link, rmse = 0)\n\n# Train the models\nfor (i in seq_len(nrow(family_link))) {\n  glm_model <- mtcars_training %>%\n    ml_generalized_linear_regression(mpg ~ .,\n      family = family_link[i, 1],\n      link = family_link[i, 2]\n    )\n\n  pred <- ml_predict(glm_model, mtcars_test)\n  family_link[i, 3] <- ml_regression_evaluator(pred, label_col = \"mpg\")\n}\n\nfamily_link"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_generalized_linear_regression.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ntidyml_model_generalized_linear_regression(x, exponentiate = FALSE, …)\ntidyml_model_linear_regression(x, …)\naugmentml_model_generalized_linear_regression( x, newdata = NULL, type.residuals = c(“working”, “deviance”, “pearson”, “response”), … )\naugmentml_model_linear_regression( x, newdata = NULL, type.residuals = c(“working”, “deviance”, “pearson”, “response”), … )\nglanceml_model_generalized_linear_regression(x, …)\nglanceml_model_linear_regression(x, …)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\nexponentiate\nFor GLM, whether to exponentiate the coefficient estimates (typical for logistic regression.)\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction.\n\n\ntype.residuals\ntype of residuals, defaults to \"working\". Must be set to\n\n\n\n\"working\" when newdata is supplied."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#details",
    "href": "packages/sparklyr/latest/reference/ml_glm_tidiers.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nThe residuals attached by augment are of type “working” by default, which is different from the default of “deviance” for residuals() or sdf_residuals()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#description",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPerform binary classification and regression using gradient boosted trees. Multiclass classification is not supported yet."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_gbt_classifier(\n  x,\n  formula = NULL,\n  max_iter = 20,\n  max_depth = 5,\n  step_size = 0.1,\n  subsampling_rate = 1,\n  feature_subset_strategy = \"auto\",\n  min_instances_per_node = 1L,\n  max_bins = 32,\n  min_info_gain = 0,\n  loss_type = \"logistic\",\n  seed = NULL,\n  thresholds = NULL,\n  checkpoint_interval = 10,\n  cache_node_ids = FALSE,\n  max_memory_in_mb = 256,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"gbt_classifier_\"),\n  ...\n)\n\nml_gradient_boosted_trees(\n  x,\n  formula = NULL,\n  type = c(\"auto\", \"regression\", \"classification\"),\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  checkpoint_interval = 10,\n  loss_type = c(\"auto\", \"logistic\", \"squared\", \"absolute\"),\n  max_bins = 32,\n  max_depth = 5,\n  max_iter = 20L,\n  min_info_gain = 0,\n  min_instances_per_node = 1,\n  step_size = 0.1,\n  subsampling_rate = 1,\n  feature_subset_strategy = \"auto\",\n  seed = NULL,\n  thresholds = NULL,\n  cache_node_ids = FALSE,\n  max_memory_in_mb = 256,\n  uid = random_string(\"gradient_boosted_trees_\"),\n  response = NULL,\n  features = NULL,\n  ...\n)\n\nml_gbt_regressor(\n  x,\n  formula = NULL,\n  max_iter = 20,\n  max_depth = 5,\n  step_size = 0.1,\n  subsampling_rate = 1,\n  feature_subset_strategy = \"auto\",\n  min_instances_per_node = 1,\n  max_bins = 32,\n  min_info_gain = 0,\n  loss_type = \"squared\",\n  seed = NULL,\n  checkpoint_interval = 10,\n  cache_node_ids = FALSE,\n  max_memory_in_mb = 256,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"gbt_regressor_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nmax_iter\nMaxmimum number of iterations.\n\n\nmax_depth\nMaximum depth of the tree (>= 0); that is, the maximum\n\n\n\nnumber of nodes separating any leaves from the root of the tree. step_size | Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default = 0.1) subsampling_rate | Fraction of the training data used for learning each decision tree, in range (0, 1]. (default = 1.0) feature_subset_strategy | The number of features to consider for splits at each tree node. See details for options. min_instances_per_node | Minimum number of instances each child must have after split. max_bins | The maximum number of bins used for discretizing continuous features and for choosing how to split on features at each node. More bins give higher granularity. min_info_gain | Minimum information gain for a split to be considered at a tree node. Should be >= 0, defaults to 0. loss_type | Loss function which GBT tries to minimize. Supported: \"squared\" (L2) and \"absolute\" (L1) (default = squared) for regression and \"logistic\" (default) for classification. For ml_gradient_boosted_trees, setting \"auto\" will default to the appropriate loss type based on model type. seed | Seed for random numbers. thresholds | Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold. checkpoint_interval | Set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10. cache_node_ids | If FALSE, the algorithm will pass trees to executors to match instances with nodes. If TRUE, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Defaults to FALSE. max_memory_in_mb | Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. Defaults to 256. features_col | Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula. label_col | Label column name. The column should be a numeric column. Usually this column is output by ft_r_formula. prediction_col | Prediction column name. probability_col | Column name for predicted class conditional probabilities. raw_prediction_col | Raw prediction (a.k.a. confidence) column name. uid | A character string used to uniquely identify the ML estimator. … | Optional arguments; see Details. type | The type of model to fit. \"regression\" treats the response as a continuous variable, while \"classification\" treats the response as a categorical variable. When \"auto\" is used, the model type is inferred based on the response variable type – if it is a numeric type, then regression is used; classification otherwise. response | (Deprecated) The name of the response column (as a length-one character vector.) features | (Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#details",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows.\nThe supported options for feature_subset_strategy are\n\n\"auto\": Choose automatically for task: If num_trees == 1, set to \"all\". If num_trees > 1 (forest), set to \"sqrt\" for classification and to \"onethird\" for regression.\n\"all\": use all features\n\"onethird\": use 1/3 of the features\n\"sqrt\": use use sqrt(number of features)\n\"log2\": use log2(number of features)\n\"n\": when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features. (default = \"auto\")\n\nml_gradient_boosted_trees is a wrapper around ml_gbt_regressor.tbl_spark and ml_gbt_classifier.tbl_spark and calls the appropriate method based on model type."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#value",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\npartitions <- iris_tbl %>%\n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\niris_training <- partitions$training\niris_test <- partitions$test\n\ngbt_model <- iris_training %>%\n  ml_gradient_boosted_trees(Sepal_Length ~ Petal_Length + Petal_Width)\n\npred <- ml_predict(gbt_model, iris_test)\n\nml_regression_evaluator(pred, label_col = \"Sepal_Length\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_gradient_boosted_trees.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#description",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCurrently implemented using parallelized pool adjacent violators algorithm. Only univariate (single feature) algorithm supported."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_isotonic_regression(\n  x,\n  formula = NULL,\n  feature_index = 0,\n  isotonic = TRUE,\n  weight_col = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"isotonic_regression_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nfeature_index\nIndex of the feature if features_col is a vector column (default: 0), no effect otherwise.\n\n\nisotonic\nWhether the output sequence should be isotonic/increasing (true) or antitonic/decreasing (false). Default: true\n\n\nweight_col\nThe name of the column to use as weights for the model fit.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#details",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#value",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\npartitions <- iris_tbl %>%\n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\niris_training <- partitions$training\niris_test <- partitions$test\n\niso_res <- iris_tbl %>%\n  ml_isotonic_regression(Petal_Length ~ Petal_Width)\n\npred <- ml_predict(iso_res, iris_test)\n\npred"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ntidyml_model_isotonic_regression(x, …)\naugmentml_model_isotonic_regression(x, newdata = NULL, …)\nglanceml_model_isotonic_regression(x, …)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_isotonic_regression_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_isotonic_regression_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans.html#description",
    "href": "packages/sparklyr/latest/reference/ml_kmeans.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nK-means clustering with support for k-means|| initialization proposed by Bahmani et al. Using ml_kmeans() with the formula interface requires Spark 2.0+."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_kmeans.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_kmeans(\n  x,\n  formula = NULL,\n  k = 2,\n  max_iter = 20,\n  tol = 1e-04,\n  init_steps = 2,\n  init_mode = \"k-means||\",\n  seed = NULL,\n  features_col = \"features\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"kmeans_\"),\n  ...\n)\n\nml_compute_cost(model, dataset)\n\nml_compute_silhouette_measure(\n  model,\n  dataset,\n  distance_measure = c(\"squaredEuclidean\", \"cosine\")\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_kmeans.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nk\nThe number of clusters to create\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\ninit_steps\nNumber of steps for the k-means\n\n\ninit_mode\nInitialization algorithm. This can be either “random” to choose random points as initial cluster centers, or “k-means\n\n\nseed\nA random seed. Set this value if you need your results to be\n\n\n\nreproducible across repeated calls. features_col | Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula. prediction_col | Prediction column name. uid | A character string used to uniquely identify the ML estimator. … | Optional arguments, see Details. model | A fitted K-means model returned by ml_kmeans() dataset | Dataset on which to calculate K-means cost distance_measure | Distance measure to apply when computing the Silhouette measure."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans.html#value",
    "href": "packages/sparklyr/latest/reference/ml_kmeans.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the clustering estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, an estimator is constructed then immediately fit with the input tbl_spark, returning a clustering model.\ntbl_spark, with formula or features specified: When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the estimator. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model. This signature does not apply to ml_lda().\n\nml_compute_cost() returns the K-means cost (sum of squared distances of points to their nearest center) for the model on the given data.\nml_compute_silhouette_measure() returns the Silhouette measure of the clustering on the given data."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_kmeans.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\nml_kmeans(iris_tbl, Species ~ .)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_kmeans.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-clustering.html for more information on the set of clustering algorithms.\nOther ml clustering algorithms: ml_bisecting_kmeans(), ml_gaussian_mixture(), ml_lda()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans_cluster_eval.html#description",
    "href": "packages/sparklyr/latest/reference/ml_kmeans_cluster_eval.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nEvaluate a K-mean clustering"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_kmeans_cluster_eval.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_kmeans_cluster_eval.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nmodel\nA fitted K-means model returned by ml_kmeans()\n\n\ndataset\nDataset on which to calculate K-means cost"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#description",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nLatent Dirichlet Allocation (LDA), a topic model designed for text documents."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_lda(\n  x,\n  formula = NULL,\n  k = 10,\n  max_iter = 20,\n  doc_concentration = NULL,\n  topic_concentration = NULL,\n  subsampling_rate = 0.05,\n  optimizer = \"online\",\n  checkpoint_interval = 10,\n  keep_last_checkpoint = TRUE,\n  learning_decay = 0.51,\n  learning_offset = 1024,\n  optimize_doc_concentration = TRUE,\n  seed = NULL,\n  features_col = \"features\",\n  topic_distribution_col = \"topicDistribution\",\n  uid = random_string(\"lda_\"),\n  ...\n)\n\nml_describe_topics(model, max_terms_per_topic = 10)\n\nml_log_likelihood(model, dataset)\n\nml_log_perplexity(model, dataset)\n\nml_topics_matrix(model)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nk\nThe number of clusters to create\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\ndoc_concentration\nConcentration parameter (commonly named “alpha”) for the prior placed on documents’ distributions over topics (“theta”). See details.\n\n\ntopic_concentration\nConcentration parameter (commonly named “beta” or “eta”) for the prior placed on topics’ distributions over terms.\n\n\nsubsampling_rate\n(For Online optimizer only) Fraction of the corpus to be sampled and used in each iteration of mini-batch gradient descent, in range (0, 1]. Note that this should be adjusted in synch with max_iter so the entire corpus is used. Specifically, set both so that maxIterations * miniBatchFraction greater than or equal to 1.\n\n\noptimizer\nOptimizer or inference algorithm used to estimate the LDA model. Supported: “online” for Online Variational Bayes (default) and “em” for Expectation-Maximization.\n\n\ncheckpoint_interval\nSet checkpoint interval (>= 1) or disable checkpoint (-1).\n\n\n\nE.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10. keep_last_checkpoint | (Spark 2.0.0+) (For EM optimizer only) If using checkpointing, this indicates whether to keep the last checkpoint. If FALSE, then the checkpoint will be deleted. Deleting the checkpoint can cause failures if a data partition is lost, so set this bit with care. Note that checkpoints will be cleaned up via reference counting, regardless. learning_decay | (For Online optimizer only) Learning rate, set as an exponential decay rate. This should be between (0.5, 1.0] to guarantee asymptotic convergence. This is called “kappa” in the Online LDA paper (Hoffman et al., 2010). Default: 0.51, based on Hoffman et al. learning_offset | (For Online optimizer only) A (positive) learning parameter that downweights early iterations. Larger values make early iterations count less. This is called “tau0” in the Online LDA paper (Hoffman et al., 2010) Default: 1024, following Hoffman et al. optimize_doc_concentration | (For Online optimizer only) Indicates whether the doc_concentration (Dirichlet parameter for document-topic distribution) will be optimized during training. Setting this to true will make the model more expressive and fit the training data better. Default: FALSE seed | A random seed. Set this value if you need your results to be reproducible across repeated calls. features_col | Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula. topic_distribution_col | Output column with estimates of the topic mixture distribution for each document (often called “theta” in the literature). Returns a vector of zeros for an empty document. uid | A character string used to uniquely identify the ML estimator. … | Optional arguments, see Details. model | A fitted LDA model returned by ml_lda(). max_terms_per_topic | Maximum number of terms to collect for each topic. Default value of 10. dataset | test corpus to use for calculating log likelihood or log perplexity"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#details",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nFor ml_lda.tbl_spark with the formula interface, you can specify named arguments in ... that will be passed ft_regex_tokenizer(), ft_stop_words_remover(), and ft_count_vectorizer(). For example, to increase the default min_token_length, you can use ml_lda(dataset, ~ text, min_token_length = 4).\nTerminology for LDA:\n\n“term” = “word”: an element of the vocabulary\n“token”: instance of a term appearing in a document\n“topic”: multinomial distribution over terms representing some concept\n“document”: one piece of text, corresponding to one row in the input data\n\nOriginal LDA paper (journal version): Blei, Ng, and Jordan. “Latent Dirichlet Allocation.” JMLR, 2003.\nInput data (features_col): LDA is given a collection of documents as input data, via the features_col parameter. Each document is specified as a Vector of length vocab_size, where each entry is the count for the corresponding term (word) in the document. Feature transformers such as ft_tokenizer and ft_count_vectorizer can be useful for converting text to word count vectors"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#value",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the clustering estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, an estimator is constructed then immediately fit with the input tbl_spark, returning a clustering model.\ntbl_spark, with formula or features specified: When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the estimator. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model. This signature does not apply to ml_lda().\n\nml_describe_topics returns a DataFrame with topics and their top-weighted terms.\nml_log_likelihood calculates a lower bound on the log likelihood of the entire corpus"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(janeaustenr)\nlibrary(dplyr)\nsc <- spark_connect(master = \"local\")\n\nlines_tbl <- sdf_copy_to(sc,\n  austen_books()[c(1:30), ],\n  name = \"lines_tbl\",\n  overwrite = TRUE\n)\n\n# transform the data in a tidy form\nlines_tbl_tidy <- lines_tbl %>%\n  ft_tokenizer(\n    input_col = \"text\",\n    output_col = \"word_list\"\n  ) %>%\n  ft_stop_words_remover(\n    input_col = \"word_list\",\n    output_col = \"wo_stop_words\"\n  ) %>%\n  mutate(text = explode(wo_stop_words)) %>%\n  filter(text != \"\") %>%\n  select(text, book)\n\nlda_model <- lines_tbl_tidy %>%\n  ml_lda(~text, k = 4)\n\n# vocabulary and topics\ntidy(lda_model)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_lda.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-clustering.html for more information on the set of clustering algorithms.\nOther ml clustering algorithms: ml_bisecting_kmeans(), ml_gaussian_mixture(), ml_kmeans()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ntidyml_model_lda(x, …)\naugmentml_model_lda(x, newdata = NULL, …)\nglanceml_model_lda(x, …)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_lda_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_regression.html#description",
    "href": "packages/sparklyr/latest/reference/ml_linear_regression.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPerform regression using linear regression."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_regression.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_linear_regression.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_linear_regression(\n  x,\n  formula = NULL,\n  fit_intercept = TRUE,\n  elastic_net_param = 0,\n  reg_param = 0,\n  max_iter = 100,\n  weight_col = NULL,\n  loss = \"squaredError\",\n  solver = \"auto\",\n  standardization = TRUE,\n  tol = 1e-06,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"linear_regression_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_regression.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_linear_regression.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nfit_intercept\nBoolean; should the model be fit with an intercept term?\n\n\nelastic_net_param\nElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n\n\nreg_param\nRegularization parameter (aka lambda)\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nweight_col\nThe name of the column to use as weights for the model fit.\n\n\nloss\nThe loss function to be optimized. Supported options: “squaredError”\n\n\n\nand “huber”. Default: “squaredError” solver | Solver algorithm for optimization. standardization | Whether to standardize the training features before fitting the model. tol | Param for the convergence tolerance for iterative algorithms. features_col | Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula. label_col | Label column name. The column should be a numeric column. Usually this column is output by ft_r_formula. prediction_col | Prediction column name. uid | A character string used to uniquely identify the ML estimator. … | Optional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_regression.html#details",
    "href": "packages/sparklyr/latest/reference/ml_linear_regression.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_regression.html#value",
    "href": "packages/sparklyr/latest/reference/ml_linear_regression.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_regression.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_linear_regression.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, name = \"mtcars_tbl\", overwrite = TRUE)\n\npartitions <- mtcars_tbl %>%\n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\nmtcars_training <- partitions$training\nmtcars_test <- partitions$test\n\nlm_model <- mtcars_training %>%\n  ml_linear_regression(mpg ~ .)\n\npred <- ml_predict(lm_model, mtcars_test)\n\nml_regression_evaluator(pred, label_col = \"mpg\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_regression.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_linear_regression.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc.html#description",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPerform classification using linear support vector machines (SVM). This binary classifier optimizes the Hinge Loss using the OWLQN optimizer. Only supports L2 regularization currently."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_linear_svc(\n  x,\n  formula = NULL,\n  fit_intercept = TRUE,\n  reg_param = 0,\n  max_iter = 100,\n  standardization = TRUE,\n  weight_col = NULL,\n  tol = 1e-06,\n  threshold = 0,\n  aggregation_depth = 2,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"linear_svc_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nfit_intercept\nBoolean; should the model be fit with an intercept term?\n\n\nreg_param\nRegularization parameter (aka lambda)\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nstandardization\nWhether to standardize the training features before fitting the model.\n\n\nweight_col\nThe name of the column to use as weights for the model fit.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nthreshold\nin binary classification prediction, in range [0, 1].\n\n\naggregation_depth\n(Spark 2.1.0+) Suggested depth for treeAggregate (>= 2).\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc.html#details",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc.html#value",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\npartitions <- iris_tbl %>%\n  filter(Species != \"setosa\") %>%\n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\niris_training <- partitions$training\niris_test <- partitions$test\n\nsvc_model <- iris_training %>%\n  ml_linear_svc(Species ~ .)\n\npred <- ml_predict(svc_model, iris_test)\n\nml_binary_classification_evaluator(pred)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ntidyml_model_linear_svc(x, …)\naugmentml_model_linear_svc(x, newdata = NULL, …)\nglanceml_model_linear_svc(x, …)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_linear_svc_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#description",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPerform classification using logistic regression."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_logistic_regression(\n  x,\n  formula = NULL,\n  fit_intercept = TRUE,\n  elastic_net_param = 0,\n  reg_param = 0,\n  max_iter = 100,\n  threshold = 0.5,\n  thresholds = NULL,\n  tol = 1e-06,\n  weight_col = NULL,\n  aggregation_depth = 2,\n  lower_bounds_on_coefficients = NULL,\n  lower_bounds_on_intercepts = NULL,\n  upper_bounds_on_coefficients = NULL,\n  upper_bounds_on_intercepts = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  family = \"auto\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"logistic_regression_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nfit_intercept\nBoolean; should the model be fit with an intercept term?\n\n\nelastic_net_param\nElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n\n\nreg_param\nRegularization parameter (aka lambda)\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nthreshold\nin binary classification prediction, in range [0, 1].\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nweight_col\nThe name of the column to use as weights for the model fit.\n\n\naggregation_depth\n(Spark 2.1.0+) Suggested depth for treeAggregate (>= 2).\n\n\nlower_bounds_on_coefficients\n(Spark 2.2.0+) Lower bounds on coefficients if fitting under bound constrained optimization.\n\n\n\nThe bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. lower_bounds_on_intercepts | (Spark 2.2.0+) Lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. upper_bounds_on_coefficients | (Spark 2.2.0+) Upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. upper_bounds_on_intercepts | (Spark 2.2.0+) Upper bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. features_col | Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula. label_col | Label column name. The column should be a numeric column. Usually this column is output by ft_r_formula. family | (Spark 2.1.0+) Param for the name of family which is a description of the label distribution to be used in the model. Supported options: “auto”, “binomial”, and “multinomial.” prediction_col | Prediction column name. probability_col | Column name for predicted class conditional probabilities. raw_prediction_col | Raw prediction (a.k.a. confidence) column name. uid | A character string used to uniquely identify the ML estimator. … | Optional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#details",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#value",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, name = \"mtcars_tbl\", overwrite = TRUE)\n\npartitions <- mtcars_tbl %>%\n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\nmtcars_training <- partitions$training\nmtcars_test <- partitions$test\n\nlr_model <- mtcars_training %>%\n  ml_logistic_regression(am ~ gear + carb)\n\npred <- ml_predict(lr_model, mtcars_test)\n\nml_binary_classification_evaluator(pred)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ntidyml_model_logistic_regression(x, …)\naugmentml_model_logistic_regression(x, newdata = NULL, …)\nglanceml_model_logistic_regression(x, …)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_logistic_regression_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_model_data.html#description",
    "href": "packages/sparklyr/latest/reference/ml_model_data.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nExtracts data associated with a Spark ML model"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_model_data.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_model_data.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_model_data(object)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_model_data.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_model_data.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nobject\na Spark ML model"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_model_data.html#value",
    "href": "packages/sparklyr/latest/reference/ml_model_data.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#description",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nClassification model based on the Multilayer Perceptron. Each layer has sigmoid activation function, output layer has softmax."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_multilayer_perceptron_classifier(\n  x,\n  formula = NULL,\n  layers = NULL,\n  max_iter = 100,\n  step_size = 0.03,\n  tol = 1e-06,\n  block_size = 128,\n  solver = \"l-bfgs\",\n  seed = NULL,\n  initial_weights = NULL,\n  thresholds = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"multilayer_perceptron_classifier_\"),\n  ...\n)\n\nml_multilayer_perceptron(\n  x,\n  formula = NULL,\n  layers,\n  max_iter = 100,\n  step_size = 0.03,\n  tol = 1e-06,\n  block_size = 128,\n  solver = \"l-bfgs\",\n  seed = NULL,\n  initial_weights = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  thresholds = NULL,\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"multilayer_perceptron_classifier_\"),\n  response = NULL,\n  features = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nlayers\nA numeric vector describing the layers – each element in the vector gives the size of a layer. For example, c(4, 5, 2) would imply three layers, with an input (feature) layer of size 4, an intermediate layer of size 5, and an output (class) layer of size 2.\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nstep_size\nStep size to be used for each iteration of optimization (> 0).\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nblock_size\nBlock size for stacking input data in matrices to speed up the computation. Data is stacked within partitions. If block size is more than remaining data in a partition then it is adjusted to the size of this data. Recommended size is between 10 and 1000. Default: 128\n\n\nsolver\nThe solver algorithm for optimization. Supported options: “gd” (minibatch gradient descent) or “l-bfgs”. Default: “l-bfgs”\n\n\nseed\nA random seed. Set this value if you need your results to be\n\n\n\nreproducible across repeated calls. initial_weights | The initial weights of the model. thresholds | Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold. features_col | Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula. label_col | Label column name. The column should be a numeric column. Usually this column is output by ft_r_formula. prediction_col | Prediction column name. probability_col | Column name for predicted class conditional probabilities. raw_prediction_col | Raw prediction (a.k.a. confidence) column name. uid | A character string used to uniquely identify the ML estimator. … | Optional arguments; see Details. response | (Deprecated) The name of the response column (as a length-one character vector.) features | (Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#details",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows.\nml_multilayer_perceptron() is an alias for ml_multilayer_perceptron_classifier() for backwards compatibility."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#value",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\n\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\npartitions <- iris_tbl %>%\n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\niris_training <- partitions$training\niris_test <- partitions$test\n\nmlp_model <- iris_training %>%\n  ml_multilayer_perceptron_classifier(Species ~ ., layers = c(4, 3, 3))\n\npred <- ml_predict(mlp_model, iris_test)\n\nml_multiclass_classification_evaluator(pred)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_classifier.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_naive_bayes(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ntidyml_model_multilayer_perceptron_classification(x, …)\naugmentml_model_multilayer_perceptron_classification(x, newdata = NULL, …)\nglanceml_model_multilayer_perceptron_classification(x, …)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_multilayer_perceptron_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#description",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nNaive Bayes Classifiers. It supports Multinomial NB (see http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.htmlhere) which can handle finitely supported discrete data. For example, by converting documents into TF-IDF vectors, it can be used for document classification. By making every vector a binary (0/1) data, it can also be used as Bernoulli NB (see http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.htmlhere). The input feature values must be nonnegative."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_naive_bayes(\n  x,\n  formula = NULL,\n  model_type = \"multinomial\",\n  smoothing = 1,\n  thresholds = NULL,\n  weight_col = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"naive_bayes_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nmodel_type\nThe model type. Supported options: \"multinomial\"\n\n\n\nand \"bernoulli\". (default = multinomial) smoothing | The (Laplace) smoothing parameter. Defaults to 1. thresholds | Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold. weight_col | (Spark 2.1.0+) Weight column name. If this is not set or empty, we treat all instance weights as 1.0. features_col | Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula. label_col | Label column name. The column should be a numeric column. Usually this column is output by ft_r_formula. prediction_col | Prediction column name. probability_col | Column name for predicted class conditional probabilities. raw_prediction_col | Raw prediction (a.k.a. confidence) column name. uid | A character string used to uniquely identify the ML estimator. … | Optional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#details",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#value",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\npartitions <- iris_tbl %>%\n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\niris_training <- partitions$training\niris_test <- partitions$test\n\nnb_model <- iris_training %>%\n  ml_naive_bayes(Species ~ .)\n\npred <- ml_predict(nb_model, iris_test)\n\nml_multiclass_classification_evaluator(pred)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_one_vs_rest(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ntidyml_model_naive_bayes(x, …)\naugmentml_model_naive_bayes(x, newdata = NULL, …)\nglanceml_model_naive_bayes(x, …)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_naive_bayes_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#description",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReduction of Multiclass Classification to Binary Classification. Performs reduction using one against all strategy. For a multiclass classification with k classes, train k models (one per class). Each example is scored against all k models and the model with highest score is picked to label the example."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_one_vs_rest(\n  x,\n  formula = NULL,\n  classifier = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"one_vs_rest_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nclassifier\nObject of class ml_estimator. Base binary classifier that we reduce multiclass classification into.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#details",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#value",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_one_vs_rest.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_random_forest_classifier()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ntidyml_model_pca(x, …)\naugmentml_model_pca(x, newdata = NULL, …)\nglanceml_model_pca(x, …)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_pca_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pipeline.html#description",
    "href": "packages/sparklyr/latest/reference/ml_pipeline.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCreate Spark ML Pipelines"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pipeline.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_pipeline.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_pipeline(x, ..., uid = random_string(\"pipeline_\"))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pipeline.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_pipeline.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nEither a spark_connection or ml_pipeline_stage objects\n\n\n…\nml_pipeline_stage objects.\n\n\nuid\nA character string used to uniquely identify the ML estimator."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_pipeline.html#value",
    "href": "packages/sparklyr/latest/reference/ml_pipeline.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nWhen x is a spark_connection, ml_pipeline() returns an empty pipeline object. When x is a ml_pipeline_stage, ml_pipeline() returns an ml_pipeline with the stages set to x and any transformers or estimators given in ...."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html#description",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPower iteration clustering (PIC) is a scalable and efficient algorithm for clustering vertices of a graph given pairwise similarities as edge properties, described in the paper “Power Iteration Clustering” by Frank Lin and William W. Cohen. It computes a pseudo-eigenvector of the normalized affinity matrix of the graph via power iteration and uses it to cluster vertices. spark.mllib includes an implementation of PIC using GraphX as its backend. It takes an RDD of (srcId, dstId, similarity) tuples and outputs a model with the clustering assignments. The similarities must be nonnegative. PIC assumes that the similarity measure is symmetric. A pair (srcId, dstId) regardless of the ordering should appear at most once in the input data. If a pair is missing from input, their similarity is treated as zero."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_power_iteration(\n  x,\n  k = 4,\n  max_iter = 20,\n  init_mode = \"random\",\n  src_col = \"src\",\n  dst_col = \"dst\",\n  weight_col = \"weight\",\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA ‘spark_connection’ or a ‘tbl_spark’.\n\n\nk\nThe number of clusters to create.\n\n\nmax_iter\nThe maximum number of iterations to run.\n\n\ninit_mode\nThis can be either “random”, which is the default, to use a random vector as vertex properties, or “degree” to use normalized sum similarities.\n\n\nsrc_col\nColumn in the input Spark dataframe containing 0-based indexes of all source vertices in the affinity matrix described in the PIC paper.\n\n\ndst_col\nColumn in the input Spark dataframe containing 0-based indexes of all destination vertices in the affinity matrix described in the PIC paper.\n\n\nweight_col\nColumn in the input Spark dataframe containing non-negative edge weights in the affinity matrix described in the PIC paper.\n\n\n…\nOptional arguments. Currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html#value",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA 2-column R dataframe with columns named “id” and “cluster” describing the resulting cluster assignments"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_power_iteration.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_power_iteration.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nr1 <- 1\nn1 <- 80L\nr2 <- 4\nn2 <- 80L\n\ngen_circle <- function(radius, num_pts) {\n  # generate evenly distributed points on a circle centered at the origin\n  seq(0, num_pts - 1) %>%\n    lapply(\n      function(pt) {\n        theta <- 2 * pi * pt / num_pts\n\n        radius * c(cos(theta), sin(theta))\n      }\n    )\n}\n\nguassian_similarity <- function(pt1, pt2) {\n  dist2 <- sum((pt2 - pt1)^2)\n\n  exp(-dist2 / 2)\n}\n\ngen_pic_data <- function() {\n  # generate points on 2 concentric circle centered at the origin and then\n  # compute pairwise Gaussian similarity values of all unordered pair of\n  # points\n  n <- n1 + n2\n  pts <- append(gen_circle(r1, n1), gen_circle(r2, n2))\n  num_unordered_pairs <- n * (n - 1) / 2\n\n  src <- rep(0L, num_unordered_pairs)\n  dst <- rep(0L, num_unordered_pairs)\n  sim <- rep(0, num_unordered_pairs)\n\n  idx <- 1\n  for (i in seq(2, n)) {\n    for (j in seq(i - 1)) {\n      src[[idx]] <- i - 1L\n      dst[[idx]] <- j - 1L\n      sim[[idx]] <- guassian_similarity(pts[[i]], pts[[j]])\n      idx <- idx + 1\n    }\n  }\n\n  tibble::tibble(src = src, dst = dst, sim = sim)\n}\n\npic_data <- copy_to(sc, gen_pic_data())\n\nclusters <- ml_power_iteration(\n  pic_data,\n  src_col = \"src\", dst_col = \"dst\", weight_col = \"sim\", k = 2, max_iter = 40\n)\nprint(clusters)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_prefixspan.html#description",
    "href": "packages/sparklyr/latest/reference/ml_prefixspan.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPrefixSpan algorithm for mining frequent itemsets."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_prefixspan.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_prefixspan.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_prefixspan(\n  x,\n  seq_col = \"sequence\",\n  min_support = 0.1,\n  max_pattern_length = 10,\n  max_local_proj_db_size = 3.2e+07,\n  uid = random_string(\"prefixspan_\"),\n  ...\n)\n\nml_freq_seq_patterns(model)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_prefixspan.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_prefixspan.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nseq_col\nThe name of the sequence column in dataset (default\n\n\n\n“sequence”). Rows with nulls in this column are ignored. min_support | The minimum support required to be considered a frequent sequential pattern. max_pattern_length | The maximum length of a frequent sequential pattern. Any frequent pattern exceeding this length will not be included in the results. max_local_proj_db_size | The maximum number of items allowed in a prefix-projected database before local iterative processing of the projected database begins. This parameter should be tuned with respect to the size of your executors. uid | A character string used to uniquely identify the ML estimator. … | Optional arguments; currently unused. model | A Prefix Span model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_prefixspan.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_prefixspan.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.4.0\")\n\nitems_df <- tibble::tibble(\n  seq = list(\n    list(list(1, 2), list(3)),\n    list(list(1), list(3, 2), list(1, 2)),\n    list(list(1, 2), list(5)),\n    list(list(6))\n  )\n)\nitems_sdf <- copy_to(sc, items_df, overwrite = TRUE)\n\nprefix_span_model <- ml_prefixspan(\n  sc,\n  seq_col = \"seq\",\n  min_support = 0.5,\n  max_pattern_length = 5,\n  max_local_proj_db_size = 32000000\n)\n\nfrequent_items <- prefix_span_model$frequent_sequential_patterns(items_sdf) %>% collect()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_random_forest.html#description",
    "href": "packages/sparklyr/latest/reference/ml_random_forest.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPerform classification and regression using random forests."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_random_forest.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_random_forest.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_random_forest_classifier(\n  x,\n  formula = NULL,\n  num_trees = 20,\n  subsampling_rate = 1,\n  max_depth = 5,\n  min_instances_per_node = 1,\n  feature_subset_strategy = \"auto\",\n  impurity = \"gini\",\n  min_info_gain = 0,\n  max_bins = 32,\n  seed = NULL,\n  thresholds = NULL,\n  checkpoint_interval = 10,\n  cache_node_ids = FALSE,\n  max_memory_in_mb = 256,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"random_forest_classifier_\"),\n  ...\n)\n\nml_random_forest(\n  x,\n  formula = NULL,\n  type = c(\"auto\", \"regression\", \"classification\"),\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  feature_subset_strategy = \"auto\",\n  impurity = \"auto\",\n  checkpoint_interval = 10,\n  max_bins = 32,\n  max_depth = 5,\n  num_trees = 20,\n  min_info_gain = 0,\n  min_instances_per_node = 1,\n  subsampling_rate = 1,\n  seed = NULL,\n  thresholds = NULL,\n  cache_node_ids = FALSE,\n  max_memory_in_mb = 256,\n  uid = random_string(\"random_forest_\"),\n  response = NULL,\n  features = NULL,\n  ...\n)\n\nml_random_forest_regressor(\n  x,\n  formula = NULL,\n  num_trees = 20,\n  subsampling_rate = 1,\n  max_depth = 5,\n  min_instances_per_node = 1,\n  feature_subset_strategy = \"auto\",\n  impurity = \"variance\",\n  min_info_gain = 0,\n  max_bins = 32,\n  seed = NULL,\n  checkpoint_interval = 10,\n  cache_node_ids = FALSE,\n  max_memory_in_mb = 256,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"random_forest_regressor_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_random_forest.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_random_forest.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nnum_trees\nNumber of trees to train (>= 1). If 1, then no bootstrapping is used. If > 1, then bootstrapping is done.\n\n\nsubsampling_rate\nFraction of the training data used for learning each decision tree, in range (0, 1]. (default = 1.0)\n\n\nmax_depth\nMaximum depth of the tree (>= 0); that is, the maximum\n\n\n\nnumber of nodes separating any leaves from the root of the tree. min_instances_per_node | Minimum number of instances each child must have after split. feature_subset_strategy | The number of features to consider for splits at each tree node. See details for options. impurity | Criterion used for information gain calculation. Supported: “entropy” and “gini” (default) for classification and “variance” (default) for regression. For ml_decision_tree, setting \"auto\" will default to the appropriate criterion based on model type. min_info_gain | Minimum information gain for a split to be considered at a tree node. Should be >= 0, defaults to 0. max_bins | The maximum number of bins used for discretizing continuous features and for choosing how to split on features at each node. More bins give higher granularity. seed | Seed for random numbers. thresholds | Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold. checkpoint_interval | Set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10. cache_node_ids | If FALSE, the algorithm will pass trees to executors to match instances with nodes. If TRUE, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Defaults to FALSE. max_memory_in_mb | Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. Defaults to 256. features_col | Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula. label_col | Label column name. The column should be a numeric column. Usually this column is output by ft_r_formula. prediction_col | Prediction column name. probability_col | Column name for predicted class conditional probabilities. raw_prediction_col | Raw prediction (a.k.a. confidence) column name. uid | A character string used to uniquely identify the ML estimator. … | Optional arguments; see Details. type | The type of model to fit. \"regression\" treats the response as a continuous variable, while \"classification\" treats the response as a categorical variable. When \"auto\" is used, the model type is inferred based on the response variable type – if it is a numeric type, then regression is used; classification otherwise. response | (Deprecated) The name of the response column (as a length-one character vector.) features | (Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_random_forest.html#details",
    "href": "packages/sparklyr/latest/reference/ml_random_forest.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows.\nThe supported options for feature_subset_strategy are\n\n\"auto\": Choose automatically for task: If num_trees == 1, set to \"all\". If num_trees > 1 (forest), set to \"sqrt\" for classification and to \"onethird\" for regression.\n\"all\": use all features\n\"onethird\": use 1/3 of the features\n\"sqrt\": use use sqrt(number of features)\n\"log2\": use log2(number of features)\n\"n\": when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features. (default = \"auto\")\n\nml_random_forest is a wrapper around ml_random_forest_regressor.tbl_spark and ml_random_forest_classifier.tbl_spark and calls the appropriate method based on model type."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_random_forest.html#value",
    "href": "packages/sparklyr/latest/reference/ml_random_forest.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_random_forest.html#examples",
    "href": "packages/sparklyr/latest/reference/ml_random_forest.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\npartitions <- iris_tbl %>%\n  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\niris_training <- partitions$training\niris_test <- partitions$test\n\nrf_model <- iris_training %>%\n  ml_random_forest(Species ~ ., type = \"classification\")\n\npred <- ml_predict(rf_model, iris_test)\n\nml_multiclass_classification_evaluator(pred)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_random_forest.html#see-also",
    "href": "packages/sparklyr/latest/reference/ml_random_forest.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression(), ml_decision_tree_classifier(), ml_gbt_classifier(), ml_generalized_linear_regression(), ml_isotonic_regression(), ml_linear_regression(), ml_linear_svc(), ml_logistic_regression(), ml_multilayer_perceptron_classifier(), ml_naive_bayes(), ml_one_vs_rest()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_stage.html#description",
    "href": "packages/sparklyr/latest/reference/ml_stage.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nExtraction of stages from a Pipeline or PipelineModel object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_stage.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_stage.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_stage(x, stage)\n\nml_stages(x, stages = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_stage.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_stage.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA ml_pipeline or a ml_pipeline_model object\n\n\nstage\nThe UID of a stage in the pipeline.\n\n\nstages\nThe UIDs of stages in the pipeline as a character vector."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_stage.html#value",
    "href": "packages/sparklyr/latest/reference/ml_stage.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nFor ml_stage(): The stage specified.\nFor ml_stages(): A list of stages. If stages is not set, the function returns all stages of the pipeline in a list."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_standardize_formula.html#description",
    "href": "packages/sparklyr/latest/reference/ml_standardize_formula.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerates a formula string from user inputs, to be used in ml_model constructor."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_standardize_formula.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_standardize_formula.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_standardize_formula(formula = NULL, response = NULL, features = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_standardize_formula.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_standardize_formula.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nformula\nThe formula argument.\n\n\nresponse\nThe response argument.\n\n\nfeatures\nThe features argument."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_summary.html#description",
    "href": "packages/sparklyr/latest/reference/ml_summary.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nExtracts a metric from the summary object of a Spark ML model."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_summary.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_summary.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_summary(x, metric = NULL, allow_null = FALSE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_summary.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_summary.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark ML model that has a summary.\n\n\nmetric\nThe name of the metric to extract. If not set, returns the summary object.\n\n\nallow_null\nWhether null results are allowed when the metric is not found in the summary."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ntidyml_model_aft_survival_regression(x, …)\naugmentml_model_aft_survival_regression(x, newdata = NULL, …)\nglanceml_model_aft_survival_regression(x, …)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_survival_regression_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ntidyml_model_decision_tree_classification(x, …)\ntidyml_model_decision_tree_regression(x, …)\naugmentml_model_decision_tree_classification(x, newdata = NULL, …)\naugmentml_model_decision_tree_regression(x, newdata = NULL, …)\nglanceml_model_decision_tree_classification(x, …)\nglanceml_model_decision_tree_regression(x, …)\ntidyml_model_random_forest_classification(x, …)\ntidyml_model_random_forest_regression(x, …)\naugmentml_model_random_forest_classification(x, newdata = NULL, …)\naugmentml_model_random_forest_regression(x, newdata = NULL, …)\nglanceml_model_random_forest_classification(x, …)\nglanceml_model_random_forest_regression(x, …)\ntidyml_model_gbt_classification(x, …)\ntidyml_model_gbt_regression(x, …)\naugmentml_model_gbt_classification(x, newdata = NULL, …)\naugmentml_model_gbt_regression(x, newdata = NULL, …)\nglanceml_model_gbt_classification(x, …)\nglanceml_model_gbt_regression(x, …)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_tree_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_uid.html#description",
    "href": "packages/sparklyr/latest/reference/ml_uid.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nExtracts the UID of an ML object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_uid.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_uid.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_uid(x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_uid.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_uid.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark ML object"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#description",
    "href": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThese methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#usage",
    "href": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ntidyml_model_kmeans(x, …)\naugmentml_model_kmeans(x, newdata = NULL, …)\nglanceml_model_kmeans(x, …)\ntidyml_model_bisecting_kmeans(x, …)\naugmentml_model_bisecting_kmeans(x, newdata = NULL, …)\nglanceml_model_bisecting_kmeans(x, …)\ntidyml_model_gaussian_mixture(x, …)\naugmentml_model_gaussian_mixture(x, newdata = NULL, …)\nglanceml_model_gaussian_mixture(x, …)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#arguments",
    "href": "packages/sparklyr/latest/reference/ml_unsupervised_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n…\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/mutate.html#description",
    "href": "packages/sparklyr/latest/reference/mutate.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee mutate for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/na.replace.html#description",
    "href": "packages/sparklyr/latest/reference/na.replace.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis S3 generic provides an interface for replacing NA values within an object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/na.replace.html#usage",
    "href": "packages/sparklyr/latest/reference/na.replace.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nna.replace(object, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/na.replace.html#arguments",
    "href": "packages/sparklyr/latest/reference/na.replace.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nobject\nAn object.\n\n\n…\nArguments passed along to implementing methods."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/nest.html#description",
    "href": "packages/sparklyr/latest/reference/nest.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee nest for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pipe.html#description",
    "href": "packages/sparklyr/latest/reference/pipe.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee %>% for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pivot_longer.html#description",
    "href": "packages/sparklyr/latest/reference/pivot_longer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee pivot_longer for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/pivot_wider.html#description",
    "href": "packages/sparklyr/latest/reference/pivot_wider.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee pivot_wider for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/print_jobj.html#description",
    "href": "packages/sparklyr/latest/reference/print_jobj.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGeneric method for print jobj for a connection type"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/print_jobj.html#usage",
    "href": "packages/sparklyr/latest/reference/print_jobj.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nprint_jobj(sc, jobj, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/print_jobj.html#arguments",
    "href": "packages/sparklyr/latest/reference/print_jobj.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nspark_connection (used for type dispatch)\n\n\njobj\nObject to print"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/quote_sql_name.html#description",
    "href": "packages/sparklyr/latest/reference/quote_sql_name.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCalls dbplyr::translate_sql_ on the input character vector or symbol to obtain the corresponding SQL identifier that is escaped and quoted properly"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/quote_sql_name.html#usage",
    "href": "packages/sparklyr/latest/reference/quote_sql_name.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nquote_sql_name(x, con = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/random_string.html#description",
    "href": "packages/sparklyr/latest/reference/random_string.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerate a random string with a given prefix."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/random_string.html#usage",
    "href": "packages/sparklyr/latest/reference/random_string.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nrandom_string(prefix = \"table\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/random_string.html#arguments",
    "href": "packages/sparklyr/latest/reference/random_string.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nprefix\nA length-one character vector."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/reactivespark.html#description",
    "href": "packages/sparklyr/latest/reference/reactivespark.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGiven a spark object, returns a reactive data source for the contents of the spark object. This function is most useful to read Spark streams."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/reactivespark.html#usage",
    "href": "packages/sparklyr/latest/reference/reactivespark.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nreactiveSpark(x, intervalMillis = 1000, session = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/reactivespark.html#arguments",
    "href": "packages/sparklyr/latest/reference/reactivespark.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\nintervalMillis\nApproximate number of milliseconds to wait to retrieve\n\n\n\nupdated data frame. This can be a numeric value, or a function that returns a numeric value. session | The user session to associate this file reader with, or NULL if none. If non-null, the reader will automatically stop when the session ends."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/reexports.html#description",
    "href": "packages/sparklyr/latest/reference/reexports.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThese objects are imported from other packages. Follow the links below to see their documentation."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/register_extension.html#description",
    "href": "packages/sparklyr/latest/reference/register_extension.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRegistering an extension package will result in the package being automatically scanned for spark dependencies when a connection to Spark is created."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/register_extension.html#usage",
    "href": "packages/sparklyr/latest/reference/register_extension.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nregister_extension(package)\n\nregistered_extensions()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/register_extension.html#arguments",
    "href": "packages/sparklyr/latest/reference/register_extension.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\npackage\nThe package(s) to register."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html#description",
    "href": "packages/sparklyr/latest/reference/registerdospark.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRegisters a parallel backend using the foreach package."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html#usage",
    "href": "packages/sparklyr/latest/reference/registerdospark.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nregisterDoSpark(spark_conn, parallelism = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html#arguments",
    "href": "packages/sparklyr/latest/reference/registerdospark.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nspark_conn\nSpark connection to use\n\n\nparallelism\nLevel of parallelism to use for task execution\n\n\n\n(if unspecified, then it will take the value of SparkContext.defaultParallelism() which by default is the number of cores available to the sparklyr application) … | additional options for sparklyr parallel backend (currently only the only valid option is nocompile = T, F)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html#value",
    "href": "packages/sparklyr/latest/reference/registerdospark.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nNone"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/registerdospark.html#examples",
    "href": "packages/sparklyr/latest/reference/registerdospark.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nsc <- spark_connect(master = \"local\")\nregisterDoSpark(sc, nocompile = FALSE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/replace_na.html#description",
    "href": "packages/sparklyr/latest/reference/replace_na.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee replace_na for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/right_join.html#description",
    "href": "packages/sparklyr/latest/reference/right_join.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee right_join for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-saveload.html#description",
    "href": "packages/sparklyr/latest/reference/sdf-saveload.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRoutines for saving and loading Spark DataFrames."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-saveload.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf-saveload.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_save_table(x, name, overwrite = FALSE, append = FALSE)\n\nsdf_load_table(sc, name)\n\nsdf_save_parquet(x, path, overwrite = FALSE, append = FALSE)\n\nsdf_load_parquet(sc, path)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-saveload.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf-saveload.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nname\nThe table name to assign to the saved Spark DataFrame.\n\n\noverwrite\nBoolean; overwrite a pre-existing table of the same name?\n\n\nappend\nBoolean; append to a pre-existing table of the same name?\n\n\nsc\nA spark_connection object.\n\n\npath\nThe path where the Spark DataFrame should be saved."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-transform-methods.html#description",
    "href": "packages/sparklyr/latest/reference/sdf-transform-methods.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nDeprecated methods for transformation, fit, and prediction. These are mirrors of the corresponding ml-transform-methods."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-transform-methods.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf-transform-methods.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_predict(x, model, ...)\n\nsdf_transform(x, transformer, ...)\n\nsdf_fit(x, estimator, ...)\n\nsdf_fit_and_transform(x, estimator, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-transform-methods.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf-transform-methods.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA tbl_spark.\n\n\nmodel\nA ml_transformer or a ml_model object.\n\n\n…\nOptional arguments passed to the corresponding ml_ methods.\n\n\ntransformer\nA ml_transformer object.\n\n\nestimator\nA ml_estimator object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf-transform-methods.html#value",
    "href": "packages/sparklyr/latest/reference/sdf-transform-methods.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nsdf_predict(), sdf_transform(), and sdf_fit_and_transform() return a transformed dataframe whereas sdf_fit() returns a ml_transformer."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_along.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_along.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCreates a DataFrame along the given object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_along.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_along.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_along(sc, along, repartition = NULL, type = c(\"integer\", \"integer64\"))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_along.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_along.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nalong\nTakes the length from the length of this argument.\n\n\nrepartition\nThe number of partitions to use when distributing the\n\n\n\ndata across the Spark cluster. type | The data type to use for the index, either \"integer\" or \"integer64\"."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nsdf_bind_rows() and sdf_bind_cols() are implementation of the common pattern of do.call(rbind, sdfs) or do.call(cbind, sdfs) for binding many Spark DataFrames into one."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_bind_rows(..., id = NULL)\n\nsdf_bind_cols(...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\n…\nSpark tbls to combine.\n\n\n\nEach argument can either be a Spark DataFrame or a list of Spark DataFrames\nWhen row-binding, columns are matched by name, and any missing columns with be filled with NA.\nWhen column-binding, rows are matched by position, so all data frames must have the same number of rows. id | Data frame identifier.\nWhen id is supplied, a new column of identifiers is created to link each row to its original Spark DataFrame. The labels are taken from the named arguments to sdf_bind_rows(). When a list of Spark DataFrames is supplied, the labels are taken from the names of the list. If no names are found a numeric sequence is used instead."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html#details",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nThe output of sdf_bind_rows() will contain a column if that column appears in any of the inputs."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_bind.html#value",
    "href": "packages/sparklyr/latest/reference/sdf_bind.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nsdf_bind_rows() and sdf_bind_cols() return tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_broadcast.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_broadcast.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nUsed to force broadcast hash joins."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_broadcast.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_broadcast.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_broadcast(x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_broadcast.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_broadcast.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_checkpoint.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_checkpoint.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCheckpoint a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_checkpoint.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_checkpoint.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_checkpoint(x, eager = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_checkpoint.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_checkpoint.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nan object coercible to a Spark DataFrame\n\n\neager\nwhether to truncate the lineage of the DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_coalesce.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_coalesce.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCoalesces a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_coalesce.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_coalesce.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_coalesce(x, partitions)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_coalesce.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_coalesce.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\npartitions\nnumber of partitions"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_collect.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_collect.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCollects a Spark dataframe into R."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_collect.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_collect.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_collect(object, impl = c(\"row-wise\", \"row-wise-iter\", \"column-wise\"), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_collect.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_collect.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nobject\nSpark dataframe to collect\n\n\nimpl\nWhich implementation to use while collecting Spark dataframe\n\n\n\n\nrow-wise: fetch the entire dataframe into memory and then process it row-by-row\nrow-wise-iter: iterate through the dataframe using RDD local iterator, processing one row at a time (hence reducing memory footprint)\ncolumn-wise: fetch the entire dataframe into memory and then process it column-by-column NOTE: (1) this will not apply to streaming or arrow use cases (2) this parameter will only affect implementation detail, and will not affect result of sdf_collect, and should only be set if performance profiling indicates any particular choice will be significantly better than the default choice (“row-wise”) … | Additional options."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_copy_to.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_copy_to.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCopy an object into Spark, and return an object wrapping the copied object (typically, a Spark DataFrame)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_copy_to.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_copy_to.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_copy_to(sc, x, name, memory, repartition, overwrite, struct_columns, ...)\n\nsdf_import(x, sc, name, memory, repartition, overwrite, struct_columns, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_copy_to.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_copy_to.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nx\nAn object from which a Spark DataFrame can be generated.\n\n\nname\nThe name to assign to the copied table in Spark.\n\n\nmemory\nBoolean; should the table be cached into memory?\n\n\nrepartition\nThe number of partitions to use when distributing the\n\n\n\ntable across the Spark cluster. The default (0) can be used to avoid partitioning. overwrite | Boolean; overwrite a pre-existing table with the name name if one already exists? struct_columns | (only supported with Spark 2.4.0 or higher) A list of columns from the source data frame that should be converted to Spark SQL StructType columns. The source columns can contain either json strings or nested lists. All rows within each source column should have identical schemas (because otherwise the conversion result will contain unexpected null values or missing values as Spark currently does not support schema discovery on individual rows within a struct column). … | Optional arguments, passed to implementing methods."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_copy_to.html#examples",
    "href": "packages/sparklyr/latest/reference/sdf_copy_to.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"spark://HOST:PORT\")\nsdf_copy_to(sc, iris)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_copy_to.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_copy_to.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_distinct(), sdf_random_split(), sdf_register(), sdf_sample(), sdf_sort(), sdf_weighted_sample()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_crosstab.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_crosstab.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nBuilds a contingency table at each combination of factor levels."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_crosstab.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_crosstab.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_crosstab(x, col1, col2)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_crosstab.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_crosstab.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame\n\n\ncol1\nThe name of the first column. Distinct items will make the first item of each row.\n\n\ncol2\nThe name of the second column. Distinct items will make the column names of the DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_crosstab.html#value",
    "href": "packages/sparklyr/latest/reference/sdf_crosstab.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA DataFrame containing the contingency table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_debug_string.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_debug_string.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPrints plan of execution to generate x. This plan will, among other things, show the number of partitions in parenthesis at the far left and indicate stages using indentation."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_debug_string.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_debug_string.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_debug_string(x, print = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_debug_string.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_debug_string.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object wrapping, or containing, a Spark DataFrame.\n\n\nprint\nPrint debug information?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_describe.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_describe.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCompute summary statistics for columns of a data frame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_describe.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_describe.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_describe(x, cols = colnames(x))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_describe.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_describe.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercible to a Spark DataFrame\n\n\ncols\nColumns to compute statistics for, given as a character vector"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_dim.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_dim.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nsdf_dim(), sdf_nrow() and sdf_ncol() provide similar functionality to dim(), nrow() and ncol()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_dim.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_dim.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_dim(x)\n\nsdf_nrow(x)\n\nsdf_ncol(x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_dim.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_dim.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object (usually a spark_tbl)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nInvoke distinct on a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_distinct(x, ..., name)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame.\n\n\n…\nOptional variables to use when determining uniqueness.\n\n\n\nIf there are multiple rows for a given combination of inputs, only the first row will be preserved. If omitted, will use all variables. name | A name to assign this table. Passed to [sdf_register()]."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_distinct.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_distinct.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_copy_to(), sdf_random_split(), sdf_register(), sdf_sample(), sdf_sort(), sdf_weighted_sample()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_drop_duplicates.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_drop_duplicates.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRemove duplicates from a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_drop_duplicates.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_drop_duplicates.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_drop_duplicates(x, cols = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_drop_duplicates.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_drop_duplicates.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercible to a Spark DataFrame\n\n\ncols\nSubset of Columns to consider, given as a character vector"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_expand_grid.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_expand_grid.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGiven one or more R vectors/factors or single-column Spark dataframes, perform an expand.grid operation on all of them and store the result in a Spark dataframe"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_expand_grid.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_expand_grid.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_expand_grid(\n  sc,\n  ...,\n  broadcast_vars = NULL,\n  memory = TRUE,\n  repartition = NULL,\n  partition_by = NULL\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_expand_grid.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_expand_grid.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\n…\nEach input variable can be either a R vector/factor or a Spark\n\n\n\ndataframe. Unnamed inputs will assume the default names of ‘Var1’, ‘Var2’, etc in the result, similar to what expand.grid does for unnamed inputs. broadcast_vars | Indicates which input(s) should be broadcasted to all nodes of the Spark cluster during the join process (default: none). memory | Boolean; whether the resulting Spark dataframe should be cached into memory (default: TRUE) repartition | Number of partitions the resulting Spark dataframe should have partition_by | Vector of column names used for partitioning the resulting Spark dataframe, only supported for Spark 2.0+"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_expand_grid.html#examples",
    "href": "packages/sparklyr/latest/reference/sdf_expand_grid.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\ngrid_sdf <- sdf_expand_grid(sc, seq(5), rnorm(10), letters)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis is a version of sdf_bind_cols that works by zipping RDDs. From the API docs: “Assumes that the two RDDs have the same number of partitions and the same number of elements in each partition (e.g. one was made through a map on the other).”"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_fast_bind_cols(...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_fast_bind_cols.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\n…\nSpark DataFrames to cbind"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_from_avro.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_from_avro.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nConvert column(s) from avro format"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_from_avro.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_from_avro.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_from_avro(x, cols)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_from_avro.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_from_avro.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercible to a Spark DataFrame\n\n\ncols\nNamed list of columns to transform from Avro format plus a valid Avro\n\n\n\nschema string for each column, where column names are keys and column schema strings are values (e.g., c(example_primitive_col = \"string\", example_complex_col = \"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"person\\\",\\\"fields\\\":[ {\\\"name\\\":\\\"person_name\\\",\\\"type\\\":\\\"string\\\"}, {\\\"name\\\":\\\"person_id\\\",\\\"type\\\":\\\"long\\\"}]}\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_is_streaming.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_is_streaming.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nIs the given Spark DataFrame a streaming data?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_is_streaming.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_is_streaming.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_is_streaming(x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_is_streaming.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_is_streaming.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_last_index.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_last_index.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReturns the last index of a Spark DataFrame. The Spark mapPartitionsWithIndex function is used to iterate through the last nonempty partition of the RDD to find the last record."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_last_index.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_last_index.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_last_index(x, id = \"id\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_last_index.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_last_index.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nid\nThe name of the index column."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_len.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_len.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCreates a DataFrame for the given length."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_len.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_len.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_len(sc, length, repartition = NULL, type = c(\"integer\", \"integer64\"))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_len.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_len.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nlength\nThe desired length of the sequence.\n\n\nrepartition\nThe number of partitions to use when distributing the\n\n\n\ndata across the Spark cluster. type | The data type to use for the index, either \"integer\" or \"integer64\"."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_num_partitions.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_num_partitions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGets number of partitions of a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_num_partitions.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_num_partitions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_num_partitions(x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_num_partitions.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_num_partitions.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCompute the number of records within each partition of a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_partition_sizes(x)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#examples",
    "href": "packages/sparklyr/latest/reference/sdf_partition_sizes.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"spark://HOST:PORT\")\nexample_sdf <- sdf_len(sc, 100L, repartition = 10L)\nexample_sdf %>%\n  sdf_partition_sizes() %>%\n  print()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_persist.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_persist.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPersist a Spark DataFrame, forcing any pending computations and (optionally) serializing the results to disk."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_persist.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_persist.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_persist(x, storage.level = \"MEMORY_AND_DISK\", name = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_persist.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_persist.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nstorage.level\nThe storage level to be used. Please view the\n\n\n\nhttps://spark.apache.org/docs/latest/programming-guide.html#rdd-persistenceSpark Documentation for information on what storage levels are accepted. name | A name to assign this table. Passed to [sdf_register()]."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_persist.html#details",
    "href": "packages/sparklyr/latest/reference/sdf_persist.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nSpark DataFrames invoke their operations lazily – pending operations are deferred until their results are actually needed. Persisting a Spark DataFrame effectively ‘forces’ any pending computations, and then persists the generated Spark DataFrame as requested (to memory, to disk, or otherwise).\nUsers of Spark should be careful to persist the results of any computations which are non-deterministic – otherwise, one might see that the values within a column seem to ‘change’ as new operations are performed on that data set."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_pivot.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_pivot.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nConstruct a pivot table over a Spark Dataframe, using a syntax similar to that from reshape2::dcast."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_pivot.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_pivot.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_pivot(x, formula, fun.aggregate = \"count\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_pivot.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_pivot.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nA two-sided formula of the form x_1 + x_2 + ... ~ y_1.\n\n\n\nThe left-hand side of the formula indicates which variables are used for grouping, and the right-hand side indicates which variable is used for pivoting. Currently, only a single pivot column is supported. fun.aggregate | How should the grouped dataset be aggregated? Can be a length-one character vector, giving the name of a Spark aggregation function to be called; a named list mapping column names to an aggregation method, or an function that is invoked on the grouped dataset."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_pivot.html#examples",
    "href": "packages/sparklyr/latest/reference/sdf_pivot.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\n# aggregating by mean\niris_tbl %>%\n  mutate(Petal_Width = ifelse(Petal_Width > 1.5, \"High\", \"Low\")) %>%\n  sdf_pivot(Petal_Width ~ Species,\n    fun.aggregate = list(Petal_Length = \"mean\")\n  )\n\n# aggregating all observations in a list\niris_tbl %>%\n  mutate(Petal_Width = ifelse(Petal_Width > 1.5, \"High\", \"Low\")) %>%\n  sdf_pivot(Petal_Width ~ Species,\n    fun.aggregate = list(Petal_Length = \"collect_list\")\n  )"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_project.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_project.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nProject features onto principal components"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_project.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_project.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_project(\n  object,\n  newdata,\n  features = dimnames(object$pc)[[1]],\n  feature_prefix = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_project.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_project.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nobject\nA Spark PCA model object\n\n\nnewdata\nAn object coercible to a Spark DataFrame\n\n\nfeatures\nA vector of names of columns to be projected\n\n\nfeature_prefix\nThe prefix used in naming the output features\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_quantile.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_quantile.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGiven a numeric column within a Spark DataFrame, compute approximate quantiles."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_quantile.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_quantile.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_quantile(\n  x,\n  column,\n  probabilities = c(0, 0.25, 0.5, 0.75, 1),\n  relative.error = 1e-05,\n  weight.column = NULL\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_quantile.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_quantile.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ncolumn\nThe column(s) for which quantiles should be computed.\n\n\n\nMultiple columns are only supported in Spark 2.0+. probabilities | A numeric vector of probabilities, for which quantiles should be computed. relative.error | The maximal possible difference between the actual percentile of a result and its expected percentile (e.g., if relative.error is 0.01 and probabilities is 0.95, then any value between the 94th and 96th percentile will be considered an acceptable approximation). weight.column | If not NULL, then a generalized version of the Greenwald- Khanna algorithm will be run to compute weighted percentiles, with each sample from column having a relative weight specified by the corresponding value in weight.column. The weights can be considered as relative frequencies of sample data points."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPartition a Spark DataFrame into multiple groups. This routine is useful for splitting a DataFrame into, for example, training and test datasets."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_random_split(\n  x,\n  ...,\n  weights = NULL,\n  seed = sample(.Machine$integer.max, 1)\n)\n\nsdf_partition(x, ..., weights = NULL, seed = sample(.Machine$integer.max, 1))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\n…\nNamed parameters, mapping table names to weights. The weights\n\n\n\nwill be normalized such that they sum to 1. weights | An alternate mechanism for supplying weights – when specified, this takes precedence over the ... arguments. seed | Random seed to use for randomly partitioning the dataset. Set this if you want your partitioning to be reproducible on repeated runs."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#details",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nThe sampling weights define the probability that a particular observation will be assigned to a particular partition, not the resulting size of the partition. This implies that partitioning a DataFrame with, for example,\nsdf_random_split(x, training = 0.5, test = 0.5)\nis not guaranteed to produce training and test partitions of equal size."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#value",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nAn list of tbl_sparks."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#examples",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n# randomly partition data into a 'training' and 'test'\n# dataset, with 60% of the observations assigned to the\n# 'training' dataset, and 40% assigned to the 'test' dataset\ndata(diamonds, package = \"ggplot2\")\ndiamonds_tbl <- copy_to(sc, diamonds, \"diamonds\")\npartitions <- diamonds_tbl %>%\n  sdf_random_split(training = 0.6, test = 0.4)\nprint(partitions)\n\n# alternate way of specifying weights\nweights <- c(training = 0.6, test = 0.4)\ndiamonds_tbl %>% sdf_random_split(weights = weights)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_random_split.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_random_split.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_copy_to(), sdf_distinct(), sdf_register(), sdf_sample(), sdf_sort(), sdf_weighted_sample()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbeta.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rbeta.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Betal distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbeta.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rbeta.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_rbeta(\n  sc,\n  n,\n  shape1,\n  shape2,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbeta.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rbeta.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nshape1\nNon-negative parameter (alpha) of the Beta distribution.\n\n\nshape2\nNon-negative parameter (beta) of the Beta distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe\n\n\n\n(default: default parallelism of the Spark cluster). seed | Random seed (default: a random long integer). output_col | Name of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbeta.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rbeta.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbinom.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rbinom.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a binomial distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbinom.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rbinom.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_rbinom(\n  sc,\n  n,\n  size,\n  prob,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbinom.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rbinom.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nsize\nNumber of trials (zero or more).\n\n\nprob\nProbability of success on each trial.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe\n\n\n\n(default: default parallelism of the Spark cluster). seed | Random seed (default: a random long integer). output_col | Name of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rbinom.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rbinom.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rcauchy.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rcauchy.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Cauchy distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rcauchy.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rcauchy.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_rcauchy(\n  sc,\n  n,\n  location = 0,\n  scale = 1,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rcauchy.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rcauchy.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nlocation\nLocation parameter of the distribution.\n\n\nscale\nScale parameter of the distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe\n\n\n\n(default: default parallelism of the Spark cluster). seed | Random seed (default: a random long integer). output_col | Name of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rcauchy.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rcauchy.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rchisq.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rchisq.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a chi-squared distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rchisq.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rchisq.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_rchisq(sc, n, df, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rchisq.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rchisq.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\ndf\nDegrees of freedom (non-negative, but can be non-integer).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe\n\n\n\n(default: default parallelism of the Spark cluster). seed | Random seed (default: a random long integer). output_col | Name of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rchisq.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rchisq.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_read_column.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_read_column.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead a single column from a Spark DataFrame, and return the contents of that column back to ."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_read_column.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_read_column.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_read_column(x, column)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_read_column.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_read_column.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ncolumn\nThe name of a column within x."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_read_column.html#details",
    "href": "packages/sparklyr/latest/reference/sdf_read_column.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nIt is expected for this operation to preserve row order."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRegisters a Spark DataFrame (giving it a table name for the Spark SQL context), and returns a tbl_spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_register(x, name = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame.\n\n\nname\nA name to assign this table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_register.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_register.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_copy_to(), sdf_distinct(), sdf_random_split(), sdf_sample(), sdf_sort(), sdf_weighted_sample()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_repartition.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_repartition.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRepartition a Spark DataFrame"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_repartition.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_repartition.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_repartition(x, partitions = NULL, partition_by = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_repartition.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_repartition.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\npartitions\nnumber of partitions\n\n\npartition_by\nvector of column names used for partitioning, only supported for Spark 2.0+"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_residuals.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_residuals.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis generic method returns a Spark DataFrame with model residuals added as a column to the model training data."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_residuals.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_residuals.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_residualsml_model_generalized_linear_regression( object, type = c(“deviance”, “pearson”, “working”, “response”), … )\nsdf_residualsml_model_linear_regression(object, …)\nsdf_residuals(object, …)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_residuals.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_residuals.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nobject\nSpark ML model object.\n\n\ntype\ntype of residuals which should be returned.\n\n\n…\nadditional arguments"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rexp.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rexp.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from an exponential distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rexp.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rexp.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_rexp(sc, n, rate = 1, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rexp.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rexp.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nrate\nRate of the exponential distribution (default: 1). The exponential\n\n\n\ndistribution with rate lambda has mean 1 / lambda and density f(x) = lambda e^- lambda x. num_partitions | Number of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster). seed | Random seed (default: a random long integer). output_col | Name of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rexp.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rexp.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgamma.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rgamma.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Gamma distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgamma.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rgamma.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_rgamma(\n  sc,\n  n,\n  shape,\n  rate = 1,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgamma.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rgamma.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nshape\nShape parameter (greater than 0) for the Gamma distribution.\n\n\nrate\nRate parameter (greater than 0) for the Gamma distribution (scale is 1/rate).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe\n\n\n\n(default: default parallelism of the Spark cluster). seed | Random seed (default: a random long integer). output_col | Name of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgamma.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rgamma.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgeom.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rgeom.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a geometric distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgeom.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rgeom.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_rgeom(sc, n, prob, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgeom.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rgeom.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nprob\nProbability of success in each trial.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe\n\n\n\n(default: default parallelism of the Spark cluster). seed | Random seed (default: a random long integer). output_col | Name of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rgeom.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rgeom.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rhyper.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rhyper.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a hypergeometric distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rhyper.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rhyper.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_rhyper(\n  sc,\n  nn,\n  m,\n  n,\n  k,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rhyper.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rhyper.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nnn\nSample Size.\n\n\nm\nThe number of successes among the population.\n\n\nn\nThe number of failures among the population.\n\n\nk\nThe number of draws.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe\n\n\n\n(default: default parallelism of the Spark cluster). seed | Random seed (default: a random long integer). output_col | Name of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rhyper.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rhyper.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rlnorm.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rlnorm.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a log normal distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rlnorm.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rlnorm.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_rlnorm(\n  sc,\n  n,\n  meanlog = 0,\n  sdlog = 1,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rlnorm.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rlnorm.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nmeanlog\nThe mean of the normally distributed natural logarithm of this distribution.\n\n\nsdlog\nThe Standard deviation of the normally distributed natural logarithm of this distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe\n\n\n\n(default: default parallelism of the Spark cluster). seed | Random seed (default: a random long integer). output_col | Name of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rlnorm.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rlnorm.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rnorm.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rnorm.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from the standard normal distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rnorm.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rnorm.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_rnorm(\n  sc,\n  n,\n  mean = 0,\n  sd = 1,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rnorm.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rnorm.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nmean\nThe mean value of the normal distribution.\n\n\nsd\nThe standard deviation of the normal distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe\n\n\n\n(default: default parallelism of the Spark cluster). seed | Random seed (default: a random long integer). output_col | Name of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rnorm.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rnorm.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rpois(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rpois.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rpois.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Poisson distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rpois.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rpois.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_rpois(sc, n, lambda, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rpois.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rpois.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nlambda\nMean, or lambda, of the Poisson distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe\n\n\n\n(default: default parallelism of the Spark cluster). seed | Random seed (default: a random long integer). output_col | Name of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rpois.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rpois.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rt(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rt.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rt.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a t-distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rt.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rt.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_rt(sc, n, df, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rt.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rt.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\ndf\nDegrees of freedom (> 0, maybe non-integer).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe\n\n\n\n(default: default parallelism of the Spark cluster). seed | Random seed (default: a random long integer). output_col | Name of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rt.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rt.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_runif(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_runif.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_runif.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from the uniform distribution U(0, 1)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_runif.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_runif.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_runif(\n  sc,\n  n,\n  min = 0,\n  max = 1,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_runif.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_runif.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nmin\nThe lower limit of the distribution.\n\n\nmax\nThe upper limit of the distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe\n\n\n\n(default: default parallelism of the Spark cluster). seed | Random seed (default: a random long integer). output_col | Name of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_runif.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_runif.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_rweibull()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rweibull.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_rweibull.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Weibull distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rweibull.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_rweibull.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_rweibull(\n  sc,\n  n,\n  shape,\n  scale = 1,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rweibull.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_rweibull.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nshape\nThe shape of the Weibull distribution.\n\n\nscale\nThe scale of the Weibull distribution (default: 1).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe\n\n\n\n(default: default parallelism of the Spark cluster). seed | Random seed (default: a random long integer). output_col | Name of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_rweibull.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_rweibull.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark statistical routines: sdf_rbeta(), sdf_rbinom(), sdf_rcauchy(), sdf_rchisq(), sdf_rexp(), sdf_rgamma(), sdf_rgeom(), sdf_rhyper(), sdf_rlnorm(), sdf_rnorm(), sdf_rpois(), sdf_rt(), sdf_runif()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sample.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_sample.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nDraw a random sample of rows (with or without replacement) from a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sample.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_sample.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_sample(x, fraction = 1, replacement = TRUE, seed = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sample.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_sample.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\nfraction\nThe fraction to sample.\n\n\nreplacement\nBoolean; sample with replacement?\n\n\nseed\nAn (optional) integer seed."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sample.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_sample.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_copy_to(), sdf_distinct(), sdf_random_split(), sdf_register(), sdf_sort(), sdf_weighted_sample()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead the schema of a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_schema(x, expand_nested_cols = FALSE, expand_struct_cols = FALSE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nexpand_nested_cols\nWhether to expand columns containing nested array\n\n\n\nof structs (which are usually created by tidyr::nest on a Spark data frame) expand_struct_cols | Whether to expand columns containing structs"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html#details",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nThe type column returned gives the string representation of the underlying Spark type for that column; for example, a vector of numeric values would be returned with the type \"DoubleType\". Please see the https://spark.apache.org/docs/latest/api/scala/index.htmlSpark Scala API Documentation for information on what types are available and exposed by Spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_schema.html#value",
    "href": "packages/sparklyr/latest/reference/sdf_schema.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nAn list, with each list element describing the name and type of a column."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_separate_column.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_separate_column.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGiven a vector column in a Spark DataFrame, split that into n separate columns, each column made up of the different elements in the column column."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_separate_column.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_separate_column.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_separate_column(x, column, into = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_separate_column.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_separate_column.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\ncolumn\nThe name of a (vector-typed) column.\n\n\ninto\nA specification of the columns that should be\n\n\n\ngenerated from column. This can either be a vector of column names, or an list mapping column names to the (1-based) index at which a particular vector element should be extracted."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_seq.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_seq.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCreates a DataFrame for the given range"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_seq.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_seq.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_seq(\n  sc,\n  from = 1L,\n  to = 1L,\n  by = 1L,\n  repartition = NULL,\n  type = c(\"integer\", \"integer64\")\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_seq.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_seq.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nfrom, to\nThe start and end to use as a range\n\n\nby\nThe increment of the sequence.\n\n\nrepartition\nThe number of partitions to use when distributing the\n\n\n\ndata across the Spark cluster. Defaults to the minimum number of partitions. type | The data type to use for the index, either \"integer\" or \"integer64\"."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSort a Spark DataFrame by one or more columns, with each column sorted in ascending order."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_sort(x, columns)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\ncolumns\nThe column(s) to sort by."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sort.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_sort.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_copy_to(), sdf_distinct(), sdf_random_split(), sdf_register(), sdf_sample(), sdf_weighted_sample()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sql.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_sql.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nDefines a Spark DataFrame from a SQL query, useful to create Spark DataFrames without collecting the results immediately."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sql.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_sql.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_sql(sc, sql)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_sql.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_sql.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nsql\na ‘SQL’ query used to generate a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_to_avro.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_to_avro.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nConvert column(s) to avro format"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_to_avro.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_to_avro.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_to_avro(x, cols = colnames(x))"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_to_avro.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_to_avro.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercible to a Spark DataFrame\n\n\ncols\nSubset of Columns to convert into avro format"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nExpand a struct column or an array column within a Spark dataframe into one or more rows, similar what to tidyr::unnest_longer does to an R dataframe. An index column, if included, will be 1-based if col is an array column."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_unnest_longer(\n  data,\n  col,\n  values_to = NULL,\n  indices_to = NULL,\n  include_indices = NULL,\n  names_repair = \"check_unique\",\n  ptype = list(),\n  transform = list()\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\ndata\nThe Spark dataframe to be unnested\n\n\ncol\nThe struct column to extract components from\n\n\nvalues_to\nName of column to store vector values. Defaults to col.\n\n\nindices_to\nA string giving the name of column which will contain the\n\n\n\ninner names or position (if not named) of the values. Defaults to col with _id suffix include_indices | Whether to include an index column. An index column will be included by default if col is a struct column. It will also be included if indices_to is not NULL. names_repair | Strategy for fixing duplicate column names (the semantic will be exactly identical to that of .name_repair option in tibble) ptype | Optionally, supply an R data frame prototype for the output. Each column of the unnested result will be casted based on the Spark equivalent of the type of the column with the same name within ptype, e.g., if ptype has a column x of type character, then column x of the unnested result will be casted from its original SQL type to StringType. transform | Optionally, a named list of transformation functions applied"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#examples",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_longer.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.4.0\")\n\n# unnesting a struct column\nsdf <- copy_to(\n  sc,\n  tibble::tibble(\n    x = 1:3,\n    y = list(list(a = 1, b = 2), list(a = 3, b = 4), list(a = 5, b = 6))\n  )\n)\n\nunnested <- sdf %>% sdf_unnest_longer(y, indices_to = \"attr\")\n\n# unnesting an array column\nsdf <- copy_to(\n  sc,\n  tibble::tibble(\n    x = 1:3,\n    y = list(1:10, 1:5, 1:2)\n  )\n)\n\nunnested <- sdf %>% sdf_unnest_longer(y, indices_to = \"array_idx\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFlatten a struct column within a Spark dataframe into one or more columns, similar what to tidyr::unnest_wider does to an R dataframe"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_unnest_wider(\n  data,\n  col,\n  names_sep = NULL,\n  names_repair = \"check_unique\",\n  ptype = list(),\n  transform = list()\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\ndata\nThe Spark dataframe to be unnested\n\n\ncol\nThe struct column to extract components from\n\n\nnames_sep\nIf NULL, the default, the names will be left as is.\n\n\n\nIf a string, the inner and outer names will be pasted together using names_sep as the delimiter. names_repair | Strategy for fixing duplicate column names (the semantic will be exactly identical to that of .name_repair option in tibble) ptype | Optionally, supply an R data frame prototype for the output. Each column of the unnested result will be casted based on the Spark equivalent of the type of the column with the same name within ptype, e.g., if ptype has a column x of type character, then column x of the unnested result will be casted from its original SQL type to StringType. transform | Optionally, a named list of transformation functions applied to each component (e.g., list(x = as.character) to cast column x to String)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#examples",
    "href": "packages/sparklyr/latest/reference/sdf_unnest_wider.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.4.0\")\n\nsdf <- copy_to(\n  sc,\n  tibble::tibble(\n    x = 1:3,\n    y = list(list(a = 1, b = 2), list(a = 3, b = 4), list(a = 5, b = 6))\n  )\n)\n\n# flatten struct column 'y' into two separate columns 'y_a' and 'y_b'\nunnested <- sdf %>% sdf_unnest_wider(y, names_sep = \"_\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nDraw a random sample of rows (with or without replacement) from a Spark DataFrame If the sampling is done without replacement, then it will be conceptually equivalent to an iterative process such that in each step the probability of adding a row to the sample set is equal to its weight divided by summation of weights of all rows that are not in the sample set yet in that step."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_weighted_sample(x, weight_col, k, replacement = TRUE, seed = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\nweight_col\nName of the weight column\n\n\nk\nSample set size\n\n\nreplacement\nWhether to sample with replacement\n\n\nseed\nAn (optional) integer seed"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#see-also",
    "href": "packages/sparklyr/latest/reference/sdf_weighted_sample.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark data frames: sdf_copy_to(), sdf_distinct(), sdf_random_split(), sdf_register(), sdf_sample(), sdf_sort()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_sequential_id.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_with_sequential_id.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nAdd a sequential ID column to a Spark DataFrame. The Spark zipWithIndex function is used to produce these. This differs from sdf_with_unique_id in that the IDs generated are independent of partitioning."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_sequential_id.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_with_sequential_id.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_with_sequential_id(x, id = \"id\", from = 1L)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_sequential_id.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_with_sequential_id.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nid\nThe name of the column to host the generated IDs.\n\n\nfrom\nThe starting value of the id column"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#description",
    "href": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nAdd a unique ID column to a Spark DataFrame. The Spark monotonicallyIncreasingId function is used to produce these and is guaranteed to produce unique, monotonically increasing ids; however, there is no guarantee that these IDs will be sequential. The table is persisted immediately after the column is generated, to ensure that the column is stable – otherwise, it can differ across new computations."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#usage",
    "href": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsdf_with_unique_id(x, id = \"id\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#arguments",
    "href": "packages/sparklyr/latest/reference/sdf_with_unique_id.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nid\nThe name of the column to host the generated IDs."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/select.html#description",
    "href": "packages/sparklyr/latest/reference/select.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee select for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/separate.html#description",
    "href": "packages/sparklyr/latest/reference/separate.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee separate for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-api.html#description",
    "href": "packages/sparklyr/latest/reference/spark-api.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nAccess the commonly-used Spark objects associated with a Spark instance. These objects provide access to different facets of the Spark API."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-api.html#usage",
    "href": "packages/sparklyr/latest/reference/spark-api.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_context(sc)\n\njava_context(sc)\n\nhive_context(sc)\n\nspark_session(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-api.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark-api.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-api.html#details",
    "href": "packages/sparklyr/latest/reference/spark-api.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nThe https://spark.apache.org/docs/latest/api/scala/Scala API documentation is useful for discovering what methods are available for each of these objects. Use invoke to call methods on these objects."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html#description",
    "href": "packages/sparklyr/latest/reference/spark-connections.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThese routines allow you to manage your connections to Spark.\nCall spark_disconnect() on each open Spark connection"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html#usage",
    "href": "packages/sparklyr/latest/reference/spark-connections.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_connect(\n  master,\n  spark_home = Sys.getenv(\"SPARK_HOME\"),\n  method = c(\"shell\", \"livy\", \"databricks\", \"test\", \"qubole\"),\n  app_name = \"sparklyr\",\n  version = NULL,\n  config = spark_config(),\n  extensions = sparklyr::registered_extensions(),\n  packages = NULL,\n  scala_version = NULL,\n  ...\n)\n\nspark_connection_is_open(sc)\n\nspark_disconnect(sc, ...)\n\nspark_disconnect_all(...)\n\nspark_submit(\n  master,\n  file,\n  spark_home = Sys.getenv(\"SPARK_HOME\"),\n  app_name = \"sparklyr\",\n  version = NULL,\n  config = spark_config(),\n  extensions = sparklyr::registered_extensions(),\n  scala_version = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark-connections.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nmaster\nSpark cluster url to connect to. Use \"local\" to\n\n\n\nconnect to a local instance of Spark installed via spark_install. spark_home | The path to a Spark installation. Defaults to the path provided by the SPARK_HOME environment variable. If SPARK_HOME is defined, it will always be used unless the version parameter is specified to force the use of a locally installed version. method | The method used to connect to Spark. Default connection method is \"shell\" to connect using spark-submit, use \"livy\" to perform remote connections using HTTP, or \"databricks\" when using a Databricks clusters. app_name | The application name to be used while running in the Spark cluster. version | The version of Spark to use. Required for \"local\" Spark connections, optional otherwise. config | Custom configuration for the generated Spark connection. See spark_config for details. extensions | Extension R packages to enable for this connection. By default, all packages enabled through the use of sparklyr::register_extension will be passed here. packages | A list of Spark packages to load. For example, \"delta\" or \"kafka\" to enable Delta Lake or Kafka. Also supports full versions like \"io.delta:delta-core_2.11:0.4.0\". This is similar to adding packages into the sparklyr.shell.packages configuration option. Notice that the version parameter is used to choose the correct package, otherwise assumes the latest version is being used. scala_version | Load the sparklyr jar file that is built with the version of Scala specified (this currently only makes sense for Spark 2.4, where sparklyr will by default assume Spark 2.4 on current host is built with Scala 2.11, and therefore scala_version = '2.12' is needed if sparklyr is connecting to Spark 2.4 built with Scala 2.12) … | Additional params to be passed to each spark_disconnect() call (e.g., terminate = TRUE) sc | A spark_connection. file | Path to R source file to submit for batch execution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html#details",
    "href": "packages/sparklyr/latest/reference/spark-connections.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nBy default, when using method = \"livy\", jars are downloaded from GitHub. But an alternative path (local to Livy server or on HDFS or HTTP(s)) to sparklyr JAR can also be specified through the sparklyr.livy.jar setting."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark-connections.html#examples",
    "href": "packages/sparklyr/latest/reference/spark-connections.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\nconf <- spark_config()\nconf$`sparklyr.shell.conf` <- c(\n  \"spark.executor.extraJavaOptions=-Duser.timezone='UTC'\",\n  \"spark.driver.extraJavaOptions=-Duser.timezone='UTC'\",\n  \"spark.sql.session.timeZone='UTC'\"\n)\n\nsc <- spark_connect(\n  master = \"spark://HOST:PORT\", config = conf\n)\nconnection_is_open(sc)\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#description",
    "href": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieves or sets whether Spark adaptive query execution is enabled"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_adaptive_query_execution(sc, enable = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nenable\nWhether to enable Spark adaptive query execution. Defaults to\n\n\n\nNULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_adaptive_query_execution.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_advisory_shuffle_partition_size(), spark_auto_broadcast_join_threshold(), spark_coalesce_initial_num_partitions(), spark_coalesce_min_num_partitions(), spark_coalesce_shuffle_partitions(), spark_session_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#description",
    "href": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieves or sets advisory size in bytes of the shuffle partition during adaptive optimization"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_advisory_shuffle_partition_size(sc, size = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nsize\nAdvisory size in bytes of the shuffle partition.\n\n\n\nDefaults to NULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_advisory_shuffle_partition_size.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_adaptive_query_execution(), spark_auto_broadcast_join_threshold(), spark_coalesce_initial_num_partitions(), spark_coalesce_min_num_partitions(), spark_coalesce_shuffle_partitions(), spark_session_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply.html#description",
    "href": "packages/sparklyr/latest/reference/spark_apply.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nApplies an R function to a Spark object (typically, a Spark DataFrame)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_apply.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_apply(\n  x,\n  f,\n  columns = NULL,\n  memory = TRUE,\n  group_by = NULL,\n  packages = NULL,\n  context = NULL,\n  name = NULL,\n  barrier = NULL,\n  fetch_result_as_sdf = TRUE,\n  partition_index_param = \"\",\n  arrow_max_records_per_batch = NULL,\n  auto_deps = FALSE,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_apply.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object (usually a spark_tbl) coercable to a Spark DataFrame.\n\n\nf\nA function that transforms a data frame partition into a data frame.\n\n\n\nThe function f has signature f(df, context, group1, group2, ...) where df is a data frame with the data to be processed, context is an optional object passed as the context parameter and group1 to groupN contain the values of the group_by values. When group_by is not specified, f takes only one argument.\nCan also be an rlang anonymous function. For example, as ~ .x + 1 to define an expression that adds one to the given .x data frame. columns | A vector of column names or a named vector of column types for the transformed object. When not specified, a sample of 10 rows is taken to infer out the output columns automatically, to avoid this performance penalty, specify the column types. The sample size is configurable using the sparklyr.apply.schema.infer configuration option. memory | Boolean; should the table be cached into memory? group_by | Column name used to group by data frame partitions. packages | Boolean to distribute .libPaths() packages to each node, a list of packages to distribute, or a package bundle created with spark_apply_bundle().\nDefaults to TRUE or the sparklyr.apply.packages value set in spark_config().\nFor clusters using Yarn cluster mode, packages can point to a package bundle created using spark_apply_bundle() and made available as a Spark file using config$sparklyr.shell.files. For clusters using Livy, packages can be manually installed on the driver node.\nFor offline clusters where available.packages() is not available, manually download the packages database from https://cran.r-project.org/web/packages/packages.rds and set Sys.setenv(sparklyr.apply.packagesdb = \"<pathl-to-rds>\"). Otherwise, all packages will be used by default.\nFor clusters where R packages already installed in every worker node, the spark.r.libpaths config entry can be set in spark_config() to the local packages library. To specify multiple paths collapse them (without spaces) with a comma delimiter (e.g., \"/lib/path/one,/lib/path/two\"). context | Optional object to be serialized and passed back to f(). name | Optional table name while registering the resulting data frame. barrier | Optional to support Barrier Execution Mode in the scheduler. fetch_result_as_sdf | Whether to return the transformed results in a Spark Dataframe (defaults to TRUE). When set to FALSE, results will be returned as a list of R objects instead.\nNOTE: fetch_result_as_sdf must be set to FALSE when the transformation function being applied is returning R objects that cannot be stored in a Spark Dataframe (e.g., complex numbers or any other R data type that does not have an equivalent representation among Spark SQL data types). partition_index_param | Optional if non-empty, then f also receives the index of the partition being processed as a named argument with this name, in addition to all positional argument(s) it will receive\nNOTE: when fetch_result_as_sdf is set to FALSE, object returned from the transformation function also must be serializable by the base::serialize function in R. arrow_max_records_per_batch | Maximum size of each Arrow record batch, ignored if Arrow serialization is not enabled. auto_deps | [Experimental] Whether to infer all required R packages by examining the closure f() and only distribute required R and their transitive dependencies to Spark worker nodes (default: FALSE). NOTE: this option will only take effect if packages is set to TRUE or is a character vector of R package names. If packages is a character vector of R package names, then both the set of packages specified by packages and the set of inferred packages will be distributed to Spark workers. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply.html#examples",
    "href": "packages/sparklyr/latest/reference/spark_apply.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local[3]\")\n\n# creates an Spark data frame with 10 elements then multiply times 10 in R\nsdf_len(sc, 10) %>% spark_apply(function(df) df * 10)\n\n# using barrier mode\nsdf_len(sc, 3, repartition = 3) %>%\n  spark_apply(nrow, barrier = TRUE, columns = c(id = \"integer\")) %>%\n  collect()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_bundle.html#description",
    "href": "packages/sparklyr/latest/reference/spark_apply_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCreates a bundle of packages for spark_apply()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_bundle.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_apply_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_apply_bundle(packages = TRUE, base_path = getwd(), session_id = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_bundle.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_apply_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\npackages\nList of packages to pack or TRUE to pack all.\n\n\nbase_path\nBase path used to store the resulting bundle.\n\n\nsession_id\nAn optional ID string to include in the bundle file name to allow the bundle to be session-specific"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_log.html#description",
    "href": "packages/sparklyr/latest/reference/spark_apply_log.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nWrites data to log under spark_apply()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_log.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_apply_log.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_apply_log(..., level = \"INFO\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_apply_log.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_apply_log.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\n…\nArguments to write to log.\n\n\nlevel\nSeverity level for this entry; recommended values: INFO,\n\n\n\nERROR or WARN."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#description",
    "href": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nConfigures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command ANALYZE TABLE <tableName> COMPUTE STATISTICS noscan has been run, and file-based data source tables where the statistics are computed directly on the files of data."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_auto_broadcast_join_threshold(sc, threshold = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nthreshold\nMaximum size in bytes for a table that will be broadcast to all worker nodes\n\n\n\nwhen performing a join. Defaults to NULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_auto_broadcast_join_threshold.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_adaptive_query_execution(), spark_advisory_shuffle_partition_size(), spark_coalesce_initial_num_partitions(), spark_coalesce_min_num_partitions(), spark_coalesce_shuffle_partitions(), spark_session_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#description",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieves or sets initial number of shuffle partitions before coalescing"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_coalesce_initial_num_partitions(sc, num_partitions = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nnum_partitions\nInitial number of shuffle partitions before coalescing.\n\n\n\nDefaults to NULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_initial_num_partitions.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_adaptive_query_execution(), spark_advisory_shuffle_partition_size(), spark_auto_broadcast_join_threshold(), spark_coalesce_min_num_partitions(), spark_coalesce_shuffle_partitions(), spark_session_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#description",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieves or sets the minimum number of shuffle partitions after coalescing"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_coalesce_min_num_partitions(sc, num_partitions = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nnum_partitions\nMinimum number of shuffle partitions after coalescing.\n\n\n\nDefaults to NULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_min_num_partitions.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_adaptive_query_execution(), spark_advisory_shuffle_partition_size(), spark_auto_broadcast_join_threshold(), spark_coalesce_initial_num_partitions(), spark_coalesce_shuffle_partitions(), spark_session_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#description",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieves or sets whether coalescing contiguous shuffle partitions is enabled"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_coalesce_shuffle_partitions(sc, enable = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nenable\nWhether to enable coalescing of contiguous shuffle partitions.\n\n\n\nDefaults to NULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_coalesce_shuffle_partitions.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_adaptive_query_execution(), spark_advisory_shuffle_partition_size(), spark_auto_broadcast_join_threshold(), spark_coalesce_initial_num_partitions(), spark_coalesce_min_num_partitions(), spark_session_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compilation_spec.html#description",
    "href": "packages/sparklyr/latest/reference/spark_compilation_spec.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFor use with compile_package_jars. The Spark compilation specification is used when compiling Spark extension Java Archives, and defines which versions of Spark, as well as which versions of Scala, should be used for compilation."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compilation_spec.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_compilation_spec.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_compilation_spec(\n  spark_version = NULL,\n  spark_home = NULL,\n  scalac_path = NULL,\n  scala_filter = NULL,\n  jar_name = NULL,\n  jar_path = NULL,\n  jar_dep = NULL,\n  embedded_srcs = \"embedded_sources.R\"\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compilation_spec.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_compilation_spec.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nspark_version\nThe Spark version to build against. This can\n\n\n\nbe left unset if the path to a suitable Spark home is supplied. spark_home | The path to a Spark home installation. This can be left unset if spark_version is supplied; in such a case, sparklyr will attempt to discover the associated Spark installation using spark_home_dir. scalac_path | The path to the scalac compiler to be used during compilation of your Spark extension. Note that you should ensure the version of scalac selected matches the version of scalac used with the version of Spark you are compiling against. scala_filter | An optional function that can be used to filter which scala files are used during compilation. This can be useful if you have auxiliary files that should only be included with certain versions of Spark. jar_name | The name to be assigned to the generated jar. jar_path | The path to the jar tool to be used during compilation of your Spark extension. jar_dep | An optional list of additional jar dependencies. embedded_srcs | Embedded source file(s) under <R package root>/java to be included in the root of the resulting jar file as resources"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compilation_spec.html#details",
    "href": "packages/sparklyr/latest/reference/spark_compilation_spec.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nMost Spark extensions won’t need to define their own compilation specification, and can instead rely on the default behavior of compile_package_jars."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compile.html#description",
    "href": "packages/sparklyr/latest/reference/spark_compile.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGiven a set of scala source files, compile them into a Java Archive (jar)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compile.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_compile.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_compile(\n  jar_name,\n  spark_home = NULL,\n  filter = NULL,\n  scalac = NULL,\n  jar = NULL,\n  jar_dep = NULL,\n  embedded_srcs = \"embedded_sources.R\"\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_compile.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_compile.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nspark_home\nThe path to the Spark sources to be used\n\n\n\nalongside compilation. filter | An optional function, used to filter out discovered scala files during compilation. This can be used to ensure that e.g. certain files are only compiled with certain versions of Spark, and so on. scalac | The path to the scalac program to be used, for compilation of scala files. jar | The path to the jar program to be used, for generating of the resulting jar. jar_dep | An optional list of additional jar dependencies. embedded_srcs | Embedded source file(s) under <R package root>/java to be included in the root of the resulting jar file as resources name | The name to assign to the target jar."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html#description",
    "href": "packages/sparklyr/latest/reference/spark_config.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead Spark Configuration"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_config.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_config(file = \"config.yml\", use_default = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_config.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nfile\nName of the configuration file\n\n\nuse_default\nTRUE to use the built-in defaults provided in this package"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html#details",
    "href": "packages/sparklyr/latest/reference/spark_config.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nRead Spark configuration using the config package."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config.html#value",
    "href": "packages/sparklyr/latest/reference/spark_config.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nNamed list with configuration data"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_exists.html#description",
    "href": "packages/sparklyr/latest/reference/spark_config_exists.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nA helper function to check value exist under spark_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_exists.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_config_exists.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_config_exists(config, name, default = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_exists.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_config_exists.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nconfig\nThe configuration list from spark_config()\n\n\nname\nThe name of the configuration entry\n\n\ndefault\nThe default value to use when entry is not present"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#description",
    "href": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nConvenience function to initialize a Kubernetes configuration instead of spark_config(), exposes common properties to set in Kubernetes clusters."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_config_kubernetes(\n  master,\n  version = \"2.3.2\",\n  image = \"spark:sparklyr\",\n  driver = random_string(\"sparklyr-\"),\n  account = \"spark\",\n  jars = \"local:///opt/sparklyr\",\n  forward = TRUE,\n  executors = NULL,\n  conf = NULL,\n  timeout = 120,\n  ports = c(8880, 8881, 4040),\n  fix_config = identical(.Platform$OS.type, \"windows\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_config_kubernetes.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nmaster\nKubernetes url to connect to, found by running kubectl cluster-info.\n\n\nversion\nThe version of Spark being used.\n\n\nimage\nContainer image to use to launch Spark and sparklyr. Also known\n\n\n\nas spark.kubernetes.container.image. driver | Name of the driver pod. If not set, the driver pod name is set to “sparklyr” suffixed by id to avoid name conflicts. Also known as spark.kubernetes.driver.pod.name. account | Service account that is used when running the driver pod. The driver pod uses this service account when requesting executor pods from the API server. Also known as spark.kubernetes.authenticate.driver.serviceAccountName. jars | Path to the sparklyr jars; either, a local path inside the container image with the sparklyr jars copied when the image was created or, a path accesible by the container where the sparklyr jars were copied. You can find a path to the sparklyr jars by running system.file(\"java/\", package = \"sparklyr\"). forward | Should ports used in sparklyr be forwarded automatically through Kubernetes? Default to TRUE which runs kubectl port-forward and pkill kubectl on disconnection. executors | Number of executors to request while connecting. conf | A named list of additional entries to add to sparklyr.shell.conf. timeout | Total seconds to wait before giving up on connection. ports | Ports to forward using kubectl. fix_config | Should the spark-defaults.conf get fixed? TRUE for Windows. … | Additional parameters, currently not in use."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_packages.html#description",
    "href": "packages/sparklyr/latest/reference/spark_config_packages.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCreates Spark Configuration"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_packages.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_config_packages.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_config_packages(config, packages, version, scala_version = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_packages.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_config_packages.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nconfig\nThe Spark configuration object.\n\n\npackages\nA list of named packages or versioned packagese to add.\n\n\nversion\nThe version of Spark being used.\n\n\nscala_version\nAcceptable Scala version of packages to be loaded\n\n\n…\nAdditional configurations"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_settings.html#description",
    "href": "packages/sparklyr/latest/reference/spark_config_settings.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieves available sparklyr settings that can be used in configuration files or spark_config()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_settings.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_config_settings.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_config_settings()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_value.html#description",
    "href": "packages/sparklyr/latest/reference/spark_config_value.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nA helper function to retrieve values from spark_config()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_value.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_config_value.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_config_value(config, name, default = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_config_value.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_config_value.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nconfig\nThe configuration list from spark_config()\n\n\nname\nThe name of the configuration entry\n\n\ndefault\nThe default value to use when entry is not present"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_configuration.html#description",
    "href": "packages/sparklyr/latest/reference/spark_configuration.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieves or sets runtime configuration entries for the Spark Session"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_configuration.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_configuration.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_session_config(sc, config = TRUE, value = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_configuration.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_configuration.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nconfig\nThe configuration entry name(s) (e.g., \"spark.sql.shuffle.partitions\").\n\n\n\nDefaults to NULL to retrieve all configuration entries. value | The configuration value to be set. Defaults to NULL to retrieve configuration entries."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_configuration.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_configuration.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark runtime configuration: spark_adaptive_query_execution(), spark_advisory_shuffle_partition_size(), spark_auto_broadcast_join_threshold(), spark_coalesce_initial_num_partitions(), spark_coalesce_min_num_partitions(), spark_coalesce_shuffle_partitions()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection-class.html#description",
    "href": "packages/sparklyr/latest/reference/spark_connection-class.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nspark_connection class"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection.html#description",
    "href": "packages/sparklyr/latest/reference/spark_connection.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieve the spark_connection associated with an object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_connection.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_connection(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_connection.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object from which a spark_connection can be obtained.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection_find.html#description",
    "href": "packages/sparklyr/latest/reference/spark_connection_find.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFinds an active spark connection in the environment given the connection parameters."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection_find.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_connection_find.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_connection_find(master = NULL, app_name = NULL, method = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_connection_find.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_connection_find.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nmaster\nThe Spark master parameter.\n\n\napp_name\nThe Spark application name.\n\n\nmethod\nThe method used to connect to Spark."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_context_config.html#description",
    "href": "packages/sparklyr/latest/reference/spark_context_config.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieves the runtime configuration interface for the Spark Context."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_context_config.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_context_config.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_context_config(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_context_config.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_context_config.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dataframe.html#description",
    "href": "packages/sparklyr/latest/reference/spark_dataframe.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis S3 generic is used to access a Spark DataFrame object (as a Java object reference) from an object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dataframe.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_dataframe.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_dataframe(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dataframe.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_dataframe.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object wrapping, or containing, a Spark DataFrame.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dataframe.html#value",
    "href": "packages/sparklyr/latest/reference/spark_dataframe.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA spark_jobj representing a Java object reference to a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#description",
    "href": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis is the default compilation specification used for Spark extensions, when used with compile_package_jars."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_default_compilation_spec(\n  pkg = infer_active_package_name(),\n  locations = NULL\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_default_compilation_spec.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\npkg\nThe package containing Spark extensions to be compiled.\n\n\nlocations\nAdditional locations to scan. By default, the\n\n\n\ndirectories /opt/scala and /usr/local/scala will be scanned."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_version.html#description",
    "href": "packages/sparklyr/latest/reference/spark_default_version.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\ndetermine the version that will be used by default if version is NULL"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_default_version.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_default_version.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_default_version()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency.html#description",
    "href": "packages/sparklyr/latest/reference/spark_dependency.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nDefine a Spark dependency consisting of a set of custom JARs, Spark packages, and customized dbplyr SQL translation env."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_dependency.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_dependency(\n  jars = NULL,\n  packages = NULL,\n  initializer = NULL,\n  catalog = NULL,\n  repositories = NULL,\n  dbplyr_sql_variant = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_dependency.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\njars\nCharacter vector of full paths to JAR files.\n\n\npackages\nCharacter vector of Spark packages names.\n\n\ninitializer\nOptional callback function called when initializing a connection.\n\n\ncatalog\nOptional location where extension JAR files can be downloaded for Livy.\n\n\nrepositories\nCharacter vector of Spark package repositories.\n\n\ndbplyr_sql_variant\nCustomization of dbplyr SQL translation env. Must be a\n\n\n\nnamed list of the following form: list( scalar = list(scalar_fn1 = …, scalar_fn2 = …, ), aggregate = list(agg_fn1 = …, agg_fn2 = …, ), window = list(wnd_fn1 = …, wnd_fn2 = …, ) ) See sql_variant for details. … | Additional optional arguments."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency.html#value",
    "href": "packages/sparklyr/latest/reference/spark_dependency.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nAn object of type spark_dependency"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#description",
    "href": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nHelper function to assist falling back to previous Spark versions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_dependency_fallback(spark_version, supported_versions)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nspark_version\nThe Spark version being requested in spark_dependencies.\n\n\nsupported_versions\nThe Spark versions that are supported by this extension."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#value",
    "href": "packages/sparklyr/latest/reference/spark_dependency_fallback.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA Spark version to use."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_extension.html#description",
    "href": "packages/sparklyr/latest/reference/spark_extension.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCreates an R package ready to be used as an Spark extension."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_extension.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_extension.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_extension(path)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_extension.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_extension.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\npath\nLocation where the extension will be created."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_get_java.html#description",
    "href": "packages/sparklyr/latest/reference/spark_get_java.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFinds the path to JAVA_HOME."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_get_java.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_get_java.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_get_java(throws = FALSE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_get_java.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_get_java.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nthrows\nThrow an error when path not found?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_dir.html#description",
    "href": "packages/sparklyr/latest/reference/spark_home_dir.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFind the SPARK_HOME directory for a given version of Spark that was previously installed using spark_install."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_dir.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_home_dir.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_home_dir(version = NULL, hadoop_version = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_dir.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_home_dir.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nversion\nVersion of Spark\n\n\nhadoop_version\nVersion of Hadoop"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_dir.html#value",
    "href": "packages/sparklyr/latest/reference/spark_home_dir.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nPath to SPARK_HOME (or NULL if the specified version was not found)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html#description",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSet the SPARK_HOME environment variable. This slightly speeds up some operations, including the connection time."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_home_set(path = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\npath\nA string containing the path to the installation location of\n\n\n\nSpark. If NULL, the path to the most latest Spark/Hadoop versions is used. … | Additional parameters not currently used."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html#value",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe function is mostly invoked for the side-effect of setting the SPARK_HOME environment variable. It also returns TRUE if the environment was successfully set, and FALSE otherwise."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_home_set.html#examples",
    "href": "packages/sparklyr/latest/reference/spark_home_set.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n# Not run due to side-effects\nspark_home_set()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install.html#description",
    "href": "packages/sparklyr/latest/reference/spark_install.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nInstall versions of Spark for use with local Spark connections (i.e. spark_connect(master = \"local\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_install.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_install(\n  version = NULL,\n  hadoop_version = NULL,\n  reset = TRUE,\n  logging = \"INFO\",\n  verbose = interactive()\n)\n\nspark_uninstall(version, hadoop_version)\n\nspark_install_dir()\n\nspark_install_tar(tarfile)\n\nspark_installed_versions()\n\nspark_available_versions(\n  show_hadoop = FALSE,\n  show_minor = FALSE,\n  show_future = FALSE\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_install.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nversion\nVersion of Spark to install. See spark_available_versions for a list of supported versions\n\n\nhadoop_version\nVersion of Hadoop to install. See spark_available_versions for a list of supported versions\n\n\nreset\nAttempts to reset settings to defaults.\n\n\nlogging\nLogging level to configure install. Supported options: “WARN”, “INFO”\n\n\nverbose\nReport information as Spark is downloaded / installed\n\n\ntarfile\nPath to TAR file conforming to the pattern spark-###-bin-(hadoop)?### where ###\n\n\n\nreference spark and hadoop versions respectively. show_hadoop | Show Hadoop distributions? show_minor | Show minor Spark versions? show_future | Should future versions which have not been released be shown?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install.html#value",
    "href": "packages/sparklyr/latest/reference/spark_install.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nList with information about the installed version."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_find.html#description",
    "href": "packages/sparklyr/latest/reference/spark_install_find.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFind a given Spark installation by version."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_find.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_install_find.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_install_find(\n  version = NULL,\n  hadoop_version = NULL,\n  installed_only = TRUE,\n  latest = FALSE,\n  hint = FALSE\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_find.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_install_find.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nversion\nVersion of Spark to install. See spark_available_versions for a list of supported versions\n\n\nhadoop_version\nVersion of Hadoop to install. See spark_available_versions for a list of supported versions\n\n\ninstalled_only\nSearch only the locally installed versions?\n\n\nlatest\nCheck for latest version?\n\n\nhint\nOn failure should the installation code be provided?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_sync.html#description",
    "href": "packages/sparklyr/latest/reference/spark_install_sync.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee: https://github.com/rstudio/spark-install"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_sync.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_install_sync.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_install_sync(project_path)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_install_sync.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_install_sync.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nproject_path\nThe path to the sparkinstall project"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj-class.html#description",
    "href": "packages/sparklyr/latest/reference/spark_jobj-class.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nspark_jobj class"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj.html#description",
    "href": "packages/sparklyr/latest/reference/spark_jobj.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis S3 generic is used for accessing the underlying Java Virtual Machine (JVM) Spark objects associated with objects. These objects act as references to Spark objects living in the JVM. Methods on these objects can be called with the invoke family of functions."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_jobj.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_jobj(x, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_jobj.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object containing, or wrapping, a spark_jobj.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_jobj.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_jobj.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\ninvoke, for calling methods on Java object references."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_load_table.html#description",
    "href": "packages/sparklyr/latest/reference/spark_load_table.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReads from a Spark Table into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_load_table.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_load_table.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_load_table(\n  sc,\n  name,\n  path,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_load_table.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_load_table.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. options | A list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration. repartition | The number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning. memory | Boolean; should the data be loaded eagerly into memory? (That is, should the table be cached?) overwrite | Boolean; overwrite the table with the given name if it already exists?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_load_table.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_load_table.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_log.html#description",
    "href": "packages/sparklyr/latest/reference/spark_log.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nView the most recent entries in the Spark log. This can be useful when inspecting output / errors produced by Spark during the invocation of various commands."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_log.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_log.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_log(sc, n = 100, filter = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_log.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_log.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nn\nThe max number of log entries to retrieve. Use NULL to\n\n\n\nretrieve all entries within the log. filter | Character string to filter log entries. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#description",
    "href": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nHelper function to create pipeline stage objects with common parameter setters."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_pipeline_stage(\n  sc,\n  class,\n  uid,\n  features_col = NULL,\n  label_col = NULL,\n  prediction_col = NULL,\n  probability_col = NULL,\n  raw_prediction_col = NULL,\n  k = NULL,\n  max_iter = NULL,\n  seed = NULL,\n  input_col = NULL,\n  input_cols = NULL,\n  output_col = NULL,\n  output_cols = NULL\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_pipeline_stage.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection object.\n\n\nclass\nClass name for the pipeline stage.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nk\nThe number of clusters to create\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nseed\nA random seed. Set this value if you need your results to be\n\n\n\nreproducible across repeated calls. input_col | The name of the input column. input_cols | Names of output columns. output_col | The name of the output column. thresholds | Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRun a custom R function on Spark workers to ingest data from one or more files into a Spark DataFrame, assuming all files follow the same schema."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_read(sc, paths, reader, columns, packages = TRUE, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npaths\nA character vector of one or more file URIs (e.g.,\n\n\n\nc(“hdfs://localhost:9000/file.txt”, “hdfs://localhost:9000/file2.txt”)) reader | A self-contained R function that takes a single file URI as argument and returns the data read from that file as a data frame. columns | a named list of column names and column types of the resulting data frame (e.g., list(column_1 = “integer”, column_2 = “character”)), or a list of column names only if column types should be inferred from the data (e.g., list(“column_1”, “column_2”), or NULL if column types should be inferred and resulting data frame can have arbitrary column names packages | A list of R packages to distribute to Spark workers … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html#examples",
    "href": "packages/sparklyr/latest/reference/spark_read.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\nsc <- spark_connect(\n  master = \"yarn\",\n  spark_home = \"~/spark/spark-2.4.5-bin-hadoop2.7\"\n)\n\n# This is a contrived example to show reader tasks will be distributed across\n# all Spark worker nodes\nspark_read(\n  sc,\n  rep(\"/dev/null\", 10),\n  reader = function(path) system(\"hostname\", intern = TRUE),\n  columns = c(hostname = \"string\")\n) %>% sdf_collect()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_avro.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_avro.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead Apache Avro data into a Spark DataFrame. Notice this functionality requires the Spark connection sc to be instantiated with either an explicitly specified Spark version (i.e., spark_connect(..., version = <version>, packages = c(\"avro\", <other package(s)>), ...)) or a specific version of Spark avro package to use (e.g., spark_connect(..., packages = c(\"org.apache.spark:spark-avro_2.12:3.0.0\", <other package(s)>), ...))."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_avro.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_avro.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_read_avro(\n  sc,\n  name = NULL,\n  path = name,\n  avro_schema = NULL,\n  ignore_extension = TRUE,\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_avro.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_avro.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. avro_schema | Optional Avro schema in JSON format ignore_extension | If enabled, all files with and without .avro extension are loaded (default: TRUE) repartition | The number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning. memory | Boolean; should the data be loaded eagerly into memory? (That is, should the table be cached?) overwrite | Boolean; overwrite the table with the given name if it already exists?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_avro.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_avro.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_binary.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_binary.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead binary files within a directory and convert each file into a record within the resulting Spark dataframe. The output will be a Spark dataframe with the following columns and possibly partition columns:\n\npath: StringType\nmodificationTime: TimestampType\nlength: LongType\ncontent: BinaryType"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_binary.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_binary.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_read_binary(\n  sc,\n  name = NULL,\n  dir = name,\n  path_glob_filter = \"*\",\n  recursive_file_lookup = FALSE,\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_binary.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_binary.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\ndir\nDirectory to read binary files from.\n\n\npath_glob_filter\nGlob pattern of binary files to be loaded\n\n\n\n(e.g., “*.jpg”). recursive_file_lookup | If FALSE (default), then partition discovery will be enabled (i.e., if a partition naming scheme is present, then partitions specified by subdirectory names such as “date=2019-07-01” will be created and files outside subdirectories following a partition naming scheme will be ignored). If TRUE, then all nested directories will be searched even if their names do not follow a partition naming scheme. repartition | The number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning. memory | Boolean; should the data be loaded eagerly into memory? (That is, should the table be cached?) overwrite | Boolean; overwrite the table with the given name if it already exists?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_binary.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_binary.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead a tabular data file into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_read_csv(\n  sc,\n  name = NULL,\n  path = name,\n  header = TRUE,\n  columns = NULL,\n  infer_schema = is.null(columns),\n  delimiter = \",\",\n  quote = \"\\\"\",\n  escape = \"\\\\\",\n  charset = \"UTF-8\",\n  null_value = NULL,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. header | Boolean; should the first row of data be used as a header? Defaults to TRUE. columns | A vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType. infer_schema | Boolean; should column types be automatically inferred? Requires one extra pass over the data. Defaults to is.null(columns). delimiter | The character used to delimit each column. Defaults to ‘,’. quote | The character used as a quote. Defaults to ’“‘. escape | The character used to escape other characters. Defaults to’'. charset | The character set. Defaults to”UTF-8”. null_value | The character to use for null, or missing, values. Defaults to NULL. options | A list of strings with additional options. repartition | The number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning. memory | Boolean; should the data be loaded eagerly into memory? (That is, should the table be cached?) overwrite | Boolean; overwrite the table with the given name if it already exists? … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html#details",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nYou can read data from HDFS (hdfs://), S3 (s3a://), as well as the local file system (file://).\nIf you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf spark.hadoop.fs.s3a.access.key, spark.hadoop.fs.s3a.secret.key or any of the methods outlined in the aws-sdk documentation https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.htmlWorking with AWS credentials In order to work with the newer s3a:// protocol also set the values for spark.hadoop.fs.s3a.impl and spark.hadoop.fs.s3a.endpoint. In addition, to support v4 of the S3 api be sure to pass the -Dcom.amazonaws.services.s3.enableV4 driver options for the config key spark.driver.extraJavaOptions For instructions on how to configure s3n:// check the hadoop documentation: https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Authentication_propertiess3n authentication properties\nWhen header is FALSE, the column names are generated with a V prefix; e.g. V1, V2, ...."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_csv.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_csv.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_delta.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_delta.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead from Delta Lake into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_delta.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_delta.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_read_delta(\n  sc,\n  path,\n  name = NULL,\n  version = NULL,\n  timestamp = NULL,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_delta.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_delta.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. name | The name to assign to the newly generated table. version | The version of the delta table to read. timestamp | The timestamp of the delta table to read. For example, \"2019-01-01\" or \"2019-01-01'T'00:00:00.000Z\". options | A list of strings with additional options. repartition | The number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning. memory | Boolean; should the data be loaded eagerly into memory? (That is, should the table be cached?) overwrite | Boolean; overwrite the table with the given name if it already exists? … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_delta.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_delta.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_image.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_image.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead image files within a directory and convert each file into a record within the resulting Spark dataframe. The output will be a Spark dataframe consisting of struct types containing the following attributes:\n\norigin: StringType\nheight: IntegerType\nwidth: IntegerType\nnChannels: IntegerType\nmode: IntegerType\ndata: BinaryType"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_image.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_image.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_read_image(\n  sc,\n  name = NULL,\n  dir = name,\n  drop_invalid = TRUE,\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_image.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_image.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\ndir\nDirectory to read binary files from.\n\n\ndrop_invalid\nWhether to drop files that are not valid images from the\n\n\n\nresult (default: TRUE). repartition | The number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning. memory | Boolean; should the data be loaded eagerly into memory? (That is, should the table be cached?) overwrite | Boolean; overwrite the table with the given name if it already exists?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_image.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_image.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_jdbc.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_jdbc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead from JDBC connection into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_jdbc.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_jdbc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_read_jdbc(\n  sc,\n  name,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  columns = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_jdbc.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_jdbc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\noptions\nA list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration.\n\n\nrepartition\nThe number of partitions used to distribute the\n\n\n\ngenerated table. Use 0 (the default) to avoid partitioning. memory | Boolean; should the data be loaded eagerly into memory? (That is, should the table be cached?) overwrite | Boolean; overwrite the table with the given name if it already exists? columns | A vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_jdbc.html#examples",
    "href": "packages/sparklyr/latest/reference/spark_read_jdbc.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(\n  master = \"local\",\n  config = list(\n    `sparklyr.shell.driver-class-path` = \"/usr/share/java/mysql-connector-java-8.0.25.jar\"\n  )\n)\nspark_read_jdbc(\n  sc,\n  name = \"my_sql_table\",\n  options = list(\n    url = \"jdbc:mysql://localhost:3306/my_sql_schema\",\n    driver = \"com.mysql.jdbc.Driver\",\n    user = \"me\",\n    password = \"******\",\n    dbtable = \"my_sql_table\"\n  )\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_jdbc.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_jdbc.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_json.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_json.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead a table serialized in the http://www.json.org/JavaScript Object Notation format into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_json.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_json.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_read_json(\n  sc,\n  name = NULL,\n  path = name,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  columns = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_json.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_json.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. options | A list of strings with additional options. repartition | The number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning. memory | Boolean; should the data be loaded eagerly into memory? (That is, should the table be cached?) overwrite | Boolean; overwrite the table with the given name if it already exists? columns | A vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_json.html#details",
    "href": "packages/sparklyr/latest/reference/spark_read_json.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nYou can read data from HDFS (hdfs://), S3 (s3a://), as well as the local file system (file://).\nIf you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf spark.hadoop.fs.s3a.access.key, spark.hadoop.fs.s3a.secret.key or any of the methods outlined in the aws-sdk documentation https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.htmlWorking with AWS credentials In order to work with the newer s3a:// protocol also set the values for spark.hadoop.fs.s3a.impl and spark.hadoop.fs.s3a.endpoint. In addition, to support v4 of the S3 api be sure to pass the -Dcom.amazonaws.services.s3.enableV4 driver options for the config key spark.driver.extraJavaOptions For instructions on how to configure s3n:// check the hadoop documentation: https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Authentication_propertiess3n authentication properties"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_json.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_json.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_libsvm.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_libsvm.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead libsvm file into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_libsvm.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_libsvm.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_read_libsvm(\n  sc,\n  name = NULL,\n  path = name,\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  options = list(),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_libsvm.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_libsvm.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. repartition | The number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning. memory | Boolean; should the data be loaded eagerly into memory? (That is, should the table be cached?) overwrite | Boolean; overwrite the table with the given name if it already exists? options | A list of strings with additional options. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_libsvm.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_libsvm.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead a https://orc.apache.org/ORC file into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_read_orc(\n  sc,\n  name = NULL,\n  path = name,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  columns = NULL,\n  schema = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. options | A list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration. repartition | The number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning. memory | Boolean; should the data be loaded eagerly into memory? (That is, should the table be cached?) overwrite | Boolean; overwrite the table with the given name if it already exists? columns | A vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType. schema | A (java) read schema. Useful for optimizing read operation on nested data. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html#details",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nYou can read data from HDFS (hdfs://), S3 (s3a://), as well as the local file system (file://)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_orc.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_orc.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead a https://parquet.apache.org/Parquet file into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_read_parquet(\n  sc,\n  name = NULL,\n  path = name,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  columns = NULL,\n  schema = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. options | A list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration. repartition | The number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning. memory | Boolean; should the data be loaded eagerly into memory? (That is, should the table be cached?) overwrite | Boolean; overwrite the table with the given name if it already exists? columns | A vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType. schema | A (java) read schema. Useful for optimizing read operation on nested data. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html#details",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nYou can read data from HDFS (hdfs://), S3 (s3a://), as well as the local file system (file://).\nIf you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf spark.hadoop.fs.s3a.access.key, spark.hadoop.fs.s3a.secret.key or any of the methods outlined in the aws-sdk documentation https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.htmlWorking with AWS credentials In order to work with the newer s3a:// protocol also set the values for spark.hadoop.fs.s3a.impl and spark.hadoop.fs.s3a.endpoint. In addition, to support v4 of the S3 api be sure to pass the -Dcom.amazonaws.services.s3.enableV4 driver options for the config key spark.driver.extraJavaOptions For instructions on how to configure s3n:// check the hadoop documentation: https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Authentication_propertiess3n authentication properties"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_parquet.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_parquet.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_source.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_source.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead from a generic source into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_source.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_source.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_read_source(\n  sc,\n  name = NULL,\n  path = name,\n  source,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  columns = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_source.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_source.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. source | A data source capable of reading data. options | A list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration. repartition | The number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning. memory | Boolean; should the data be loaded eagerly into memory? (That is, should the table be cached?) overwrite | Boolean; overwrite the table with the given name if it already exists? columns | A vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_source.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_source.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_table.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_table.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReads from a Spark Table into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_table.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_table.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_read_table(\n  sc,\n  name,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  columns = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_table.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_table.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\noptions\nA list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration.\n\n\nrepartition\nThe number of partitions used to distribute the\n\n\n\ngenerated table. Use 0 (the default) to avoid partitioning. memory | Boolean; should the data be loaded eagerly into memory? (That is, should the table be cached?) columns | A vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_table.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_table.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html#description",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead a text file into a Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_read_text(\n  sc,\n  name = NULL,\n  path = name,\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  options = list(),\n  whole = FALSE,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. repartition | The number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning. memory | Boolean; should the data be loaded eagerly into memory? (That is, should the table be cached?) overwrite | Boolean; overwrite the table with the given name if it already exists? options | A list of strings with additional options. whole | Read the entire text file as a single entry? Defaults to FALSE. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html#details",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nYou can read data from HDFS (hdfs://), S3 (s3a://), as well as the local file system (file://).\nIf you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf spark.hadoop.fs.s3a.access.key, spark.hadoop.fs.s3a.secret.key or any of the methods outlined in the aws-sdk documentation https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.htmlWorking with AWS credentials In order to work with the newer s3a:// protocol also set the values for spark.hadoop.fs.s3a.impl and spark.hadoop.fs.s3a.endpoint. In addition, to support v4 of the S3 api be sure to pass the -Dcom.amazonaws.services.s3.enableV4 driver options for the config key spark.driver.extraJavaOptions For instructions on how to configure s3n:// check the hadoop documentation: https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Authentication_propertiess3n authentication properties"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_read_text.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_read_text.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_save_table.html#description",
    "href": "packages/sparklyr/latest/reference/spark_save_table.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSaves a Spark DataFrame and as a Spark table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_save_table.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_save_table.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_save_table(x, path, mode = NULL, options = list())"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_save_table.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_save_table.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. mode | A character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.\nFor more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark. options | A list of strings with additional options."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_save_table.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_save_table.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_statistical_routines.html#description",
    "href": "packages/sparklyr/latest/reference/spark_statistical_routines.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerator methods for creating single-column Spark dataframes comprised of i.i.d. samples from some distribution."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_statistical_routines.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_statistical_routines.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe\n\n\n\n(default: default parallelism of the Spark cluster). seed | Random seed (default: a random long integer). output_col | Name of the output column containing sample values (default: “x”)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_table_name.html#description",
    "href": "packages/sparklyr/latest/reference/spark_table_name.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nAttempts to generate a table name from an expression; otherwise, assigns an auto-generated generic name with “sparklyr_” prefix."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_table_name.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_table_name.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_table_name(expr)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_table_name.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_table_name.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nexpr\nThe expression to attempt to use as name"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html#description",
    "href": "packages/sparklyr/latest/reference/spark_version.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieve the version of Spark associated with a Spark connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_version.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_version(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_version.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html#details",
    "href": "packages/sparklyr/latest/reference/spark_version.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nSuffixes for e.g. preview versions, or snapshotted versions, are trimmed – if you require the full Spark version, you can retrieve it with invoke(spark_context(sc), \"version\")."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version.html#value",
    "href": "packages/sparklyr/latest/reference/spark_version.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe Spark version as a numeric_version."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version_from_home.html#description",
    "href": "packages/sparklyr/latest/reference/spark_version_from_home.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieve the version of Spark associated with a Spark installation."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version_from_home.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_version_from_home.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_version_from_home(spark_home, default = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_version_from_home.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_version_from_home.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nspark_home\nThe path to a Spark installation.\n\n\ndefault\nThe default version to be inferred, in case\n\n\n\nversion lookup failed, e.g. no Spark installation was found at spark_home."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_versions.html#description",
    "href": "packages/sparklyr/latest/reference/spark_versions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieves a dataframe available Spark versions that van be installed."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_versions.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_versions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_versions(latest = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_versions.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_versions.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nlatest\nCheck for latest version?"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_web.html#description",
    "href": "packages/sparklyr/latest/reference/spark_web.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nOpen the Spark web interface"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_web.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_web.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_web(sc, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_web.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_web.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRun a custom R function on Spark worker to write a Spark DataFrame into file(s). If Spark’s speculative execution feature is enabled (i.e., spark.speculation is true), then each write task may be executed more than once and the user-defined writer function will need to ensure no concurrent writes happen to the same file path (e.g., by appending UUID to each file name)."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_write(x, writer, paths, packages = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark Dataframe to be saved into file(s)\n\n\nwriter\nA writer function with the signature function(partition, path)\n\n\n\nwhere partition is a R dataframe containing all rows from one partition of the original Spark Dataframe x and path is a string specifying the file to write partition to paths | A single destination path or a list of destination paths, each one specifying a location for a partition from x to be written to. If number of partition(s) in x is not equal to length(paths) then x will be re-partitioned to contain length(paths) partition(s) packages | Boolean to distribute .libPaths() packages to each node, a list of packages to distribute, or a package bundle created with"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write.html#examples",
    "href": "packages/sparklyr/latest/reference/spark_write.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local[3]\")\n\n# copy some test data into a Spark Dataframe\nsdf <- sdf_copy_to(sc, iris, overwrite = TRUE)\n\n# create a writer function\nwriter <- function(df, path) {\n  write.csv(df, path)\n}\n\nspark_write(\n  sdf,\n  writer,\n  # re-partition sdf into 3 partitions and write them to 3 separate files\n  paths = list(\"file:///tmp/file1\", \"file:///tmp/file2\", \"file:///tmp/file3\"),\n)\n\nspark_write(\n  sdf,\n  writer,\n  # save all rows into a single file\n  paths = list(\"file:///tmp/all_rows\")\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_avro.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_avro.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSerialize a Spark DataFrame into Apache Avro format. Notice this functionality requires the Spark connection sc to be instantiated with either an explicitly specified Spark version (i.e., spark_connect(..., version = <version>, packages = c(\"avro\", <other package(s)>), ...)) or a specific version of Spark avro package to use (e.g., spark_connect(..., packages = c(\"org.apache.spark:spark-avro_2.12:3.0.0\", <other package(s)>), ...))."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_avro.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_avro.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_write_avro(\n  x,\n  path,\n  avro_schema = NULL,\n  record_name = \"topLevelRecord\",\n  record_namespace = \"\",\n  compression = \"snappy\",\n  partition_by = NULL\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_avro.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_avro.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. avro_schema | Optional Avro schema in JSON format record_name | Optional top level record name in write result (default: “topLevelRecord”) record_namespace | Record namespace in write result (default: ““) compression | Compression codec to use (default:”snappy”) partition_by | A character vector. Partitions the output by the given columns on the file system."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_avro.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_avro.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_csv.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_csv.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nWrite a Spark DataFrame to a tabular (typically, comma-separated) file."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_csv.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_csv.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_write_csv(\n  x,\n  path,\n  header = TRUE,\n  delimiter = \",\",\n  quote = \"\\\"\",\n  escape = \"\\\\\",\n  charset = \"UTF-8\",\n  null_value = NULL,\n  options = list(),\n  mode = NULL,\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_csv.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_csv.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. header | Should the first row of data be used as a header? Defaults to TRUE. delimiter | The character used to delimit each column, defaults to ,. quote | The character used as a quote. Defaults to ‘“’. escape | The character used to escape other characters, defaults to \\. charset | The character set, defaults to \"UTF-8\". null_value | The character to use for default values, defaults to NULL. options | A list of strings with additional options. mode | A character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.\nFor more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark. partition_by | A character vector. Partitions the output by the given columns on the file system. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_csv.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_csv.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_delta.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_delta.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nWrites a Spark DataFrame into Delta Lake."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_delta.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_delta.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_write_delta(\n  x,\n  path,\n  mode = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_delta.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_delta.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. mode | A character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.\nFor more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark. options | A list of strings with additional options. partition_by | A character vector. Partitions the output by the given columns on the file system. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_delta.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_delta.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_jdbc.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_jdbc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nWrites a Spark DataFrame into a JDBC table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_jdbc.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_jdbc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_write_jdbc(\n  x,\n  name,\n  mode = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_jdbc.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_jdbc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nname\nThe name to assign to the newly generated table.\n\n\nmode\nA character element. Specifies the behavior when data or\n\n\n\ntable already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.\nFor more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark. options | A list of strings with additional options. partition_by | A character vector. Partitions the output by the given columns on the file system. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_jdbc.html#examples",
    "href": "packages/sparklyr/latest/reference/spark_write_jdbc.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(\n  master = \"local\",\n  config = list(\n    `sparklyr.shell.driver-class-path` = \"/usr/share/java/mysql-connector-java-8.0.25.jar\"\n  )\n)\nspark_write_jdbc(\n  sdf_len(sc, 10),\n  name = \"my_sql_table\",\n  options = list(\n    url = \"jdbc:mysql://localhost:3306/my_sql_schema\",\n    driver = \"com.mysql.jdbc.Driver\",\n    user = \"me\",\n    password = \"******\",\n    dbtable = \"my_sql_table\"\n  )\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_jdbc.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_jdbc.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_json.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_json.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSerialize a Spark DataFrame to the http://www.json.org/JavaScript Object Notation format."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_json.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_json.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_write_json(\n  x,\n  path,\n  mode = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_json.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_json.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. mode | A character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.\nFor more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark. options | A list of strings with additional options. partition_by | A character vector. Partitions the output by the given columns on the file system. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_json.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_json.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_orc.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_orc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSerialize a Spark DataFrame to the https://orc.apache.org/ORC format."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_orc.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_orc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_write_orc(\n  x,\n  path,\n  mode = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_orc.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_orc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. mode | A character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.\nFor more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark. options | A list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration. partition_by | A character vector. Partitions the output by the given columns on the file system. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_orc.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_orc.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_parquet(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_parquet.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_parquet.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSerialize a Spark DataFrame to the https://parquet.apache.org/Parquet format."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_parquet.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_parquet.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_write_parquet(\n  x,\n  path,\n  mode = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_parquet.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_parquet.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. mode | A character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.\nFor more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark. options | A list of strings with additional options. See https://spark.apache.org/docs/latest/sql-programming-guide.html#configuration. partition_by | A character vector. Partitions the output by the given columns on the file system. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_parquet.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_parquet.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_source(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_rds.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_rds.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nWrite Spark dataframe to RDS files. Each partition of the dataframe will be exported to a separate RDS file so that all partitions can be processed in parallel."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_rds.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_rds.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_write_rds(x, dest_uri)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_rds.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_rds.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame to be exported\n\n\ndest_uri\nCan be a URI template containing “partitionId” (e.g.,\n\n\n\n“hdfs://my_data_part_partitionId.rds”) where “partitionId” will be substituted with ID of each partition using glue, or a list of URIs to be assigned to RDS output from all partitions (e.g., “hdfs://my_data_part_0.rds”, “hdfs://my_data_part_1.rds”, and so on) If working with a Spark instance running locally, then all URIs should be in “file://” form. Otherwise the scheme of the URI should reflect the underlying file system the Spark instance is working with (e.g., “hdfs://”). If the resulting list of URI(s) does not contain unique values, then it will be post-processed with make.unique() to ensure uniqueness."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_rds.html#value",
    "href": "packages/sparklyr/latest/reference/spark_write_rds.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA tibble containing partition ID and RDS file location for each partition of the input Spark dataframe."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_source.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_source.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nWrites a Spark DataFrame into a generic source."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_source.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_source.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_write_source(\n  x,\n  source,\n  mode = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_source.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_source.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nsource\nA data source capable of reading data.\n\n\nmode\nA character element. Specifies the behavior when data or\n\n\n\ntable already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.\nFor more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark. options | A list of strings with additional options. partition_by | A character vector. Partitions the output by the given columns on the file system. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_source.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_source.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_table(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_table.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_table.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nWrites a Spark DataFrame into a Spark table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_table.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_table.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_write_table(\n  x,\n  name,\n  mode = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_table.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_table.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nname\nThe name to assign to the newly generated table.\n\n\nmode\nA character element. Specifies the behavior when data or\n\n\n\ntable already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.\nFor more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark. options | A list of strings with additional options. partition_by | A character vector. Partitions the output by the given columns on the file system. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_table.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_table.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_text.html#description",
    "href": "packages/sparklyr/latest/reference/spark_write_text.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSerialize a Spark DataFrame to the plain text format."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_text.html#usage",
    "href": "packages/sparklyr/latest/reference/spark_write_text.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_write_text(\n  x,\n  path,\n  mode = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_text.html#arguments",
    "href": "packages/sparklyr/latest/reference/spark_write_text.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. mode | A character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure.\nFor more details see also https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark. options | A list of strings with additional options. partition_by | A character vector. Partitions the output by the given columns on the file system. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/spark_write_text.html#see-also",
    "href": "packages/sparklyr/latest/reference/spark_write_text.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark serialization routines: collect_from_rds(), spark_load_table(), spark_read_avro(), spark_read_binary(), spark_read_csv(), spark_read_delta(), spark_read_image(), spark_read_jdbc(), spark_read_json(), spark_read_libsvm(), spark_read_orc(), spark_read_parquet(), spark_read_source(), spark_read_table(), spark_read_text(), spark_read(), spark_save_table(), spark_write_avro(), spark_write_csv(), spark_write_delta(), spark_write_jdbc(), spark_write_json(), spark_write_orc(), spark_write_parquet(), spark_write_source(), spark_write_table()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#description",
    "href": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieve the port number of the sparklyr backend associated with a Spark connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#usage",
    "href": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsparklyr_get_backend_port(sc)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#arguments",
    "href": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#value",
    "href": "packages/sparklyr/latest/reference/sparklyr_get_backend_port.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe port number of the sparklyr backend associated with sc."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sql-transformer.html#description",
    "href": "packages/sparklyr/latest/reference/sql-transformer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nImplements the transformations which are defined by SQL statement. Currently we only support SQL syntax like ‘SELECT … FROM THIS …’ where ‘THIS’ represents the underlying table of the input dataset. The select clause specifies the fields, constants, and expressions to display in the output, it can be any select clause that Spark SQL supports. Users can also use Spark SQL built-in function and UDFs to operate on these selected columns."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sql-transformer.html#usage",
    "href": "packages/sparklyr/latest/reference/sql-transformer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nft_sql_transformer(\n  x,\n  statement = NULL,\n  uid = random_string(\"sql_transformer_\"),\n  ...\n)\n\nft_dplyr_transformer(x, tbl, uid = random_string(\"dplyr_transformer_\"), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sql-transformer.html#arguments",
    "href": "packages/sparklyr/latest/reference/sql-transformer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nstatement\nA SQL statement.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n…\nOptional arguments; currently unused.\n\n\ntbl\nA tbl_spark generated using dplyr transformations."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sql-transformer.html#details",
    "href": "packages/sparklyr/latest/reference/sql-transformer.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nft_dplyr_transformer() is mostly a wrapper around ft_sql_transformer() that takes a tbl_spark instead of a SQL statement. Internally, the ft_dplyr_transformer() extracts the dplyr transformations used to generate tbl as a SQL statement or a sampling operation. Note that only single-table dplyr verbs are supported and that the sdf_ family of functions are not."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sql-transformer.html#value",
    "href": "packages/sparklyr/latest/reference/sql-transformer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns a ml_transformer, a ml_estimator, or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a transformer is constructed then immediately applied to the input tbl_spark, returning a tbl_spark"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sql-transformer.html#see-also",
    "href": "packages/sparklyr/latest/reference/sql-transformer.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee https://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer(), ft_bucketizer(), ft_chisq_selector(), ft_count_vectorizer(), ft_dct(), ft_elementwise_product(), ft_feature_hasher(), ft_hashing_tf(), ft_idf(), ft_imputer(), ft_index_to_string(), ft_interaction(), ft_lsh, ft_max_abs_scaler(), ft_min_max_scaler(), ft_ngram(), ft_normalizer(), ft_one_hot_encoder_estimator(), ft_one_hot_encoder(), ft_pca(), ft_polynomial_expansion(), ft_quantile_discretizer(), ft_r_formula(), ft_regex_tokenizer(), ft_robust_scaler(), ft_standard_scaler(), ft_stop_words_remover(), ft_string_indexer(), ft_tokenizer(), ft_vector_assembler(), ft_vector_indexer(), ft_vector_slicer(), ft_word2vec()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/src_databases.html#description",
    "href": "packages/sparklyr/latest/reference/src_databases.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nShow database list"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/src_databases.html#usage",
    "href": "packages/sparklyr/latest/reference/src_databases.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nsrc_databases(sc, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/src_databases.html#arguments",
    "href": "packages/sparklyr/latest/reference/src_databases.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_find.html#description",
    "href": "packages/sparklyr/latest/reference/stream_find.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFinds and returns a stream based on the stream’s identifier."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_find.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_find.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_find(sc, id)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_find.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_find.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nid\nThe stream identifier to find."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_find.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_find.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\nsdf_len(sc, 10) %>%\n  spark_write_parquet(path = \"parquet-in\")\n\nstream <- stream_read_parquet(sc, \"parquet-in\") %>%\n  stream_write_parquet(\"parquet-out\")\n\nstream_id <- stream_id(stream)\nstream_find(sc, stream_id)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_generate_test.html#description",
    "href": "packages/sparklyr/latest/reference/stream_generate_test.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerates a local test stream, useful when testing streams locally."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_generate_test.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_generate_test.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_generate_test(\n  df = rep(1:1000),\n  path = \"source\",\n  distribution = floor(10 + 1e+05 * stats::dbinom(1:20, 20, 0.5)),\n  iterations = 50,\n  interval = 1\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_generate_test.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_generate_test.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\ndf\nThe data frame used as a source of rows to the stream, will\n\n\n\nbe cast to data frame if needed. Defaults to a sequence of one thousand entries. path | Path to save stream of files to, defaults to \"source\". distribution | The distribution of rows to use over each iteration, defaults to a binomial distribution. The stream will cycle through the distribution if needed. iterations | Number of iterations to execute before stopping, defaults to fifty. interval | The inverval in seconds use to write the stream, defaults to one second."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_generate_test.html#details",
    "href": "packages/sparklyr/latest/reference/stream_generate_test.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nThis function requires the callr package to be installed."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_id.html#description",
    "href": "packages/sparklyr/latest/reference/stream_id.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieves the identifier of the Spark stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_id.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_id.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_id(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_id.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_id.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nstream\nThe spark stream object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_lag.html#description",
    "href": "packages/sparklyr/latest/reference/stream_lag.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGiven a streaming Spark dataframe as input, this function will return another streaming dataframe that contains all columns in the input and column(s) that are shifted behind by the offset(s) specified in ... (see example)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_lag.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_lag.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_lag(x, cols, thresholds = NULL)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_lag.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_lag.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a Spark Streaming DataFrame.\n\n\ncols\nA list of expressions of the form\n\n\n\n =  ~  (e.g., prev_value = value ~ 1 will create a new column prev_value containing all values from the source column value shifted behind by 1 thresholds | Optional named list of timestamp column(s) and corresponding time duration(s) for deterimining whether a previous record is sufficiently recent relative to the current record. If the any of the time difference(s) between the current and a previous record is greater than the maximal duration allowed, then the previous record is discarded and will not be part of the query result. The durations can be specified with numeric types (which will be interpreted as max difference allowed in number of milliseconds between 2 UNIX timestamps) or time duration strings such as “5s”, “5sec”, “5min”, “5hour”, etc. Any timestamp column in x that is not of timestamp of date Spark SQL types will be interepreted as number of milliseconds since the UNIX epoch."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_lag.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_lag.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\", version = \"2.2.0\")\n\nstreaming_path <- tempfile(\"days_df_\")\ndays_df <- tibble::tibble(\n  today = weekdays(as.Date(seq(7), origin = \"1970-01-01\"))\n)\nnum_iters <- 7\nstream_generate_test(\n  df = days_df,\n  path = streaming_path,\n  distribution = rep(nrow(days_df), num_iters),\n  iterations = num_iters\n)\n\nstream_read_csv(sc, streaming_path) %>%\n  stream_lag(cols = c(yesterday = today ~ 1, two_days_ago = today ~ 2)) %>%\n  collect() %>%\n  print(n = 10L)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_name.html#description",
    "href": "packages/sparklyr/latest/reference/stream_name.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieves the name of the Spark stream if available."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_name.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_name.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_name(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_name.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_name.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nstream\nThe spark stream object."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReads a CSV stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_read_csv(\n  sc,\n  path,\n  name = NULL,\n  header = TRUE,\n  columns = NULL,\n  delimiter = \",\",\n  quote = \"\\\"\",\n  escape = \"\\\\\",\n  charset = \"UTF-8\",\n  null_value = NULL,\n  options = list(),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. name | The name to assign to the newly generated stream. header | Boolean; should the first row of data be used as a header? Defaults to TRUE. columns | A vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType. delimiter | The character used to delimit each column. Defaults to ‘,’. quote | The character used as a quote. Defaults to ’“‘. escape | The character used to escape other characters. Defaults to’'. charset | The character set. Defaults to”UTF-8”. null_value | The character to use for null, or missing, values. Defaults to NULL. options | A list of strings with additional options. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nsc <- spark_connect(master = \"local\")\n\ndir.create(\"csv-in\")\nwrite.csv(iris, \"csv-in/data.csv\", row.names = FALSE)\n\ncsv_path <- file.path(\"file://\", getwd(), \"csv-in\")\n\nstream <- stream_read_csv(sc, csv_path) %>% stream_write_csv(\"csv-out\")\n\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_csv.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_csv.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReads a Delta Lake table as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_read_delta(sc, path, name = NULL, options = list(), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. name | The name to assign to the newly generated stream. options | A list of strings with additional options. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#details",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nPlease note that Delta Lake requires installing the appropriate package by setting the packages parameter to \"delta\" in spark_connect()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.4.0\", packages = \"delta\")\n\nsdf_len(sc, 5) %>% spark_write_delta(path = \"delta-test\")\n\nstream <- stream_read_delta(sc, \"delta-test\") %>%\n  stream_write_json(\"json-out\")\n\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_delta.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_delta.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_json.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_json.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReads a JSON stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_json.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_json.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_read_json(sc, path, name = NULL, columns = NULL, options = list(), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_json.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_json.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. name | The name to assign to the newly generated stream. columns | A vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType. options | A list of strings with additional options. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_json.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_json.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nsc <- spark_connect(master = \"local\")\n\ndir.create(\"json-in\")\njsonlite::write_json(list(a = c(1, 2), b = c(10, 20)), \"json-in/data.json\")\n\njson_path <- file.path(\"file://\", getwd(), \"json-in\")\n\nstream <- stream_read_json(sc, json_path) %>% stream_write_json(\"json-out\")\n\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_json.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_json.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReads a Kafka stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_read_kafka(sc, name = NULL, options = list(), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\noptions\nA list of strings with additional options.\n\n\n…\nOptional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#details",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nPlease note that Kafka requires installing the appropriate package by setting the packages parameter to \"kafka\" in spark_connect()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.3\", packages = \"kafka\")\n\nread_options <- list(kafka.bootstrap.servers = \"localhost:9092\", subscribe = \"topic1\")\nwrite_options <- list(kafka.bootstrap.servers = \"localhost:9092\", topic = \"topic2\")\n\nstream <- stream_read_kafka(sc, options = read_options) %>%\n  stream_write_kafka(options = write_options)\n\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_kafka.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_kafka.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_orc.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_orc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReads an https://orc.apache.org/ORC stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_orc.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_orc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_read_orc(sc, path, name = NULL, columns = NULL, options = list(), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_orc.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_orc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. name | The name to assign to the newly generated stream. columns | A vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType. options | A list of strings with additional options. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_orc.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_orc.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nsc <- spark_connect(master = \"local\")\n\nsdf_len(sc, 10) %>% spark_write_orc(\"orc-in\")\n\nstream <- stream_read_orc(sc, \"orc-in\") %>% stream_write_orc(\"orc-out\")\n\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_orc.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_orc.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReads a parquet stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_read_parquet(\n  sc,\n  path,\n  name = NULL,\n  columns = NULL,\n  options = list(),\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. name | The name to assign to the newly generated stream. columns | A vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType. options | A list of strings with additional options. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nsc <- spark_connect(master = \"local\")\n\nsdf_len(sc, 10) %>% spark_write_parquet(\"parquet-in\")\n\nstream <- stream_read_parquet(sc, \"parquet-in\") %>% stream_write_parquet(\"parquet-out\")\n\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_parquet.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_parquet.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReads a Socket stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_read_socket(sc, name = NULL, columns = NULL, options = list(), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\ncolumns\nA vector of column names or a named vector of column types.\n\n\n\nIf specified, the elements can be \"binary\" for BinaryType, \"boolean\" for BooleanType, \"byte\" for ByteType, \"integer\" for IntegerType, \"integer64\" for LongType, \"double\" for DoubleType, \"character\" for StringType, \"timestamp\" for TimestampType and \"date\" for DateType. options | A list of strings with additional options. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nsc <- spark_connect(master = \"local\")\n\n# Start socket server from terminal, example: nc -lk 9999\nstream <- stream_read_socket(sc, options = list(host = \"localhost\", port = 9999))\nstream"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_socket.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_socket.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html#description",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReads a text stream as a Spark dataframe stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_read_text(sc, path, name = NULL, options = list(), ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. name | The name to assign to the newly generated stream. options | A list of strings with additional options. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nsc <- spark_connect(master = \"local\")\n\ndir.create(\"text-in\")\nwriteLines(\"A text entry\", \"text-in/text.txt\")\n\ntext_path <- file.path(\"file://\", getwd(), \"text-in\")\n\nstream <- stream_read_text(sc, text_path) %>% stream_write_text(\"text-out\")\n\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_read_text.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_read_text.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_render.html#description",
    "href": "packages/sparklyr/latest/reference/stream_render.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCollects streaming statistics to render the stream as an ‘htmlwidget’."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_render.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_render.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_render(stream = NULL, collect = 10, stats = NULL, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_render.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_render.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nstream\nThe stream to render\n\n\ncollect\nThe interval in seconds to collect data before rendering the\n\n\n\n‘htmlwidget’. stats | Optional stream statistics collected using stream_stats(), when specified, stream should be omitted. … | Additional optional arguments."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_render.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_render.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\n\ndir.create(\"iris-in\")\nwrite.csv(iris, \"iris-in/iris.csv\", row.names = FALSE)\n\nstream <- stream_read_csv(sc, \"iris-in/\") %>%\n  stream_write_csv(\"iris-out/\")\n\nstream_render(stream)\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stats.html#description",
    "href": "packages/sparklyr/latest/reference/stream_stats.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCollects streaming statistics, usually, to be used with stream_render() to render streaming statistics."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stats.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_stats.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_stats(stream, stats = list())"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stats.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_stats.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nstream\nThe stream to collect statistics from.\n\n\nstats\nAn optional stats object generated using stream_stats()."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stats.html#value",
    "href": "packages/sparklyr/latest/reference/stream_stats.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA stats object containing streaming statistics that can be passed back to the stats parameter to continue aggregating streaming stats."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stats.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_stats.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nsc <- spark_connect(master = \"local\")\nsdf_len(sc, 10) %>%\n  spark_write_parquet(path = \"parquet-in\")\n\nstream <- stream_read_parquet(sc, \"parquet-in\") %>%\n  stream_write_parquet(\"parquet-out\")\n\nstream_stats(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stop.html#description",
    "href": "packages/sparklyr/latest/reference/stream_stop.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nStops processing data from a Spark stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stop.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_stop.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_stop.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_stop.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nstream\nThe spark stream object to be stopped."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#description",
    "href": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCreates a Spark structured streaming trigger to execute continuously. This mode is the most performant but not all operations are supported."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_trigger_continuous(checkpoint = 5000)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\ncheckpoint\nThe checkpoint interval specified in milliseconds."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_trigger_continuous.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nstream_trigger_interval"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_interval.html#description",
    "href": "packages/sparklyr/latest/reference/stream_trigger_interval.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCreates a Spark structured streaming trigger to execute over the specified interval."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_interval.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_trigger_interval.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_trigger_interval(interval = 1000)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_interval.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_trigger_interval.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\ninterval\nThe execution interval specified in milliseconds."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_trigger_interval.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_trigger_interval.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nstream_trigger_continuous"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_view.html#description",
    "href": "packages/sparklyr/latest/reference/stream_view.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nOpens a Shiny gadget to visualize the given stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_view.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_view.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_view(stream, ...)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_view.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_view.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nstream\nThe stream to visualize.\n\n\n…\nAdditional optional arguments."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_view.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_view.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\n\ndir.create(\"iris-in\")\nwrite.csv(iris, \"iris-in/iris.csv\", row.names = FALSE)\n\nstream_read_csv(sc, \"iris-in/\") %>%\n  stream_write_csv(\"iris-out/\") %>%\n  stream_view() %>%\n  stream_stop()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_watermark.html#description",
    "href": "packages/sparklyr/latest/reference/stream_watermark.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nEnsures a stream has a watermark defined, which is required for some operations over streams."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_watermark.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_watermark.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_watermark(x, column = \"timestamp\", threshold = \"10 minutes\")"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_watermark.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_watermark.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a Spark Streaming DataFrame.\n\n\ncolumn\nThe name of the column that contains the event time of the row,\n\n\n\nif the column is missing, a column with the current time will be added. threshold | The minimum delay to wait to data to arrive late, defaults to ten minutes."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into console logs."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_write_console(\n  x,\n  mode = c(\"append\", \"complete\", \"update\"),\n  options = list(),\n  trigger = stream_trigger_interval(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are\n\n\n\n\"append\", \"complete\" or \"update\". options | A list of strings with additional options. trigger | The trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous. partition_by | Partitions the output by the given list of columns. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nsc <- spark_connect(master = \"local\")\n\nsdf_len(sc, 10) %>%\n  dplyr::transmute(text = as.character(id)) %>%\n  spark_write_text(\"text-in\")\n\nstream <- stream_read_text(sc, \"text-in\") %>% stream_write_console()\n\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_console.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_console.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into a tabular (typically, comma-separated) stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_write_csv(\n  x,\n  path,\n  mode = c(\"append\", \"complete\", \"update\"),\n  trigger = stream_trigger_interval(),\n  checkpoint = file.path(path, \"checkpoint\"),\n  header = TRUE,\n  delimiter = \",\",\n  quote = \"\\\"\",\n  escape = \"\\\\\",\n  charset = \"UTF-8\",\n  null_value = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. mode | Specifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\". trigger | The trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous. checkpoint | The location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance. header | Should the first row of data be used as a header? Defaults to TRUE. delimiter | The character used to delimit each column, defaults to ,. quote | The character used as a quote. Defaults to ‘“’. escape | The character used to escape other characters, defaults to \\. charset | The character set, defaults to \"UTF-8\". null_value | The character to use for default values, defaults to NULL. options | A list of strings with additional options. partition_by | Partitions the output by the given list of columns. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nsc <- spark_connect(master = \"local\")\n\ndir.create(\"csv-in\")\nwrite.csv(iris, \"csv-in/data.csv\", row.names = FALSE)\n\ncsv_path <- file.path(\"file://\", getwd(), \"csv-in\")\n\nstream <- stream_read_csv(sc, csv_path) %>% stream_write_csv(\"csv-out\")\n\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_csv.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_csv.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_delta.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_delta.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into a Delta Lake table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_delta.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_delta.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_write_delta(\n  x,\n  path,\n  mode = c(\"append\", \"complete\", \"update\"),\n  checkpoint = file.path(\"checkpoints\", random_string(\"\")),\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_delta.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_delta.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. mode | Specifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\". checkpoint | The location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance. options | A list of strings with additional options. partition_by | Partitions the output by the given list of columns. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_delta.html#details",
    "href": "packages/sparklyr/latest/reference/stream_write_delta.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nPlease note that Delta Lake requires installing the appropriate package by setting the packages parameter to \"delta\" in spark_connect()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_delta.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_delta.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.4.0\", packages = \"delta\")\n\ndir.create(\"text-in\")\nwriteLines(\"A text entry\", \"text-in/text.txt\")\n\ntext_path <- file.path(\"file://\", getwd(), \"text-in\")\n\nstream <- stream_read_text(sc, text_path) %>% stream_write_delta(path = \"delta-test\")\n\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_delta.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_delta.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_json.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_json.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into a JSON stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_json.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_json.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_write_json(\n  x,\n  path,\n  mode = c(\"append\", \"complete\", \"update\"),\n  trigger = stream_trigger_interval(),\n  checkpoint = file.path(path, \"checkpoints\", random_string(\"\")),\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_json.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_json.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe destination path. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. mode | Specifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\". trigger | The trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous. checkpoint | The location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance. options | A list of strings with additional options. partition_by | Partitions the output by the given list of columns. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_json.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_json.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nsc <- spark_connect(master = \"local\")\n\ndir.create(\"json-in\")\njsonlite::write_json(list(a = c(1, 2), b = c(10, 20)), \"json-in/data.json\")\n\njson_path <- file.path(\"file://\", getwd(), \"json-in\")\n\nstream <- stream_read_json(sc, json_path) %>% stream_write_json(\"json-out\")\n\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_json.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_json.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_kafka.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_kafka.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into an kafka stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_kafka.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_kafka.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_write_kafka(\n  x,\n  mode = c(\"append\", \"complete\", \"update\"),\n  trigger = stream_trigger_interval(),\n  checkpoint = file.path(\"checkpoints\", random_string(\"\")),\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_kafka.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_kafka.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are\n\n\n\n\"append\", \"complete\" or \"update\". trigger | The trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous. checkpoint | The location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance. options | A list of strings with additional options. partition_by | Partitions the output by the given list of columns. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_kafka.html#details",
    "href": "packages/sparklyr/latest/reference/stream_write_kafka.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nPlease note that Kafka requires installing the appropriate package by setting the packages parameter to \"kafka\" in spark_connect()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_kafka.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_kafka.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.3\", packages = \"kafka\")\n\nread_options <- list(kafka.bootstrap.servers = \"localhost:9092\", subscribe = \"topic1\")\nwrite_options <- list(kafka.bootstrap.servers = \"localhost:9092\", topic = \"topic2\")\n\nstream <- stream_read_kafka(sc, options = read_options) %>%\n  stream_write_kafka(options = write_options)\n\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_kafka.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_kafka.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_memory(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_memory.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_memory.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into a memory stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_memory.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_memory.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_write_memory(\n  x,\n  name = random_string(\"sparklyr_tmp_\"),\n  mode = c(\"append\", \"complete\", \"update\"),\n  trigger = stream_trigger_interval(),\n  checkpoint = file.path(\"checkpoints\", name, random_string(\"\")),\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_memory.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_memory.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nname\nThe name to assign to the newly generated stream.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are\n\n\n\n\"append\", \"complete\" or \"update\". trigger | The trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous. checkpoint | The location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance. options | A list of strings with additional options. partition_by | Partitions the output by the given list of columns. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_memory.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_memory.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nsc <- spark_connect(master = \"local\")\n\ndir.create(\"csv-in\")\nwrite.csv(iris, \"csv-in/data.csv\", row.names = FALSE)\n\ncsv_path <- file.path(\"file://\", getwd(), \"csv-in\")\n\nstream <- stream_read_csv(sc, csv_path) %>% stream_write_memory(\"csv-out\")\n\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_memory.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_memory.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_orc(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_orc.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_orc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into an https://orc.apache.org/ORC stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_orc.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_orc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_write_orc(\n  x,\n  path,\n  mode = c(\"append\", \"complete\", \"update\"),\n  trigger = stream_trigger_interval(),\n  checkpoint = file.path(path, \"checkpoints\", random_string(\"\")),\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_orc.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_orc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe destination path. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. mode | Specifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\". trigger | The trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous. checkpoint | The location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance. options | A list of strings with additional options. partition_by | Partitions the output by the given list of columns. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_orc.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_orc.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nsc <- spark_connect(master = \"local\")\n\nsdf_len(sc, 10) %>% spark_write_orc(\"orc-in\")\n\nstream <- stream_read_orc(sc, \"orc-in\") %>% stream_write_orc(\"orc-out\")\n\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_orc.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_orc.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_parquet(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into a parquet stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_write_parquet(\n  x,\n  path,\n  mode = c(\"append\", \"complete\", \"update\"),\n  trigger = stream_trigger_interval(),\n  checkpoint = file.path(path, \"checkpoints\", random_string(\"\")),\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe destination path. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. mode | Specifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\". trigger | The trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous. checkpoint | The location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance. options | A list of strings with additional options. partition_by | Partitions the output by the given list of columns. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nsc <- spark_connect(master = \"local\")\n\nsdf_len(sc, 10) %>% spark_write_parquet(\"parquet-in\")\n\nstream <- stream_read_parquet(sc, \"parquet-in\") %>% stream_write_parquet(\"parquet-out\")\n\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_parquet.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_parquet.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_text()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html#description",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nWrites a Spark dataframe stream into a text stream."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html#usage",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nstream_write_text(\n  x,\n  path,\n  mode = c(\"append\", \"complete\", \"update\"),\n  trigger = stream_trigger_interval(),\n  checkpoint = file.path(path, \"checkpoints\", random_string(\"\")),\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html#arguments",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe destination path. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://” and “file://” protocols. mode | Specifies how data is written to a streaming sink. Valid values are \"append\", \"complete\" or \"update\". trigger | The trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous. checkpoint | The location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance. options | A list of strings with additional options. partition_by | Partitions the output by the given list of columns. … | Optional arguments; currently unused."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html#examples",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n\nsc <- spark_connect(master = \"local\")\n\ndir.create(\"text-in\")\nwriteLines(\"A text entry\", \"text-in/text.txt\")\n\ntext_path <- file.path(\"file://\", getwd(), \"text-in\")\n\nstream <- stream_read_text(sc, text_path) %>% stream_write_text(\"text-out\")\n\nstream_stop(stream)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/stream_write_text.html#see-also",
    "href": "packages/sparklyr/latest/reference/stream_write_text.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nOther Spark stream serialization: stream_read_csv(), stream_read_delta(), stream_read_json(), stream_read_kafka(), stream_read_orc(), stream_read_parquet(), stream_read_socket(), stream_read_text(), stream_write_console(), stream_write_csv(), stream_write_delta(), stream_write_json(), stream_write_kafka(), stream_write_memory(), stream_write_orc(), stream_write_parquet()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#description",
    "href": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSusetting operator for Spark dataframe allowing a subset of column(s) to be selected using syntaxes similar to those supported by R dataframes"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#usage",
    "href": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\n[tbl_spark(x, i)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#arguments",
    "href": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nThe Spark dataframe\n\n\ni\nExpression specifying subset of column(s) to include or exclude\n\n\n\nfrom the result (e.g., [\"col1\"], [c(\"col1\", \"col2\")], [1:10], [-1], [NULL], or [])"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#examples",
    "href": "packages/sparklyr/latest/reference/sub-.tbl_spark.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"spark://HOST:PORT\")\nexample_sdf <- copy_to(sc, tibble::tibble(a = 1, b = 2))\nexample_sdf[\"a\"] %>% print()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_cache.html#description",
    "href": "packages/sparklyr/latest/reference/tbl_cache.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nForce a Spark table with name name to be loaded into memory. Operations on cached tables should normally (although not always) be more performant than the same operation performed on an uncached table."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_cache.html#usage",
    "href": "packages/sparklyr/latest/reference/tbl_cache.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ntbl_cache(sc, name, force = TRUE)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_cache.html#arguments",
    "href": "packages/sparklyr/latest/reference/tbl_cache.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe table name.\n\n\nforce\nForce the data to be loaded into memory? This is accomplished\n\n\n\nby calling the count API on the associated Spark DataFrame."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_change_db.html#description",
    "href": "packages/sparklyr/latest/reference/tbl_change_db.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nUse specific database"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_change_db.html#usage",
    "href": "packages/sparklyr/latest/reference/tbl_change_db.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ntbl_change_db(sc, name)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_change_db.html#arguments",
    "href": "packages/sparklyr/latest/reference/tbl_change_db.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe database name."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_uncache.html#description",
    "href": "packages/sparklyr/latest/reference/tbl_uncache.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nForce a Spark table with name name to be unloaded from memory."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_uncache.html#usage",
    "href": "packages/sparklyr/latest/reference/tbl_uncache.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ntbl_uncache(sc, name)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/tbl_uncache.html#arguments",
    "href": "packages/sparklyr/latest/reference/tbl_uncache.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark_connection.\n\n\nname\nThe table name."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/transform_sdf.html#description",
    "href": "packages/sparklyr/latest/reference/transform_sdf.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\ntransform a subset of column(s) in a Spark Dataframe"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/transform_sdf.html#usage",
    "href": "packages/sparklyr/latest/reference/transform_sdf.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ntransform_sdf(x, cols, fn)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/transform_sdf.html#arguments",
    "href": "packages/sparklyr/latest/reference/transform_sdf.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercible to a Spark DataFrame\n\n\ncols\nSubset of columns to apply transformation to\n\n\nfn\nTransformation function taking column name as the 1st parameter, the\n\n\n\ncorresponding org.apache.spark.sql.Column object as the 2nd parameter, and returning a transformed org.apache.spark.sql.Column object"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/unite.html#description",
    "href": "packages/sparklyr/latest/reference/unite.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee unite for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/unnest.html#description",
    "href": "packages/sparklyr/latest/reference/unnest.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSee unnest for more details."
  },
  {
    "objectID": "packages/sparklyr/latest/reference/worker_spark_apply_unbundle.html#description",
    "href": "packages/sparklyr/latest/reference/worker_spark_apply_unbundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nExtracts a bundle of dependencies required by spark_apply()"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/worker_spark_apply_unbundle.html#usage",
    "href": "packages/sparklyr/latest/reference/worker_spark_apply_unbundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nworker_spark_apply_unbundle(bundle_path, base_path, bundle_name)"
  },
  {
    "objectID": "packages/sparklyr/latest/reference/worker_spark_apply_unbundle.html#arguments",
    "href": "packages/sparklyr/latest/reference/worker_spark_apply_unbundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nbundle_path\nPath to the bundle created using spark_apply_bundle()\n\n\nbase_path\nBase path to use while extracting bundles"
  },
  {
    "objectID": "packages/sparktf/latest/index.html#overview",
    "href": "packages/sparktf/latest/index.html#overview",
    "title": "sparklyr",
    "section": "Overview",
    "text": "Overview\nsparktf is a sparklyr extension that allows writing of Spark DataFrames to TFRecord, the recommended format for persisting data to be used in training with TensorFlow."
  },
  {
    "objectID": "packages/sparktf/latest/index.html#installation",
    "href": "packages/sparktf/latest/index.html#installation",
    "title": "sparklyr",
    "section": "Installation",
    "text": "Installation\nYou can install the development version of sparktf from GitHub with:\ndevtools::install_github(\"rstudio/sparktf\")"
  },
  {
    "objectID": "packages/sparktf/latest/index.html#example",
    "href": "packages/sparktf/latest/index.html#example",
    "title": "sparklyr",
    "section": "Example",
    "text": "Example\nWe first attach the required packages and establish a Spark connection.\nlibrary(sparktf)\nlibrary(sparklyr)\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution()\nlibrary(tfdatasets)\n\nsc <- spark_connect(master = \"local\")\nCopied a sample dataset to Spark then write it to disk via spark_write_tfrecord().\ndata_path <- file.path(tempdir(), \"iris\")\niris_tbl <- sdf_copy_to(sc, iris)\n\niris_tbl %>%\n  ft_string_indexer_model(\n    \"Species\", \"label\",\n    labels = c(\"setosa\", \"versicolor\", \"virginica\")\n  ) %>%\n  spark_write_tfrecord(\n    path = data_path,\n    write_locality = \"local\"\n  )\nWe now read the saved TFRecord file and parse the contents to create a dataset object. For details, refer to the package website for tfdatasets.\ndataset <- tfrecord_dataset(list.files(data_path, full.names = TRUE)) %>%\n  dataset_map(function(example_proto) {\n    features <- list(\n      label = tf$FixedLenFeature(shape(), tf$float32),\n      Sepal_Length = tf$FixedLenFeature(shape(), tf$float32),\n      Sepal_Width = tf$FixedLenFeature(shape(), tf$float32),\n      Petal_Length = tf$FixedLenFeature(shape(), tf$float32),\n      Petal_Width = tf$FixedLenFeature(shape(), tf$float32)\n    )\n\n    features <- tf$parse_single_example(example_proto, features)\n    x <- list(\n      features$Sepal_Length, features$Sepal_Width,\n      features$Petal_Length, features$Petal_Width\n      )\n    y <- tf$one_hot(tf$cast(features$label, tf$int32), 3L)\n    list(x, y)\n  }) %>%\n  dataset_shuffle(150) %>%\n  dataset_batch(16)\nNow, we can define a Keras model using the keras package and fit it by feeding the dataset object defined above.\nmodel <- keras_model_sequential() %>%\n  layer_dense(32, activation = \"relu\", input_shape = 4) %>%\n  layer_dense(3, activation = \"softmax\")\n\nmodel %>%\n  compile(loss = \"categorical_crossentropy\", optimizer = tf$train$AdamOptimizer())\n\nhistory <- model %>%\n  fit(dataset, epochs = 100, verbose = 0)\nFinally, we can use the trained model to make some predictions.\nnew_data <- tf$constant(c(4.9, 3.2, 1.4, 0.2), shape = c(1, 4))\nmodel(new_data)\n#> tf.Tensor([[0.76382965 0.19407341 0.04209692]], shape=(1, 3), dtype=float32)"
  },
  {
    "objectID": "packages/sparktf/latest/news.html",
    "href": "packages/sparktf/latest/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "sparktf 0.1.0\nsparkdf is a sparklyr extension for reading and writing TensorFlow TFRecord files via Apache Spark."
  },
  {
    "objectID": "packages/sparktf/latest/reference/index.html",
    "href": "packages/sparktf/latest/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\nspark_read_tfrecord()\nRead a TFRecord File\n\n\nspark_write_tfrecord()\nWrite a Spark DataFrame to a TFRecord file"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_read_tfrecord.html#description",
    "href": "packages/sparktf/latest/reference/spark_read_tfrecord.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead a TFRecord file as a Spark DataFrame."
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_read_tfrecord.html#usage",
    "href": "packages/sparktf/latest/reference/spark_read_tfrecord.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_read_tfrecord(sc, name, path, schema = NULL,\n  record_type = c(\"Example\", \"SequenceExample\"), overwrite = TRUE)"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_read_tfrecord.html#arguments",
    "href": "packages/sparktf/latest/reference/spark_read_tfrecord.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark conneciton.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://”, “s3a://” and “file://” protocols.\n\n\nschema\n(Currently unsupported.) Schema of TensorFlow records. If not provided, the schema is inferred from TensorFlow records.\n\n\nrecord_type\nInput format of TensorFlow records. By default it is Example.\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_write_tfrecord.html#description",
    "href": "packages/sparktf/latest/reference/spark_write_tfrecord.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSerialize a Spark DataFrame to the TensorFlow TFRecord format for training or inference."
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_write_tfrecord.html#usage",
    "href": "packages/sparktf/latest/reference/spark_write_tfrecord.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_write_tfrecord(x, path, record_type = c(\"Example\",\n  \"SequenceExample\"), write_locality = c(\"distributed\", \"local\"),\n  mode = NULL)"
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_write_tfrecord.html#arguments",
    "href": "packages/sparktf/latest/reference/spark_write_tfrecord.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://”, and “file://” protocols. record_type | Output format of TensorFlow records. One of \"Example\" and \"SequenceExample\". write_locality | Determines whether the TensorFlow records are written locally on the workers or on a distributed file system. One of \"distributed\" and \"local\". See Details for more information. mode | A character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ‘ignore’. Notice that ‘overwrite’ will also change the column structure.\nFor more details see also http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark."
  },
  {
    "objectID": "packages/sparktf/latest/reference/spark_write_tfrecord.html#details",
    "href": "packages/sparktf/latest/reference/spark_write_tfrecord.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nFor write_locality = local, each of the workers stores on the local disk a subset of the data. The subset that is stored on each worker is determined by the partitioning of the DataFrame. Each of the partitions is coalesced into a single TFRecord file and written on the node where the partition lives. This is useful in the context of distributed training, in which each of the workers gets a subset of the data to work on. When this mode is activated, the path provided to the writer is interpreted as a base path that is created on each of the worker nodes, and that will be populated with data from the DataFrame."
  },
  {
    "objectID": "packages/sparkxgb/latest/index.html#overview",
    "href": "packages/sparkxgb/latest/index.html#overview",
    "title": "sparklyr",
    "section": "Overview",
    "text": "Overview\nsparkxgb is a sparklyr extension that provides an interface to XGBoost on Spark."
  },
  {
    "objectID": "packages/sparkxgb/latest/index.html#installation",
    "href": "packages/sparkxgb/latest/index.html#installation",
    "title": "sparklyr",
    "section": "Installation",
    "text": "Installation\nYou can install the development version of sparkxgb with:\n# sparkxgb requires the development version of sparklyr\ndevtools::install_github(\"rstudio/sparklyr\")\ndevtools::install_github(\"rstudio/sparkxgb\")"
  },
  {
    "objectID": "packages/sparkxgb/latest/index.html#example",
    "href": "packages/sparkxgb/latest/index.html#example",
    "title": "sparklyr",
    "section": "Example",
    "text": "Example\nsparkxgb supports the familiar formula interface for specifying models:\nlibrary(sparkxgb)\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris)\n\nxgb_model <- xgboost_classifier(\n  iris_tbl,\n  Species ~ .,\n  num_class = 3,\n  num_round = 50,\n  max_depth = 4\n)\n\nxgb_model %>%\n  ml_predict(iris_tbl) %>%\n  select(Species, predicted_label, starts_with(\"probability_\")) %>%\n  glimpse()\n#> Observations: ??\n#> Variables: 5\n#> Database: spark_connection\n#> $ Species                <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"…\n#> $ predicted_label        <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"…\n#> $ probability_versicolor <dbl> 0.003566429, 0.003564076, 0.003566429, 0.…\n#> $ probability_virginica  <dbl> 0.001423170, 0.002082058, 0.001423170, 0.…\n#> $ probability_setosa     <dbl> 0.9950104, 0.9943539, 0.9950104, 0.995010…\nIt also provides a Pipelines API, which means you can use a xgboost_classifier or xgboost_regressor in a pipeline as any Estimator, and do things like hyperparameter tuning:\npipeline <- ml_pipeline(sc) %>%\n  ft_r_formula(Species ~ .) %>%\n  xgboost_classifier(num_class = 3)\n\nparam_grid <- list(\n  xgboost = list(\n    max_depth = c(1, 5),\n    num_round = c(10, 50)\n  )\n)\n\ncv <- ml_cross_validator(\n  sc,\n  estimator = pipeline,\n  evaluator = ml_multiclass_classification_evaluator(\n    sc,\n    label_col = \"label\",\n    raw_prediction_col = \"rawPrediction\"\n  ),\n  estimator_param_maps = param_grid\n)\n\ncv_model <- cv %>%\n  ml_fit(iris_tbl)\n\nsummary(cv_model)\n#> Summary for CrossValidatorModel\n#>             <cross_validator_ebc61803a06b>\n#>\n#> Tuned Pipeline\n#>   with metric f1\n#>   over 4 hyperparameter sets\n#>   via 3-fold cross validation\n#>\n#> Estimator: Pipeline\n#>            <pipeline_ebc62f635bb6>\n#> Evaluator: MulticlassClassificationEvaluator\n#>            <multiclass_classification_evaluator_ebc65fbf8a19>\n#>\n#> Results Summary:\n#>          f1 num_round_1 max_depth_1\n#> 1 0.9549670          10           1\n#> 2 0.9674460          10           5\n#> 3 0.9488665          50           1\n#> 4 0.9613854          50           5"
  },
  {
    "objectID": "packages/sparkxgb/latest/news.html",
    "href": "packages/sparkxgb/latest/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "sparkxgb 0.1.2\n\nEdgar Ruiz (https://github.com/edgararuiz) will be the new maintainer of this package moving forward.\n\n\n\nsparkxgb 0.1.1\n\nsparkxgb is a sparklyr extension that provides an interface to XGBoost on Spark."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/index.html",
    "href": "packages/sparkxgb/latest/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\nxgboost_classifier()\nXGBoost Classifier\n\n\nxgboost_regressor()\nXGBoost Regressor"
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_classifier.html#description",
    "href": "packages/sparkxgb/latest/reference/xgboost_classifier.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nXGBoost classifier for Spark."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_classifier.html#usage",
    "href": "packages/sparkxgb/latest/reference/xgboost_classifier.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nxgboost_classifier(\n  x,\n  formula = NULL,\n  eta = 0.3,\n  gamma = 0,\n  max_depth = 6,\n  min_child_weight = 1,\n  max_delta_step = 0,\n  grow_policy = \"depthwise\",\n  max_bins = 16,\n  subsample = 1,\n  colsample_bytree = 1,\n  colsample_bylevel = 1,\n  lambda = 1,\n  alpha = 0,\n  tree_method = \"auto\",\n  sketch_eps = 0.03,\n  scale_pos_weight = 1,\n  sample_type = \"uniform\",\n  normalize_type = \"tree\",\n  rate_drop = 0,\n  skip_drop = 0,\n  lambda_bias = 0,\n  tree_limit = 0,\n  num_round = 1,\n  num_workers = 1,\n  nthread = 1,\n  use_external_memory = FALSE,\n  silent = 0,\n  custom_obj = NULL,\n  custom_eval = NULL,\n  missing = NaN,\n  seed = 0,\n  timeout_request_workers = 30 * 60 * 1000,\n  checkpoint_path = \"\",\n  checkpoint_interval = -1,\n  objective = \"multi:softprob\",\n  base_score = 0.5,\n  train_test_ratio = 1,\n  num_early_stopping_rounds = 0,\n  objective_type = \"classification\",\n  eval_metric = NULL,\n  maximize_evaluation_metrics = FALSE,\n  num_class = NULL,\n  base_margin_col = NULL,\n  thresholds = NULL,\n  weight_col = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"xgboost_classifier_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_classifier.html#arguments",
    "href": "packages/sparkxgb/latest/reference/xgboost_classifier.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\neta\nStep size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features and eta actually shrinks the feature weights to make the boosting process more conservative. [default=0.3] range: [0,1]\n\n\ngamma\nMinimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be. [default=0]\n\n\nmax_depth\nMaximum depth of a tree, increase this value will make model more complex / likely to be overfitting. [default=6]\n\n\nmin_child_weight\nMinimum sum of instance weight(hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be. [default=1]\n\n\nmax_delta_step\nMaximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update. [default=0]\n\n\ngrow_policy\nGrowth policy for fast histogram algorithm.\n\n\nmax_bins\nMaximum number of bins in histogram.\n\n\nsubsample\nSubsample ratio of the training instance. Setting it to 0.5 means that XGBoost randomly collected half of the data instances to grow trees and this will prevent overfitting. [default=1] range:(0,1]\n\n\ncolsample_bytree\nSubsample ratio of columns when constructing each tree. [default=1] range: (0,1]\n\n\ncolsample_bylevel\nSubsample ratio of columns for each split, in each level. [default=1] range: (0,1]\n\n\nlambda\nL2 regularization term on weights, increase this value will make model more conservative. [default=1]\n\n\nalpha\nL1 regularization term on weights, increase this value will make model more conservative, defaults to 0.\n\n\ntree_method\nThe tree construction algorithm used in XGBoost. options: ‘auto’, ‘exact’, ‘approx’ [default=‘auto’]\n\n\nsketch_eps\nThis is only used for approximate greedy algorithm. This roughly translated into O(1 / sketch_eps) number of bins. Compared to directly select number of bins, this comes with theoretical guarantee with sketch accuracy. [default=0.03] range: (0, 1)\n\n\nscale_pos_weight\nControl the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative cases) / sum(positive cases). [default=1]\n\n\nsample_type\nParameter for Dart booster. Type of sampling algorithm. “uniform”: dropped trees are selected uniformly. “weighted”: dropped trees are selected in proportion to weight. [default=“uniform”]\n\n\nnormalize_type\nParameter of Dart booster. type of normalization algorithm, options: ‘tree’, ‘forest’. [default=“tree”]\n\n\nrate_drop\nParameter of Dart booster. dropout rate. [default=0.0] range: [0.0, 1.0]\n\n\nskip_drop\nParameter of Dart booster. probability of skip dropout. If a dropout is skipped, new trees are added in the same manner as gbtree. [default=0.0] range: [0.0, 1.0]\n\n\nlambda_bias\nParameter of linear booster L2 regularization term on bias, default 0 (no L1 reg on bias because it is not important.)\n\n\ntree_limit\nLimit number of trees in the prediction; defaults to 0 (use all trees.)\n\n\nnum_round\nThe number of rounds for boosting.\n\n\nnum_workers\nnumber of workers used to train xgboost model. Defaults to 1.\n\n\nnthread\nNumber of threads used by per worker. Defaults to 1.\n\n\nuse_external_memory\nThe tree construction algorithm used in XGBoost. options: ‘auto’, ‘exact’, ‘approx’ [default=‘auto’]\n\n\nsilent\n0 means printing running messages, 1 means silent mode. default: 0\n\n\ncustom_obj\nCustomized objective function provided by user. Currently unsupported.\n\n\ncustom_eval\nCustomized evaluation function provided by user. Currently unsupported.\n\n\nmissing\nThe value treated as missing. default: Float.NaN\n\n\nseed\nRandom seed for the C++ part of XGBoost and train/test splitting.\n\n\ntimeout_request_workers\nthe maximum time to wait for the job requesting new workers. default: 30 minutes\n\n\ncheckpoint_path\nThe hdfs folder to load and save checkpoint boosters.\n\n\ncheckpoint_interval\nParam for set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the trained model will get checkpointed every 10 iterations. Note: checkpoint_path must also be set if the checkpoint interval is greater than 0.\n\n\nobjective\nSpecify the learning task and the corresponding learning objective. options: reg:linear, reg:logistic, binary:logistic, binary:logitraw, count:poisson, multi:softmax, multi:softprob, rank:pairwise, reg:gamma. default: reg:linear.\n\n\nbase_score\nParam for initial prediction (aka base margin) column name. Defaults to 0.5.\n\n\ntrain_test_ratio\nFraction of training points to use for testing.\n\n\nnum_early_stopping_rounds\nIf non-zero, the training will be stopped after a specified number of consecutive increases in any evaluation metric.\n\n\nobjective_type\nThe learning objective type of the specified custom objective and eval. Corresponding type will be assigned if custom objective is defined options: regression, classification.\n\n\neval_metric\nEvaluation metrics for validation data, a default metric will be assigned according to objective(rmse for regression, and error for classification, mean average precision for ranking). options: rmse, mae, logloss, error, merror, mlogloss, auc, aucpr, ndcg, map, gamma-deviance\n\n\nmaximize_evaluation_metrics\nWhether to maximize evaluation metrics. Defaults to FALSE (for minization.)\n\n\nnum_class\nNumber of classes.\n\n\nbase_margin_col\nParam for initial prediction (aka base margin) column name.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\nweight_col\nWeight column.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#description",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nXGBoost regressor for Spark."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#usage",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nxgboost_regressor(\n  x,\n  formula = NULL,\n  eta = 0.3,\n  gamma = 0,\n  max_depth = 6,\n  min_child_weight = 1,\n  max_delta_step = 0,\n  grow_policy = \"depthwise\",\n  max_bins = 16,\n  subsample = 1,\n  colsample_bytree = 1,\n  colsample_bylevel = 1,\n  lambda = 1,\n  alpha = 0,\n  tree_method = \"auto\",\n  sketch_eps = 0.03,\n  scale_pos_weight = 1,\n  sample_type = \"uniform\",\n  normalize_type = \"tree\",\n  rate_drop = 0,\n  skip_drop = 0,\n  lambda_bias = 0,\n  tree_limit = 0,\n  num_round = 1,\n  num_workers = 1,\n  nthread = 1,\n  use_external_memory = FALSE,\n  silent = 0,\n  custom_obj = NULL,\n  custom_eval = NULL,\n  missing = NaN,\n  seed = 0,\n  timeout_request_workers = 30 * 60 * 1000,\n  checkpoint_path = \"\",\n  checkpoint_interval = -1,\n  objective = \"reg:linear\",\n  base_score = 0.5,\n  train_test_ratio = 1,\n  num_early_stopping_rounds = 0,\n  objective_type = \"regression\",\n  eval_metric = NULL,\n  maximize_evaluation_metrics = FALSE,\n  base_margin_col = NULL,\n  weight_col = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"xgboost_regressor_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#arguments",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\neta\nStep size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features and eta actually shrinks the feature weights to make the boosting process more conservative. [default=0.3] range: [0,1]\n\n\ngamma\nMinimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be. [default=0]\n\n\nmax_depth\nMaximum depth of a tree, increase this value will make model more complex / likely to be overfitting. [default=6]\n\n\nmin_child_weight\nMinimum sum of instance weight(hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be. [default=1]\n\n\nmax_delta_step\nMaximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update. [default=0]\n\n\ngrow_policy\nGrowth policy for fast histogram algorithm.\n\n\nmax_bins\nMaximum number of bins in histogram.\n\n\nsubsample\nSubsample ratio of the training instance. Setting it to 0.5 means that XGBoost randomly collected half of the data instances to grow trees and this will prevent overfitting. [default=1] range:(0,1]\n\n\ncolsample_bytree\nSubsample ratio of columns when constructing each tree. [default=1] range: (0,1]\n\n\ncolsample_bylevel\nSubsample ratio of columns for each split, in each level. [default=1] range: (0,1]\n\n\nlambda\nL2 regularization term on weights, increase this value will make model more conservative. [default=1]\n\n\nalpha\nL1 regularization term on weights, increase this value will make model more conservative, defaults to 0.\n\n\ntree_method\nThe tree construction algorithm used in XGBoost. options: ‘auto’, ‘exact’, ‘approx’ [default=‘auto’]\n\n\nsketch_eps\nThis is only used for approximate greedy algorithm. This roughly translated into O(1 / sketch_eps) number of bins. Compared to directly select number of bins, this comes with theoretical guarantee with sketch accuracy. [default=0.03] range: (0, 1)\n\n\nscale_pos_weight\nControl the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative cases) / sum(positive cases). [default=1]\n\n\nsample_type\nParameter for Dart booster. Type of sampling algorithm. “uniform”: dropped trees are selected uniformly. “weighted”: dropped trees are selected in proportion to weight. [default=“uniform”]\n\n\nnormalize_type\nParameter of Dart booster. type of normalization algorithm, options: ‘tree’, ‘forest’. [default=“tree”]\n\n\nrate_drop\nParameter of Dart booster. dropout rate. [default=0.0] range: [0.0, 1.0]\n\n\nskip_drop\nParameter of Dart booster. probability of skip dropout. If a dropout is skipped, new trees are added in the same manner as gbtree. [default=0.0] range: [0.0, 1.0]\n\n\nlambda_bias\nParameter of linear booster L2 regularization term on bias, default 0 (no L1 reg on bias because it is not important.)\n\n\ntree_limit\nLimit number of trees in the prediction; defaults to 0 (use all trees.)\n\n\nnum_round\nThe number of rounds for boosting.\n\n\nnum_workers\nnumber of workers used to train xgboost model. Defaults to 1.\n\n\nnthread\nNumber of threads used by per worker. Defaults to 1.\n\n\nuse_external_memory\nThe tree construction algorithm used in XGBoost. options: ‘auto’, ‘exact’, ‘approx’ [default=‘auto’]\n\n\nsilent\n0 means printing running messages, 1 means silent mode. default: 0\n\n\ncustom_obj\nCustomized objective function provided by user. Currently unsupported.\n\n\ncustom_eval\nCustomized evaluation function provided by user. Currently unsupported.\n\n\nmissing\nThe value treated as missing. default: Float.NaN\n\n\nseed\nRandom seed for the C++ part of XGBoost and train/test splitting.\n\n\ntimeout_request_workers\nthe maximum time to wait for the job requesting new workers. default: 30 minutes\n\n\ncheckpoint_path\nThe hdfs folder to load and save checkpoint boosters.\n\n\ncheckpoint_interval\nParam for set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the trained model will get checkpointed every 10 iterations. Note: checkpoint_path must also be set if the checkpoint interval is greater than 0.\n\n\nobjective\nSpecify the learning task and the corresponding learning objective. options: reg:linear, reg:logistic, binary:logistic, binary:logitraw, count:poisson, multi:softmax, multi:softprob, rank:pairwise, reg:gamma. default: reg:linear.\n\n\nbase_score\nParam for initial prediction (aka base margin) column name. Defaults to 0.5.\n\n\ntrain_test_ratio\nFraction of training points to use for testing.\n\n\nnum_early_stopping_rounds\nIf non-zero, the training will be stopped after a specified number of consecutive increases in any evaluation metric.\n\n\nobjective_type\nThe learning objective type of the specified custom objective and eval. Corresponding type will be assigned if custom objective is defined options: regression, classification.\n\n\neval_metric\nEvaluation metrics for validation data, a default metric will be assigned according to objective(rmse for regression, and error for classification, mean average precision for ranking). options: rmse, mae, logloss, error, merror, mlogloss, auc, aucpr, ndcg, map, gamma-deviance\n\n\nmaximize_evaluation_metrics\nWhether to maximize evaluation metrics. Defaults to FALSE (for minization.)\n\n\nbase_margin_col\nParam for initial prediction (aka base margin) column name.\n\n\nweight_col\nWeight column.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#details",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#value",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparkxgb/latest/reference/xgboost_regressor.html#see-also",
    "href": "packages/sparkxgb/latest/reference/xgboost_regressor.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee http://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms."
  },
  {
    "objectID": "guides/tidymodels.html#broom",
    "href": "guides/tidymodels.html#broom",
    "title": "tidymodels and Spark",
    "section": "broom",
    "text": "broom\nThe broom package offers great ways to get summarized information about a fitted model. sparklyr offers integration for parsnip based,\n\ntidy() - Summarizes information about the components of a model. A model component might be a single term in a regression, a single hypothesis, a cluster, or a class.\nglance() - Returns a tibble::tibble() with exactly one row of model summaries. The summaries are typically goodness of fit measures, p-values for hypothesis tests on residuals, or model convergence information.\n\n\nList of supported models\nCurrently, 20 Spark models support broom via sparklyr. Here is the current list of models and the corresponding sparklyr function:\n\n\n\n\n\n  \n    \n      Models that support glance(), tidy(), and augment()\n    \n    \n  \n  \n    \n      Model\n      Function\n    \n  \n  \n     ALS\nml_als()\n\n     Bisecting K-Means Clustering\nml_bisecting_kmeans()\n\n     Decision Trees\nml_decision_tree()\n\n     Gaussian Mixture clustering.\nml_gaussian_mixture()\n\n     Generalized Linear Regression\nml_generalized_linear_regression()\n\n     Gradient Boosted Trees\nml_gradient_boosted_trees()\n\n     Isotonic Regression\nml_isotonic_regression()\n\n     K-Means Clustering\nml_kmeans()\n\n     Latent Dirichlet Allocation\nml_lda()\n\n     Linear Regression\nml_linear_regression()\n\n     LinearSVC\nml_linear_svc()\n\n     Logistic Regression\nml_logistic_regression()\n\n     Multilayer Perceptron\nml_multilayer_perceptron()\n\n     Naive-Bayes\nml_naive_bayes()\n\n     Random Forest\nml_random_forest()\n\n     Survival Regression\nml_aft_survival_regression()\n\n    PCA (Estimator)\nml_pca()\n\n  \n  \n  \n\n\n\n\n\n\nExample\n\ntidy(mtcars_model)\n\nWarning: `select_vars()` was deprecated in dplyr 0.8.4.\nPlease use `tidyselect::vars_select()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\n# A tibble: 11 × 5\n   term        estimate std.error statistic p.value\n   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)  12.3      18.7        0.657  0.518 \n 2 cyl          -0.111     1.05      -0.107  0.916 \n 3 disp          0.0133    0.0179     0.747  0.463 \n 4 hp           -0.0215    0.0218    -0.987  0.335 \n 5 drat          0.787     1.64       0.481  0.635 \n 6 wt           -3.72      1.89      -1.96   0.0633\n 7 qsec          0.821     0.731      1.12   0.274 \n 8 vs            0.318     2.10       0.151  0.881 \n 9 am            2.52      2.06       1.23   0.234 \n10 gear          0.655     1.49       0.439  0.665 \n11 carb         -0.199     0.829     -0.241  0.812 \n\n\n\nglance(mtcars_model)\n\n# A tibble: 1 × 5\n  explained.varian… mean.absolute.e… mean.squared.er… r.squared root.mean.squar…\n              <dbl>            <dbl>            <dbl>     <dbl>            <dbl>\n1              30.6             1.72             4.61     0.869             2.15\n\n\n\ntidy(iris_model)\n\n# A tibble: 4 × 2\n  feature      importance\n  <chr>             <dbl>\n1 Petal_Length     0.478 \n2 Petal_Width      0.380 \n3 Sepal_Length     0.120 \n4 Sepal_Width      0.0227\n\n\n\nglance(iris_model)\n\n# A tibble: 1 × 5\n  num_trees total_num_nodes max_depth impurity subsampling_rate\n      <int>           <int>     <int> <chr>               <dbl>\n1       100            1450         5 gini                    1"
  },
  {
    "objectID": "guides/tidymodels.html#parsnip",
    "href": "guides/tidymodels.html#parsnip",
    "title": "tidymodels and Spark",
    "section": "parsnip",
    "text": "parsnip\nparsnip provides a common interface to models. This enables us to run the same model against multiple engines. With one change, we can easily run a Random Forest model against the randomForest, ranger and sparklyr packages. parsnip contains translation for each of these packages, so we do not have to remember, or find out, how to setup each argument in the respective package.\n\nList of supported models\n\n\n\n\n\n  \n  \n    \n      Model\n      parsnip function\n      Classification\n      Regression\n    \n  \n  \n    Boosted trees\nboost_tree()\n\nYes\nYes\n    Decision trees\ndecision_tree()\n\nYes\nYes\n    Linear regression\nlinear_reg()\n\n\nYes\n    Logistic regression\nlogistic_reg()\n\nYes\n\n    Multinomial regression\nmultinom_reg()\n\nYes\n\n    Random forest\nrand_forest()\n\nYes\nYes\n  \n  \n  \n\n\n\n\n\n\nExamples\n\nsc <- spark_connect(\"local\")\n\nspark_mtcars <- copy_to(sc, mtcars)\n\n\nmtcars_model <- linear_reg() %>%\n  set_engine(\"spark\") %>%\n  fit(mpg ~ ., data = spark_mtcars)\n\nmtcars_model\n\nparsnip model object\n\nFit time:  4.9s \nFormula: mpg ~ .\n\nCoefficients:\n(Intercept)         cyl        disp          hp        drat          wt \n12.30337416 -0.11144048  0.01333524 -0.02148212  0.78711097 -3.71530393 \n       qsec          vs          am        gear        carb \n 0.82104075  0.31776281  2.52022689  0.65541302 -0.19941925 \n\n\n\nlinear_reg() %>%\n  set_engine(\"spark\") %>%\n  translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: spark \n\nModel fit template:\nsparklyr::ml_linear_regression(x = missing_arg(), formula = missing_arg(), \n    weight_col = missing_arg())\n\n\n\nspark_iris <- copy_to(sc, iris)\n\n\niris_model <- rand_forest(trees = 100) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"spark\") %>% \n  fit(Species ~., data = spark_iris)\n\niris_model\n\nparsnip model object\n\nFit time:  5.8s \nFormula: Species ~ .\n\nRandomForestClassificationModel: uid=random_forest__66e83d53_ea61_43b6_8a66_f68adacb2f94, numTrees=100, numClasses=3, numFeatures=4\n\n\n\nrand_forest(trees = 100) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"spark\") %>% \n  translate()\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 100\n\nComputational engine: spark \n\nModel fit template:\nsparklyr::ml_random_forest(x = missing_arg(), formula = missing_arg(), \n    type = \"classification\", num_trees = 100, seed = sample.int(10^5, \n        1))"
  },
  {
    "objectID": "guides/tidymodels.html#model-preparation-with-parsnip",
    "href": "guides/tidymodels.html#model-preparation-with-parsnip",
    "title": "tidymodels and Spark",
    "section": "Model preparation with parsnip",
    "text": "Model preparation with parsnip\nparsnip provides a common interface to models. This enables us to run the same model against multiple engines. parsnip contains translation for each of these packages, so we do not have to remember, or find out, how to setup each argument in the respective package.\n\nWhy use in Spark?\nIn some cases, it is better to try out model parameters on a smaller, local, data set in R. Once we are happy with the parameters, we can then run the model over the entire data set in Spark.\nFor example, doing this for a Linear Regression model, we would use lm() locally, and then we would have to re-write the model using ml_linear_regression(). Both of these functions have different sets of function arguments that we would need to set.\nparsnip allows us to use the exact same set of functions and arguments when running against either back-end. With a couple of small changes, we can change the target engine (R vs Spark) and the target data set (local vs remote). Here is an example of what the model fitting looks like locally in R:\n\nlinear_reg() %>%\n  set_engine(\"lm\") %>%           # << Engine set to `lm`\n  fit(mpg ~ ., data = mtcars)    # << Local `mtcars`\n\nTo switch to Spark, we just need to change the engine to spark, and the training data set to the remote Spark data set:\n\nlinear_reg() %>%\n  set_engine(\"spark\") %>%           # << Engine set to `spark`\n  fit(mpg ~ ., data = spark_mtcars) # << Remote `mtcars`\n\n\n\nList of supported models\nThere are six parsnip models that currently support sparklyr equivalent models. Here is the list:\n\n\n\n\n\n  \n  \n    \n      Model\n      parsnip function\n      Classification\n      Regression\n    \n  \n  \n    Boosted trees\nboost_tree()\n\nYes\nYes\n    Decision trees\ndecision_tree()\n\nYes\nYes\n    Linear regression\nlinear_reg()\n\n\nYes\n    Logistic regression\nlogistic_reg()\n\nYes\n\n    Multinomial regression\nmultinom_reg()\n\nYes\n\n    Random forest\nrand_forest()\n\nYes\nYes\n  \n  \n  \n\n\n\n\n\n\nExamples\nThis article will use the same Spark session in all the examples.\n\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc <- spark_connect(\"local\")\n\nWe will upload the mtcars data set to the Spark session:\n\nspark_mtcars <- copy_to(sc, mtcars)\n\nA Linear Regression model is prepared against spark_mtcars:\n\nlibrary(parsnip)\n\nmtcars_model <- linear_reg() %>%\n  set_engine(\"spark\") %>%\n  fit(mpg ~ ., data = spark_mtcars)\n\nmtcars_model\n\nparsnip model object\n\nFit time:  4.5s \nFormula: mpg ~ .\n\nCoefficients:\n(Intercept)         cyl        disp          hp        drat          wt \n12.30337416 -0.11144048  0.01333524 -0.02148212  0.78711097 -3.71530393 \n       qsec          vs          am        gear        carb \n 0.82104075  0.31776281  2.52022689  0.65541302 -0.19941925 \n\n\nIt is also possible to see how parsnip plans to translate the model against the given engine. Use translate() so view the translation:\n\nlinear_reg() %>%\n  set_engine(\"spark\") %>%\n  translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: spark \n\nModel fit template:\nsparklyr::ml_linear_regression(x = missing_arg(), formula = missing_arg(), \n    weight_col = missing_arg())\n\n\nNow, we will show an example with a classification model. We will fit a Random Forest model. To start, we will copy the iris data set to the Spark session:\n\nspark_iris <- copy_to(sc, iris)\n\nWe can prepare the model by piping the initial setup of 100 trees, then then to set the mode to “classification”, and then the engine to “spark” and lastly, fit the model:\n\niris_model <- rand_forest(trees = 100) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"spark\") %>% \n  fit(Species ~., data = spark_iris)\n\niris_model\n\nparsnip model object\n\nFit time:  5.7s \nFormula: Species ~ .\n\nRandomForestClassificationModel: uid=random_forest__d0cf55ae_2749_4b70_8238_143bf17cd567, numTrees=100, numClasses=3, numFeatures=4"
  },
  {
    "objectID": "guides/tidymodels.html#model-results-with-broom",
    "href": "guides/tidymodels.html#model-results-with-broom",
    "title": "tidymodels and Spark",
    "section": "Model results with broom",
    "text": "Model results with broom\nThe broom package offers great ways to get summarized information about a fitted model. There is support for three broom functions in sparklyr:\n\ntidy() - Summarizes information about the components of a model. A model component might be a single term in a regression, a single hypothesis, a cluster, or a class.\nglance() - Returns a data frame with exactly one row of model summaries. The summaries are typically goodness of fit measures, p-values for hypothesis tests on residuals, or model convergence information.\naugment() - It is similar to how ml_predict() works. Instead of returning a vector of predictions, like predict() works, augment() adds the prediction columns to the data set.\n\n\nWhy use in Spark?\ntidy() and glance() offer a very good, concise way to view the model results in a rectangular data frame. This is very helpful when we want to compare different model runs side-by-side.\n\n\nList of supported models\nCurrently, 20 Spark models support broom via sparklyr. Here is the current list of models and the corresponding sparklyr function:\n\n\n\n\n\n  \n    \n      Models that support glance(), tidy(), and augment()\n    \n    \n  \n  \n    \n      Model\n      Function\n    \n  \n  \n     ALS\nml_als()\n\n     Bisecting K-Means Clustering\nml_bisecting_kmeans()\n\n     Decision Trees\nml_decision_tree()\n\n     Gaussian Mixture clustering.\nml_gaussian_mixture()\n\n     Generalized Linear Regression\nml_generalized_linear_regression()\n\n     Gradient Boosted Trees\nml_gradient_boosted_trees()\n\n     Isotonic Regression\nml_isotonic_regression()\n\n     K-Means Clustering\nml_kmeans()\n\n     Latent Dirichlet Allocation\nml_lda()\n\n     Linear Regression\nml_linear_regression()\n\n     LinearSVC\nml_linear_svc()\n\n     Logistic Regression\nml_logistic_regression()\n\n     Multilayer Perceptron\nml_multilayer_perceptron()\n\n     Naive-Bayes\nml_naive_bayes()\n\n     Random Forest\nml_random_forest()\n\n     Survival Regression\nml_aft_survival_regression()\n\n    PCA (Estimator)\nml_pca()\n\n  \n  \n  \n\n\n\n\n\n\nExamples\nUsing the same Spark session and models created in the previous section we start by loading broom:\n\nlibrary(broom)\n\nTo view the estimates for each term simply pass mtcars_model to the tidy() function:\n\ntidy(mtcars_model)\n\n# A tibble: 11 × 5\n   term        estimate std.error statistic p.value\n   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)  12.3      18.7        0.657  0.518 \n 2 cyl          -0.111     1.05      -0.107  0.916 \n 3 disp          0.0133    0.0179     0.747  0.463 \n 4 hp           -0.0215    0.0218    -0.987  0.335 \n 5 drat          0.787     1.64       0.481  0.635 \n 6 wt           -3.72      1.89      -1.96   0.0633\n 7 qsec          0.821     0.731      1.12   0.274 \n 8 vs            0.318     2.10       0.151  0.881 \n 9 am            2.52      2.06       1.23   0.234 \n10 gear          0.655     1.49       0.439  0.665 \n11 carb         -0.199     0.829     -0.241  0.812 \n\n\nglance() returns the the models R Squared, error means, and variance:\n\nglance(mtcars_model)\n\n# A tibble: 1 × 5\n  explained.varian… mean.absolute.e… mean.squared.er… r.squared root.mean.squar…\n              <dbl>            <dbl>            <dbl>     <dbl>            <dbl>\n1              30.6             1.72             4.61     0.869             2.15\n\n\n\naugment(mtcars_model)\n\n\n\n\n\n\n\nCaution\n\n\n\nAs of sparklyr version 1.7.5, a sparklyr model fitted through parsnip will not work with augment(). That is a bug that we will work to resolve.\nPass the fit element to augment(). A model fitted using the ML function, such as ml_linear_regression() currently works.\n\naugment(mtcars_model$fit)\n\n# Source: spark<?> [?? x 13]\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb fitted\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4   22.6\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4   22.1\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1   26.3\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1   21.2\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2   17.7\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1   20.4\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4   14.4\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2   22.5\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2   24.4\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4   18.7\n# … with more rows, and 1 more variable: resid <dbl>\n\n\n\n\nFor our classification model, tidy() returns each feature’s importance:\n\ntidy(iris_model)\n\n# A tibble: 4 × 2\n  feature      importance\n  <chr>             <dbl>\n1 Petal_Width      0.460 \n2 Petal_Length     0.423 \n3 Sepal_Length     0.0996\n4 Sepal_Width      0.0176\n\n\nThe glance() model returns the number of trees, nodes depth, sub-sampling rate and impurtiy mode:\n\nglance(iris_model)\n\n# A tibble: 1 × 5\n  num_trees total_num_nodes max_depth impurity subsampling_rate\n      <int>           <int>     <int> <chr>               <dbl>\n1       100            1436         5 gini                    1"
  },
  {
    "objectID": "guides/tidymodels.html#correlations-using-corrr",
    "href": "guides/tidymodels.html#correlations-using-corrr",
    "title": "tidymodels and Spark",
    "section": "Correlations using corrr",
    "text": "Correlations using corrr\nThe corrr package helps with exploring data correlations in R. It returns a data frame with all of the correlations.\n\nWhy use in Spark?\nFor sparklyr, corrr wraps the ml_cor() function, and returns a data frame with the exact same format as if the correlation would have been calculated in R. This allows us to use all the other functions inside corrr, such as filtering, and plotting without having to re-run the correlation inside Spark.\n\n\nExample\nWe start by loading the package corrr:\n\nlibrary(corrr)\n\nWe will pipe spark_mtcars into the correlate() function. That runs the correlations inside Spark, and returning the results into R. Those results are saved into a data frame:\n\ncorr_mtcars <- spark_mtcars %>% \n  correlate()\n\nThe corr_mtcars variable is now a local data set. So we do not need to go back to Spark if we wish to use it for other things that corrr can do:\n\ncorr_mtcars\n\n# A tibble: 11 × 12\n   term     mpg    cyl   disp     hp    drat     wt    qsec     vs      am\n   <chr>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>  <dbl>   <dbl>  <dbl>   <dbl>\n 1 mpg   NA     -0.852 -0.848 -0.776  0.681  -0.868  0.419   0.664  0.600 \n 2 cyl   -0.852 NA      0.902  0.832 -0.700   0.782 -0.591  -0.811 -0.523 \n 3 disp  -0.848  0.902 NA      0.791 -0.710   0.888 -0.434  -0.710 -0.591 \n 4 hp    -0.776  0.832  0.791 NA     -0.449   0.659 -0.708  -0.723 -0.243 \n 5 drat   0.681 -0.700 -0.710 -0.449 NA      -0.712  0.0912  0.440  0.713 \n 6 wt    -0.868  0.782  0.888  0.659 -0.712  NA     -0.175  -0.555 -0.692 \n 7 qsec   0.419 -0.591 -0.434 -0.708  0.0912 -0.175 NA       0.745 -0.230 \n 8 vs     0.664 -0.811 -0.710 -0.723  0.440  -0.555  0.745  NA      0.168 \n 9 am     0.600 -0.523 -0.591 -0.243  0.713  -0.692 -0.230   0.168 NA     \n10 gear   0.480 -0.493 -0.556 -0.126  0.700  -0.583 -0.213   0.206  0.794 \n11 carb  -0.551  0.527  0.395  0.750 -0.0908  0.428 -0.656  -0.570  0.0575\n# … with 2 more variables: gear <dbl>, carb <dbl>\n\n\nFor example, share() removes the duplicate correlations from the data set, making it easier to read:\n\ncorr_mtcars %>% \n  shave()\n\n# A tibble: 11 × 12\n   term     mpg    cyl   disp     hp    drat     wt   qsec     vs      am   gear\n   <chr>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>  <dbl>\n 1 mpg   NA     NA     NA     NA     NA      NA     NA     NA     NA      NA    \n 2 cyl   -0.852 NA     NA     NA     NA      NA     NA     NA     NA      NA    \n 3 disp  -0.848  0.902 NA     NA     NA      NA     NA     NA     NA      NA    \n 4 hp    -0.776  0.832  0.791 NA     NA      NA     NA     NA     NA      NA    \n 5 drat   0.681 -0.700 -0.710 -0.449 NA      NA     NA     NA     NA      NA    \n 6 wt    -0.868  0.782  0.888  0.659 -0.712  NA     NA     NA     NA      NA    \n 7 qsec   0.419 -0.591 -0.434 -0.708  0.0912 -0.175 NA     NA     NA      NA    \n 8 vs     0.664 -0.811 -0.710 -0.723  0.440  -0.555  0.745 NA     NA      NA    \n 9 am     0.600 -0.523 -0.591 -0.243  0.713  -0.692 -0.230  0.168 NA      NA    \n10 gear   0.480 -0.493 -0.556 -0.126  0.700  -0.583 -0.213  0.206  0.794  NA    \n11 carb  -0.551  0.527  0.395  0.750 -0.0908  0.428 -0.656 -0.570  0.0575  0.274\n# … with 1 more variable: carb <dbl>\n\n\nrplot() provides a nice way to visualize the correlations. Again, because corr_mtcars’s data it is currently locally in R, plotting requires no extra steps:\n\ncorr_mtcars %>% \n  rplot()"
  },
  {
    "objectID": "guides/tidymodels.html#intro",
    "href": "guides/tidymodels.html#intro",
    "title": "tidymodels and Spark",
    "section": "Intro",
    "text": "Intro\ntidymodels is a collection of packages for modeling and Machine Learning. Just as sparklyr, tidymodels uses tidyverse principles.\nsparklyr allows us to use dplyr verbs to manipulate data. We use the same commands in R when manipulating local data or Spark data. Similarly, sparklyr and some packages in the tidymodels ecosystem offer integration.\nAs with any evolving framework, the integration does not apply to all functions. This article aims at enumerating what is available today, and why should we consider using the tidymodels implementation in our day-to-day work with Spark.\nOur expectation is that this article will be constantly updated as the integration between tidymodels and sparklyr grows and improves."
  }
]