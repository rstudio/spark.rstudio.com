{
  "hash": "c734823d1ed30404dae173adf360f6df",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Compute (Approximate) Quantiles with a Spark DataFrame\"\nexecute:\n  freeze: true\n---\n\n\n\n*R/sdf_interface.R*\n\n## sdf_quantile\n\n## Description\n Given a numeric column within a Spark DataFrame, compute approximate quantiles. \n\n\n## Usage\n```r\n \nsdf_quantile( \n  x, \n  column, \n  probabilities = c(0, 0.25, 0.5, 0.75, 1), \n  relative.error = 1e-05, \n  weight.column = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | A `spark_connection`, `ml_pipeline`, or a `tbl_spark`. |\n| column | The column(s) for which quantiles should be computed. Multiple columns are only supported in Spark 2.0+. |\n| probabilities | A numeric vector of probabilities, for which quantiles should be computed. |\n| relative.error | The maximal possible difference between the actual percentile of a result and its expected percentile (e.g., if `relative.error` is 0.01 and `probabilities` is 0.95, then any value between the 94th and 96th percentile will be considered an acceptable approximation). |\n| weight.column | If not NULL, then a generalized version of the Greenwald- Khanna algorithm will be run to compute weighted percentiles, with each sample from `column` having a relative weight specified by the corresponding value in `weight.column`. The weights can be considered as relative frequencies of sample data points. |\n\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}