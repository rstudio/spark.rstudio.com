{
  "hash": "d82fcb7677126d56a98e600c7595c9ea",
  "result": {
    "markdown": "---\ntitle: \"Create a Spark dataframe containing all combinations of inputs\"\nexecute:\n  freeze: true\n---\n\n\n\n\n*R/sdf_interface.R*\n\n## sdf_expand_grid\n\n## Description\n Given one or more R vectors/factors or single-column Spark dataframes, perform an expand.grid operation on all of them and store the result in a Spark dataframe \n\n\n## Usage\n```r\n \nsdf_expand_grid( \n  sc, \n  ..., \n  broadcast_vars = NULL, \n  memory = TRUE, \n  repartition = NULL, \n  partition_by = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| sc | The associated Spark connection. |\n| ... | Each input variable can be either a R vector/factor or a Spark dataframe. Unnamed inputs will assume the default names of 'Var1', 'Var2', etc in the result, similar to what `expand.grid` does for unnamed inputs. |\n| broadcast_vars | Indicates which input(s) should be broadcasted to all nodes of the Spark cluster during the join process (default: none). |\n| memory | Boolean; whether the resulting Spark dataframe should be cached into memory (default: TRUE) |\n| repartition | Number of partitions the resulting Spark dataframe should have |\n| partition_by | Vector of column names used for partitioning the resulting Spark dataframe, only supported for Spark 2.0+ |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n \nsc <- spark_connect(master = \"local\") \ngrid_sdf <- sdf_expand_grid(sc, seq(5), rnorm(10), letters) \n \n \n```\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}