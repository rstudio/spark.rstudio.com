---
title: Run R inside Databricks Connect
format:
  html:
    theme: default
    toc: true
execute:
    eval: true
    freeze: true
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| include: false

library(remotes)
library(dplyr)
library(dbplyr)
library(sparklyr)
library(pysparklyr)
```

*Last updated: `r lubridate::date()`*

## Intro

Support for `spark_apply()` is currently available in the development
versions of `sparklyr`, and `pysparklyr`. To install, run the following:

``` r
remotes::install_github("sparklyr/sparklyr")
remotes::install_github("mlverse/pysparklyr")
```

Databricks Connect is now able to run regular Python code inside Spark.
`sparklyr` takes advantage of this capability by having Python transport
and run the R code. It does this via the `rpy2` Python library. Using
this library also guarantees Arrow support.

::: {#fig-connect}
```{mermaid}
%%| fig-width: 6
%%| eval: true
flowchart LR
  subgraph mm[My machine]
    sp[R <br> **********  <br>sparklyr]
    rp[Python<br> **************** <br>rpy2 'packages'<br> the R code]
  end
  subgraph db[Databricks]
    subgraph sr[Spark]
      pt[Python<br> ********************* <br>rpy2 runs the R code]
    end
  end

sp --> rp
rp --> sr

style mm   fill:#fff,stroke:#666,color:#000
style sp   fill:#fff,stroke:#666,color:#000
style rp   fill:#fff,stroke:#666,color:#000
style db   fill:#fff,stroke:#666,color:#000
style sr   fill:#fff,stroke:#666,color:#000
style pt   fill:#fff,stroke:#666,color:#000
```

How `sparklyr` uses rpy2 to run R code in Databricks Connect
:::

In Python, `sparklyr` uses the `mapInPandas()` and `applyInPandas()` to
run the Python code, that in turn, runs the R code. `mapInPandas()` is
used to run R code against un-grouped data, and `applyInPandas()` is
used for grouped data, meaning when the `group_by` argument, in
`spark_apply()` is used.

## Getting started

If you have been using `sparklyr` with Databricks Connect v2 already,
then after upgrading the packages, you will be prompted to install
`rpy2` in your Python environment. The prompt will occur the first time
you use `spark_apply()` in an interactive R session. If this is the
first time you are using `sparklyr` with Databricks Connect v2, please
refer to our intro article["Databricks Connect
v2"](/deployment/databricks-connect.qmd) to learn how to setup your
environment.

As shown in the diagram on the previous section, `rpy2` is needed on the
Databricks cluster you plan to use. This means that you will need to
"manually" install the library in the cluster. This is a simple
operation that is done via your Databricks web portal. Here are the
instructions that shows you how to do that: [Databricks - Cluster
Libraries](https://docs.databricks.com/en/libraries/cluster-libraries.html).

## What is supported

| Argument                      | Supported? | Notes                                                                                                                                                                                                                                                                                                                                                                                                 |
|------------------------|------------|------------------------------------|
| `x`                           | Yes        |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `f`                           | Yes        |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `columns`                     | Yes        | Requires a string entry that contains the name of the column and its Spark variable type. Accepted values are: `long`, `decimal`, `string`, `datetime` and `bool`. Example: `columns = "x long, y string"`. If not provided, `sparklyr` will automatically create one, by examining the first 10 records of `x`, and it will provide a `columns` spec you can use when running `spark_apply()` again. |
| `memory`                      | Yes        |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `group_by`                    | Yes        |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `packages`                    | **No**     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `context`                     | **No**     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `name`                        | Yes        |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `barrier`                     | Yes        | Supports only on not-grouped data. In other words, it is valid when the `group_by` argument is used.                                                                                                                                                                                                                                                                                                  |
| `fetch_result_as_sdf`         | Yes        | At this time, `spark_apply()` inside Databricks Connect only supports rectangular data, so seeing to `FALSE` will always return a data frame.                                                                                                                                                                                                                                                         |
| `partition_index_param`       | **No**     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `arrow_max_records_per_batch` | Yes        | Supports only on not-grouped data. In other words, it is valid when the `group_by` argument is used.                                                                                                                                                                                                                                                                                                  |
| `auto_deps`                   | **No**     |                                                                                                                                                                                                                                                                                                                                                                                                       |
| `...`                         |            |                                                                                                                                                                                                                                                                                                                                                                                                       |
