<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>sparklyr</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../site-news.html" rel="next">
<link href="../../../images/favicon/apple-touch-icon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Google Tag Manager (noscript) -->
 <noscript></noscript></head><body class="nav-sidebar floating nav-fixed"><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-KHBDBW7" height="0" width="0" style="display:none;visibility:hidden"></iframe>
<!-- End Google Tag Manager (noscript) -->

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">




<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../images/favicon/apple-touch-icon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">sparklyr</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../get-started/index.html"> 
<span class="menu-text">Get Started</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../guides/index.html"> 
<span class="menu-text">Guides</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../deployment/databricks-connect.html"> 
<span class="menu-text">Databricks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../deployment/index.html"> 
<span class="menu-text">Deployment</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../../../packages/sparklyr/latest/news.html" aria-current="page"> 
<span class="menu-text">News</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../packages/sparklyr/latest/reference/index.html"> 
<span class="menu-text">Reference</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../packages/index.html"> 
<span class="menu-text">Packages</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../learn-more.html"> 
<span class="menu-text">Learn more</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/sparklyr/sparklyr"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../packages/sparklyr/latest/news.html">Package News</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../packages/sparklyr/latest/news.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Package News</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../site-news.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Site News</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sparklyr-1.8.6" id="toc-sparklyr-1.8.6" class="nav-link active" data-scroll-target="#sparklyr-1.8.6">Sparklyr 1.8.6</a></li>
  <li><a href="#sparklyr-1.8.5" id="toc-sparklyr-1.8.5" class="nav-link" data-scroll-target="#sparklyr-1.8.5">Sparklyr 1.8.5</a>
  <ul class="collapse">
  <li><a href="#fixes" id="toc-fixes" class="nav-link" data-scroll-target="#fixes">Fixes</a></li>
  <li><a href="#package-improvements" id="toc-package-improvements" class="nav-link" data-scroll-target="#package-improvements">Package improvements</a></li>
  <li><a href="#spark-improvements" id="toc-spark-improvements" class="nav-link" data-scroll-target="#spark-improvements">Spark improvements</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.8.4" id="toc-sparklyr-1.8.4" class="nav-link" data-scroll-target="#sparklyr-1.8.4">Sparklyr 1.8.4</a>
  <ul class="collapse">
  <li><a href="#compatability-with-new-dbplyr-version" id="toc-compatability-with-new-dbplyr-version" class="nav-link" data-scroll-target="#compatability-with-new-dbplyr-version">Compatability with new <code>dbplyr</code> version</a></li>
  <li><a href="#fixes-1" id="toc-fixes-1" class="nav-link" data-scroll-target="#fixes-1">Fixes</a></li>
  <li><a href="#improvements" id="toc-improvements" class="nav-link" data-scroll-target="#improvements">Improvements</a></li>
  <li><a href="#test-improvements" id="toc-test-improvements" class="nav-link" data-scroll-target="#test-improvements">Test improvements</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.8.3" id="toc-sparklyr-1.8.3" class="nav-link" data-scroll-target="#sparklyr-1.8.3">Sparklyr 1.8.3</a>
  <ul class="collapse">
  <li><a href="#improvements-1" id="toc-improvements-1" class="nav-link" data-scroll-target="#improvements-1">Improvements</a></li>
  <li><a href="#java" id="toc-java" class="nav-link" data-scroll-target="#java">Java</a></li>
  <li><a href="#fixes-2" id="toc-fixes-2" class="nav-link" data-scroll-target="#fixes-2">Fixes</a></li>
  <li><a href="#package-integration" id="toc-package-integration" class="nav-link" data-scroll-target="#package-integration">Package integration</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.8.2" id="toc-sparklyr-1.8.2" class="nav-link" data-scroll-target="#sparklyr-1.8.2">Sparklyr 1.8.2</a>
  <ul class="collapse">
  <li><a href="#new-features" id="toc-new-features" class="nav-link" data-scroll-target="#new-features">New Features</a></li>
  <li><a href="#fixes-3" id="toc-fixes-3" class="nav-link" data-scroll-target="#fixes-3">Fixes</a></li>
  <li><a href="#misc" id="toc-misc" class="nav-link" data-scroll-target="#misc">Misc</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.8.1" id="toc-sparklyr-1.8.1" class="nav-link" data-scroll-target="#sparklyr-1.8.1">Sparklyr 1.8.1</a>
  <ul class="collapse">
  <li><a href="#bug-fixes" id="toc-bug-fixes" class="nav-link" data-scroll-target="#bug-fixes">Bug Fixes</a></li>
  <li><a href="#internal-functionality" id="toc-internal-functionality" class="nav-link" data-scroll-target="#internal-functionality">Internal functionality</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.8.0" id="toc-sparklyr-1.8.0" class="nav-link" data-scroll-target="#sparklyr-1.8.0">Sparklyr 1.8.0</a>
  <ul class="collapse">
  <li><a href="#bug-fixes-1" id="toc-bug-fixes-1" class="nav-link" data-scroll-target="#bug-fixes-1">Bug Fixes</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.7.9" id="toc-sparklyr-1.7.9" class="nav-link" data-scroll-target="#sparklyr-1.7.9">Sparklyr 1.7.9</a>
  <ul class="collapse">
  <li><a href="#bug-fixes-2" id="toc-bug-fixes-2" class="nav-link" data-scroll-target="#bug-fixes-2">Bug Fixes</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.7.8" id="toc-sparklyr-1.7.8" class="nav-link" data-scroll-target="#sparklyr-1.7.8">Sparklyr 1.7.8</a>
  <ul class="collapse">
  <li><a href="#new-features-1" id="toc-new-features-1" class="nav-link" data-scroll-target="#new-features-1">New features</a></li>
  <li><a href="#bug-fixes-3" id="toc-bug-fixes-3" class="nav-link" data-scroll-target="#bug-fixes-3">Bug Fixes</a></li>
  <li><a href="#spark" id="toc-spark" class="nav-link" data-scroll-target="#spark">Spark</a></li>
  <li><a href="#internal-functionality-1" id="toc-internal-functionality-1" class="nav-link" data-scroll-target="#internal-functionality-1">Internal functionality</a></li>
  <li><a href="#misc-1" id="toc-misc-1" class="nav-link" data-scroll-target="#misc-1">Misc</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.7.7" id="toc-sparklyr-1.7.7" class="nav-link" data-scroll-target="#sparklyr-1.7.7">Sparklyr 1.7.7</a>
  <ul class="collapse">
  <li><a href="#dplyr" id="toc-dplyr" class="nav-link" data-scroll-target="#dplyr">dplyr</a></li>
  <li><a href="#misc-2" id="toc-misc-2" class="nav-link" data-scroll-target="#misc-2">Misc</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.7.6" id="toc-sparklyr-1.7.6" class="nav-link" data-scroll-target="#sparklyr-1.7.6">Sparklyr 1.7.6</a>
  <ul class="collapse">
  <li><a href="#misc-3" id="toc-misc-3" class="nav-link" data-scroll-target="#misc-3">Misc</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.7.5" id="toc-sparklyr-1.7.5" class="nav-link" data-scroll-target="#sparklyr-1.7.5">Sparklyr 1.7.5</a>
  <ul class="collapse">
  <li><a href="#misc-4" id="toc-misc-4" class="nav-link" data-scroll-target="#misc-4">Misc</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.7.4" id="toc-sparklyr-1.7.4" class="nav-link" data-scroll-target="#sparklyr-1.7.4">Sparklyr 1.7.4</a>
  <ul class="collapse">
  <li><a href="#misc-5" id="toc-misc-5" class="nav-link" data-scroll-target="#misc-5">Misc</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.7.3" id="toc-sparklyr-1.7.3" class="nav-link" data-scroll-target="#sparklyr-1.7.3">Sparklyr 1.7.3</a>
  <ul class="collapse">
  <li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data">Data</a></li>
  <li><a href="#misc-6" id="toc-misc-6" class="nav-link" data-scroll-target="#misc-6">Misc</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.7.2" id="toc-sparklyr-1.7.2" class="nav-link" data-scroll-target="#sparklyr-1.7.2">Sparklyr 1.7.2</a>
  <ul class="collapse">
  <li><a href="#connections" id="toc-connections" class="nav-link" data-scroll-target="#connections">Connections</a></li>
  <li><a href="#data-1" id="toc-data-1" class="nav-link" data-scroll-target="#data-1">Data</a></li>
  <li><a href="#documentation" id="toc-documentation" class="nav-link" data-scroll-target="#documentation">Documentation</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.7.1" id="toc-sparklyr-1.7.1" class="nav-link" data-scroll-target="#sparklyr-1.7.1">Sparklyr 1.7.1</a>
  <ul class="collapse">
  <li><a href="#connections-1" id="toc-connections-1" class="nav-link" data-scroll-target="#connections-1">Connections</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.7.0" id="toc-sparklyr-1.7.0" class="nav-link" data-scroll-target="#sparklyr-1.7.0">Sparklyr 1.7.0</a>
  <ul class="collapse">
  <li><a href="#data-2" id="toc-data-2" class="nav-link" data-scroll-target="#data-2">Data</a></li>
  <li><a href="#distributed-r" id="toc-distributed-r" class="nav-link" data-scroll-target="#distributed-r">Distributed R</a></li>
  <li><a href="#extensions" id="toc-extensions" class="nav-link" data-scroll-target="#extensions">Extensions</a></li>
  <li><a href="#serialization" id="toc-serialization" class="nav-link" data-scroll-target="#serialization">Serialization</a></li>
  <li><a href="#spark-ml" id="toc-spark-ml" class="nav-link" data-scroll-target="#spark-ml">Spark ML</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.6.3" id="toc-sparklyr-1.6.3" class="nav-link" data-scroll-target="#sparklyr-1.6.3">Sparklyr 1.6.3</a>
  <ul class="collapse">
  <li><a href="#data-3" id="toc-data-3" class="nav-link" data-scroll-target="#data-3">Data</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.6.2" id="toc-sparklyr-1.6.2" class="nav-link" data-scroll-target="#sparklyr-1.6.2">Sparklyr 1.6.2</a>
  <ul class="collapse">
  <li><a href="#data-4" id="toc-data-4" class="nav-link" data-scroll-target="#data-4">Data</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.6.1" id="toc-sparklyr-1.6.1" class="nav-link" data-scroll-target="#sparklyr-1.6.1">Sparklyr 1.6.1</a>
  <ul class="collapse">
  <li><a href="#data-5" id="toc-data-5" class="nav-link" data-scroll-target="#data-5">Data</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.6.0" id="toc-sparklyr-1.6.0" class="nav-link" data-scroll-target="#sparklyr-1.6.0">Sparklyr 1.6.0</a>
  <ul class="collapse">
  <li><a href="#data-6" id="toc-data-6" class="nav-link" data-scroll-target="#data-6">Data</a></li>
  <li><a href="#serialization-1" id="toc-serialization-1" class="nav-link" data-scroll-target="#serialization-1">Serialization</a></li>
  <li><a href="#connections-2" id="toc-connections-2" class="nav-link" data-scroll-target="#connections-2">Connections</a></li>
  <li><a href="#spark-ml-1" id="toc-spark-ml-1" class="nav-link" data-scroll-target="#spark-ml-1">Spark ML</a></li>
  <li><a href="#misc-7" id="toc-misc-7" class="nav-link" data-scroll-target="#misc-7">Misc</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.5.2" id="toc-sparklyr-1.5.2" class="nav-link" data-scroll-target="#sparklyr-1.5.2">Sparklyr 1.5.2</a>
  <ul class="collapse">
  <li><a href="#connections-3" id="toc-connections-3" class="nav-link" data-scroll-target="#connections-3">Connections</a></li>
  <li><a href="#data-7" id="toc-data-7" class="nav-link" data-scroll-target="#data-7">Data</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.5.1" id="toc-sparklyr-1.5.1" class="nav-link" data-scroll-target="#sparklyr-1.5.1">Sparklyr 1.5.1</a>
  <ul class="collapse">
  <li><a href="#connections-4" id="toc-connections-4" class="nav-link" data-scroll-target="#connections-4">Connections</a></li>
  <li><a href="#data-8" id="toc-data-8" class="nav-link" data-scroll-target="#data-8">Data</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.5.0" id="toc-sparklyr-1.5.0" class="nav-link" data-scroll-target="#sparklyr-1.5.0">Sparklyr 1.5.0</a>
  <ul class="collapse">
  <li><a href="#connections-5" id="toc-connections-5" class="nav-link" data-scroll-target="#connections-5">Connections</a></li>
  <li><a href="#data-9" id="toc-data-9" class="nav-link" data-scroll-target="#data-9">Data</a></li>
  <li><a href="#serialization-2" id="toc-serialization-2" class="nav-link" data-scroll-target="#serialization-2">Serialization</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.4.0" id="toc-sparklyr-1.4.0" class="nav-link" data-scroll-target="#sparklyr-1.4.0">Sparklyr 1.4.0</a>
  <ul class="collapse">
  <li><a href="#connections-6" id="toc-connections-6" class="nav-link" data-scroll-target="#connections-6">Connections</a></li>
  <li><a href="#data-10" id="toc-data-10" class="nav-link" data-scroll-target="#data-10">Data</a></li>
  <li><a href="#distributed-r-1" id="toc-distributed-r-1" class="nav-link" data-scroll-target="#distributed-r-1">Distributed R</a></li>
  <li><a href="#misc-8" id="toc-misc-8" class="nav-link" data-scroll-target="#misc-8">Misc</a></li>
  <li><a href="#spark-ml-2" id="toc-spark-ml-2" class="nav-link" data-scroll-target="#spark-ml-2">Spark ML</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.3.1" id="toc-sparklyr-1.3.1" class="nav-link" data-scroll-target="#sparklyr-1.3.1">Sparklyr 1.3.1</a>
  <ul class="collapse">
  <li><a href="#distributed-r-2" id="toc-distributed-r-2" class="nav-link" data-scroll-target="#distributed-r-2">Distributed R</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.3.0" id="toc-sparklyr-1.3.0" class="nav-link" data-scroll-target="#sparklyr-1.3.0">Sparklyr 1.3.0</a>
  <ul class="collapse">
  <li><a href="#spark-ml-3" id="toc-spark-ml-3" class="nav-link" data-scroll-target="#spark-ml-3">Spark ML</a></li>
  <li><a href="#distributed-r-3" id="toc-distributed-r-3" class="nav-link" data-scroll-target="#distributed-r-3">Distributed R</a></li>
  <li><a href="#connections-7" id="toc-connections-7" class="nav-link" data-scroll-target="#connections-7">Connections</a></li>
  <li><a href="#serialization-3" id="toc-serialization-3" class="nav-link" data-scroll-target="#serialization-3">Serialization</a></li>
  <li><a href="#data-11" id="toc-data-11" class="nav-link" data-scroll-target="#data-11">Data</a></li>
  <li><a href="#extensions-1" id="toc-extensions-1" class="nav-link" data-scroll-target="#extensions-1">Extensions</a></li>
  <li><a href="#misc-9" id="toc-misc-9" class="nav-link" data-scroll-target="#misc-9">Misc</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.2.0" id="toc-sparklyr-1.2.0" class="nav-link" data-scroll-target="#sparklyr-1.2.0">Sparklyr 1.2.0</a>
  <ul class="collapse">
  <li><a href="#distributed-r-4" id="toc-distributed-r-4" class="nav-link" data-scroll-target="#distributed-r-4">Distributed R</a></li>
  <li><a href="#data-12" id="toc-data-12" class="nav-link" data-scroll-target="#data-12">Data</a></li>
  <li><a href="#connection" id="toc-connection" class="nav-link" data-scroll-target="#connection">Connection</a></li>
  <li><a href="#misc-10" id="toc-misc-10" class="nav-link" data-scroll-target="#misc-10">Misc</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.1.0" id="toc-sparklyr-1.1.0" class="nav-link" data-scroll-target="#sparklyr-1.1.0">Sparklyr 1.1.0</a>
  <ul class="collapse">
  <li><a href="#distributed-r-5" id="toc-distributed-r-5" class="nav-link" data-scroll-target="#distributed-r-5">Distributed R</a></li>
  <li><a href="#streaming" id="toc-streaming" class="nav-link" data-scroll-target="#streaming">Streaming</a></li>
  <li><a href="#data-13" id="toc-data-13" class="nav-link" data-scroll-target="#data-13">Data</a></li>
  <li><a href="#livy" id="toc-livy" class="nav-link" data-scroll-target="#livy">Livy</a></li>
  <li><a href="#data-14" id="toc-data-14" class="nav-link" data-scroll-target="#data-14">Data</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.0.5" id="toc-sparklyr-1.0.5" class="nav-link" data-scroll-target="#sparklyr-1.0.5">Sparklyr 1.0.5</a>
  <ul class="collapse">
  <li><a href="#serialization-4" id="toc-serialization-4" class="nav-link" data-scroll-target="#serialization-4">Serialization</a></li>
  <li><a href="#data-15" id="toc-data-15" class="nav-link" data-scroll-target="#data-15">Data</a></li>
  <li><a href="#connections-8" id="toc-connections-8" class="nav-link" data-scroll-target="#connections-8">Connections</a></li>
  <li><a href="#configuration" id="toc-configuration" class="nav-link" data-scroll-target="#configuration">Configuration</a></li>
  <li><a href="#compilation" id="toc-compilation" class="nav-link" data-scroll-target="#compilation">Compilation</a></li>
  <li><a href="#yarn" id="toc-yarn" class="nav-link" data-scroll-target="#yarn">YARN</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.0.4" id="toc-sparklyr-1.0.4" class="nav-link" data-scroll-target="#sparklyr-1.0.4">Sparklyr 1.0.4</a>
  <ul class="collapse">
  <li><a href="#arrow" id="toc-arrow" class="nav-link" data-scroll-target="#arrow">Arrow</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.0.3" id="toc-sparklyr-1.0.3" class="nav-link" data-scroll-target="#sparklyr-1.0.3">Sparklyr 1.0.3</a>
  <ul class="collapse">
  <li><a href="#kuberenetes" id="toc-kuberenetes" class="nav-link" data-scroll-target="#kuberenetes">Kuberenetes</a></li>
  <li><a href="#dplyr-1" id="toc-dplyr-1" class="nav-link" data-scroll-target="#dplyr-1">dplyr</a></li>
  <li><a href="#data-16" id="toc-data-16" class="nav-link" data-scroll-target="#data-16">Data</a></li>
  <li><a href="#connections-9" id="toc-connections-9" class="nav-link" data-scroll-target="#connections-9">Connections</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.0.2" id="toc-sparklyr-1.0.2" class="nav-link" data-scroll-target="#sparklyr-1.0.2">Sparklyr 1.0.2</a>
  <ul class="collapse">
  <li><a href="#connections-10" id="toc-connections-10" class="nav-link" data-scroll-target="#connections-10">Connections</a></li>
  <li><a href="#extensions-2" id="toc-extensions-2" class="nav-link" data-scroll-target="#extensions-2">Extensions</a></li>
  <li><a href="#rstudio" id="toc-rstudio" class="nav-link" data-scroll-target="#rstudio">RStudio</a></li>
  <li><a href="#distributed" id="toc-distributed" class="nav-link" data-scroll-target="#distributed">Distributed</a></li>
  <li><a href="#ml" id="toc-ml" class="nav-link" data-scroll-target="#ml">ML</a></li>
  <li><a href="#misc-11" id="toc-misc-11" class="nav-link" data-scroll-target="#misc-11">Misc</a></li>
  <li><a href="#data-17" id="toc-data-17" class="nav-link" data-scroll-target="#data-17">Data</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.0.1" id="toc-sparklyr-1.0.1" class="nav-link" data-scroll-target="#sparklyr-1.0.1">Sparklyr 1.0.1</a>
  <ul class="collapse">
  <li><a href="#ml-1" id="toc-ml-1" class="nav-link" data-scroll-target="#ml-1">ML</a></li>
  <li><a href="#misc-12" id="toc-misc-12" class="nav-link" data-scroll-target="#misc-12">Misc</a></li>
  <li><a href="#connections-11" id="toc-connections-11" class="nav-link" data-scroll-target="#connections-11">Connections</a></li>
  <li><a href="#batches" id="toc-batches" class="nav-link" data-scroll-target="#batches">Batches</a></li>
  <li><a href="#distributed-r-6" id="toc-distributed-r-6" class="nav-link" data-scroll-target="#distributed-r-6">Distributed R</a></li>
  <li><a href="#extensions-3" id="toc-extensions-3" class="nav-link" data-scroll-target="#extensions-3">Extensions</a></li>
  <li><a href="#dataframes" id="toc-dataframes" class="nav-link" data-scroll-target="#dataframes">DataFrames</a></li>
  <li><a href="#kubernetes" id="toc-kubernetes" class="nav-link" data-scroll-target="#kubernetes">Kubernetes</a></li>
  </ul></li>
  <li><a href="#sparklyr-1.0.0" id="toc-sparklyr-1.0.0" class="nav-link" data-scroll-target="#sparklyr-1.0.0">Sparklyr 1.0.0</a>
  <ul class="collapse">
  <li><a href="#arrow-1" id="toc-arrow-1" class="nav-link" data-scroll-target="#arrow-1">Arrow</a></li>
  <li><a href="#ml-2" id="toc-ml-2" class="nav-link" data-scroll-target="#ml-2">ML</a></li>
  <li><a href="#livy-1" id="toc-livy-1" class="nav-link" data-scroll-target="#livy-1">Livy</a></li>
  <li><a href="#data-18" id="toc-data-18" class="nav-link" data-scroll-target="#data-18">Data</a></li>
  <li><a href="#broom" id="toc-broom" class="nav-link" data-scroll-target="#broom">Broom</a></li>
  <li><a href="#connections-12" id="toc-connections-12" class="nav-link" data-scroll-target="#connections-12">Connections</a></li>
  <li><a href="#serialization-5" id="toc-serialization-5" class="nav-link" data-scroll-target="#serialization-5">Serialization</a></li>
  <li><a href="#yarn-1" id="toc-yarn-1" class="nav-link" data-scroll-target="#yarn-1">YARN</a></li>
  <li><a href="#distributed-r-7" id="toc-distributed-r-7" class="nav-link" data-scroll-target="#distributed-r-7">Distributed R</a></li>
  <li><a href="#other" id="toc-other" class="nav-link" data-scroll-target="#other">Other</a></li>
  </ul></li>
  <li><a href="#sparklyr-0.9.4" id="toc-sparklyr-0.9.4" class="nav-link" data-scroll-target="#sparklyr-0.9.4">Sparklyr 0.9.4</a></li>
  <li><a href="#sparklyr-0.9.3" id="toc-sparklyr-0.9.3" class="nav-link" data-scroll-target="#sparklyr-0.9.3">Sparklyr 0.9.3</a></li>
  <li><a href="#sparklyr-0.9.2" id="toc-sparklyr-0.9.2" class="nav-link" data-scroll-target="#sparklyr-0.9.2">Sparklyr 0.9.2</a>
  <ul class="collapse">
  <li><a href="#broom-1" id="toc-broom-1" class="nav-link" data-scroll-target="#broom-1">Broom</a></li>
  </ul></li>
  <li><a href="#sparklyr-0.9.2-1" id="toc-sparklyr-0.9.2-1" class="nav-link" data-scroll-target="#sparklyr-0.9.2-1">Sparklyr 0.9.2</a></li>
  <li><a href="#sparklyr-0.9.2-2" id="toc-sparklyr-0.9.2-2" class="nav-link" data-scroll-target="#sparklyr-0.9.2-2">Sparklyr 0.9.2</a></li>
  <li><a href="#sparklyr-0.9.1" id="toc-sparklyr-0.9.1" class="nav-link" data-scroll-target="#sparklyr-0.9.1">Sparklyr 0.9.1</a></li>
  <li><a href="#sparklyr-0.9.0" id="toc-sparklyr-0.9.0" class="nav-link" data-scroll-target="#sparklyr-0.9.0">Sparklyr 0.9.0</a>
  <ul class="collapse">
  <li><a href="#streaming-1" id="toc-streaming-1" class="nav-link" data-scroll-target="#streaming-1">Streaming</a></li>
  <li><a href="#monitoring" id="toc-monitoring" class="nav-link" data-scroll-target="#monitoring">Monitoring</a></li>
  <li><a href="#kubernetes-1" id="toc-kubernetes-1" class="nav-link" data-scroll-target="#kubernetes-1">Kubernetes</a></li>
  <li><a href="#batches-1" id="toc-batches-1" class="nav-link" data-scroll-target="#batches-1">Batches</a>
  <ul class="collapse">
  <li><a href="#spark-ml-4" id="toc-spark-ml-4" class="nav-link" data-scroll-target="#spark-ml-4">Spark ML</a></li>
  <li><a href="#data-19" id="toc-data-19" class="nav-link" data-scroll-target="#data-19">Data</a></li>
  <li><a href="#livy-2" id="toc-livy-2" class="nav-link" data-scroll-target="#livy-2">Livy</a></li>
  <li><a href="#distributed-r-8" id="toc-distributed-r-8" class="nav-link" data-scroll-target="#distributed-r-8">Distributed R</a></li>
  <li><a href="#connections-13" id="toc-connections-13" class="nav-link" data-scroll-target="#connections-13">Connections</a></li>
  <li><a href="#extensions-4" id="toc-extensions-4" class="nav-link" data-scroll-target="#extensions-4">Extensions</a></li>
  <li><a href="#serialization-6" id="toc-serialization-6" class="nav-link" data-scroll-target="#serialization-6">Serialization</a></li>
  <li><a href="#documentation-1" id="toc-documentation-1" class="nav-link" data-scroll-target="#documentation-1">Documentation</a></li>
  <li><a href="#broom-2" id="toc-broom-2" class="nav-link" data-scroll-target="#broom-2">Broom</a></li>
  <li><a href="#configuration-1" id="toc-configuration-1" class="nav-link" data-scroll-target="#configuration-1">Configuration</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sparklyr-0.8.4" id="toc-sparklyr-0.8.4" class="nav-link" data-scroll-target="#sparklyr-0.8.4">Sparklyr 0.8.4</a></li>
  <li><a href="#sparklyr-0.8.3" id="toc-sparklyr-0.8.3" class="nav-link" data-scroll-target="#sparklyr-0.8.3">Sparklyr 0.8.3</a></li>
  <li><a href="#sparklyr-0.8.2" id="toc-sparklyr-0.8.2" class="nav-link" data-scroll-target="#sparklyr-0.8.2">Sparklyr 0.8.2</a></li>
  <li><a href="#sparklyr-0.8.1" id="toc-sparklyr-0.8.1" class="nav-link" data-scroll-target="#sparklyr-0.8.1">Sparklyr 0.8.1</a></li>
  <li><a href="#sparklyr-0.8.0" id="toc-sparklyr-0.8.0" class="nav-link" data-scroll-target="#sparklyr-0.8.0">Sparklyr 0.8.0</a>
  <ul class="collapse">
  <li><a href="#spark-ml-5" id="toc-spark-ml-5" class="nav-link" data-scroll-target="#spark-ml-5">Spark ML</a></li>
  <li><a href="#extensions-5" id="toc-extensions-5" class="nav-link" data-scroll-target="#extensions-5">Extensions</a></li>
  <li><a href="#distributed-r-9" id="toc-distributed-r-9" class="nav-link" data-scroll-target="#distributed-r-9">Distributed R</a></li>
  <li><a href="#miscellaneous" id="toc-miscellaneous" class="nav-link" data-scroll-target="#miscellaneous">Miscellaneous</a></li>
  </ul></li>
  <li><a href="#sparklyr-0.7.0" id="toc-sparklyr-0.7.0" class="nav-link" data-scroll-target="#sparklyr-0.7.0">Sparklyr 0.7.0</a></li>
  <li><a href="#sparklyr-0.6.4" id="toc-sparklyr-0.6.4" class="nav-link" data-scroll-target="#sparklyr-0.6.4">Sparklyr 0.6.4</a></li>
  <li><a href="#sparklyr-0.6.3" id="toc-sparklyr-0.6.3" class="nav-link" data-scroll-target="#sparklyr-0.6.3">Sparklyr 0.6.3</a></li>
  <li><a href="#sparklyr-0.6.2" id="toc-sparklyr-0.6.2" class="nav-link" data-scroll-target="#sparklyr-0.6.2">Sparklyr 0.6.2</a></li>
  <li><a href="#sparklyr-0.6.1" id="toc-sparklyr-0.6.1" class="nav-link" data-scroll-target="#sparklyr-0.6.1">Sparklyr 0.6.1</a></li>
  <li><a href="#sparklyr-0.6.0" id="toc-sparklyr-0.6.0" class="nav-link" data-scroll-target="#sparklyr-0.6.0">Sparklyr 0.6.0</a>
  <ul class="collapse">
  <li><a href="#distributed-r-10" id="toc-distributed-r-10" class="nav-link" data-scroll-target="#distributed-r-10">Distributed R</a></li>
  <li><a href="#external-data" id="toc-external-data" class="nav-link" data-scroll-target="#external-data">External Data</a></li>
  <li><a href="#dplyr-2" id="toc-dplyr-2" class="nav-link" data-scroll-target="#dplyr-2">dplyr</a></li>
  <li><a href="#databases" id="toc-databases" class="nav-link" data-scroll-target="#databases">Databases</a></li>
  <li><a href="#dataframes-1" id="toc-dataframes-1" class="nav-link" data-scroll-target="#dataframes-1">DataFrames</a></li>
  <li><a href="#mllib" id="toc-mllib" class="nav-link" data-scroll-target="#mllib">MLlib</a></li>
  <li><a href="#broom-3" id="toc-broom-3" class="nav-link" data-scroll-target="#broom-3">Broom</a></li>
  <li><a href="#r-compatibility" id="toc-r-compatibility" class="nav-link" data-scroll-target="#r-compatibility">R Compatibility</a></li>
  <li><a href="#connections-14" id="toc-connections-14" class="nav-link" data-scroll-target="#connections-14">Connections</a></li>
  <li><a href="#compilation-1" id="toc-compilation-1" class="nav-link" data-scroll-target="#compilation-1">Compilation</a></li>
  <li><a href="#backend" id="toc-backend" class="nav-link" data-scroll-target="#backend">Backend</a></li>
  <li><a href="#miscellaneous-1" id="toc-miscellaneous-1" class="nav-link" data-scroll-target="#miscellaneous-1">Miscellaneous</a></li>
  <li><a href="#bug-fixes-4" id="toc-bug-fixes-4" class="nav-link" data-scroll-target="#bug-fixes-4">Bug Fixes</a></li>
  </ul></li>
  <li><a href="#sparklyr-0.5.5" id="toc-sparklyr-0.5.5" class="nav-link" data-scroll-target="#sparklyr-0.5.5">Sparklyr 0.5.5</a></li>
  <li><a href="#sparklyr-0.5.4" id="toc-sparklyr-0.5.4" class="nav-link" data-scroll-target="#sparklyr-0.5.4">Sparklyr 0.5.4</a></li>
  <li><a href="#sparklyr-0.5.3" id="toc-sparklyr-0.5.3" class="nav-link" data-scroll-target="#sparklyr-0.5.3">Sparklyr 0.5.3</a></li>
  <li><a href="#sparklyr-0.5.2" id="toc-sparklyr-0.5.2" class="nav-link" data-scroll-target="#sparklyr-0.5.2">Sparklyr 0.5.2</a></li>
  <li><a href="#sparklyr-0.5.0" id="toc-sparklyr-0.5.0" class="nav-link" data-scroll-target="#sparklyr-0.5.0">Sparklyr 0.5.0</a></li>
  <li><a href="#sparklyr-0.4.0" id="toc-sparklyr-0.4.0" class="nav-link" data-scroll-target="#sparklyr-0.4.0">Sparklyr 0.4.0</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
 <!-- Google Tag Manager -->
 <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-KHBDBW7');</script>
 <!-- End Google Tag Manager -->
 




<section id="sparklyr-1.8.6" class="level1">
<h1>Sparklyr 1.8.6</h1>
<ul>
<li>Addresses issues with R 4.4.0. The root cause was that version checking functions changed how the work.
<ul>
<li><code>package_version()</code> no longer accepts <code>numeric_version()</code> output. Wrapped the <code>package_version()</code> function to coerce the argument if it’s a <code>numeric_version</code> class</li>
<li>Comparison operators (<code>&lt;</code>, <code>&gt;=</code>, etc.) for <code>packageVersion()</code> do no longer accept numeric values. The changes were to pass the version as a character</li>
</ul></li>
<li>Adding support for Databricks “<a href="https://docs.databricks.com/en/ingestion/auto-loader/options.html">autoloader</a>” (format: <code>cloudFiles</code>) for streaming ingestion of files(<code>stream_read_cloudfiles</code>)(<span class="citation" data-cites="zacdav-db">@zacdav-db</span> #3432):
<ul>
<li><code>stream_write_table()</code></li>
<li><code>stream_read_table()</code></li>
</ul></li>
<li>Made changes to <code>stream_write_generic</code> (<span class="citation" data-cites="zacdav-db">@zacdav-db</span> #3432):
<ul>
<li><code>toTable</code> method doesn’t allow calling <code>start</code>, added <code>to_table</code> param that adjusts logic</li>
<li><code>path</code> option not propagated when <code>to_table</code> is <code>TRUE</code></li>
</ul></li>
<li>Upgrades to Roxygen version 7.3.1</li>
</ul>
</section>
<section id="sparklyr-1.8.5" class="level1">
<h1>Sparklyr 1.8.5</h1>
<section id="fixes" class="level3">
<h3 class="anchored" data-anchor-id="fixes">Fixes</h3>
<ul>
<li><p>Fixes quoting issue with <code>dbplyr</code> 2.5.0 (#3429)</p></li>
<li><p>Fixes Windows OS identification (#3426)</p></li>
</ul>
</section>
<section id="package-improvements" class="level3">
<h3 class="anchored" data-anchor-id="package-improvements">Package improvements</h3>
<ul>
<li><p>Removes dependency on <code>tibble</code>, all calls are now redirected to <code>dplyr</code> (#3399)</p></li>
<li><p>Removes dependency on <code>rapddirs</code> (#3401):</p>
<ul>
<li>Backwards compatibility with <code>sparklyr</code> 0.5 is no longer needed</li>
<li>Replicates selection of cache directory</li>
</ul></li>
<li><p>Converts <code>spark_apply()</code> to a method (#3418)</p></li>
</ul>
</section>
<section id="spark-improvements" class="level3">
<h3 class="anchored" data-anchor-id="spark-improvements">Spark improvements</h3>
<ul>
<li>Spark 2.3 is no longer considered maintained as of September 2019
<ul>
<li>Removes Java folder for versions 2.3 and below</li>
<li>Merges Scala file sets into Spark version 2.4</li>
<li>Re-compiles JARs for version 2.4 and above</li>
</ul></li>
<li>Updates Delta-to-Spark version matching when using <code>delta</code> as one of the <code>packages</code> when connecting (#3414)</li>
</ul>
</section>
</section>
<section id="sparklyr-1.8.4" class="level1">
<h1>Sparklyr 1.8.4</h1>
<section id="compatability-with-new-dbplyr-version" class="level3">
<h3 class="anchored" data-anchor-id="compatability-with-new-dbplyr-version">Compatability with new <code>dbplyr</code> version</h3>
<ul>
<li><p>Fixes <code>db_connection_describe()</code> S3 consistency error (<span class="citation" data-cites="t-kalinowski">@t-kalinowski</span>)</p></li>
<li><p>Addresses new error from <code>dbplyr</code> that fails when you try to access components from a remote <code>tbl</code> using <code>$</code></p></li>
<li><p>Bumps the version of <code>dbplyr</code> to switch between the two methods to create temporary tables</p></li>
<li><p>Addresses new <code>translate_sql()</code> hard requirement to pass a <code>con</code> object. Done by passing the current connection or <code>simulate_hive()</code></p></li>
</ul>
</section>
<section id="fixes-1" class="level3">
<h3 class="anchored" data-anchor-id="fixes-1">Fixes</h3>
<ul>
<li><p>Small fix to spark_connect_method() arguments. Removes ‘hadoop_version’</p></li>
<li><p>Improvements to handling <code>pysparklyr</code> load (<span class="citation" data-cites="t-kalinowski">@t-kalinowski</span>)</p></li>
<li><p>Fixes ‘subscript out of bounds’ issue found by <code>pysparklyr</code> (<span class="citation" data-cites="t-kalinowski">@t-kalinowski</span>)</p></li>
<li><p>Updates available Spark download links</p></li>
</ul>
</section>
<section id="improvements" class="level3">
<h3 class="anchored" data-anchor-id="improvements">Improvements</h3>
<ul>
<li>Removes dependency on the following packages:
<ul>
<li><code>digest</code></li>
<li><code>base64enc</code></li>
<li><code>ellipsis</code></li>
</ul></li>
<li>Converts <code>ml_fit()</code> into a S3 method for <code>pysparklyr</code> compatibility</li>
</ul>
</section>
<section id="test-improvements" class="level3">
<h3 class="anchored" data-anchor-id="test-improvements">Test improvements</h3>
<ul>
<li><p>Improvements and fixes to tests (<span class="citation" data-cites="t-kalinowski">@t-kalinowski</span>)</p></li>
<li><p>Fixes test jobs that include should have included Arrow but did not</p></li>
<li><p>Updates to the Spark versions to be tested</p></li>
<li><p>Re-adds tests for development <code>dbplyr</code></p></li>
</ul>
</section>
</section>
<section id="sparklyr-1.8.3" class="level1">
<h1>Sparklyr 1.8.3</h1>
<section id="improvements-1" class="level3">
<h3 class="anchored" data-anchor-id="improvements-1">Improvements</h3>
<ul>
<li><p>Spark error message relays are now cached instead of the entire content displayed as an R error. This used to overwhelm the interactive session’s console or Notebook, because of the amount of lines returned by the Spark message. Now, by default, it will return the top of the Spark error message, which is typically the most relevant part. The full error can still be accessed using a new function called <code>spark_last_error()</code></p></li>
<li><p>Reduces redundancy on several tests</p></li>
<li><p>Handles SQL quoting when the table reference contains multiple levels. The common time someone would encounter an issue is when a table name is passed using <code>in_catalog()</code>, or <code>in_schema()</code>.</p></li>
</ul>
</section>
<section id="java" class="level3">
<h3 class="anchored" data-anchor-id="java">Java</h3>
<ul>
<li>Adds Scala scripts to handle changes in the upcoming version of Spark (3.5)</li>
<li>Adds new JAR file to handle Spark 3.0 to 3.4</li>
<li>Adds new JAR file to handle Spark 3.5 and above</li>
</ul>
</section>
<section id="fixes-2" class="level3">
<h3 class="anchored" data-anchor-id="fixes-2">Fixes</h3>
<ul>
<li><p>It prevents an error when <code>na.rm = TRUE</code> is explicitly set within <code>pmax()</code> and <code>pmin()</code>. It will now also purposely fail if <code>na.rm</code> is set to <code>FALSE</code>. The default of these functions in base R is for <code>na.rm</code> to be <code>FALSE</code>, but ever since these functions were released, there has been no warning or error. For now, we will keep that behavior until a better approach can be figured out. (#3353)</p></li>
<li><p><code>spark_install()</code> will now properly match when a partial version is passed to the function. The issue was that passing ‘2.3’ would match to ‘3.2.3’, instead of ‘2.3.x’ (#3370)</p></li>
</ul>
</section>
<section id="package-integration" class="level3">
<h3 class="anchored" data-anchor-id="package-integration">Package integration</h3>
<ul>
<li><p>Adds functionality to allow other packages to provide <code>sparklyr</code> additional back-ends. This effort is mainly focused on adding the ability to integrate with Spark Connect and Databricks Connect through a new package.</p></li>
<li><p>New exported functions to integrate with the RStudio IDE. They all have the same <code>spark_ide_</code> prefix</p></li>
<li><p>Modifies several read functions to become exported methods, such as <code>sdf_read_column()</code>.</p></li>
<li><p>Adds <code>spark_integ_test_skip()</code> function. This is to allow other packages to use <code>sparklyr</code>’s test suite. It enables a way to the external package to indicate if a given test should run or be skipped.</p></li>
<li><p>If installed, <code>sparklyr</code> will load the <code>pysparklyr</code> package</p></li>
</ul>
</section>
</section>
<section id="sparklyr-1.8.2" class="level1">
<h1>Sparklyr 1.8.2</h1>
<section id="new-features" class="level3">
<h3 class="anchored" data-anchor-id="new-features">New Features</h3>
<ul>
<li><p>Adds Azure Synapse Analytics connectivity (<span class="citation" data-cites="Bob-Chou">@Bob-Chou</span> , #3336)</p></li>
<li><p>Adds support for “parameterized” queries now available in Spark 3.4 (<span class="citation" data-cites="gregleleu">@gregleleu</span> #3335)</p></li>
<li><p>Adds new DBI methods: <code>dbValid</code> and <code>dbDisconnect</code> (<span class="citation" data-cites="alibell">@alibell</span>, #3296)</p></li>
<li><p>Adds <code>overwrite</code> parameter to <code>dbWriteTable()</code> (<span class="citation" data-cites="alibell">@alibell</span>, #3296)</p></li>
<li><p>Adds <code>database</code> parameter to <code>dbListTables()</code> (<span class="citation" data-cites="alibell">@alibell</span>, #3296)</p></li>
<li><p>Adds ability to turn off predicate support (where(), across()) using options(“sparklyr.support.predicates” = FALSE). Defaults to TRUE. This should accelerate <code>dplyr</code> commands because it won’t need to process column types for every single piped command</p></li>
</ul>
</section>
<section id="fixes-3" class="level3">
<h3 class="anchored" data-anchor-id="fixes-3">Fixes</h3>
<ul>
<li><p>Fixes Spark download locations (#3331)</p></li>
<li><p>Fix various rlang deprecation warnings (<span class="citation" data-cites="mgirlich">@mgirlich</span>, #3333).</p></li>
</ul>
</section>
<section id="misc" class="level3">
<h3 class="anchored" data-anchor-id="misc">Misc</h3>
<ul>
<li>Switches upper version of Spark to 3.4, and updates JARS (#3334)</li>
</ul>
</section>
</section>
<section id="sparklyr-1.8.1" class="level1">
<h1>Sparklyr 1.8.1</h1>
<section id="bug-fixes" class="level3">
<h3 class="anchored" data-anchor-id="bug-fixes">Bug Fixes</h3>
<ul>
<li>Fixes consistency issues with dplyr’s sample_n(), slice(), op_vars(), and sample_frac()</li>
</ul>
</section>
<section id="internal-functionality" class="level3">
<h3 class="anchored" data-anchor-id="internal-functionality">Internal functionality</h3>
<ul>
<li>Adds R-devel to GHA testing</li>
</ul>
</section>
</section>
<section id="sparklyr-1.8.0" class="level1">
<h1>Sparklyr 1.8.0</h1>
<section id="bug-fixes-1" class="level3">
<h3 class="anchored" data-anchor-id="bug-fixes-1">Bug Fixes</h3>
<ul>
<li><p>Addresses Warning from CRAN checks</p></li>
<li><p>Addresses option(stringsAsFactors) usage</p></li>
<li><p>Fixes root cause of issue processing pivot wider and distinct (#3317 &amp; #3320)</p></li>
<li><p>Updates local Spark download sources</p></li>
</ul>
</section>
</section>
<section id="sparklyr-1.7.9" class="level1">
<h1>Sparklyr 1.7.9</h1>
<section id="bug-fixes-2" class="level3">
<h3 class="anchored" data-anchor-id="bug-fixes-2">Bug Fixes</h3>
<ul>
<li><p>Better resolves intermediate column names when using <code>dplyr</code> verbs for data transformation (#3286)</p></li>
<li><p>Fixes <code>pivot_wider()</code> issues with simpler cases (#3289)</p></li>
<li><p>Updates Spark download locations (#3298)</p></li>
<li><p>Better resolution of intermediate column names (#3286)</p></li>
</ul>
</section>
</section>
<section id="sparklyr-1.7.8" class="level1">
<h1>Sparklyr 1.7.8</h1>
<section id="new-features-1" class="level3">
<h3 class="anchored" data-anchor-id="new-features-1">New features</h3>
<ul>
<li><p>Adds new metric extraction functions: <code>ml_metrics_binary()</code>, <code>ml_metrics_regression()</code> and <code>ml_metrics_multiclass()</code>. They work closer to how <code>yardstick</code> metric extraction functions work. They expect a table with the predictions and actual values, and returns a concise <code>tibble</code> with the metrics. (#3281)</p></li>
<li><p>Adds new <code>spark_insert_table()</code> function. This allows one to insert data into an existing table definition without redefining the table, even when overwriting the existing data. (#3272 <span class="citation" data-cites="jimhester">@jimhester</span>)</p></li>
</ul>
</section>
<section id="bug-fixes-3" class="level3">
<h3 class="anchored" data-anchor-id="bug-fixes-3">Bug Fixes</h3>
<ul>
<li>Restores “validator” functions to regression models. Removing them in a previous version broke <code>ml_cross_validator()</code> for regression models. (#3273)</li>
</ul>
</section>
<section id="spark" class="level3">
<h3 class="anchored" data-anchor-id="spark">Spark</h3>
<ul>
<li><p>Adds support to Spark 3.3 local installation. This includes the ability to enable and setup log4j version 2. (#3269)</p></li>
<li><p>Updates the JSON file that <code>sparklyr</code> uses to find and download Spark for local use. It is worth mentioning that starting with Spark 3.3, the Hadoop version number is no longer using a minor version for its download link. So, instead of requesting 3.2, the version to request is 3.</p></li>
</ul>
</section>
<section id="internal-functionality-1" class="level3">
<h3 class="anchored" data-anchor-id="internal-functionality-1">Internal functionality</h3>
<ul>
<li><p>Removes workaround for older versions of <code>arrow</code>. Bumps <code>arrow</code> version dependency, from 0.14.0 to 0.17.0 (#3283 <span class="citation" data-cites="nealrichardson">@nealrichardson</span>)</p></li>
<li><p>Removes code related to backwards compatibility with <code>dbplyr</code>. <code>sparklyr</code> requires <code>dbplyr</code> version 2.2.1 or above, so the code is no longer needed. (#3277)</p></li>
<li><p>Begins centralizing ML parameter validation into a single function that will run the proper <code>cast</code> function for each Spark parameter. It also starts using S3 methods, instead of searching for a concatenated function name, to find the proper parameter validator. Regression models are the first ones to use this new method. (#3279)</p></li>
<li><p><code>sparklyr</code> compilation routines have been improved and simplified.<br>
<code>spark_compile()</code> now provides more informative output when used. It also adds tests to compilation to make sure. It also adds a step to install Scala in the corresponding GHAs. This is so that the new JAR build tests are able to run. (#3275)</p></li>
<li><p>Stops using package environment variables directly. Any package level variable will be handled by a <code>genv</code> prefixed function to set and retrieve values. This avoids the risk of having the exact same variable initialized on more than on R script. (#3274)</p></li>
<li><p>Adds more tests to improve coverage.</p></li>
</ul>
</section>
<section id="misc-1" class="level3">
<h3 class="anchored" data-anchor-id="misc-1">Misc</h3>
<ul>
<li>Addresses new CRAN HTML check NOTEs. It also adds a new GHA action to run the same checks to make sure we avoid new issues with this in the future.</li>
</ul>
</section>
</section>
<section id="sparklyr-1.7.7" class="level1">
<h1>Sparklyr 1.7.7</h1>
<section id="dplyr" class="level3">
<h3 class="anchored" data-anchor-id="dplyr">dplyr</h3>
<ul>
<li>Makes sure to run previous <code>dplyr</code> actions before sampling (#3276)</li>
</ul>
</section>
<section id="misc-2" class="level3">
<h3 class="anchored" data-anchor-id="misc-2">Misc</h3>
<ul>
<li>Ensures compatibility with the upcoming, and current, versions of <code>dbplyr</code></li>
</ul>
</section>
</section>
<section id="sparklyr-1.7.6" class="level1">
<h1>Sparklyr 1.7.6</h1>
<section id="misc-3" class="level3">
<h3 class="anchored" data-anchor-id="misc-3">Misc</h3>
<ul>
<li><p>Ensures compatibility with Spark version 3.2 (#3261)</p></li>
<li><p>Compatibility with new <code>dbplyr</code> version (<span class="citation" data-cites="mgirlich">@mgirlich</span>)</p></li>
<li><p>Removes <code>stringr</code> dependency</p></li>
<li><p>Fixes <code>augment()</code> when the model was fitted via <code>parsnip</code> (#3233)</p></li>
</ul>
</section>
</section>
<section id="sparklyr-1.7.5" class="level1">
<h1>Sparklyr 1.7.5</h1>
<section id="misc-4" class="level3">
<h3 class="anchored" data-anchor-id="misc-4">Misc</h3>
<ul>
<li><p>Addresses deprecation of <code>rlang::is_env()</code> function. (<span class="citation" data-cites="lionel">@lionel</span>- #3217)</p></li>
<li><p>Updates <code>pivot_wider()</code> to support new version of <code>tidyr</code> (<span class="citation" data-cites="DavisVaughan">@DavisVaughan</span> #3215)</p></li>
</ul>
</section>
</section>
<section id="sparklyr-1.7.4" class="level1">
<h1>Sparklyr 1.7.4</h1>
<section id="misc-5" class="level3">
<h3 class="anchored" data-anchor-id="misc-5">Misc</h3>
<ul>
<li>Edgar Ruiz (https://github.com/edgararuiz) will be the new maintainer of {sparklyr} moving forward.</li>
</ul>
</section>
</section>
<section id="sparklyr-1.7.3" class="level1">
<h1>Sparklyr 1.7.3</h1>
<section id="data" class="level3">
<h3 class="anchored" data-anchor-id="data">Data</h3>
<ul>
<li><p>Implemented support for the <code>.groups</code> parameter for <code>dplyr::summarize()</code> operations on Spark dataframes</p></li>
<li><p>Fixed the incorrect handling of the <code>remove = TRUE</code> option for <code>separate.tbl_spark()</code></p></li>
<li><p>Optimized away an extra count query when collecting Spark dataframes from Spark to R.</p></li>
</ul>
</section>
<section id="misc-6" class="level3">
<h3 class="anchored" data-anchor-id="misc-6">Misc</h3>
<ul>
<li><p>By default, use links from the https://dlcdn.apache.org site for downloading Apache Spark when possible.</p></li>
<li><p>Attempt to continue <code>spark_install()</code> process even if the Spark version specified is not present in <code>inst/extdata/versions*.json</code> files (in which case <code>sparklyr</code> will guess the URL of the tar ball based on the existing and well-known naming convention used by https://archive.apache.org, i.e., https://archive.apache.org/dist/spark/spark-<span class="math inline">\({spark version}/spark-\)</span>{spark version}-bin-hadoop${hadoop version}.tgz)</p></li>
<li><p>Revised <code>inst/extdata/versions*.json</code> files to reflect recent releases of Apache Spark.</p></li>
<li><p>Implemented <code>sparklyr_get_backend_port()</code> for querying the port number used by the <code>sparklyr</code> backend.</p></li>
</ul>
</section>
</section>
<section id="sparklyr-1.7.2" class="level1">
<h1>Sparklyr 1.7.2</h1>
<section id="connections" class="level3">
<h3 class="anchored" data-anchor-id="connections">Connections</h3>
<ul>
<li><p>Added support for notebook-scoped libraries on Databricks connections. R library tree paths (i.e., those returned from <code>.libPaths()</code>) are now shared between driver and worker in sparklyr for Databricks connection use cases.</p></li>
<li><p>Java version validation function of <code>sparklyr</code> was revised to be able to parse <code>java -version</code> outputs containing only major version or outputs containing data values.</p></li>
<li><p>Spark configuration logic was revised to ensure “sparklyr.cores.local” takes precedence over “sparklyr.connect.cores.local”, as the latter is deprecated.</p></li>
<li><p>Renamed “sparklyr.backend.threads” (an undocumented, non-user-facing, <code>sparklyr</code> internal-only configuration) to “spark.sparklyr-backend.threads” so that it has the required “spark.” prefix and is configurable through <code>sparklyr::spark_config()</code>.</p></li>
<li><p>For Spark 2.0 or above, if <code>org.apache.spark.SparkEnv.get()</code> returns a non- null env object, then <code>sparklyr</code> will use that env object to configure “spark.sparklyr-backend.threads”.</p></li>
<li><p>Support for running custom callbacks before the <code>sparklyr</code> backend starts processing JVM method calls was added for Databricks-related use cases, which will be useful for implementing ADL credential pass-through.</p></li>
</ul>
</section>
<section id="data-1" class="level3">
<h3 class="anchored" data-anchor-id="data-1">Data</h3>
<ul>
<li><p>Revised <code>spark_write_delta()</code> to use <code>delta.io</code> library version 1.0 when working with Apache Spark 3.1 or above.</p></li>
<li><p>Fixed a problem with <code>dbplyr::remote_name()</code> returning <code>NULL</code> on Spark dataframes returned from a <code>dplyr::arrange()</code> operation followed by <code>dplyr::compute()</code> (e.g., <code>&lt;a spark_dataframe&gt; %&gt;% arrange(&lt;some column&gt;) %&gt;% compute()</code>).</p></li>
<li><p>Implemented <code>tidyr::replace_na()</code> interface for Spark dataframes.</p></li>
<li><p>The <code>n_distinct()</code> summarizer for Spark dataframes was revised substantially to properly support <code>na.rm = TRUE</code> or <code>na.rm = FALSE</code> use cases when performing <code>dplyr::summarize(&lt;colname&gt; = n_distinct(...))</code> types of operations on Spark dataframes.</p></li>
<li><p>Spark data interface functions that create Spark dataframes will no longer check whether any Spark dataframe with identical name exists when the dataframe being created has a randomly generated name (as randomly generated table name will contain a UUID and any chance of name collision is vanishingly small).</p></li>
</ul>
</section>
<section id="documentation" class="level3">
<h3 class="anchored" data-anchor-id="documentation">Documentation</h3>
<ul>
<li>Create usage example for <code>ml_prefixspan()</code>.</li>
</ul>
</section>
</section>
<section id="sparklyr-1.7.1" class="level1">
<h1>Sparklyr 1.7.1</h1>
<section id="connections-1" class="level3">
<h3 class="anchored" data-anchor-id="connections-1">Connections</h3>
<ul>
<li>Fixed an issue with connecting to Apache Spark 3.1 or above.</li>
</ul>
</section>
</section>
<section id="sparklyr-1.7.0" class="level1">
<h1>Sparklyr 1.7.0</h1>
<section id="data-2" class="level3">
<h3 class="anchored" data-anchor-id="data-2">Data</h3>
<ul>
<li><p>Revised <code>tidyr::fill()</code> implementation to respect any ‘ORDER BY’ clause from the input while ensuring the same ‘ORDER BY’ operation is never duplicated twice in the generated Spark SQL query</p></li>
<li><p>Helper functions such as <code>sdf_rbeta()</code>, <code>sdf_rbinom()</code>, etc were implemented for generating Spark dataframes containing i.i.d. samples from commonly used probability distributions.</p></li>
<li><p>Fixed a bug with <code>compute.tbl_spark()</code>’s handling of positional args.</p></li>
<li><p>Fixed a bug that previously affected <code>dplyr::tbl()</code> when the source table is specified using <code>dbplyr::in_schema()</code>.</p></li>
<li><p>Internal calls to <code>sdf_schema.tbl_spark()</code> and <code>spark_dataframe.tbl_spark()</code> are memoized to reduce performance overhead from repeated <code>spark_invoke()</code>s.</p></li>
<li><p><code>spark_read_image()</code> was implemented to support image files as data sources.</p></li>
<li><p><code>spark_read_binary()</code> was implemented to support binary data sources.</p></li>
<li><p>A specialized version of <code>tbl_ptype()</code> was implemented so that no data will be collected from Spark to R when <code>dplyr</code> calls <code>tbl_ptype()</code> on a Spark dataframe.</p></li>
<li><p>Added support for <code>database</code> parameter to <code>src_tbls.spark_connection()</code> (e.g., <code>src_tbls(sc, database = "default")</code> where <code>sc</code> is a Spark connection).</p></li>
<li><p>Fixed a null pointer issue with <code>spark_read_jdbc()</code> and <code>spark_write_jdbc()</code>.</p></li>
</ul>
</section>
<section id="distributed-r" class="level3">
<h3 class="anchored" data-anchor-id="distributed-r">Distributed R</h3>
<ul>
<li><p><code>spark_apply()</code> was improved to support <code>tibble</code> inputs containing list columns.</p></li>
<li><p>Spark dataframes created by <code>spark_apply()</code> will be cached by default to avoid re-computations.</p></li>
<li><p><code>spark_apply()</code> and <code>do_spark()</code> now support <code>qs</code> and custom serializations.</p></li>
<li><p>The experimental <code>auto_deps = TRUE</code> mode was implemented for <code>spark_apply()</code> to infer required R packages for the closure, and to only copy required R packages to Spark worker nodes when executing the closure.</p></li>
</ul>
</section>
<section id="extensions" class="level3">
<h3 class="anchored" data-anchor-id="extensions">Extensions</h3>
<ul>
<li><p>Sparklyr extensions can now customize dbplyr SQL translator env used by <code>sparklyr</code> by supplying their own dbplyr SQL variant when calling <code>spark_dependency()</code> (see https://github.com/r-spark/sparklyr.sedona/blob/1455d3dea51ad16114a8112f2990ec542458aee2/R/dependencies.R#L38 for an example).</p></li>
<li><p><code>jarray()</code> was implemented to convert a R vector into an <code>Array[T]</code> reference. A reference returned by <code>jarray()</code> can be passed to <code>invoke*</code> family of functions requiring an <code>Array[T]</code> as a parameter where T is some type that is more specific than <code>java.lang.Object</code>.</p></li>
<li><p><code>jfloat()</code> function was implemented to cast any numeric type in R to <code>java.lang.Float</code>.</p></li>
<li><p><code>jfloat_array()</code> was implemented to instantiate <code>Array[java.lang.Float]</code> from numeric values in R.</p></li>
</ul>
</section>
<section id="serialization" class="level3">
<h3 class="anchored" data-anchor-id="serialization">Serialization</h3>
<ul>
<li><p>Added null checks that were previously missing when collecting array columns from Spark dataframe to R.</p></li>
<li><p><code>array&lt;byte&gt;</code> and <code>array&lt;boolean&gt;</code> columns in a Spark dataframe will be collected as <code>raw()</code> and <code>logical()</code> vectors, respectively, in R rather than integer arrays.</p></li>
<li><p>Fixed a bug that previously caused invoke params containing <code>NaN</code>s to be serialized incorrectly.</p></li>
</ul>
</section>
<section id="spark-ml" class="level3">
<h3 class="anchored" data-anchor-id="spark-ml">Spark ML</h3>
<ul>
<li><p><code>ml_compute_silhouette_measure()</code> was implemented to evaluate the <a href="https://en.wikipedia.org/wiki/Silhouette_(clustering)">Silhouette measure</a> of k-mean clustering results.</p></li>
<li><p><code>spark_read_libsvm()</code> now supports specifications of additional options via the <code>options</code> parameter. Additional libsvm data source options currently supported by Spark include <code>numFeatures</code> and <code>vectorType</code> (see https://spark.apache.org/docs/latest/api/java/org/apache/spark/ml/source/libsvm/LibSVMDataSource.html).</p></li>
<li><p><code>ml_linear_svc()</code> will emit a warning if <code>weight_col</code> is specified while working with Spark 3.0 or above, as it is no longer supported in recent versions of Spark.</p></li>
<li><p>Fixed an issue with <code>ft_one_hot_encoder.ml_pipeline()</code> not working as expected.</p></li>
</ul>
</section>
</section>
<section id="sparklyr-1.6.3" class="level1">
<h1>Sparklyr 1.6.3</h1>
<section id="data-3" class="level3">
<h3 class="anchored" data-anchor-id="data-3">Data</h3>
<ul>
<li><p>Reduced the number of <code>invoke()</code> calls needed for <code>sdf_schema()</code> to avoid performance issues when processing Spark dataframes with non-trivial number of columns</p></li>
<li><p>Implement memoization for <code>spark_dataframe.tbl_spark()</code> and <code>sdf_schema.tbl_spark()</code> to reduce performance overhead for some <code>dplyr</code> use cases involving Spark dataframes with non-trivial number of columns</p></li>
</ul>
</section>
</section>
<section id="sparklyr-1.6.2" class="level1">
<h1>Sparklyr 1.6.2</h1>
<section id="data-4" class="level3">
<h3 class="anchored" data-anchor-id="data-4">Data</h3>
<ul>
<li>A previous bug fix related to <code>dplyr::compute()</code> caching a Spark view needed to be further revised to take effect with dbplyr backend API edition 2</li>
</ul>
</section>
</section>
<section id="sparklyr-1.6.1" class="level1">
<h1>Sparklyr 1.6.1</h1>
<section id="data-5" class="level3">
<h3 class="anchored" data-anchor-id="data-5">Data</h3>
<ul>
<li><p><code>sdf_distinct()</code> is implemented to be an R interface for <code>distinct()</code> operation on Spark dataframes (NOTE: this is different from the <code>dplyr::distinct()</code> operation, as <code>dplyr::distinct()</code> operation on a Spark dataframe now supports <code>.keep_all = TRUE</code> and has more complex ordering requirements)</p></li>
<li><p>Fixed a problem of some expressions being evaluated twice in <code>transmute.tbl_spark()</code> (see tidyverse/dbplyr#605)</p></li>
<li><p><code>dbExistsTable()</code> now performs case insensitive comparison with table names to be consistent with how table names are handled by Spark catalog API</p></li>
<li><p>Fixed a bug with <code>sql_query_save()</code> not overwriting a temp table with identical name</p></li>
<li><p>Revised <code>sparklyr:::process_tbl_name()</code> to correctly handle inputs that are not table names</p></li>
<li><p>Bug fix: <code>db_save_query.spark_connection()</code> should also cache the view it created in Spark</p></li>
</ul>
</section>
</section>
<section id="sparklyr-1.6.0" class="level1">
<h1>Sparklyr 1.6.0</h1>
<section id="data-6" class="level3">
<h3 class="anchored" data-anchor-id="data-6">Data</h3>
<ul>
<li><p>Made <code>sparklyr</code> compatible with both dbplyr edition 1 and edition 2 APIs</p></li>
<li><p>Revised <code>sparklyr</code>’s integration with <code>dbplyr</code> API so that <code>dplyr::select()</code>, <code>dplyr::mutate()</code>, and <code>dplyr::summarize()</code> verbs on Spark dataframes involving <code>where()</code> predicates can be correctly translated to Spark SQL (e.g., one can have <code>sdf %&gt;% select(where(is.numeric))</code> and <code>sdf %&gt;% summarize(across(starts_with("Petal"), mean))</code>, etc)</p></li>
<li><p>Implemented <code>dplyr::if_all()</code> and <code>dplyr::if_any()</code> support for Spark dataframes</p></li>
<li><p>Added support for <code>partition_by</code> option in <code>stream_write_*</code> methods</p></li>
<li><p>Fixed a bug with URI handling affecting all <code>spark_read_*</code> methods</p></li>
<li><p>Avoided repeated creations of SimpleDataFormat objects and setTimeZone calls while collecting Data columns from a Spark dataframe</p></li>
<li><p>Schema specification for struct columns in <code>spark_read_*()</code> methods are now supported (e.g., <code>spark_read_json(sc, path, columns = list(s = list(a = "integer, b = "double")))</code> says expect a struct column named <code>s</code> with each element containing a field named <code>a</code> and a field named <code>b</code>)</p></li>
<li><p><code>sdf_quantile()</code> and <code>ft_quantile_discretizer()</code> now support approximation of weighted quantiles using a modified version of the Greenwald-Khanna algorithm that takes relative weight of each data point into consideration.</p></li>
<li><p>Fixed a problem of some expressions being evaluated twice in <code>transmute.tbl_spark()</code> (see tidyverse/dbplyr#605)</p></li>
<li><p>Made <code>dplyr::distinct()</code> behavior for Spark dataframes configurable: setting <code>options(sparklyr.dplyr_distinct.impl = "tbl_lazy)</code> will switch <code>dplyr::distinct()</code> implementation to a basic one that only adds ‘DISTINCT’ clause to the current Spark SQL query, does not support the <code>.keep_all = TRUE</code> option, and (3) does not have any ordering guarantee for the output.</p></li>
</ul>
</section>
<section id="serialization-1" class="level3">
<h3 class="anchored" data-anchor-id="serialization-1">Serialization</h3>
<ul>
<li><p><code>spark_write_rds()</code> was implemented to support exporting all partitions of a Spark dataframe in parallel into RDS (version 2) files. Such RDS files will be written to the default file system of the Spark instance (i.e., local file if the Spark instance is running locally, or a distributed file system such as HDFS if the Spark instance is deployed over a cluster). The resulting RDS files, once downloaded onto the local file system, should be deserialized into R dataframes using <code>collect_from_rds()</code> (which calls <code>readRDS()</code> internally and also performs some important post-processing steps to support timestamp columns, date columns, and struct columns properly in R).</p></li>
<li><p><code>copy_to()</code> can now import list columns of temporal values within a R dataframe as arrays of Spark SQL date/timestamp types when working with Spark 3.0 or above</p></li>
<li><p>Fixed a bug with <code>copy_to()</code>’s handling of NA values in list columns of a R dataframe</p></li>
<li><p>Spark map type will be collected as list instead of environment in R in order to support empty string as key</p></li>
<li><p>Fixed a configuration-related bug in <code>sparklyr:::arrow_enabled()</code></p></li>
<li><p>Implemented spark-apply-specific configuration option for Arrow max records per batch, which can be different from the <code>spark.sql.execution.arrow.maxRecordsPerBatch</code> value from Spark session config</p></li>
</ul>
</section>
<section id="connections-2" class="level3">
<h3 class="anchored" data-anchor-id="connections-2">Connections</h3>
<ul>
<li><p>Created convenience functions for working with Spark runtime configurations</p></li>
<li><p>Fixed buggy exit code from the <code>spark-submit</code> process launched by sparklyr</p></li>
</ul>
</section>
<section id="spark-ml-1" class="level3">
<h3 class="anchored" data-anchor-id="spark-ml-1">Spark ML</h3>
<ul>
<li><p>Implemented R interface for Power Iteration Clustering</p></li>
<li><p>The <code>handle_invalid</code> option is added to <code>ft_vector_indexer()</code> (supported by Spark 2.3 or above)</p></li>
</ul>
</section>
<section id="misc-7" class="level3">
<h3 class="anchored" data-anchor-id="misc-7">Misc</h3>
<ul>
<li>Fixed a bug with <code>~</code> within some path components not being normalized in <code>sparklyr::livy_install()</code></li>
</ul>
</section>
</section>
<section id="sparklyr-1.5.2" class="level1">
<h1>Sparklyr 1.5.2</h1>
<section id="connections-3" class="level3">
<h3 class="anchored" data-anchor-id="connections-3">Connections</h3>
<ul>
<li><p>Fixed <code>op_vars()</code> specification in <code>dplyr::distinct()</code> verb for Spark dataframes</p></li>
<li><p><code>spark_disconnect()</code> now closes the Spark monitoring connection correctly</p></li>
</ul>
</section>
<section id="data-7" class="level3">
<h3 class="anchored" data-anchor-id="data-7">Data</h3>
<ul>
<li><p>Implement support for stratified sampling in <code>ft_dplyr_transformer()</code></p></li>
<li><p>Added support for <code>na.rm</code> in dplyr <code>rowSums()</code> function for Spark dataframes</p></li>
</ul>
</section>
</section>
<section id="sparklyr-1.5.1" class="level1">
<h1>Sparklyr 1.5.1</h1>
<section id="connections-4" class="level3">
<h3 class="anchored" data-anchor-id="connections-4">Connections</h3>
<ul>
<li><p>A bug in how multiple <code>--conf</code> values were handled in some scenarios within the spark-submit shell args which was introduced in sparklyr 1.4 has been fixed now.</p></li>
<li><p>A bug with <code>livy.jars</code> configuration was fixed (#2843)</p></li>
</ul>
</section>
<section id="data-8" class="level3">
<h3 class="anchored" data-anchor-id="data-8">Data</h3>
<ul>
<li><code>tbl()</code> methods were revised to be compatible with <code>dbplyr</code> 2.0 when handling inputs of the form <code>"&lt;schema name&gt;.&lt;table name&gt;"</code></li>
</ul>
</section>
</section>
<section id="sparklyr-1.5.0" class="level1">
<h1>Sparklyr 1.5.0</h1>
<section id="connections-5" class="level3">
<h3 class="anchored" data-anchor-id="connections-5">Connections</h3>
<ul>
<li><p><code>spark_web()</code> has been revised to work correctly in environments such as RStudio Server or RStudio Cloud where the Spark web UI URLs such as “http://localhost:4040/jobs/” needs to be translated with <code>rstudioapi::translateLocalUrl()</code> to be accessible.</p></li>
<li><p>The problem with bundle file name collisions when <code>session_id</code> is not provided has been fixed in <code>spark_apply_bundle()</code>.</p></li>
<li><p>Support for <code>sparklyr.livy.sources</code> is removed completely as it is no longer needed as a workaround when Spark version is specified.</p></li>
</ul>
</section>
<section id="data-9" class="level3">
<h3 class="anchored" data-anchor-id="data-9">Data</h3>
<ul>
<li><p><code>stream_lag()</code> is implemented to provide the equivalent functionality of <code>dplyr::lag()</code> for streaming Spark dataframes while also supporting additional filtering of “outdated” records based on timestamp threshold.</p></li>
<li><p>A specialized version of <code>dplyr::distinct()</code> is implemented for Spark dataframes that supports <code>.keep_all = TRUE</code> and correctly satisfies the “rows are a subset of the input but appear in the same order” requirement stated in the <code>dplyr</code> documentation.</p></li>
<li><p>The default value for the <code>repartition</code> parameter of <code>sdf_seq()</code> has been corrected.</p></li>
<li><p>Some implementation detail was revised to make <code>sparklyr</code> 1.5 fully compatible with <code>dbplyr</code> 2.0.</p></li>
<li><p><code>sdf_expand_grid()</code> was implemented to support roughly the equivalent of <code>expand.grid()</code> for Spark dataframes while also offering additional Spark- specific options such as broadcast hash joins, repartitioning, and caching of the resulting Spark dataframe in memory.</p></li>
<li><p><code>sdf_quantile()</code> now supports calculation for multiple columns.</p></li>
<li><p>Both <code>lead()</code> and <code>lag()</code> methods for dplyr interface of <code>sparklyr</code> are fixed to correctly accept the <code>order_by</code> parameter.</p></li>
<li><p>The <code>cumprod()</code> window aggregation function for dplyr was reimplemented to correctly handle null values in Spark dataframes.</p></li>
<li><p>Support for <code>missing</code> parameter is implemented for the <code>ifelse()</code>/<code>if_else()</code> function for dplyr.</p></li>
<li><p>A <code>weighted.mean()</code> summarizer was implemented for dplyr interface of <code>sparklyr</code>.</p></li>
<li><p>A workaround was created to ensure <code>NA_real_</code> is handled correctly within the contexts of <code>dplyr::mutate()</code> and <code>dplyr::transmute()</code> methods (e.g., <code>sdf %&gt;% dplyr::mutate(z = NA_real_)</code> should result in a column named “z” with double-precision SQL type)</p></li>
<li><p>Support for R-like subsetting operator (<code>[</code>) was implemented for selecting a subset of columns from a Spark dataframe.</p></li>
<li><p>The <code>rowSums()</code> function was implemented for dplyr interface of <code>sparklyr</code>.</p></li>
<li><p>The <code>sdf_partition_sizes()</code> function was created to enable efficient query of partition sizes within a Spark dataframe.</p></li>
<li><p>Stratified sampling for Spark dataframes has been implemented and can be expressed using dplyr grammar as <code>&lt;spark dataframe&gt; %&gt;% dplyr::group_by(&lt;columns&gt;) %&gt;% dplyr::sample_n(...)</code> or <code>&lt;spark dataframe&gt; %&gt;% dplyr::group_by(&lt;columns&gt;) %&gt;% dplyr::sample_frac(...)</code> where <code>&lt;columns&gt;</code> is a list of grouping column(s) defining the strata (i.e., the sampling specified by <code>dplyr::sample_n()</code> or <code>dplyr::sample_frac()</code> will be applied to each group defined by <code>dplyr::group_by(&lt;columns&gt;)</code>)</p></li>
<li><p>The implementations of <code>dplyr::sample_n()</code> and <code>dplyr::sample_frac()</code> have been revised to first perform aggregations on individual partitions before merging aggregated results from all partitions, which is more efficient than <code>mapPartitions()</code> followed by <code>reduce()</code>.</p></li>
<li><p><code>sdf_unnest_longer()</code> and <code>sdf_unnest_wider()</code> were implemented and offer the equivalents of <code>tidyr::unnest_longer()</code> and <code>tidyr::unnest_wider()</code> for for Spark dataframes.</p></li>
</ul>
</section>
<section id="serialization-2" class="level3">
<h3 class="anchored" data-anchor-id="serialization-2">Serialization</h3>
<ul>
<li><p><code>copy_to()</code> now serializes R dataframes into RDS format instead of CSV format if <code>arrow</code> is unavailable. RDS serialization is approximately 48% faster than CSV and allows multiple correctness issues related to CSV serialization to be fixed easily in <code>sparklyr</code>.</p></li>
<li><p><code>copy_to()</code> and <code>collect()</code> now correctly preserve <code>NA_real_</code> (<code>NA_real_</code> from a R dataframe, once translated as <code>null</code> in a Spark dataframe, used to be incorrectly collected as <code>NaN</code> in previous versions of <code>sparklyr</code>).</p></li>
<li><p><code>copy_to()</code> can now distinguish <code>"NA"</code> from <code>NA</code> as expected.</p></li>
<li><p><code>copy_to()</code> now supports importing binary columns from R dataframes to Spark.</p></li>
<li><p>Reduced serialization overhead in Spark-based <code>foreach</code> parallel backend created with <code>registerDoSpark()</code>.</p></li>
</ul>
</section>
</section>
<section id="sparklyr-1.4.0" class="level1">
<h1>Sparklyr 1.4.0</h1>
<section id="connections-6" class="level3">
<h3 class="anchored" data-anchor-id="connections-6">Connections</h3>
<ul>
<li><p>RAPIDS GPU acceleration plugin can now be enabled with <code>spark_connect(..., package = "rapids")</code> and configured with <code>spark_config</code> options prefixed with “spark.rapids.”</p></li>
<li><p>Enabled support for http{,s} proxy plus additional CURL options for Livy connections</p></li>
<li><p>In sparklyr error message, suggest <code>options(sparklyr.log.console = TRUE)</code> as a trouble-shooting step whenever the “sparklyr gateway not responding” error occurs</p></li>
<li><p>Addressed an inter-op issue with Livy + Spark 2.4 (https://github.com/sparklyr/sparklyr/issues/2641)</p></li>
<li><p>Added configurable retries for Gateway ports query (https://github.com/sparklyr/sparklyr/pull/2654)</p></li>
<li><p>App name setting now takes effect as expected in YARN cluster mode (https://github.com/sparklyr/sparklyr/pull/2675)</p></li>
</ul>
</section>
<section id="data-10" class="level3">
<h3 class="anchored" data-anchor-id="data-10">Data</h3>
<ul>
<li><p>Support for newly introduced higher-order functions in Spark 3.0 (e.g., <code>array_sort</code>, <code>map_filter</code>, <code>map_zip_with</code>, and many others)</p></li>
<li><p>Implemented parallelizable weighted sampling methods for sampling from a Spark data frames with and without replacement using exponential variates</p></li>
<li><p>Replaced <code>dplyr::sample_*</code> implementations based on <code>TABLESAMPLE</code> with alternative implementation that can return exactly the number of rows or fraction specified and also properly support sampling with-replacement, without-replacement, and repeatable sampling use cases</p></li>
<li><p>All higher-order functions and sampling methods are made directly accessible through <code>dplyr</code> verbs</p></li>
<li><p>Made <code>grepl</code> part of the <code>dplyr</code> interface for Spark data frames</p></li>
<li><p>Tidyr verbs such as <code>pivot_wider</code>, <code>pivot_longer</code>, <code>nest</code>, <code>unnest</code>, <code>separate</code>, <code>unite</code>, and <code>fill</code> now have specialized implementations in <code>sparklyr</code> for working with Spark data frames</p></li>
<li><p>Made <code>dplyr::inner_join</code>, <code>dplyr::left_join</code>, <code>dplyr::right_join</code>, and <code>dplyr::full_join</code> replace <code>'.'</code> with <code>'_'</code> in <code>suffix</code> parameter when working with Spark data frames (https://github.com/sparklyr/sparklyr/issues/2648)</p></li>
</ul>
</section>
<section id="distributed-r-1" class="level3">
<h3 class="anchored" data-anchor-id="distributed-r-1">Distributed R</h3>
<ul>
<li><p>Fixed an issue with global variables in <code>registerDoSpark</code> (https://github.com/sparklyr/sparklyr/pull/2608)</p></li>
<li><p>Revised <code>spark_read_compat_param</code> to avoid collision on names assigned to different Spark data frames</p></li>
</ul>
</section>
<section id="misc-8" class="level3">
<h3 class="anchored" data-anchor-id="misc-8">Misc</h3>
<ul>
<li><p>Fixed a rendering issue with HTML reference pages</p></li>
<li><p>Made test reporting in Github CI workflows more informative (https://github.com/sparklyr/sparklyr/pull/2672)</p></li>
</ul>
</section>
<section id="spark-ml-2" class="level3">
<h3 class="anchored" data-anchor-id="spark-ml-2">Spark ML</h3>
<ul>
<li><code>ft_robust_scaler</code> was created as the R interface for the <code>RobustScaler</code> functionality in Spark 3 or above</li>
</ul>
</section>
</section>
<section id="sparklyr-1.3.1" class="level1">
<h1>Sparklyr 1.3.1</h1>
<section id="distributed-r-2" class="level3">
<h3 class="anchored" data-anchor-id="distributed-r-2">Distributed R</h3>
<ul>
<li>Fixed a bug in ordering of parameters for a lamba expression when the lambda expression passed to a <code>hof_*</code> method is specified with a R formula and the lambda takes 2 parameters</li>
</ul>
</section>
</section>
<section id="sparklyr-1.3.0" class="level1">
<h1>Sparklyr 1.3.0</h1>
<section id="spark-ml-3" class="level3">
<h3 class="anchored" data-anchor-id="spark-ml-3">Spark ML</h3>
<ul>
<li><code>ml_evaluate()</code> methods are implemented for ML clustering and classification models</li>
</ul>
</section>
<section id="distributed-r-3" class="level3">
<h3 class="anchored" data-anchor-id="distributed-r-3">Distributed R</h3>
<ul>
<li><p>Created helper methods to integrate Spark SQL higher-order functions with <code>dplyr::mutate</code></p></li>
<li><p>Implemented option to pass partition index as a named parameter to <code>spark_apply()</code> transform function</p></li>
<li><p>Enabled transform function of <code>spark_apply()</code> to return nested lists</p></li>
<li><p>Added option to return R objects instead of Spark data frame rows from transform function of <code>spark_apply</code></p></li>
<li><p><code>sdf_collect()</code> now supports fetching Spark data frame row-by-row rather than column-by-column, and fetching rows using iterator instead of collecting all rows into memory</p></li>
<li><p>Support for <code>partition</code> when using barrier execution in <code>spark_apply</code> (#2454)</p></li>
</ul>
</section>
<section id="connections-7" class="level3">
<h3 class="anchored" data-anchor-id="connections-7">Connections</h3>
<ul>
<li><p>Sparklyr can now connect with Spark 2.4 built with Scala 2.12 using <code>spark_connect(..., scala_version = "2.12")</code></p></li>
<li><p>Hive integration can now be disabled by configuration in <code>spark_connect()</code> (#2465)</p></li>
<li><p>A JVM object reference counting bug affecting secondary Spark connections was fixed (#2515)</p></li>
<li><p>Revised JObj envs initialization for Databricks connections (#2533)</p></li>
</ul>
</section>
<section id="serialization-3" class="level3">
<h3 class="anchored" data-anchor-id="serialization-3">Serialization</h3>
<ul>
<li><p>Timezones, if present in data, are correctly represented now in Arrow serialization</p></li>
<li><p>Embedded nul bytes are removed from strings when reading strings from Spark to R (#2250)</p></li>
<li><p>Support to collect objectts of type <code>SeqWrapper</code> (#2441)</p></li>
</ul>
</section>
<section id="data-11" class="level3">
<h3 class="anchored" data-anchor-id="data-11">Data</h3>
<ul>
<li><p>Created helper methods to integrate Spark SQL higher-order functions with <code>dplyr::mutate</code></p></li>
<li><p>New <code>spark_read()</code> method to allow user-defined R functions to be run on Spark workers to import data into a Spark data frame</p></li>
<li><p><code>spark_write()</code> method is implemented allow user-defined functions to be run on Spark workers to export data from a Spark data frame</p></li>
<li><p>Avro functionalities such as <code>spark_read_avro()</code>, <code>spark_write_avro()</code>, <code>sdf_from_avro()</code>, and <code>sdf_to_avro()</code> are implemented and can be optionally enabled with <code>spark_connect(..., package = "avro")</code></p></li>
</ul>
</section>
<section id="extensions-1" class="level3">
<h3 class="anchored" data-anchor-id="extensions-1">Extensions</h3>
<ul>
<li>Fixed a bug where Spark package repositories specification was not honored by <code>spark_dependency()</code>. The <code>repositories</code> parameter of <code>spark_dependency()</code> now works as expected.</li>
</ul>
</section>
<section id="misc-9" class="level3">
<h3 class="anchored" data-anchor-id="misc-9">Misc</h3>
<ul>
<li><p>Fixed warnings for deprecated functions (#2431)</p></li>
<li><p>More test coverage for Databricks Connect and Databricks Notebook modes</p></li>
<li><p>Embedded R sources are now included as resources rather than as a Scala string literal in <code>sparklyr-*.jar</code> files, so that they can be updated without re-compilation of Scala source files</p></li>
<li><p>A mechanism is created to verify embedded sources in <code>sparklyr-*.jar</code> files are in-sync with current R source files and this verification is now part of the Github CI workflow for <code>sparklyr</code></p></li>
</ul>
</section>
</section>
<section id="sparklyr-1.2.0" class="level1">
<h1>Sparklyr 1.2.0</h1>
<section id="distributed-r-4" class="level3">
<h3 class="anchored" data-anchor-id="distributed-r-4">Distributed R</h3>
<ul>
<li><p>Add support for using Spark as a foreach parallel backend</p></li>
<li><p>Fixed a bug with how <code>columns</code> parameter was interpreted in <code>spark_apply</code></p></li>
</ul>
</section>
<section id="data-12" class="level3">
<h3 class="anchored" data-anchor-id="data-12">Data</h3>
<ul>
<li><p>Allow <code>sdf_query_plan</code> to also get analyzed plan</p></li>
<li><p>Add support for serialization of R date values into corresponding Hive date values</p></li>
<li><p>Fixed the issue of date or timestamp values representing the UNIX epoch (1970-01-01) being deserialized incorrectly into NAs</p></li>
<li><p>Better support for querying and deserializing Spark SQL struct columns when working with Spark 2.4 or above</p></li>
<li><p>Add support in <code>copy_to()</code> for columns with nested lists (#2247).</p></li>
<li><p>Significantly improve <code>collect()</code> performance for columns with nested lists (#2252).</p></li>
</ul>
</section>
<section id="connection" class="level3">
<h3 class="anchored" data-anchor-id="connection">Connection</h3>
<ul>
<li><p>Add support for Databricks Connect</p></li>
<li><p>Add support for <code>copy_to</code> in Databricks connection</p></li>
<li><p>Ensure spark apply bundle files created by multiple Spark sessions don’t overwrite each other</p></li>
<li><p>Fixed an interop issue with spark-submit when running with Spark 3 preview</p></li>
<li><p>Fixed an interop issue with Sparklyr gateway connection when running with Spark 3 preview</p></li>
<li><p>Fixed a race condition of JVM object with refcount 1 being removed from JVM object tracker before pending method invocation(s) on them could be initiated (NOTE: previously this would only happen when the R process was running under high memory pressure)</p></li>
<li><p>Allow a chain of JVM method invocations to be batched into 1 <code>invoke</code> call</p></li>
<li><p>Removal of unneeded objects from JVM object tracker no longer blocks subsequent JVM method invocations</p></li>
<li><p>Add support for JDK11 for Spark 3 preview.</p></li>
</ul>
</section>
<section id="misc-10" class="level3">
<h3 class="anchored" data-anchor-id="misc-10">Misc</h3>
<ul>
<li><p>Support for installing Spark 3.0 Preview 2.</p></li>
<li><p>Emit more informative error message if network interface required for <code>spark_connect</code> is not up</p></li>
<li><p>Fixed a bug preventing more than 10 rows of a Spark table to be printed from R</p></li>
<li><p>Fixed a spelling error in <code>print</code> method for <code>ml_model_naive_bayes</code> objects</p></li>
<li><p>Made <code>sdf_drop_duplicates</code> an exported function (previously it was not exported by mistake)</p></li>
<li><p>Fixed a bug in <code>summary()</code> of <code>ml_linear_regression</code></p></li>
</ul>
</section>
</section>
<section id="sparklyr-1.1.0" class="level1">
<h1>Sparklyr 1.1.0</h1>
<section id="distributed-r-5" class="level3">
<h3 class="anchored" data-anchor-id="distributed-r-5">Distributed R</h3>
<ul>
<li>Add support for barrier execution mode with <code>barrier = TRUE</code> in <code>spark_apply()</code> (<span class="citation" data-cites="samuelmacedo83">@samuelmacedo83</span>, #2216).</li>
</ul>
</section>
<section id="streaming" class="level3">
<h3 class="anchored" data-anchor-id="streaming">Streaming</h3>
<ul>
<li><p>Add support for <code>stream_read_delta()</code> and <code>stream_write_delta()</code>.</p></li>
<li><p>Fixed typo in <code>stream_read_socket()</code>.</p></li>
</ul>
</section>
<section id="data-13" class="level3">
<h3 class="anchored" data-anchor-id="data-13">Data</h3>
<ul>
<li><p>Allow using Scala types in schema specifications. For example, <code>StringType</code> in the <code>columns</code> parameter for <code>spark_read_csv()</code> (<span class="citation" data-cites="jozefhajnala">@jozefhajnala</span>, #2226)</p></li>
<li><p>Add support for <code>DBI 1.1</code> to implement missing <code>dbQuoteLiteral</code> signature (#2227).</p></li>
</ul>
</section>
<section id="livy" class="level3">
<h3 class="anchored" data-anchor-id="livy">Livy</h3>
<ul>
<li><p>Add support for Livy 0.6.0.</p></li>
<li><p>Deprecate uploading sources to Livy, a jar is now always used and the <code>version</code> parameter in <code>spark_connect()</code> is always required.</p></li>
<li><p>Add config <code>sparklyr.livy.branch</code> to specify the branch used for the sparklyr JAR.</p></li>
<li><p>Add config <code>sparklyr.livy.jar</code> to configure path or URL to sparklyr JAR.</p></li>
</ul>
</section>
<section id="data-14" class="level3">
<h3 class="anchored" data-anchor-id="data-14">Data</h3>
<ul>
<li>Add support for <code>partition_by</code> when using <code>spark_write_delta()</code> (#2228).</li>
</ul>
</section>
</section>
<section id="sparklyr-1.0.5" class="level1">
<h1>Sparklyr 1.0.5</h1>
<section id="serialization-4" class="level3">
<h3 class="anchored" data-anchor-id="serialization-4">Serialization</h3>
<ul>
<li>R environments are now sent to Scala Maps rather than <code>java.util.Map[Object, Object]</code> (#1058).</li>
</ul>
</section>
<section id="data-15" class="level3">
<h3 class="anchored" data-anchor-id="data-15">Data</h3>
<ul>
<li><p>Allow <code>sdf_sql()</code> to accept glue strings (<span class="citation" data-cites="yutannihilation">@yutannihilation</span>, #2171).</p></li>
<li><p>Support to read and write from Delta Lake using <code>spark_read_delta()</code> and <code>spark_write_delta()</code> (#2148).</p></li>
</ul>
</section>
<section id="connections-8" class="level3">
<h3 class="anchored" data-anchor-id="connections-8">Connections</h3>
<ul>
<li><p><code>spark_connect()</code> supports new <code>packages</code> parameter to easily enable <code>kafka</code> and <code>delta</code> (#2148).</p></li>
<li><p><code>spark_disconnect()</code> returns invisibly (#2028).</p></li>
</ul>
</section>
<section id="configuration" class="level3">
<h3 class="anchored" data-anchor-id="configuration">Configuration</h3>
<ul>
<li>Support to specify config file location using the <code>SPARKLYR_CONFIG_FILE</code> environment variable (<span class="citation" data-cites="AgrawalAmey">@AgrawalAmey</span>, #2153).</li>
</ul>
</section>
<section id="compilation" class="level3">
<h3 class="anchored" data-anchor-id="compilation">Compilation</h3>
<ul>
<li>Support for Scala 12 (<span class="citation" data-cites="lu-wang-dl">@lu-wang-dl</span>, #2154).</li>
</ul>
</section>
<section id="yarn" class="level3">
<h3 class="anchored" data-anchor-id="yarn">YARN</h3>
<ul>
<li>Fix <code>curl_fetch_memory</code> error when using YARN Cluster mode (#2157).</li>
</ul>
</section>
</section>
<section id="sparklyr-1.0.4" class="level1">
<h1>Sparklyr 1.0.4</h1>
<section id="arrow" class="level3">
<h3 class="anchored" data-anchor-id="arrow">Arrow</h3>
<ul>
<li>Support for Apache Arrow 0.15 (<span class="citation" data-cites="nealrichardson">@nealrichardson</span>, #2132).</li>
</ul>
</section>
</section>
<section id="sparklyr-1.0.3" class="level1">
<h1>Sparklyr 1.0.3</h1>
<section id="kuberenetes" class="level3">
<h3 class="anchored" data-anchor-id="kuberenetes">Kuberenetes</h3>
<ul>
<li>Support for port forwarding in Windows using RStudio terminal.</li>
</ul>
</section>
<section id="dplyr-1" class="level3">
<h3 class="anchored" data-anchor-id="dplyr-1">dplyr</h3>
<ul>
<li>Fix support for <code>compute()</code> in Spark 1.6 (#2099)</li>
</ul>
</section>
<section id="data-16" class="level3">
<h3 class="anchored" data-anchor-id="data-16">Data</h3>
<ul>
<li>The <code>spark_read_()</code> functions now support multiple parameters (<span class="citation" data-cites="jozefhajnala">@jozefhajnala</span>, #2118).</li>
</ul>
</section>
<section id="connections-9" class="level3">
<h3 class="anchored" data-anchor-id="connections-9">Connections</h3>
<ul>
<li>Fix for Qubole connections for single user and multiple sessions (<span class="citation" data-cites="vipul1409">@vipul1409</span>, #2128).</li>
</ul>
</section>
</section>
<section id="sparklyr-1.0.2" class="level1">
<h1>Sparklyr 1.0.2</h1>
<section id="connections-10" class="level3">
<h3 class="anchored" data-anchor-id="connections-10">Connections</h3>
<ul>
<li>Support for Qubole connections using <code>mode = "quobole"</code> (<span class="citation" data-cites="vipul1409">@vipul1409</span>, #2039).</li>
</ul>
</section>
<section id="extensions-2" class="level3">
<h3 class="anchored" data-anchor-id="extensions-2">Extensions</h3>
<ul>
<li>When <code>invoke()</code> fails due to mismatched parameters, warning with info is logged.</li>
</ul>
</section>
<section id="rstudio" class="level3">
<h3 class="anchored" data-anchor-id="rstudio">RStudio</h3>
<ul>
<li>Spark UI path can now be accessed even when the R session and Spark are bussy.</li>
</ul>
</section>
<section id="distributed" class="level3">
<h3 class="anchored" data-anchor-id="distributed">Distributed</h3>
<ul>
<li><p>Configuration setting <code>sparklyr.apply.serializer</code> can be used to select serializer version in <code>spark_apply()</code>.</p></li>
<li><p>Fix for <code>spark_apply_log()</code> and use <code>RClosure</code> as logging component.</p></li>
</ul>
</section>
<section id="ml" class="level3">
<h3 class="anchored" data-anchor-id="ml">ML</h3>
<ul>
<li><code>ml_corr()</code> retrieve a <code>tibble</code> for better formatting.</li>
</ul>
</section>
<section id="misc-11" class="level3">
<h3 class="anchored" data-anchor-id="misc-11">Misc</h3>
<ul>
<li>Support for Spark 2.3.3 and 2.4.3.</li>
</ul>
</section>
<section id="data-17" class="level3">
<h3 class="anchored" data-anchor-id="data-17">Data</h3>
<ul>
<li><p>The <code>infer_schema</code> parameter now defaults to <code>is.null(column)</code>.</p></li>
<li><p>The <code>spark_read_()</code> functions support loading data with named <code>path</code> but no explicit <code>name</code>.</p></li>
</ul>
</section>
</section>
<section id="sparklyr-1.0.1" class="level1">
<h1>Sparklyr 1.0.1</h1>
<section id="ml-1" class="level3">
<h3 class="anchored" data-anchor-id="ml-1">ML</h3>
<ul>
<li><p><code>ml_lda()</code>: Allow passing of optional arguments via <code>...</code> to regex tokenizer, stop words remover, and count vectorizer components in the formula API.</p></li>
<li><p>Implemented <code>ml_evaluate()</code> for logistic regression, linear regression, and GLM models.</p></li>
<li><p>Implemented <code>print()</code> method for <code>ml_summary</code> objects.</p></li>
<li><p>Deprecated <code>compute_cost()</code> for KMeans in Spark 2.4 (#1772).</p></li>
<li><p>Added missing internal constructor for clustering evaluator (#1936).</p></li>
<li><p><code>sdf_partition()</code> has been renamed to <code>sdf_random_split()</code>.</p></li>
<li><p>Added <code>ft_one_hot_encoder_estimator()</code> (#1337).</p></li>
</ul>
</section>
<section id="misc-12" class="level3">
<h3 class="anchored" data-anchor-id="misc-12">Misc</h3>
<ul>
<li><p>Added <code>sdf_crosstab()</code> to create contingency tables.</p></li>
<li><p>Fix <code>tibble::as.tibble()</code> deprecation warning.</p></li>
</ul>
</section>
<section id="connections-11" class="level3">
<h3 class="anchored" data-anchor-id="connections-11">Connections</h3>
<ul>
<li>Reduced default memory for local connections when Java x64 is not installed (#1931).</li>
</ul>
</section>
<section id="batches" class="level3">
<h3 class="anchored" data-anchor-id="batches">Batches</h3>
<ul>
<li>Add support in <code>spark-submit</code> with R file to pass additional arguments to R file (#1942).</li>
</ul>
</section>
<section id="distributed-r-6" class="level3">
<h3 class="anchored" data-anchor-id="distributed-r-6">Distributed R</h3>
<ul>
<li>Fix support for multiple library paths when using <code>spark.r.libpaths</code> (<span class="citation" data-cites="mattpollock">@mattpollock</span>, #1956).</li>
</ul>
</section>
<section id="extensions-3" class="level3">
<h3 class="anchored" data-anchor-id="extensions-3">Extensions</h3>
<ul>
<li><p>Support for creating an Spark extension package using <code>spark_extension()</code>.</p></li>
<li><p>Add support for repositories in <code>spark_dependency()</code>.</p></li>
</ul>
</section>
<section id="dataframes" class="level3">
<h3 class="anchored" data-anchor-id="dataframes">DataFrames</h3>
<ul>
<li>Fix <code>sdf_bind_cols()</code> when using <code>dbplyr</code> 1.4.0.</li>
</ul>
</section>
<section id="kubernetes" class="level3">
<h3 class="anchored" data-anchor-id="kubernetes">Kubernetes</h3>
<ul>
<li>Fix regression in <code>spark_config_kubernetes()</code> configuration helper.</li>
</ul>
</section>
</section>
<section id="sparklyr-1.0.0" class="level1">
<h1>Sparklyr 1.0.0</h1>
<section id="arrow-1" class="level3">
<h3 class="anchored" data-anchor-id="arrow-1">Arrow</h3>
<ul>
<li>Support for Apache Arrow using the <code>arrow</code> package.</li>
</ul>
</section>
<section id="ml-2" class="level3">
<h3 class="anchored" data-anchor-id="ml-2">ML</h3>
<ul>
<li><p>The <code>dataset</code> parameter for estimator feature transformers has been deprecated (#1891).</p></li>
<li><p><code>ml_multilayer_perceptron_classifier()</code> gains probabilistic classifier parameters (#1798).</p></li>
<li><p>Removed support for all undocumented/deprecated parameters. These are mostly dot case parameters from pre-0.7.</p></li>
<li><p>Remove support for deprecated <code>function(pipeline_stage, data)</code> signature in <code>sdf_predict/transform/fit</code> functions.</p></li>
<li><p>Soft deprecate <code>sdf_predict/transform/fit</code> functions. Users are advised to use <code>ml_predict/transform/fit</code> functions instead.</p></li>
<li><p>Utilize the ellipsis package to provide warnings when unsupported arguments are specified in ML functions.</p></li>
</ul>
</section>
<section id="livy-1" class="level3">
<h3 class="anchored" data-anchor-id="livy-1">Livy</h3>
<ul>
<li><p>Support for sparklyr extensions when using Livy.</p></li>
<li><p>Significant performance improvements by using <code>version</code> in <code>spark_connect()</code> which enables using the sparklyr JAR rather than sources.</p></li>
<li><p>Improved memory use in Livy by using string builders and avoid print backs.</p></li>
</ul>
</section>
<section id="data-18" class="level3">
<h3 class="anchored" data-anchor-id="data-18">Data</h3>
<ul>
<li><p>Fix for <code>DBI::sqlInterpolate()</code> and related methods to properly quote parameterized queries.</p></li>
<li><p><code>copy_to()</code> names tables <code>sparklyr_tmp_</code> instead of <code>sparklyr_</code> for consistency with other temp tables and to avoid rendering them under the connections pane.</p></li>
<li><p><code>copy_to()</code> and <code>collect()</code> are not re-exported since they are commonly used even when using <code>DBI</code> or outside data analysis use cases.</p></li>
<li><p>Support for reading <code>path</code> as the second parameter in <code>spark_read_*()</code> when no name is specified (e.g.&nbsp;<code>spark_read_csv(sc, "data.csv")</code>).</p></li>
<li><p>Support for batches in <code>sdf_collect()</code> and <code>dplyr::collect()</code> to retrieve data incrementally using a callback function provided through a <code>callback</code> parameter. Useful when retrieving larger datasets.</p></li>
<li><p>Support for batches in <code>sdf_copy_to()</code> and <code>dplyr::copy_to()</code> by passing a list of callbacks that retrieve data frames. Useful when uploading larger datasets.</p></li>
<li><p><code>spark_read_source()</code> now has a <code>path</code> parameter for specifying file path.</p></li>
<li><p>Support for <code>whole</code> parameter for <code>spark_read_text()</code> to read an entire text file without splitting contents by line.</p></li>
</ul>
</section>
<section id="broom" class="level3">
<h3 class="anchored" data-anchor-id="broom">Broom</h3>
<ul>
<li>Implemented <code>tidy()</code>, <code>augment()</code>, and <code>glance()</code> for <code>ml_lda()</code>and <code>ml_als()</code> models (<span class="citation" data-cites="samuelmacedo83">@samuelmacedo83</span>)</li>
</ul>
</section>
<section id="connections-12" class="level3">
<h3 class="anchored" data-anchor-id="connections-12">Connections</h3>
<ul>
<li><p>Local connection defaults now to 2GB.</p></li>
<li><p>Support to install and connect based on major Spark versions, for instance: <code>spark_connect(master = "local", version = "2.4")</code>.</p></li>
<li><p>Support for installing and connecting to Spark 2.4.</p></li>
</ul>
</section>
<section id="serialization-5" class="level3">
<h3 class="anchored" data-anchor-id="serialization-5">Serialization</h3>
<ul>
<li>Faster retrieval of string arrays.</li>
</ul>
</section>
<section id="yarn-1" class="level3">
<h3 class="anchored" data-anchor-id="yarn-1">YARN</h3>
<ul>
<li><p>New YARN action under RStudio connection pane extension to launch YARN UI. Configurable through the <code>sparklyr.web.yarn</code> configuration setting.</p></li>
<li><p>Support for property expansion in <code>yarn-site.xml</code> (<span class="citation" data-cites="lgongmsft">@lgongmsft</span>, #1876).</p></li>
</ul>
</section>
<section id="distributed-r-7" class="level2">
<h2 class="anchored" data-anchor-id="distributed-r-7">Distributed R</h2>
<ul>
<li>The <code>memory</code> parameter in <code>spark_apply()</code> now defaults to <code>FALSE</code> when the <code>name</code> parameter is not specified.</li>
</ul>
</section>
<section id="other" class="level2">
<h2 class="anchored" data-anchor-id="other">Other</h2>
<ul>
<li><p>Removed dreprecated <code>sdf_mutate()</code>.</p></li>
<li><p>Remove exported <code>ensure_</code> functions which were deprecated.</p></li>
<li><p>Fixed missing Hive tables not rendering under some Spark distributions (#1823).</p></li>
<li><p>Remove dependency on broom.</p></li>
<li><p>Fixed re-entrancy job progress issues when running RStudio 1.2.</p></li>
<li><p>Tables with periods supported by setting <code>sparklyr.dplyr.period.splits</code> to <code>FALSE</code>.</p></li>
<li><p><code>sdf_len()</code>, <code>sdf_along()</code> and <code>sdf_seq()</code> default to 32 bit integers but allow support for 64 bits through <code>bits</code> parameter.</p></li>
<li><p>Support for detecting Spark version using <code>spark-submit</code>.</p></li>
</ul>
</section>
</section>
<section id="sparklyr-0.9.4" class="level1">
<h1>Sparklyr 0.9.4</h1>
<ul>
<li><p>Improved multiple streaming documentation examples (#1801, #1805, #1806).</p></li>
<li><p>Fix issue while printing Spark data frames under <code>tibble</code> 2.0.0 (#1829).</p></li>
<li><p>Support for <code>stream_write_console()</code> to write to console log.</p></li>
<li><p>Support for <code>stream_read_scoket()</code> to read socket streams.</p></li>
<li><p>Fix to <code>spark_read_kafka()</code> to remove unused <code>path</code>.</p></li>
</ul>
</section>
<section id="sparklyr-0.9.3" class="level1">
<h1>Sparklyr 0.9.3</h1>
<ul>
<li><p>Fix to make <code>spark_config_kubernetes()</code> work with variable <code>jar</code> parameters.</p></li>
<li><p>Support to install and use Spark 2.4.0.</p></li>
<li><p>Improvements and fixes to <code>spark_config_kubernetes()</code> parameters.</p></li>
<li><p>Support for <code>sparklyr.connect.ondisconnect</code> config setting to allow cleanup of resources when using kubernetes.</p></li>
<li><p><code>spark_apply()</code> and <code>spark_apply_bundle()</code> properly dereference symlinks when creating package bundle (<span class="citation" data-cites="awblocker">@awblocker</span>, #1785)</p></li>
<li><p>Fix <code>tableName</code> warning triggered while connecting.</p></li>
<li><p>Deprecate <code>sdf_mutate()</code> (#1754).</p></li>
<li><p>Fix requirement to specify <code>SPARK_HOME_VERSION</code> when <code>version</code> parameter is set in <code>spark_connect()</code>.</p></li>
<li><p>Cloudera autodetect Spark version improvements.</p></li>
<li><p>Fixed default for <code>session</code> in <code>reactiveSpark()</code>.</p></li>
<li><p>Removed <code>stream_read_jdbc()</code> and <code>stream_write_jdbc()</code> since they are not yet implemented in Spark.</p></li>
<li><p>Support for collecting NA values from logical columns (#1729).</p></li>
<li><p>Proactevely clean JVM objects when R object is deallocated.</p></li>
</ul>
</section>
<section id="sparklyr-0.9.2" class="level1">
<h1>Sparklyr 0.9.2</h1>
<ul>
<li><p>Support for Spark 2.3.2.</p></li>
<li><p>Fix installation error with older versions of <code>rstudioapi</code> (#1716).</p></li>
<li><p>Fix missing callstack and error case while logging in <code>spark_apply()</code>.</p></li>
<li><p>Proactevely clean JVM objects when R object is deallocated.</p></li>
</ul>
<section id="broom-1" class="level3">
<h3 class="anchored" data-anchor-id="broom-1">Broom</h3>
<ul>
<li>Implemented <code>tidy()</code>, <code>augment()</code>, and <code>glance()</code> for <code>ml_linear_svc()</code>and <code>ml_pca()</code> models (<span class="citation" data-cites="samuelmacedo83">@samuelmacedo83</span>)</li>
</ul>
</section>
</section>
<section id="sparklyr-0.9.2-1" class="level1">
<h1>Sparklyr 0.9.2</h1>
<ul>
<li><p>Support for Spark 2.3.2.</p></li>
<li><p>Fix installation error with older versions of <code>rstudioapi</code> (#1716).</p></li>
<li><p>Fix missing callstack and error case while logging in <code>spark_apply()</code>.</p></li>
<li><p>Fix regression in <code>sdf_collect()</code> failing to collect tables.</p></li>
<li><p>Fix new connection RStudio selectors colors when running under OS X Mojave.</p></li>
<li><p>Support for launching Livy logs from connection pane.</p></li>
</ul>
</section>
<section id="sparklyr-0.9.2-2" class="level1">
<h1>Sparklyr 0.9.2</h1>
<ul>
<li><p>Removed <code>overwrite</code> parameter in <code>spark_read_table()</code> (#1698).</p></li>
<li><p>Fix regression preventing using R 3.2 (#1695).</p></li>
<li><p>Additional jar search paths under Spark 2.3.1 (#1694)</p></li>
</ul>
</section>
<section id="sparklyr-0.9.1" class="level1">
<h1>Sparklyr 0.9.1</h1>
<ul>
<li><p>Terminate streams when Shiny app terminates.</p></li>
<li><p>Fix <code>dplyr::collect()</code> with Spark streams and improve printing.</p></li>
<li><p>Fix regression in <code>sparklyr.sanitize.column.names.verbose</code> setting which would cause verbose column renames.</p></li>
<li><p>Fix to <code>stream_write_kafka()</code> and <code>stream_write_jdbc()</code>.</p></li>
</ul>
</section>
<section id="sparklyr-0.9.0" class="level1">
<h1>Sparklyr 0.9.0</h1>
<section id="streaming-1" class="level3">
<h3 class="anchored" data-anchor-id="streaming-1">Streaming</h3>
<ul>
<li><p>Support for <code>stream_read_*()</code> and <code>stream_write_*()</code> to read from and to Spark structured streams.</p></li>
<li><p>Support for <code>dplyr</code>, <code>sdf_sql()</code>, <code>spark_apply()</code> and scoring pipeline in Spark streams.</p></li>
<li><p>Support for <code>reactiveSpark()</code> to create a <code>shiny</code> reactive over a Spark stream.</p></li>
<li><p>Support for convenience functions <code>stream_*()</code> to stop, change triggers, print, generate test streams, etc.</p></li>
</ul>
</section>
<section id="monitoring" class="level3">
<h3 class="anchored" data-anchor-id="monitoring">Monitoring</h3>
<ul>
<li><p>Support for interrupting long running operations and recover gracefully using the same connection.</p></li>
<li><p>Support cancelling Spark jobs by interrupting R session.</p></li>
<li><p>Support for monitoring job progress within RStudio, required RStudio 1.2.</p></li>
<li><p>Progress reports can be turned off by setting <code>sparklyr.progress</code> to <code>FALSE</code> in <code>spark_config()</code>.</p></li>
</ul>
</section>
<section id="kubernetes-1" class="level3">
<h3 class="anchored" data-anchor-id="kubernetes-1">Kubernetes</h3>
<ul>
<li><p>Added config <code>sparklyr.gateway.routing</code> to avoid routing to ports since Kubernetes clusters have unique spark masters.</p></li>
<li><p>Change backend ports to be choosen deterministically by searching for free ports starting on <code>sparklyr.gateway.port</code> which default to <code>8880</code>. This allows users to enable port forwarding with <code>kubectl port-forward</code>.</p></li>
<li><p>Added support to set config <code>sparklyr.events.aftersubmit</code> to a function that is called after <code>spark-submit</code> which can be used to automatically configure port forwarding.</p></li>
</ul>
</section>
<section id="batches-1" class="level2">
<h2 class="anchored" data-anchor-id="batches-1">Batches</h2>
<ul>
<li>Added support for <code>spark_submit()</code> to assist submitting non-interactive Spark jobs.</li>
</ul>
<section id="spark-ml-4" class="level3">
<h3 class="anchored" data-anchor-id="spark-ml-4">Spark ML</h3>
<ul>
<li><strong>(Breaking change)</strong> The formula API for ML classification algorithms no longer indexes numeric labels, to avoid the confusion of <code>0</code> being mapped to <code>"1"</code> and vice versa. This means that if the largest numeric label is <code>N</code>, Spark will fit a <code>N+1</code>-class classification model, regardless of how many distinct labels there are in the provided training set (#1591).</li>
<li>Fix retrieval of coefficients in <code>ml_logistic_regression()</code> (<span class="citation" data-cites="shabbybanks">@shabbybanks</span>, #1596).</li>
<li><strong>(Breaking change)</strong> For model objects, <code>lazy val</code> and <code>def</code> attributes have been converted to closures, so they are not evaluated at object instantiation (#1453).</li>
<li>Input and output column names are no longer required to construct pipeline objects to be consistent with Spark (#1513).</li>
<li>Vector attributes of pipeline stages are now printed correctly (#1618).</li>
<li>Deprecate various aliases favoring method names in Spark.
<ul>
<li><code>ml_binary_classification_eval()</code></li>
<li><code>ml_classification_eval()</code></li>
<li><code>ml_multilayer_perceptron()</code></li>
<li><code>ml_survival_regression()</code></li>
<li><code>ml_als_factorization()</code></li>
</ul></li>
<li>Deprecate incompatible signatures for <code>sdf_transform()</code> and <code>ml_transform()</code> families of methods; the former should take a <code>tbl_spark</code> as the first argument while the latter should take a model object as the first argument.</li>
<li>Input and output column names are no longer required to construct pipeline objects to be consistent with Spark (#1513).</li>
</ul>
</section>
<section id="data-19" class="level3">
<h3 class="anchored" data-anchor-id="data-19">Data</h3>
<ul>
<li><p>Implemented support for <code>DBI::db_explain()</code> (#1623).</p></li>
<li><p>Fixed for <code>timestamp</code> fields when using <code>copy_to()</code> (#1312, <span class="citation" data-cites="yutannihilation">@yutannihilation</span>).</p></li>
<li><p>Added support to read and write ORC files using <code>spark_read_orc()</code> and <code>spark_write_orc()</code> (#1548).</p></li>
</ul>
</section>
<section id="livy-2" class="level3">
<h3 class="anchored" data-anchor-id="livy-2">Livy</h3>
<ul>
<li><p>Fixed <code>must share the same src</code> error for <code>sdf_broadcast()</code> and other functions when using Livy connections.</p></li>
<li><p>Added support for logging <code>sparklyr</code> server events and logging sparklyr invokes as comments in the Livy UI.</p></li>
<li><p>Added support to open the Livy UI from the connections viewer while using RStudio.</p></li>
<li><p>Improve performance in Livy for long execution queries, fixed <code>livy.session.command.timeout</code> and support for <code>livy.session.command.interval</code> to control max polling while waiting for command response (#1538).</p></li>
<li><p>Fixed Livy version with MapR distributions.</p></li>
<li><p>Removed <code>install</code> column from <code>livy_available_versions()</code>.</p></li>
</ul>
</section>
<section id="distributed-r-8" class="level3">
<h3 class="anchored" data-anchor-id="distributed-r-8">Distributed R</h3>
<ul>
<li><p>Added <code>name</code> parameter to <code>spark_apply()</code> to optionally name resulting table.</p></li>
<li><p>Fix to <code>spark_apply()</code> to retain column types when NAs are present (#1665).</p></li>
<li><p><code>spark_apply()</code> now supports <code>rlang</code> anonymous functions. For example, <code>sdf_len(sc, 3) %&gt;% spark_apply(~.x+1)</code>.</p></li>
<li><p>Breaking Change: <code>spark_apply()</code> no longer defaults to the input column names when the <code>columns</code> parameter is nos specified.</p></li>
<li><p>Support for reading column names from the R data frame returned by <code>spark_apply()</code>.</p></li>
<li><p>Fix to support retrieving empty data frames in grouped <code>spark_apply()</code> operations (#1505).</p></li>
<li><p>Added support for <code>sparklyr.apply.packages</code> to configure default behavior for <code>spark_apply()</code> parameters (#1530).</p></li>
<li><p>Added support for <code>spark.r.libpaths</code> to configure package library in <code>spark_apply()</code> (#1530).</p></li>
</ul>
</section>
<section id="connections-13" class="level3">
<h3 class="anchored" data-anchor-id="connections-13">Connections</h3>
<ul>
<li><p>Default to Spark 2.3.1 for installation and local connections (#1680).</p></li>
<li><p><code>ml_load()</code> no longer keeps extraneous table views which was cluttering up the RStudio Connections pane (<span class="citation" data-cites="randomgambit">@randomgambit</span>, #1549).</p></li>
<li><p>Avoid preparing windows environment in non-local connections.</p></li>
</ul>
</section>
<section id="extensions-4" class="level3">
<h3 class="anchored" data-anchor-id="extensions-4">Extensions</h3>
<ul>
<li><p>The <code>ensure_*</code> family of functions is deprecated in favor of <a href="https://github.com/rstudio/forge">forge</a> which doesn’t use NSE and provides more informative errors messages for debugging (#1514).</p></li>
<li><p>Support for <code>sparklyr.invoke.trace</code> and <code>sparklyr.invoke.trace.callstack</code> configuration options to trace all <code>invoke()</code> calls.</p></li>
<li><p>Support to invoke methods with <code>char</code> types using single character strings (<span class="citation" data-cites="lawremi">@lawremi</span>, #1395).</p></li>
</ul>
</section>
<section id="serialization-6" class="level3">
<h3 class="anchored" data-anchor-id="serialization-6">Serialization</h3>
<ul>
<li>Fixed collection of <code>Date</code> types to support correct local JVM timezone to UTC ().</li>
</ul>
</section>
<section id="documentation-1" class="level3">
<h3 class="anchored" data-anchor-id="documentation-1">Documentation</h3>
<ul>
<li>Many new examples for <code>ft_binarizer()</code>, <code>ft_bucketizer()</code>, <code>ft_min_max_scaler</code>, <code>ft_max_abs_scaler()</code>, <code>ft_standard_scaler()</code>, <code>ml_kmeans()</code>, <code>ml_pca()</code>, <code>ml_bisecting_kmeans()</code>, <code>ml_gaussian_mixture()</code>, <code>ml_naive_bayes()</code>, <code>ml_decision_tree()</code>, <code>ml_random_forest()</code>, <code>ml_multilayer_perceptron_classifier()</code>, <code>ml_linear_regression()</code>, <code>ml_logistic_regression()</code>, <code>ml_gradient_boosted_trees()</code>, <code>ml_generalized_linear_regression()</code>, <code>ml_cross_validator()</code>, <code>ml_evaluator()</code>, <code>ml_clustering_evaluator()</code>, <code>ml_corr()</code>, <code>ml_chisquare_test()</code> and <code>sdf_pivot()</code> (<span class="citation" data-cites="samuelmacedo83">@samuelmacedo83</span>).</li>
</ul>
</section>
<section id="broom-2" class="level3">
<h3 class="anchored" data-anchor-id="broom-2">Broom</h3>
<ul>
<li>Implemented <code>tidy()</code>, <code>augment()</code>, and <code>glance()</code> for <code>ml_aft_survival_regression()</code>, <code>ml_isotonic_regression()</code>, <code>ml_naive_bayes()</code>, <code>ml_logistic_regression()</code>, <code>ml_decision_tree()</code>, <code>ml_random_forest()</code>, <code>ml_gradient_boosted_trees()</code>, <code>ml_bisecting_kmeans()</code>, <code>ml_kmeans()</code>and <code>ml_gaussian_mixture()</code> models (<span class="citation" data-cites="samuelmacedo83">@samuelmacedo83</span>)</li>
</ul>
</section>
<section id="configuration-1" class="level3">
<h3 class="anchored" data-anchor-id="configuration-1">Configuration</h3>
<ul>
<li><p>Deprecated configuration option <code>sparklyr.dplyr.compute.nocache</code>.</p></li>
<li><p>Added <code>spark_config_settings()</code> to list all <code>sparklyr</code> configuration settings and describe them, cleaned all settings and grouped by area while maintaining support for previous settings.</p></li>
<li><p>Static SQL configuration properties are now respected for Spark 2.3, and <code>spark.sql.catalogImplementation</code> defaults to <code>hive</code> to maintain Hive support (#1496, #415).</p></li>
<li><p><code>spark_config()</code> values can now also be specified as <code>options()</code>.</p></li>
<li><p>Support for functions as values in entries to <code>spark_config()</code> to enable advanced configuration workflows.</p></li>
</ul>
</section>
</section>
</section>
<section id="sparklyr-0.8.4" class="level1">
<h1>Sparklyr 0.8.4</h1>
<ul>
<li><p>Added support for <code>spark_session_config()</code> to modify spark session settings.</p></li>
<li><p>Added support for <code>sdf_debug_string()</code> to print execution plan for a Spark DataFrame.</p></li>
<li><p>Fixed DESCRIPTION file to include test packages as requested by CRAN.</p></li>
<li><p>Support for <code>sparklyr.spark-submit</code> as <code>config</code> entry to allow customizing the <code>spark-submit</code> command.</p></li>
<li><p>Changed <code>spark_connect()</code> to give precedence to the <code>version</code> parameter over <code>SPARK_HOME_VERSION</code> and other automatic version detection mechanisms, improved automatic version detection in Spark 2.X.</p></li>
<li><p>Fixed <code>sdf_bind_rows()</code> with <code>dplyr 0.7.5</code> and prepend id column instead of appending it to match behavior.</p></li>
<li><p><code>broom::tidy()</code> for linear regression and generalized linear regression models now give correct results (#1501).</p></li>
</ul>
</section>
<section id="sparklyr-0.8.3" class="level1">
<h1>Sparklyr 0.8.3</h1>
<ul>
<li>Support for Spark 2.3 in local windows clusters (#1473).</li>
</ul>
</section>
<section id="sparklyr-0.8.2" class="level1">
<h1>Sparklyr 0.8.2</h1>
<ul>
<li><p>Support for resource managers using <code>https</code> in <code>yarn-cluster</code> mode (#1459).</p></li>
<li><p>Fixed regression for connections using Livy and Spark 1.6.X.</p></li>
</ul>
</section>
<section id="sparklyr-0.8.1" class="level1">
<h1>Sparklyr 0.8.1</h1>
<ul>
<li>Fixed regression for connections using <code>mode</code> with <code>databricks</code>.</li>
</ul>
</section>
<section id="sparklyr-0.8.0" class="level1">
<h1>Sparklyr 0.8.0</h1>
<section id="spark-ml-5" class="level3">
<h3 class="anchored" data-anchor-id="spark-ml-5">Spark ML</h3>
<ul>
<li><p>Added <code>ml_validation_metrics()</code> to extract validation metrics from cross validator and train split validator models.</p></li>
<li><p><code>ml_transform()</code> now also takes a list of transformers, e.g.&nbsp;the result of <code>ml_stages()</code> on a <code>PipelineModel</code> (#1444).</p></li>
<li><p>Added <code>collect_sub_models</code> parameter to <code>ml_cross_validator()</code> and <code>ml_train_validation_split()</code> and helper function <code>ml_sub_models()</code> to allow inspecting models trained for each fold/parameter set (#1362).</p></li>
<li><p>Added <code>parallelism</code> parameter to <code>ml_cross_validator()</code> and <code>ml_train_validation_split()</code> to allow tuning in parallel (#1446).</p></li>
<li><p>Added support for <code>feature_subset_strategy</code> parameter in GBT algorithms (#1445).</p></li>
<li><p>Added <code>string_order_type</code> to <code>ft_string_indexer()</code> to allow control over how strings are indexed (#1443).</p></li>
<li><p>Added <code>ft_string_indexer_model()</code> constructor for the string indexer transformer (#1442).</p></li>
<li><p>Added <code>ml_feature_importances()</code> for extracing feature importances from tree-based models (#1436). <code>ml_tree_feature_importance()</code> is maintained as an alias.</p></li>
<li><p>Added <code>ml_vocabulary()</code> to extract vocabulary from count vectorizer model and <code>ml_topics_matrix()</code> to extract matrix from LDA model.</p></li>
<li><p><code>ml_tree_feature_importance()</code> now works properly with decision tree classification models (#1401).</p></li>
<li><p>Added <code>ml_corr()</code> for calculating correlation matrices and <code>ml_chisquare_test()</code> for performing chi-square hypothesis testing (#1247).</p></li>
<li><p><code>ml_save()</code> outputs message when model is successfully saved (#1348).</p></li>
<li><p><code>ml_</code> routines no longer capture the calling expression (#1393).</p></li>
<li><p>Added support for <code>offset</code> argument in <code>ml_generalized_linear_regression()</code> (#1396).</p></li>
<li><p>Fixed regression blocking use of response-features syntax in some <code>ml_</code>functions (#1302).</p></li>
<li><p>Added support for Huber loss for linear regression (#1335).</p></li>
<li><p><code>ft_bucketizer()</code> and <code>ft_quantile_discretizer()</code> now support multiple input columns (#1338, #1339).</p></li>
<li><p>Added <code>ft_feature_hasher()</code> (#1336).</p></li>
<li><p>Added <code>ml_clustering_evaluator()</code> (#1333).</p></li>
<li><p><code>ml_default_stop_words()</code> now returns English stop words by default (#1280).</p></li>
<li><p>Support the <code>sdf_predict(ml_transformer, dataset)</code> signature with a deprecation warning. Also added a deprecation warning to the usage of <code>sdf_predict(ml_model, dataset)</code>. (#1287)</p></li>
<li><p>Fixed regression blocking use of <code>ml_kmeans()</code> in Spark 1.6.x.</p></li>
</ul>
</section>
<section id="extensions-5" class="level3">
<h3 class="anchored" data-anchor-id="extensions-5">Extensions</h3>
<ul>
<li><p><code>invoke*()</code> method dispatch now supports <code>Char</code> and <code>Short</code> parameters. Also, <code>Long</code> parameters now allow numeric arguments, but integers are supported for backwards compatibility (#1395).</p></li>
<li><p><code>invoke_static()</code> now supports calling Scala’s package objects (#1384).</p></li>
<li><p><code>spark_connection</code> and <code>spark_jobj</code> classes are now exported (#1374).</p></li>
</ul>
</section>
<section id="distributed-r-9" class="level3">
<h3 class="anchored" data-anchor-id="distributed-r-9">Distributed R</h3>
<ul>
<li><p>Added support for <code>profile</code> parameter in <code>spark_apply()</code> that collects a profile to measure perpformance that can be rendered using the <code>profvis</code> package.</p></li>
<li><p>Added support for <code>spark_apply()</code> under Livy connections.</p></li>
<li><p>Fixed file not found error in <code>spark_apply()</code> while working under low disk space.</p></li>
<li><p>Added support for <code>sparklyr.apply.options.rscript.before</code> to run a custom command before launching the R worker role.</p></li>
<li><p>Added support for <code>sparklyr.apply.options.vanilla</code> to be set to <code>FALSE</code> to avoid using <code>--vanilla</code> while launching R worker role.</p></li>
<li><p>Fixed serialization issues most commonly hit while using <code>spark_apply()</code> with NAs (#1365, #1366).</p></li>
<li><p>Fixed issue with dates or date-times not roundtripping with `spark_apply() (#1376).</p></li>
<li><p>Fixed data frame provided by <code>spark_apply()</code> to not provide characters not factors (#1313).</p></li>
</ul>
</section>
<section id="miscellaneous" class="level3">
<h3 class="anchored" data-anchor-id="miscellaneous">Miscellaneous</h3>
<ul>
<li><p>Fixed typo in <code>sparklyr.yarn.cluster.hostaddress.timeot</code> (#1318).</p></li>
<li><p>Fixed regression blocking use of <code>livy.session.start.timeout</code> parameter in Livy connections.</p></li>
<li><p>Added support for Livy 0.4 and Livy 0.5.</p></li>
<li><p>Livy now supports Kerberos authentication.</p></li>
<li><p>Default to Spark 2.3.0 for installation and local connections (#1449).</p></li>
<li><p><code>yarn-cluster</code> now supported by connecting with <code>master="yarn"</code> and <code>config</code> entry <code>sparklyr.shell.deploy-mode</code> set to <code>cluster</code> (#1404).</p></li>
<li><p><code>sample_frac()</code> and <code>sample_n()</code> now work properly in nontrivial queries (#1299)</p></li>
<li><p><code>sdf_copy_to()</code> no longer gives a spurious warning when user enters a multiline expression for <code>x</code> (#1386).</p></li>
<li><p><code>spark_available_versions()</code> was changed to only return available Spark versions, Hadoop versions can be still retrieved using <code>hadoop = TRUE</code>.</p></li>
<li><p><code>spark_installed_versions()</code> was changed to retrieve the full path to the installation folder.</p></li>
<li><p><code>cbind()</code> and <code>sdf_bind_cols()</code> don’t use NSE internally anymore and no longer output names of mismatched data frames on error (#1363).</p></li>
</ul>
</section>
</section>
<section id="sparklyr-0.7.0" class="level1">
<h1>Sparklyr 0.7.0</h1>
<ul>
<li><p>Added support for Spark 2.2.1.</p></li>
<li><p>Switched <code>copy_to</code> serializer to use Scala implementation, this change can be reverted by setting the <code>sparklyr.copy.serializer</code> option to <code>csv_file</code>.</p></li>
<li><p>Added support for <code>spark_web()</code> for Livy and Databricks connections when using Spark 2.X.</p></li>
<li><p>Fixed <code>SIGPIPE</code> error under <code>spark_connect()</code> immediately after a <code>spark_disconnect()</code> operation.</p></li>
<li><p><code>spark_web()</code> is is more reliable under Spark 2.X by making use of a new API to programmatically find the right address.</p></li>
<li><p>Added support in <code>dbWriteTable()</code> for <code>temporary = FALSE</code> to allow persisting table across connections. Changed default value for <code>temporary</code> to <code>TRUE</code> to match <code>DBI</code> specification, for compatibility, default value can be reverted back to <code>FALSE</code> using the <code>sparklyr.dbwritetable.temp</code> option.</p></li>
<li><p><code>ncol()</code> now returns the number of columns instead of <code>NA</code>, and <code>nrow()</code> now returns <code>NA_real_</code>.</p></li>
<li><p>Added support to collect <code>VectorUDT</code> column types with nested arrays.</p></li>
<li><p>Fixed issue in which connecting to Livy would fail due to long user names or long passwords.</p></li>
<li><p>Fixed error in the Spark connection dialog for clusters using a proxy.</p></li>
<li><p>Improved support for Spark 2.X under Cloudera clusters by prioritizing use of <code>spark2-submit</code> over <code>spark-submit</code>.</p></li>
<li><p>Livy new connection dialog now prompts for password using <code>rstudioapi::askForPassword()</code>.</p></li>
<li><p>Added <code>schema</code> parameter to <code>spark_read_parquet()</code> that enables reading a subset of the schema to increase performance.</p></li>
<li><p>Implemented <code>sdf_describe()</code> to easily compute summary statistics for data frames.</p></li>
<li><p>Fixed data frames with dates in <code>spark_apply()</code> retrieved as <code>Date</code> instead of doubles.</p></li>
<li><p>Added support to use <code>invoke()</code> with arrays of POSIXlt and POSIXct.</p></li>
<li><p>Added support for <code>context</code> parameter in <code>spark_apply()</code> to allow callers to pass additional contextual information to the <code>f()</code> closure.</p></li>
<li><p>Implemented workaround to support in <code>spark_write_table()</code> for <code>mode = 'append'</code>.</p></li>
<li><p>Various ML improvements, including support for pipelines, additional algorithms, hyper-parameter tuning, and better model persistence.</p></li>
<li><p>Added <code>spark_read_libsvm()</code> for reading libsvm files.</p></li>
<li><p>Added support for separating struct columns in <code>sdf_separate_column()</code>.</p></li>
<li><p>Fixed collection of <code>short</code>, <code>float</code> and <code>byte</code> to properly return NAs.</p></li>
<li><p>Added <code>sparklyr.collect.datechars</code> option to enable collecting <code>DateType</code> and <code>TimestampTime</code> as <code>characters</code> to support compatibility with previos versions.</p></li>
<li><p>Fixed collection of <code>DateType</code> and <code>TimestampTime</code> from <code>character</code> to proper <code>Date</code> and <code>POSIXct</code> types.</p></li>
</ul>
</section>
<section id="sparklyr-0.6.4" class="level1">
<h1>Sparklyr 0.6.4</h1>
<ul>
<li><p>Added support for HTTPS for <code>yarn-cluster</code> which is activated by setting <code>yarn.http.policy</code> to <code>HTTPS_ONLY</code> in <code>yarn-site.xml</code>.</p></li>
<li><p>Added support for <code>sparklyr.yarn.cluster.accepted.timeout</code> under <code>yarn-cluster</code> to allow users to wait for resources under cluster with high waiting times.</p></li>
<li><p>Fix to <code>spark_apply()</code> when package distribution deadlock triggers in environments where multiple executors run under the same node.</p></li>
<li><p>Added support in <code>spark_apply()</code> for specifying a list of <code>packages</code> to distribute to each worker node.</p></li>
<li><p>Added support in<code>yarn-cluster</code> for <code>sparklyr.yarn.cluster.lookup.prefix</code>, <code>sparklyr.yarn.cluster.lookup.username</code> and <code>sparklyr.yarn.cluster.lookup.byname</code> to control the new application lookup behavior.</p></li>
</ul>
</section>
<section id="sparklyr-0.6.3" class="level1">
<h1>Sparklyr 0.6.3</h1>
<ul>
<li><p>Enabled support for Java 9 for clusters configured with Hadoop 2.8. Java 9 blocked on ‘master=local’ unless ‘options(sparklyr.java9 = TRUE)’ is set.</p></li>
<li><p>Fixed issue in <code>spark_connect()</code> where using <code>set.seed()</code> before connection would cause session ids to be duplicates and connections to be reused.</p></li>
<li><p>Fixed issue in <code>spark_connect()</code> blocking gateway port when connection was never started to the backend, for isntasnce, while interrupting the r session while connecting.</p></li>
<li><p>Performance improvement for quering field names from tables impacting tables and <code>dplyr</code> queries, most noticeable in <code>na.omit</code> with several columns.</p></li>
<li><p>Fix to <code>spark_apply()</code> when closure returns a <code>data.frame</code> that contains no rows and has one or more columns.</p></li>
<li><p>Fix to <code>spark_apply()</code> while using <code>tryCatch()</code> within closure and increased callstack printed to logs when error triggers within closure.</p></li>
<li><p>Added support for the <code>SPARKLYR_LOG_FILE</code> environment variable to specify the file used for log output.</p></li>
<li><p>Fixed regression for <code>union_all()</code> affecting Spark 1.6.X.</p></li>
<li><p>Added support for <code>na.omit.cache</code> option that when set to <code>FALSE</code> will prevent <code>na.omit</code> from caching results when rows are dropped.</p></li>
<li><p>Added support in <code>spark_connect()</code> for <code>yarn-cluster</code> with hight-availability enabled.</p></li>
<li><p>Added support for <code>spark_connect()</code> with <code>master="yarn-cluster"</code> to query YARN resource manager API and retrieve the correct container host name.</p></li>
<li><p>Fixed issue in <code>invoke()</code> calls while using integer arrays that contain <code>NA</code> which can be commonly experienced while using <code>spark_apply()</code>.</p></li>
<li><p>Added <code>topics.description</code> under <code>ml_lda()</code> result.</p></li>
<li><p>Added support for <code>ft_stop_words_remover()</code> to strip out stop words from tokens.</p></li>
<li><p>Feature transformers (<code>ft_*</code> functions) now explicitly require <code>input.col</code> and <code>output.col</code> to be specified.</p></li>
<li><p>Added support for <code>spark_apply_log()</code> to enable logging in worker nodes while using <code>spark_apply()</code>.</p></li>
<li><p>Fix to <code>spark_apply()</code> for <code>SparkUncaughtExceptionHandler</code> exception while running over large jobs that may overlap during an, now unnecesary, unregister operation.</p></li>
<li><p>Fix race-condition first time <code>spark_apply()</code> is run when more than one partition runs in a worker and both processes try to unpack the packages bundle at the same time.</p></li>
<li><p><code>spark_apply()</code> now adds generic column names when needed and validates <code>f</code> is a <code>function</code>.</p></li>
<li><p>Improved documentation and error cases for <code>metric</code> argument in <code>ml_classification_eval()</code> and <code>ml_binary_classification_eval()</code>.</p></li>
<li><p>Fix to <code>spark_install()</code> to use the <code>/logs</code> subfolder to store local <code>log4j</code> logs.</p></li>
<li><p>Fix to <code>spark_apply()</code> when R is used from a worker node since worker node already contains packages but still might be triggering different R session.</p></li>
<li><p>Fix connection from closing when <code>invoke()</code> attempts to use a class with a method that contains a reference to an undefined class.</p></li>
<li><p>Implemented all tuning options from Spark ML for <code>ml_random_forest()</code>, <code>ml_gradient_boosted_trees()</code>, and <code>ml_decision_tree()</code>.</p></li>
<li><p>Avoid tasks failing under <code>spark_apply()</code> and multiple concurrent partitions running while selecting backend port.</p></li>
<li><p>Added support for numeric arguments for <code>n</code> in <code>lead()</code> for dplyr.</p></li>
<li><p>Added unsupported error message to <code>sample_n()</code> and <code>sample_frac()</code> when Spark is not 2.0 or higher.</p></li>
<li><p>Fixed <code>SIGPIPE</code> error under <code>spark_connect()</code> immediately after a <code>spark_disconnect()</code> operation.</p></li>
<li><p>Added support for <code>sparklyr.apply.env.</code> under <code>spark_config()</code> to allow <code>spark_apply()</code> to initializae environment varaibles.</p></li>
<li><p>Added support for <code>spark_read_text()</code> and <code>spark_write_text()</code> to read from and to plain text files.</p></li>
<li><p>Addesd support for RStudio project templates to create an “R Package using sparklyr”.</p></li>
<li><p>Fix <code>compute()</code> to trigger refresh of the connections view.</p></li>
<li><p>Added a <code>k</code> argument to <code>ml_pca()</code> to enable specification of number of principal components to extract. Also implemented <code>sdf_project()</code> to project datasets using the results of <code>ml_pca()</code> models.</p></li>
<li><p>Added support for additional livy session creation parameters using the <code>livy_config()</code> function.</p></li>
</ul>
</section>
<section id="sparklyr-0.6.2" class="level1">
<h1>Sparklyr 0.6.2</h1>
<ul>
<li>Fix connection_spark_shinyapp() under RStudio 1.1 to avoid error while listing Spark installation options for the first time.</li>
</ul>
</section>
<section id="sparklyr-0.6.1" class="level1">
<h1>Sparklyr 0.6.1</h1>
<ul>
<li><p>Fixed error in <code>spark_apply()</code> that may triggered when multiple CPUs are used in a single node due to race conditions while accesing the gateway service and another in the <code>JVMObjectTracker</code>.</p></li>
<li><p><code>spark_apply()</code> now supports explicit column types using the <code>columns</code> argument to avoid sampling types.</p></li>
<li><p><code>spark_apply()</code> with <code>group_by</code> no longer requires persisting to disk nor memory.</p></li>
<li><p>Added support for Spark 1.6.3 under <code>spark_install()</code>.</p></li>
<li><p>Added support for Spark 1.6.3 under <code>spark_install()</code></p></li>
<li><p><code>spark_apply()</code> now logs the current callstack when it fails.</p></li>
<li><p>Fixed error triggered while processing empty partitions in <code>spark_apply()</code>.</p></li>
<li><p>Fixed slow printing issue caused by <code>print</code> calculating the total row count, which is expensive for some tables.</p></li>
<li><p>Fixed <code>sparklyr 0.6</code> issue blocking concurrent <code>sparklyr</code> connections, which required to set <code>config$sparklyr.gateway.remote = FALSE</code> as workaround.</p></li>
</ul>
</section>
<section id="sparklyr-0.6.0" class="level1">
<h1>Sparklyr 0.6.0</h1>
<section id="distributed-r-10" class="level3">
<h3 class="anchored" data-anchor-id="distributed-r-10">Distributed R</h3>
<ul>
<li><p>Added <code>packages</code> parameter to <code>spark_apply()</code> to distribute packages across worker nodes automatically.</p></li>
<li><p>Added <code>sparklyr.closures.rlang</code> as a <code>spark_config()</code> value to support generic closures provided by the <code>rlang</code> package.</p></li>
<li><p>Added config options <code>sparklyr.worker.gateway.address</code> and <code>sparklyr.worker.gateway.port</code> to configure gateway used under worker nodes.</p></li>
<li><p>Added <code>group_by</code> parameter to <code>spark_apply()</code>, to support operations over groups of dataframes.</p></li>
<li><p>Added <code>spark_apply()</code>, allowing users to use R code to directly manipulate and transform Spark DataFrames.</p></li>
</ul>
</section>
<section id="external-data" class="level3">
<h3 class="anchored" data-anchor-id="external-data">External Data</h3>
<ul>
<li><p>Added <code>spark_write_source()</code>. This function writes data into a Spark data source which can be loaded through an Spark package.</p></li>
<li><p>Added <code>spark_write_jdbc()</code>. This function writes from a Spark DataFrame into a JDBC connection.</p></li>
<li><p>Added <code>columns</code> parameter to <code>spark_read_*()</code> functions to load data with named columns or explicit column types.</p></li>
<li><p>Added <code>partition_by</code> parameter to <code>spark_write_csv()</code>, <code>spark_write_json()</code>, <code>spark_write_table()</code> and <code>spark_write_parquet()</code>.</p></li>
<li><p>Added <code>spark_read_source()</code>. This function reads data from a Spark data source which can be loaded through an Spark package.</p></li>
<li><p>Added support for <code>mode = "overwrite"</code> and <code>mode = "append"</code> to <code>spark_write_csv()</code>.</p></li>
<li><p><code>spark_write_table()</code> now supports saving to default Hive path.</p></li>
<li><p>Improved performance of <code>spark_read_csv()</code> reading remote data when <code>infer_schema = FALSE</code>.</p></li>
<li><p>Added <code>spark_read_jdbc()</code>. This function reads from a JDBC connection into a Spark DataFrame.</p></li>
<li><p>Renamed <code>spark_load_table()</code> and <code>spark_save_table()</code> into <code>spark_read_table()</code> and <code>spark_write_table()</code> for consistency with existing <code>spark_read_*()</code> and <code>spark_write_*()</code> functions.</p></li>
<li><p>Added support to specify a vector of column names in <code>spark_read_csv()</code> to specify column names without having to set the type of each column.</p></li>
<li><p>Improved <code>copy_to()</code>, <code>sdf_copy_to()</code> and <code>dbWriteTable()</code> performance under <code>yarn-client</code> mode.</p></li>
</ul>
</section>
<section id="dplyr-2" class="level3">
<h3 class="anchored" data-anchor-id="dplyr-2">dplyr</h3>
<ul>
<li><p>Support for <code>cumprod()</code> to calculate cumulative products.</p></li>
<li><p>Support for <code>cor()</code>, <code>cov()</code>, <code>sd()</code> and <code>var()</code> as window functions.</p></li>
<li><p>Support for Hive built-in operators <code>%like%</code>, <code>%rlike%</code>, and <code>%regexp%</code> for matching regular expressions in <code>filter()</code> and <code>mutate()</code>.</p></li>
<li><p>Support for dplyr (&gt;= 0.6) which among many improvements, increases performance in some queries by making use of a new query optimizer.</p></li>
<li><p><code>sample_frac()</code> takes a fraction instead of a percent to match dplyr.</p></li>
<li><p>Improved performance of <code>sample_n()</code> and <code>sample_frac()</code> through the use of <code>TABLESAMPLE</code> in the generated query.</p></li>
</ul>
</section>
<section id="databases" class="level3">
<h3 class="anchored" data-anchor-id="databases">Databases</h3>
<ul>
<li><p>Added <code>src_databases()</code>. This function list all the available databases.</p></li>
<li><p>Added <code>tbl_change_db()</code>. This function changes current database.</p></li>
</ul>
</section>
<section id="dataframes-1" class="level3">
<h3 class="anchored" data-anchor-id="dataframes-1">DataFrames</h3>
<ul>
<li><p>Added <code>sdf_len()</code>, <code>sdf_seq()</code> and <code>sdf_along()</code> to help generate numeric sequences as Spark DataFrames.</p></li>
<li><p>Added <code>spark_set_checkpoint_dir()</code>, <code>spark_get_checkpoint_dir()</code>, and <code>sdf_checkpoint()</code> to enable checkpointing.</p></li>
<li><p>Added <code>sdf_broadcast()</code> which can be used to hint the query optimizer to perform a broadcast join in cases where a shuffle hash join is planned but not optimal.</p></li>
<li><p>Added <code>sdf_repartition()</code>, <code>sdf_coalesce()</code>, and <code>sdf_num_partitions()</code> to support repartitioning and getting the number of partitions of Spark DataFrames.</p></li>
<li><p>Added <code>sdf_bind_rows()</code> and <code>sdf_bind_cols()</code> – these functions are the <code>sparklyr</code> equivalent of <code>dplyr::bind_rows()</code> and <code>dplyr::bind_cols()</code>.</p></li>
<li><p>Added <code>sdf_separate_column()</code> – this function allows one to separate components of an array / vector column into separate scalar-valued columns.</p></li>
<li><p><code>sdf_with_sequential_id()</code> now supports <code>from</code> parameter to choose the starting value of the id column.</p></li>
<li><p>Added <code>sdf_pivot()</code>. This function provides a mechanism for constructing pivot tables, using Spark’s ‘groupBy’ + ‘pivot’ functionality, with a formula interface similar to that of <code>reshape2::dcast()</code>.</p></li>
</ul>
</section>
<section id="mllib" class="level3">
<h3 class="anchored" data-anchor-id="mllib">MLlib</h3>
<ul>
<li><p>Added <code>vocabulary.only</code> to <code>ft_count_vectorizer()</code> to retrieve the vocabulary with ease.</p></li>
<li><p>GLM type models now support <code>weights.column</code> to specify weights in model fitting. (#217)</p></li>
<li><p><code>ml_logistic_regression()</code> now supports multinomial regression, in addition to binomial regression [requires Spark 2.1.0 or greater]. (#748)</p></li>
<li><p>Implemented <code>residuals()</code> and <code>sdf_residuals()</code> for Spark linear regression and GLM models. The former returns a R vector while the latter returns a <code>tbl_spark</code> of training data with a <code>residuals</code> column added.</p></li>
<li><p>Added <code>ml_model_data()</code>, used for extracting data associated with Spark ML models.</p></li>
<li><p>The <code>ml_save()</code> and <code>ml_load()</code> functions gain a <code>meta</code> argument, allowing users to specify where R-level model metadata should be saved independently of the Spark model itself. This should help facilitate the saving and loading of Spark models used in non-local connection scenarios.</p></li>
<li><p><code>ml_als_factorization()</code> now supports the implicit matrix factorization and nonnegative least square options.</p></li>
<li><p>Added <code>ft_count_vectorizer()</code>. This function can be used to transform columns of a Spark DataFrame so that they might be used as input to <code>ml_lda()</code>. This should make it easier to invoke <code>ml_lda()</code> on Spark data sets.</p></li>
</ul>
</section>
<section id="broom-3" class="level3">
<h3 class="anchored" data-anchor-id="broom-3">Broom</h3>
<ul>
<li>Implemented <code>tidy()</code>, <code>augment()</code>, and <code>glance()</code> from tidyverse/broom for <code>ml_model_generalized_linear_regression</code> and <code>ml_model_linear_regression</code> models.</li>
</ul>
</section>
<section id="r-compatibility" class="level3">
<h3 class="anchored" data-anchor-id="r-compatibility">R Compatibility</h3>
<ul>
<li>Implemented <code>cbind.tbl_spark()</code>. This method works by first generating index columns using <code>sdf_with_sequential_id()</code> then performing <code>inner_join()</code>. Note that dplyr <code>_join()</code> functions should still be used for DataFrames with common keys since they are less expensive.</li>
</ul>
</section>
<section id="connections-14" class="level3">
<h3 class="anchored" data-anchor-id="connections-14">Connections</h3>
<ul>
<li><p>Increased default number of concurrent connections by setting default for <code>spark.port.maxRetries</code> from 16 to 128.</p></li>
<li><p>Support for gateway connections <code>sparklyr://hostname:port/session</code> and using <code>spark-submit --class sparklyr.Shell sparklyr-2.1-2.11.jar &lt;port&gt; &lt;id&gt; --remote</code>.</p></li>
<li><p>Added support for <code>sparklyr.gateway.service</code> and <code>sparklyr.gateway.remote</code> to enable/disable the gateway in service and to accept remote connections required for Yarn Cluster mode.</p></li>
<li><p>Added support for Yarn Cluster mode using <code>master = "yarn-cluster"</code>. Either, explicitly set <code>config = list(sparklyr.gateway.address = "&lt;driver-name&gt;")</code> or implicitly <code>sparklyr</code> will read the <code>site-config.xml</code> for the <code>YARN_CONF_DIR</code> environment variable.</p></li>
<li><p>Added <code>spark_context_config()</code> and <code>hive_context_config()</code> to retrieve runtime configurations for the Spark and Hive contexts.</p></li>
<li><p>Added <code>sparklyr.log.console</code> to redirect logs to console, useful to troubleshooting <code>spark_connect</code>.</p></li>
<li><p>Added <code>sparklyr.backend.args</code> as config option to enable passing parameters to the <code>sparklyr</code> backend.</p></li>
<li><p>Improved logging while establishing connections to <code>sparklyr</code>.</p></li>
<li><p>Improved <code>spark_connect()</code> performance.</p></li>
<li><p>Implemented new configuration checks to proactively report connection errors in Windows.</p></li>
<li><p>While connecting to spark from Windows, setting the <code>sparklyr.verbose</code> option to <code>TRUE</code> prints detailed configuration steps.</p></li>
<li><p>Added <code>custom_headers</code> to <code>livy_config()</code> to add custom headers to the REST call to the Livy server</p></li>
</ul>
</section>
<section id="compilation-1" class="level3">
<h3 class="anchored" data-anchor-id="compilation-1">Compilation</h3>
<ul>
<li><p>Added support for <code>jar_dep</code> in the compilation specification to support additional <code>jars</code> through <code>spark_compile()</code>.</p></li>
<li><p><code>spark_compile()</code> now prints deprecation warnings.</p></li>
<li><p>Added <code>download_scalac()</code> to assist downloading all the Scala compilers required to build using <code>compile_package_jars</code> and provided support for using any <code>scalac</code> minor versions while looking for the right compiler.</p></li>
</ul>
</section>
<section id="backend" class="level3">
<h3 class="anchored" data-anchor-id="backend">Backend</h3>
<ul>
<li>Improved backend logging by adding type and session id prefix.</li>
</ul>
</section>
<section id="miscellaneous-1" class="level3">
<h3 class="anchored" data-anchor-id="miscellaneous-1">Miscellaneous</h3>
<ul>
<li><p><code>copy_to()</code> and <code>sdf_copy_to()</code> auto generate a <code>name</code> when an expression can’t be transformed into a table name.</p></li>
<li><p>Implemented <code>type_sum.jobj()</code> (from tibble) to enable better printing of jobj objects embedded in data frames.</p></li>
<li><p>Added the <code>spark_home_set()</code> function, to help facilitate the setting of the <code>SPARK_HOME</code> environment variable. This should prove useful in teaching environments, when teaching the basics of Spark and sparklyr.</p></li>
<li><p>Added support for the <code>sparklyr.ui.connections</code> option, which adds additional connection options into the new connections dialog. The <code>rstudio.spark.connections</code> option is now deprecated.</p></li>
<li><p>Implemented the “New Connection Dialog” as a Shiny application to be able to support newer versions of RStudio that deprecate current connections UI.</p></li>
</ul>
</section>
<section id="bug-fixes-4" class="level3">
<h3 class="anchored" data-anchor-id="bug-fixes-4">Bug Fixes</h3>
<ul>
<li><p>When using <code>spark_connect()</code> in local clusters, it validates that <code>java</code> exists under <code>JAVA_HOME</code> to help troubleshoot systems that have an incorrect <code>JAVA_HOME</code>.</p></li>
<li><p>Improved <code>argument is of length zero</code> error triggered while retrieving data with no columns to display.</p></li>
<li><p>Fixed <code>Path does not exist</code> referencing <code>hdfs</code> exception during <code>copy_to</code> under systems configured with <code>HADOOP_HOME</code>.</p></li>
<li><p>Fixed session crash after “No status is returned” error by terminating invalid connection and added support to print log trace during this error.</p></li>
<li><p><code>compute()</code> now caches data in memory by default. To revert this beavior use <code>sparklyr.dplyr.compute.nocache</code> set to <code>TRUE</code>.</p></li>
<li><p><code>spark_connect()</code> with <code>master = "local"</code> and a given <code>version</code> overrides <code>SPARK_HOME</code> to avoid existing installation mismatches.</p></li>
<li><p>Fixed <code>spark_connect()</code> under Windows issue when <code>newInstance0</code> is present in the logs.</p></li>
<li><p>Fixed collecting <code>long</code> type columns when NAs are present (#463).</p></li>
<li><p>Fixed backend issue that affects systems where <code>localhost</code> does not resolve properly to the loopback address.</p></li>
<li><p>Fixed issue collecting data frames containing newlines <code>\n</code>.</p></li>
<li><p>Spark Null objects (objects of class NullType) discovered within numeric vectors are now collected as NAs, rather than lists of NAs.</p></li>
<li><p>Fixed warning while connecting with livy and improved 401 message.</p></li>
<li><p>Fixed issue in <code>spark_read_parquet()</code> and other read methods in which <code>spark_normalize_path()</code> would not work in some platforms while loading data using custom protocols like <code>s3n://</code> for Amazon S3.</p></li>
<li><p>Resolved issue in <code>spark_save()</code> / <code>load_table()</code> to support saving / loading data and added path parameter in <code>spark_load_table()</code> for consistency with other functions.</p></li>
</ul>
</section>
</section>
<section id="sparklyr-0.5.5" class="level1">
<h1>Sparklyr 0.5.5</h1>
<ul>
<li>Implemented support for <code>connectionViewer</code> interface required in RStudio 1.1 and <code>spark_connect</code> with <code>mode="databricks"</code>.</li>
</ul>
</section>
<section id="sparklyr-0.5.4" class="level1">
<h1>Sparklyr 0.5.4</h1>
<ul>
<li>Implemented support for <code>dplyr 0.6</code> and Spark 2.1.x.</li>
</ul>
</section>
<section id="sparklyr-0.5.3" class="level1">
<h1>Sparklyr 0.5.3</h1>
<ul>
<li>Implemented support for <code>DBI 0.6</code>.</li>
</ul>
</section>
<section id="sparklyr-0.5.2" class="level1">
<h1>Sparklyr 0.5.2</h1>
<ul>
<li><p>Fix to <code>spark_connect</code> affecting Windows users and Spark 1.6.x.</p></li>
<li><p>Fix to Livy connections which would cause connections to fail while connection is on ‘waiting’ state.</p></li>
</ul>
</section>
<section id="sparklyr-0.5.0" class="level1">
<h1>Sparklyr 0.5.0</h1>
<ul>
<li><p>Implemented basic authorization for Livy connections using <code>livy_config_auth()</code>.</p></li>
<li><p>Added support to specify additional <code>spark-submit</code> parameters using the <code>sparklyr.shell.args</code> environment variable.</p></li>
<li><p>Renamed <code>sdf_load()</code> and <code>sdf_save()</code> to <code>spark_read()</code> and <code>spark_write()</code> for consistency.</p></li>
<li><p>The functions <code>tbl_cache()</code> and <code>tbl_uncache()</code> can now be using without requiring the <code>dplyr</code> namespace to be loaded.</p></li>
<li><p><code>spark_read_csv(..., columns = &lt;...&gt;, header = FALSE)</code> should now work as expected – previously, <code>sparklyr</code> would still attempt to normalize the column names provided.</p></li>
<li><p>Support to configure Livy using the <code>livy.</code> prefix in the <code>config.yml</code> file.</p></li>
<li><p>Implemented experimental support for Livy through: <code>livy_install()</code>, <code>livy_service_start()</code>, <code>livy_service_stop()</code> and <code>spark_connect(method = "livy")</code>.</p></li>
<li><p>The <code>ml</code> routines now accept <code>data</code> as an optional argument, to support calls of the form e.g.&nbsp;<code>ml_linear_regression(y ~ x, data = data)</code>. This should be especially helpful in conjunction with <code>dplyr::do()</code>.</p></li>
<li><p>Spark <code>DenseVector</code> and <code>SparseVector</code> objects are now deserialized as R numeric vectors, rather than Spark objects. This should make it easier to work with the output produced by <code>sdf_predict()</code> with Random Forest models, for example.</p></li>
<li><p>Implemented <code>dim.tbl_spark()</code>. This should ensure that <code>dim()</code>, <code>nrow()</code> and <code>ncol()</code> all produce the expected result with <code>tbl_spark</code>s.</p></li>
<li><p>Improved Spark 2.0 installation in Windows by creating <code>spark-defaults.conf</code> and configuring <code>spark.sql.warehouse.dir</code>.</p></li>
<li><p>Embedded Apache Spark package dependencies to avoid requiring internet connectivity while connecting for the first through <code>spark_connect</code>. The <code>sparklyr.csv.embedded</code> config setting was added to configure a regular expression to match Spark versions where the embedded package is deployed.</p></li>
<li><p>Increased exception callstack and message length to include full error details when an exception is thrown in Spark.</p></li>
<li><p>Improved validation of supported Java versions.</p></li>
<li><p>The <code>spark_read_csv()</code> function now accepts the <code>infer_schema</code> parameter, controlling whether the columns schema should be inferred from the underlying file itself. Disabling this should improve performance when the schema is known beforehand.</p></li>
<li><p>Added a <code>do_.tbl_spark</code> implementation, allowing for the execution of <code>dplyr::do</code> statements on Spark DataFrames. Currently, the computation is performed in serial across the different groups specified on the Spark DataFrame; in the future we hope to explore a parallel implementation. Note that <code>do_</code> always returns a <code>tbl_df</code> rather than a <code>tbl_spark</code>, as the objects produced within a <code>do_</code> query may not necessarily be Spark objects.</p></li>
<li><p>Improved errors, warnings and fallbacks for unsupported Spark versions.</p></li>
<li><p><code>sparklyr</code> now defaults to <code>tar = "internal"</code> in its calls to <code>untar()</code>. This should help resolve issues some Windows users have seen related to an inability to connect to Spark, which ultimately were caused by a lack of permissions on the Spark installation.</p></li>
<li><p>Resolved an issue where <code>copy_to()</code> and other R =&gt; Spark data transfer functions could fail when the last column contained missing / empty values. (#265)</p></li>
<li><p>Added <code>sdf_persist()</code> as a wrapper to the Spark DataFrame <code>persist()</code> API.</p></li>
<li><p>Resolved an issue where <code>predict()</code> could produce results in the wrong order for large Spark DataFrames.</p></li>
<li><p>Implemented support for <code>na.action</code> with the various Spark ML routines. The value of <code>getOption("na.action")</code> is used by default. Users can customize the <code>na.action</code> argument through the <code>ml.options</code> object accepted by all ML routines.</p></li>
<li><p>On Windows, long paths, and paths containing spaces, are now supported within calls to <code>spark_connect()</code>.</p></li>
<li><p>The <code>lag()</code> window function now accepts numeric values for <code>n</code>. Previously, only integer values were accepted. (#249)</p></li>
<li><p>Added support to configure Ppark environment variables using <code>spark.env.*</code> config.</p></li>
<li><p>Added support for the <code>Tokenizer</code> and <code>RegexTokenizer</code> feature transformers. These are exported as the <code>ft_tokenizer()</code> and <code>ft_regex_tokenizer()</code> functions.</p></li>
<li><p>Resolved an issue where attempting to call <code>copy_to()</code> with an R <code>data.frame</code> containing many columns could fail with a Java StackOverflow. (#244)</p></li>
<li><p>Resolved an issue where attempting to call <code>collect()</code> on a Spark DataFrame containing many columns could produce the wrong result. (#242)</p></li>
<li><p>Added support to parameterize network timeouts using the <code>sparklyr.backend.timeout</code>, <code>sparklyr.gateway.start.timeout</code> and <code>sparklyr.gateway.connect.timeout</code> config settings.</p></li>
<li><p>Improved logging while establishing connections to <code>sparklyr</code>.</p></li>
<li><p>Added <code>sparklyr.gateway.port</code> and <code>sparklyr.gateway.address</code> as config settings.</p></li>
<li><p>The <code>spark_log()</code> function now accepts the <code>filter</code> parameter. This can be used to filter entries within the Spark log.</p></li>
<li><p>Increased network timeout for <code>sparklyr.backend.timeout</code>.</p></li>
<li><p>Moved <code>spark.jars.default</code> setting from options to Spark config.</p></li>
<li><p><code>sparklyr</code> now properly respects the Hive metastore directory with the <code>sdf_save_table()</code> and <code>sdf_load_table()</code> APIs for Spark &lt; 2.0.0.</p></li>
<li><p>Added <code>sdf_quantile()</code> as a means of computing (approximate) quantiles for a column of a Spark DataFrame.</p></li>
<li><p>Added support for <code>n_distinct(...)</code> within the <code>dplyr</code> interface, based on call to Hive function <code>count(DISTINCT ...)</code>. (#220)</p></li>
</ul>
</section>
<section id="sparklyr-0.4.0" class="level1">
<h1>Sparklyr 0.4.0</h1>
<ul>
<li>First release to CRAN.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/spark\.rstudio\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../site-news.html" class="pagination-link" aria-label="Site News">
        <span class="nav-page-text">Site News</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>