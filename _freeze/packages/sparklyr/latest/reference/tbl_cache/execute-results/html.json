{
  "hash": "8c3faf326768f8b77a4a7cf944212acc",
  "result": {
    "markdown": "---\ntitle: \"Cache a Spark Table\"\nexecute:\n  freeze: true\n---\n\n\n\n\n*R/tables_spark.R*\n\n## tbl_cache\n\n## Description\nForce a Spark table with name `name` to be loaded into memory. Operations on cached tables should normally (although not always) be more performant than the same operation performed on an uncached table. \n\n\n## Usage\n```r\ntbl_cache(sc, name, force = TRUE) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| sc | A `spark_connection`. |\n| name | The table name. |\n| force | Force the data to be loaded into memory? This is accomplished by calling the `count` API on the associated Spark DataFrame. |\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}