[
  {
    "href": "post/2018-04-09-graphframes.html",
    "title": "Graph Analysis using the graphframes package",
    "section": "",
    "text": "We are very excited to announce that the graphframes package is now on CRAN!\n\nWhat does graphframes do?\n\nIt adds support for GraphFrames which aims to provide the functionality of GraphX.\nPerform graph algorithms like: PageRank, ShortestPaths and many others.\nDesigned to work with sparklyr and the sparklyr extensions.\n\n\n\nNew article\nA new article has been added to the site that walk through how to get started with Graph Analysis inside Spark: R interface for GraphFrames"
  },
  {
    "href": "post/2018-04-03-introduction-to-ml-pipelines.html",
    "title": "New ML Pipelines Article",
    "section": "",
    "text": "A new article is now available to provide further background and guidance on how to use and implement ML Pipelines via sparklyr: Spark ML Pipelines"
  },
  {
    "href": "post/2019-10-02-sparklyr-0-9.html",
    "title": "sparklyr 0.9",
    "section": "",
    "text": "Today we are excited to share that a new release of sparklyr is available on CRAN! This 0.9 release enables you to:\n\nCreate Spark structured streams to process real time data from many data sources using dplyr, SQL, pipelines, and arbitrary R code.\nMonitor connection progress with upcoming RStudio Preview 1.2 features and support for properly interrupting Spark jobs from R.\nUse Kubernetes clusters with sparklyr to simplify deployment and maintenance.\n\nThe full blog post is available in the RStudio Blog site: https://blog.rstudio.com/2018/10/01/sparklyr-0-9/"
  },
  {
    "href": "post/2018-03-15-data-day-mexico-city.html",
    "title": "Data Day Mexico City",
    "section": "",
    "text": "We had lots of fun attending Data Day Mexico 2018, and presenting a workshop in Spanish, which focused on basic sparklyr operations. The full deck and code are available in this GitHub repo: https://github.com/edgararuiz/datadaymx\nArturo Cardenas was kind enough to tweet during the session too!\n\n\nUn poco de sparklyr con @theotheredgar en el #datadaymx pic.twitter.com/EytW79Lj4r\n\n— Arturo Cárdenas (@arturocm) March 15, 2018"
  },
  {
    "href": "post/2018-04-05-sparklyr-0-7.html",
    "title": "sparklyr 0.7",
    "section": "",
    "text": "We are excited to share that sparklyr 0.7 is now available on CRAN! Sparklyr provides an R interface to Apache Spark. It supports dplyr syntax for working with Spark DataFrames and exposes the full range of machine learning algorithms available in Spark. Features in this release:\n\nAdds support for ML Pipelines which provide a uniform set of high-level APIs to help create, tune, and deploy machine learning pipelines at scale.\nEnhances Machine Learning capabilities by supporting the full range of ML algorithms and feature transformers.\nImproves Data Serialization, specifically by adding support for date columns.\nAdds support for YARN cluster mode connections.\n\nThe full blog post is available in the RStudio Blog site: https://blog.rstudio.com/2018/01/29/sparklyr-0-7/"
  },
  {
    "href": "troubleshooting.html#help-with-code-debugging",
    "title": "sparklyr",
    "section": "Help with code debugging",
    "text": "For general programming questions with sparklyr, please ask on Stack Overflow."
  },
  {
    "href": "troubleshooting.html#code-does-not-work-after-upgrading-to-the-latest-sparklyr-version",
    "title": "sparklyr",
    "section": "Code does not work after upgrading to the latest sparklyr version",
    "text": "Please refer to the NEWS section of the sparklyr package to find out if any of the updates listed may have changed the way your code needs to work.\nIf it seems that current version of the package has a bug, or the new functionality does not perform as stated, please refer to the sparklyr ISSUES page. If no existing issue matches to what your problem is, please open a new issue."
  },
  {
    "href": "troubleshooting.html#not-able-to-connect-or-the-jobs-take-a-long-time-when-working-with-a-data-lake",
    "title": "sparklyr",
    "section": "Not able to connect, or the jobs take a long time when working with a Data Lake",
    "text": "The Configuration connections contains an overview and recommendations for requesting resources form the cluster.\nThe articles in the Guides section provide best-practice information about specific operations that may match to the intent of your code.\nTo verify your infrastructure, please review the Deployment Examples section."
  },
  {
    "href": "news.html#distributed-r-7",
    "title": "sparklyr",
    "section": "Distributed R",
    "text": "The memory parameter in spark_apply() now defaults to FALSE when the name parameter is not specified."
  },
  {
    "href": "news.html#other",
    "title": "sparklyr",
    "section": "Other",
    "text": "Removed dreprecated sdf_mutate().\nRemove exported ensure_ functions which were deprecated.\nFixed missing Hive tables not rendering under some Spark distributions (#1823).\nRemove dependency on broom.\nFixed re-entrancy job progress issues when running RStudio 1.2.\nTables with periods supported by setting sparklyr.dplyr.period.splits to FALSE.\nsdf_len(), sdf_along() and sdf_seq() default to 32 bit integers but allow support for 64 bits through bits parameter.\nSupport for detecting Spark version using spark-submit."
  },
  {
    "href": "news.html#batches-1",
    "title": "sparklyr",
    "section": "Batches",
    "text": "Added support for spark_submit() to assist submitting non-interactive Spark jobs.\n\n\nSpark ML\n\n(Breaking change) The formula API for ML classification algorithms no longer indexes numeric labels, to avoid the confusion of 0 being mapped to \"1\" and vice versa. This means that if the largest numeric label is N, Spark will fit a N+1-class classification model, regardless of how many distinct labels there are in the provided training set (#1591).\nFix retrieval of coefficients in ml_logistic_regression() (@shabbybanks, #1596).\n(Breaking change) For model objects, lazy val and def attributes have been converted to closures, so they are not evaluated at object instantiation (#1453).\nInput and output column names are no longer required to construct pipeline objects to be consistent with Spark (#1513).\nVector attributes of pipeline stages are now printed correctly (#1618).\nDeprecate various aliases favoring method names in Spark.\n\nml_binary_classification_eval()\nml_classification_eval()\nml_multilayer_perceptron()\nml_survival_regression()\nml_als_factorization()\n\nDeprecate incompatible signatures for sdf_transform() and ml_transform() families of methods; the former should take a tbl_spark as the first argument while the latter should take a model object as the first argument.\nInput and output column names are no longer required to construct pipeline objects to be consistent with Spark (#1513).\n\n\n\nData\n\nImplemented support for DBI::db_explain() (#1623).\nFixed for timestamp fields when using copy_to() (#1312, @yutannihilation).\nAdded support to read and write ORC files using spark_read_orc() and spark_write_orc() (#1548).\n\n\n\nLivy\n\nFixed must share the same src error for sdf_broadcast() and other functions when using Livy connections.\nAdded support for logging sparklyr server events and logging sparklyr invokes as comments in the Livy UI.\nAdded support to open the Livy UI from the connections viewer while using RStudio.\nImprove performance in Livy for long execution queries, fixed livy.session.command.timeout and support for livy.session.command.interval to control max polling while waiting for command response (#1538).\nFixed Livy version with MapR distributions.\nRemoved install column from livy_available_versions().\n\n\n\nDistributed R\n\nAdded name parameter to spark_apply() to optionally name resulting table.\nFix to spark_apply() to retain column types when NAs are present (#1665).\nspark_apply() now supports rlang anonymous functions. For example, sdf_len(sc, 3) %>% spark_apply(~.x+1).\nBreaking Change: spark_apply() no longer defaults to the input column names when the columns parameter is nos specified.\nSupport for reading column names from the R data frame returned by spark_apply().\nFix to support retrieving empty data frames in grouped spark_apply() operations (#1505).\nAdded support for sparklyr.apply.packages to configure default behavior for spark_apply() parameters (#1530).\nAdded support for spark.r.libpaths to configure package library in spark_apply() (#1530).\n\n\n\nConnections\n\nDefault to Spark 2.3.1 for installation and local connections (#1680).\nml_load() no longer keeps extraneous table views which was cluttering up the RStudio Connections pane (@randomgambit, #1549).\nAvoid preparing windows environment in non-local connections.\n\n\n\nExtensions\n\nThe ensure_* family of functions is deprecated in favor of forge which doesn’t use NSE and provides more informative errors messages for debugging (#1514).\nSupport for sparklyr.invoke.trace and sparklyr.invoke.trace.callstack configuration options to trace all invoke() calls.\nSupport to invoke methods with char types using single character strings (@lawremi, #1395).\n\n\n\nSerialization\n\nFixed collection of Date types to support correct local JVM timezone to UTC ().\n\n\n\nDocumentation\n\nMany new examples for ft_binarizer(), ft_bucketizer(), ft_min_max_scaler, ft_max_abs_scaler(), ft_standard_scaler(), ml_kmeans(), ml_pca(), ml_bisecting_kmeans(), ml_gaussian_mixture(), ml_naive_bayes(), ml_decision_tree(), ml_random_forest(), ml_multilayer_perceptron_classifier(), ml_linear_regression(), ml_logistic_regression(), ml_gradient_boosted_trees(), ml_generalized_linear_regression(), ml_cross_validator(), ml_evaluator(), ml_clustering_evaluator(), ml_corr(), ml_chisquare_test() and sdf_pivot() (@samuelmacedo83).\n\n\n\nBroom\n\nImplemented tidy(), augment(), and glance() for ml_aft_survival_regression(), ml_isotonic_regression(), ml_naive_bayes(), ml_logistic_regression(), ml_decision_tree(), ml_random_forest(), ml_gradient_boosted_trees(), ml_bisecting_kmeans(), ml_kmeans()and ml_gaussian_mixture() models (@samuelmacedo83)\n\n\n\nConfiguration\n\nDeprecated configuration option sparklyr.dplyr.compute.nocache.\nAdded spark_config_settings() to list all sparklyr configuration settings and describe them, cleaned all settings and grouped by area while maintaining support for previous settings.\nStatic SQL configuration properties are now respected for Spark 2.3, and spark.sql.catalogImplementation defaults to hive to maintain Hive support (#1496, #415).\nspark_config() values can now also be specified as options().\nSupport for functions as values in entries to spark_config() to enable advanced configuration workflows."
  },
  {
    "href": "dplyr.html#overview",
    "title": "sparklyr",
    "section": "Overview",
    "text": "dplyr is an R package for working with structured data both in and outside of R. dplyr makes data manipulation for R users easy, consistent, and performant. With dplyr as an interface to manipulating Spark DataFrames, you can:\n\nSelect, filter, and aggregate data\nUse window functions (e.g. for sampling)\nPerform joins on DataFrames\nCollect data from Spark into R\n\nStatements in dplyr can be chained together using pipes defined by the magrittr R package. dplyr also supports non-standard evalution of its arguments. For more information on dplyr, see the introduction, a guide for connecting to databases, and a variety of vignettes."
  },
  {
    "href": "dplyr.html#reading-data",
    "title": "sparklyr",
    "section": "Reading Data",
    "text": "You can read data into Spark DataFrames using the following functions:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nspark_read_csv\nReads a CSV file and provides a data source compatible with dplyr\n\n\nspark_read_json\nReads a JSON file and provides a data source compatible with dplyr\n\n\nspark_read_parquet\nReads a parquet file and provides a data source compatible with dplyr\n\n\n\nRegardless of the format of your data, Spark supports reading data from a variety of different data sources. These include data stored on HDFS (hdfs:// protocol), Amazon S3 (s3n:// protocol), or local files available to the Spark worker nodes (file:// protocol)\nEach of these functions returns a reference to a Spark DataFrame which can be used as a dplyr table (tbl).\n\nFlights Data\nThis guide will demonstrate some of the basic data manipulation verbs of dplyr by using data from the nycflights13 R package. This package contains data for all 336,776 flights departing New York City in 2013. It also includes useful metadata on airlines, airports, weather, and planes. The data comes from the US Bureau of Transportation Statistics, and is documented in ?nycflights13\nConnect to the cluster and copy the flights data using the copy_to function. Caveat: The flight data in nycflights13 is convenient for dplyr demonstrations because it is small, but in practice large data should rarely be copied directly from R objects.\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(nycflights13)\nlibrary(ggplot2)\n\nsc <- spark_connect(master=\"local\")\nflights <- copy_to(sc, flights, \"flights\")\nairlines <- copy_to(sc, airlines, \"airlines\")\ndplyr::src_tbls(sc)\n## [1] \"airlines\" \"flights\""
  },
  {
    "href": "dplyr.html#dplyr-verbs",
    "title": "sparklyr",
    "section": "dplyr Verbs",
    "text": "Verbs are dplyr commands for manipulating data. When connected to a Spark DataFrame, dplyr translates the commands into Spark SQL statements. Remote data sources use exactly the same five verbs as local data sources. Here are the five verbs with their corresponding SQL commands:\n\nselect ~ SELECT\nfilter ~ WHERE\narrange ~ ORDER\nsummarise ~ aggregators: sum, min, sd, etc.\nmutate ~ operators: +, *, log, etc.\n\n\nselect(flights, year:day, arr_delay, dep_delay)\n## # Source: lazy query [?? x 5]\n## # Database: spark_connection\n##     year month   day arr_delay dep_delay\n##    <int> <int> <int>     <dbl>     <dbl>\n##  1  2013     1     1     11.0       2.00\n##  2  2013     1     1     20.0       4.00\n##  3  2013     1     1     33.0       2.00\n##  4  2013     1     1    -18.0      -1.00\n##  5  2013     1     1    -25.0      -6.00\n##  6  2013     1     1     12.0      -4.00\n##  7  2013     1     1     19.0      -5.00\n##  8  2013     1     1    -14.0      -3.00\n##  9  2013     1     1    - 8.00     -3.00\n## 10  2013     1     1      8.00     -2.00\n## # ... with more rows\nfilter(flights, dep_delay > 1000)\n## # Source: lazy query [?? x 19]\n## # Database: spark_connection\n##    year month   day dep_t~ sche~ dep_~ arr_~ sche~ arr_~ carr~ flig~ tail~\n##   <int> <int> <int>  <int> <int> <dbl> <int> <int> <dbl> <chr> <int> <chr>\n## 1  2013     1     9    641   900  1301  1242  1530  1272 HA       51 N384~\n## 2  2013     1    10   1121  1635  1126  1239  1810  1109 MQ     3695 N517~\n## 3  2013     6    15   1432  1935  1137  1607  2120  1127 MQ     3535 N504~\n## 4  2013     7    22    845  1600  1005  1044  1815   989 MQ     3075 N665~\n## 5  2013     9    20   1139  1845  1014  1457  2210  1007 AA      177 N338~\n## # ... with 7 more variables: origin <chr>, dest <chr>, air_time <dbl>,\n## #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dbl>\narrange(flights, desc(dep_delay))\n## # Source: table<flights> [?? x 19]\n## # Database: spark_connection\n## # Ordered by: desc(dep_delay)\n##     year month   day dep_~ sche~ dep_~ arr_~ sche~ arr_~ carr~ flig~ tail~\n##    <int> <int> <int> <int> <int> <dbl> <int> <int> <dbl> <chr> <int> <chr>\n##  1  2013     1     9   641   900  1301  1242  1530  1272 HA       51 N384~\n##  2  2013     6    15  1432  1935  1137  1607  2120  1127 MQ     3535 N504~\n##  3  2013     1    10  1121  1635  1126  1239  1810  1109 MQ     3695 N517~\n##  4  2013     9    20  1139  1845  1014  1457  2210  1007 AA      177 N338~\n##  5  2013     7    22   845  1600  1005  1044  1815   989 MQ     3075 N665~\n##  6  2013     4    10  1100  1900   960  1342  2211   931 DL     2391 N959~\n##  7  2013     3    17  2321   810   911   135  1020   915 DL     2119 N927~\n##  8  2013     6    27   959  1900   899  1236  2226   850 DL     2007 N376~\n##  9  2013     7    22  2257   759   898   121  1026   895 DL     2047 N671~\n## 10  2013    12     5   756  1700   896  1058  2020   878 AA      172 N5DM~\n## # ... with more rows, and 7 more variables: origin <chr>, dest <chr>,\n## #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour\n## #   <dbl>\nsummarise(flights, mean_dep_delay = mean(dep_delay))\n## Warning: Missing values are always removed in SQL.\n## Use `AVG(x, na.rm = TRUE)` to silence this warning\n\n## # Source: lazy query [?? x 1]\n## # Database: spark_connection\n##   mean_dep_delay\n##            <dbl>\n## 1           12.6\nmutate(flights, speed = distance / air_time * 60)\n## # Source: lazy query [?? x 20]\n## # Database: spark_connection\n##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~\n##    <int> <int> <int>  <int>   <int>  <dbl> <int>  <int>  <dbl> <chr> <int>\n##  1  2013     1     1    517     515   2.00   830    819  11.0  UA     1545\n##  2  2013     1     1    533     529   4.00   850    830  20.0  UA     1714\n##  3  2013     1     1    542     540   2.00   923    850  33.0  AA     1141\n##  4  2013     1     1    544     545  -1.00  1004   1022 -18.0  B6      725\n##  5  2013     1     1    554     600  -6.00   812    837 -25.0  DL      461\n##  6  2013     1     1    554     558  -4.00   740    728  12.0  UA     1696\n##  7  2013     1     1    555     600  -5.00   913    854  19.0  B6      507\n##  8  2013     1     1    557     600  -3.00   709    723 -14.0  EV     5708\n##  9  2013     1     1    557     600  -3.00   838    846 - 8.00 B6       79\n## 10  2013     1     1    558     600  -2.00   753    745   8.00 AA      301\n## # ... with more rows, and 9 more variables: tailnum <chr>, origin <chr>,\n## #   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n## #   time_hour <dbl>, speed <dbl>"
  },
  {
    "href": "dplyr.html#laziness",
    "title": "sparklyr",
    "section": "Laziness",
    "text": "When working with databases, dplyr tries to be as lazy as possible:\n\nIt never pulls data into R unless you explicitly ask for it.\nIt delays doing any work until the last possible moment: it collects together everything you want to do and then sends it to the database in one step.\n\nFor example, take the following code:\nc1 <- filter(flights, day == 17, month == 5, carrier %in% c('UA', 'WN', 'AA', 'DL'))\nc2 <- select(c1, year, month, day, carrier, dep_delay, air_time, distance)\nc3 <- arrange(c2, year, month, day, carrier)\nc4 <- mutate(c3, air_time_hours = air_time / 60)\nThis sequence of operations never actually touches the database. It’s not until you ask for the data (e.g. by printing c4) that dplyr requests the results from the database.\nc4\n## # Source: lazy query [?? x 8]\n## # Database: spark_connection\n## # Ordered by: year, month, day, carrier\n##     year month   day carrier dep_delay air_time distance air_time_hours\n##    <int> <int> <int> <chr>       <dbl>    <dbl>    <dbl>          <dbl>\n##  1  2013     5    17 AA          -2.00      294     2248           4.90\n##  2  2013     5    17 AA          -1.00      146     1096           2.43\n##  3  2013     5    17 AA          -2.00      185     1372           3.08\n##  4  2013     5    17 AA          -9.00      186     1389           3.10\n##  5  2013     5    17 AA           2.00      147     1096           2.45\n##  6  2013     5    17 AA          -4.00      114      733           1.90\n##  7  2013     5    17 AA          -7.00      117      733           1.95\n##  8  2013     5    17 AA          -7.00      142     1089           2.37\n##  9  2013     5    17 AA          -6.00      148     1089           2.47\n## 10  2013     5    17 AA          -7.00      137      944           2.28\n## # ... with more rows"
  },
  {
    "href": "dplyr.html#piping",
    "title": "sparklyr",
    "section": "Piping",
    "text": "You can use magrittr pipes to write cleaner syntax. Using the same example from above, you can write a much cleaner version like this:\nc4 <- flights %>%\n  filter(month == 5, day == 17, carrier %in% c('UA', 'WN', 'AA', 'DL')) %>%\n  select(carrier, dep_delay, air_time, distance) %>%\n  arrange(carrier) %>%\n  mutate(air_time_hours = air_time / 60)"
  },
  {
    "href": "dplyr.html#grouping",
    "title": "sparklyr",
    "section": "Grouping",
    "text": "The group_by function corresponds to the GROUP BY statement in SQL.\nc4 %>%\n  group_by(carrier) %>%\n  summarize(count = n(), mean_dep_delay = mean(dep_delay))\n## Warning: Missing values are always removed in SQL.\n## Use `AVG(x, na.rm = TRUE)` to silence this warning\n\n## # Source: lazy query [?? x 3]\n## # Database: spark_connection\n##   carrier count mean_dep_delay\n##   <chr>   <dbl>          <dbl>\n## 1 AA       94.0           1.47\n## 2 DL      136             6.24\n## 3 UA      172             9.63\n## 4 WN       34.0           7.97"
  },
  {
    "href": "dplyr.html#collecting-to-r",
    "title": "sparklyr",
    "section": "Collecting to R",
    "text": "You can copy data from Spark into R’s memory by using collect().\ncarrierhours <- collect(c4)\ncollect() executes the Spark query and returns the results to R for further analysis and visualization.\n# Test the significance of pairwise differences and plot the results\nwith(carrierhours, pairwise.t.test(air_time, carrier))\n## \n##  Pairwise comparisons using t tests with pooled SD \n## \n## data:  air_time and carrier \n## \n##    AA      DL      UA     \n## DL 0.25057 -       -      \n## UA 0.07957 0.00044 -      \n## WN 0.07957 0.23488 0.00041\n## \n## P value adjustment method: holm\nggplot(carrierhours, aes(carrier, air_time_hours)) + geom_boxplot()"
  },
  {
    "href": "dplyr.html#sql-translation",
    "title": "sparklyr",
    "section": "SQL Translation",
    "text": "It’s relatively straightforward to translate R code to SQL (or indeed to any programming language) when doing simple mathematical operations of the form you normally use when filtering, mutating and summarizing. dplyr knows how to convert the following R functions to Spark SQL:\n# Basic math operators\n+, -, *, /, %%, ^\n  \n# Math functions\nabs, acos, asin, asinh, atan, atan2, ceiling, cos, cosh, exp, floor, log, log10, round, sign, sin, sinh, sqrt, tan, tanh\n\n# Logical comparisons\n<, <=, !=, >=, >, ==, %in%\n\n# Boolean operations\n&, &&, |, ||, !\n\n# Character functions\npaste, tolower, toupper, nchar\n\n# Casting\nas.double, as.integer, as.logical, as.character, as.date\n\n# Basic aggregations\nmean, sum, min, max, sd, var, cor, cov, n"
  },
  {
    "href": "dplyr.html#window-functions",
    "title": "sparklyr",
    "section": "Window Functions",
    "text": "dplyr supports Spark SQL window functions. Window functions are used in conjunction with mutate and filter to solve a wide range of problems. You can compare the dplyr syntax to the query it has generated by using dbplyr::sql_render().\n# Find the most and least delayed flight each day\nbestworst <- flights %>%\n  group_by(year, month, day) %>%\n  select(dep_delay) %>% \n  filter(dep_delay == min(dep_delay) || dep_delay == max(dep_delay))\ndbplyr::sql_render(bestworst)\n## Warning: Missing values are always removed in SQL.\n## Use `min(x, na.rm = TRUE)` to silence this warning\n## Warning: Missing values are always removed in SQL.\n## Use `max(x, na.rm = TRUE)` to silence this warning\n## <SQL> SELECT `year`, `month`, `day`, `dep_delay`\n## FROM (SELECT `year`, `month`, `day`, `dep_delay`, min(`dep_delay`) OVER (PARTITION BY `year`, `month`, `day`) AS `zzz3`, max(`dep_delay`) OVER (PARTITION BY `year`, `month`, `day`) AS `zzz4`\n## FROM (SELECT `year`, `month`, `day`, `dep_delay`\n## FROM `flights`) `coaxmtqqbj`) `efznnpuovy`\n## WHERE (`dep_delay` = `zzz3` OR `dep_delay` = `zzz4`)\nbestworst\n## Warning: Missing values are always removed in SQL.\n## Use `min(x, na.rm = TRUE)` to silence this warning\n\n## Warning: Missing values are always removed in SQL.\n## Use `max(x, na.rm = TRUE)` to silence this warning\n## # Source: lazy query [?? x 4]\n## # Database: spark_connection\n## # Groups: year, month, day\n##     year month   day dep_delay\n##    <int> <int> <int>     <dbl>\n##  1  2013     1     1     853  \n##  2  2013     1     1   -  15.0\n##  3  2013     1     1   -  15.0\n##  4  2013     1     9    1301  \n##  5  2013     1     9   -  17.0\n##  6  2013     1    24   -  15.0\n##  7  2013     1    24     329  \n##  8  2013     1    29   -  27.0\n##  9  2013     1    29     235  \n## 10  2013     2     1   -  15.0\n## # ... with more rows\n# Rank each flight within a daily\nranked <- flights %>%\n  group_by(year, month, day) %>%\n  select(dep_delay) %>% \n  mutate(rank = rank(desc(dep_delay)))\ndbplyr::sql_render(ranked)\n## <SQL> SELECT `year`, `month`, `day`, `dep_delay`, rank() OVER (PARTITION BY `year`, `month`, `day` ORDER BY `dep_delay` DESC) AS `rank`\n## FROM (SELECT `year`, `month`, `day`, `dep_delay`\n## FROM `flights`) `mauqwkxuam`\nranked\n## # Source: lazy query [?? x 5]\n## # Database: spark_connection\n## # Groups: year, month, day\n##     year month   day dep_delay  rank\n##    <int> <int> <int>     <dbl> <int>\n##  1  2013     1     1       853     1\n##  2  2013     1     1       379     2\n##  3  2013     1     1       290     3\n##  4  2013     1     1       285     4\n##  5  2013     1     1       260     5\n##  6  2013     1     1       255     6\n##  7  2013     1     1       216     7\n##  8  2013     1     1       192     8\n##  9  2013     1     1       157     9\n## 10  2013     1     1       155    10\n## # ... with more rows"
  },
  {
    "href": "dplyr.html#peforming-joins",
    "title": "sparklyr",
    "section": "Peforming Joins",
    "text": "It’s rare that a data analysis involves only a single table of data. In practice, you’ll normally have many tables that contribute to an analysis, and you need flexible tools to combine them. In dplyr, there are three families of verbs that work with two tables at a time:\n\nMutating joins, which add new variables to one table from matching rows in another.\nFiltering joins, which filter observations from one table based on whether or not they match an observation in the other table.\nSet operations, which combine the observations in the data sets as if they were set elements.\n\nAll two-table verbs work similarly. The first two arguments are x and y, and provide the tables to combine. The output is always a new table with the same type as x.\nThe following statements are equivalent:\nflights %>% left_join(airlines)\n## Joining, by = \"carrier\"\n\n## # Source: lazy query [?? x 20]\n## # Database: spark_connection\n##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~\n##    <int> <int> <int>  <int>   <int>  <dbl> <int>  <int>  <dbl> <chr> <int>\n##  1  2013     1     1    517     515   2.00   830    819  11.0  UA     1545\n##  2  2013     1     1    533     529   4.00   850    830  20.0  UA     1714\n##  3  2013     1     1    542     540   2.00   923    850  33.0  AA     1141\n##  4  2013     1     1    544     545  -1.00  1004   1022 -18.0  B6      725\n##  5  2013     1     1    554     600  -6.00   812    837 -25.0  DL      461\n##  6  2013     1     1    554     558  -4.00   740    728  12.0  UA     1696\n##  7  2013     1     1    555     600  -5.00   913    854  19.0  B6      507\n##  8  2013     1     1    557     600  -3.00   709    723 -14.0  EV     5708\n##  9  2013     1     1    557     600  -3.00   838    846 - 8.00 B6       79\n## 10  2013     1     1    558     600  -2.00   753    745   8.00 AA      301\n## # ... with more rows, and 9 more variables: tailnum <chr>, origin <chr>,\n## #   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n## #   time_hour <dbl>, name <chr>\nflights %>% left_join(airlines, by = \"carrier\")\n## # Source: lazy query [?? x 20]\n## # Database: spark_connection\n##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~\n##    <int> <int> <int>  <int>   <int>  <dbl> <int>  <int>  <dbl> <chr> <int>\n##  1  2013     1     1    517     515   2.00   830    819  11.0  UA     1545\n##  2  2013     1     1    533     529   4.00   850    830  20.0  UA     1714\n##  3  2013     1     1    542     540   2.00   923    850  33.0  AA     1141\n##  4  2013     1     1    544     545  -1.00  1004   1022 -18.0  B6      725\n##  5  2013     1     1    554     600  -6.00   812    837 -25.0  DL      461\n##  6  2013     1     1    554     558  -4.00   740    728  12.0  UA     1696\n##  7  2013     1     1    555     600  -5.00   913    854  19.0  B6      507\n##  8  2013     1     1    557     600  -3.00   709    723 -14.0  EV     5708\n##  9  2013     1     1    557     600  -3.00   838    846 - 8.00 B6       79\n## 10  2013     1     1    558     600  -2.00   753    745   8.00 AA      301\n## # ... with more rows, and 9 more variables: tailnum <chr>, origin <chr>,\n## #   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n## #   time_hour <dbl>, name <chr>\nflights %>% left_join(airlines, by = c(\"carrier\", \"carrier\"))\n## # Source: lazy query [?? x 20]\n## # Database: spark_connection\n##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~\n##    <int> <int> <int>  <int>   <int>  <dbl> <int>  <int>  <dbl> <chr> <int>\n##  1  2013     1     1    517     515   2.00   830    819  11.0  UA     1545\n##  2  2013     1     1    533     529   4.00   850    830  20.0  UA     1714\n##  3  2013     1     1    542     540   2.00   923    850  33.0  AA     1141\n##  4  2013     1     1    544     545  -1.00  1004   1022 -18.0  B6      725\n##  5  2013     1     1    554     600  -6.00   812    837 -25.0  DL      461\n##  6  2013     1     1    554     558  -4.00   740    728  12.0  UA     1696\n##  7  2013     1     1    555     600  -5.00   913    854  19.0  B6      507\n##  8  2013     1     1    557     600  -3.00   709    723 -14.0  EV     5708\n##  9  2013     1     1    557     600  -3.00   838    846 - 8.00 B6       79\n## 10  2013     1     1    558     600  -2.00   753    745   8.00 AA      301\n## # ... with more rows, and 9 more variables: tailnum <chr>, origin <chr>,\n## #   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n## #   time_hour <dbl>, name <chr>"
  },
  {
    "href": "dplyr.html#sampling",
    "title": "sparklyr",
    "section": "Sampling",
    "text": "You can use sample_n() and sample_frac() to take a random sample of rows: use sample_n() for a fixed number and sample_frac() for a fixed fraction.\nsample_n(flights, 10)\n## # Source: lazy query [?? x 19]\n## # Database: spark_connection\n##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~\n##    <int> <int> <int>  <int>   <int>  <dbl> <int>  <int>  <dbl> <chr> <int>\n##  1  2013     1     1    517     515   2.00   830    819  11.0  UA     1545\n##  2  2013     1     1    533     529   4.00   850    830  20.0  UA     1714\n##  3  2013     1     1    542     540   2.00   923    850  33.0  AA     1141\n##  4  2013     1     1    544     545  -1.00  1004   1022 -18.0  B6      725\n##  5  2013     1     1    554     600  -6.00   812    837 -25.0  DL      461\n##  6  2013     1     1    554     558  -4.00   740    728  12.0  UA     1696\n##  7  2013     1     1    555     600  -5.00   913    854  19.0  B6      507\n##  8  2013     1     1    557     600  -3.00   709    723 -14.0  EV     5708\n##  9  2013     1     1    557     600  -3.00   838    846 - 8.00 B6       79\n## 10  2013     1     1    558     600  -2.00   753    745   8.00 AA      301\n## # ... with more rows, and 8 more variables: tailnum <chr>, origin <chr>,\n## #   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n## #   time_hour <dbl>\nsample_frac(flights, 0.01)\n## # Source: lazy query [?? x 19]\n## # Database: spark_connection\n##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~\n##    <int> <int> <int>  <int>   <int>  <dbl> <int>  <int>  <dbl> <chr> <int>\n##  1  2013     1     1    655     655   0     1021   1030 - 9.00 DL     1415\n##  2  2013     1     1    656     700 - 4.00   854    850   4.00 AA      305\n##  3  2013     1     1   1044    1045 - 1.00  1231   1212  19.0  EV     4322\n##  4  2013     1     1   1056    1059 - 3.00  1203   1209 - 6.00 EV     4479\n##  5  2013     1     1   1317    1325 - 8.00  1454   1505 -11.0  MQ     4475\n##  6  2013     1     1   1708    1700   8.00  2037   2005  32.0  WN     1066\n##  7  2013     1     1   1825    1829 - 4.00  2056   2053   3.00 9E     3286\n##  8  2013     1     1   1843    1845 - 2.00  1955   2024 -29.0  DL      904\n##  9  2013     1     1   2108    2057  11.0     25     39 -14.0  UA     1517\n## 10  2013     1     2    557     605 - 8.00   832    823   9.00 DL      544\n## # ... with more rows, and 8 more variables: tailnum <chr>, origin <chr>,\n## #   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n## #   time_hour <dbl>"
  },
  {
    "href": "dplyr.html#writing-data",
    "title": "sparklyr",
    "section": "Writing Data",
    "text": "It is often useful to save the results of your analysis or the tables that you have generated on your Spark cluster into persistent storage. The best option in many scenarios is to write the table out to a Parquet file using the spark_write_parquet function. For example:\nspark_write_parquet(tbl, \"hdfs://hdfs.company.org:9000/hdfs-path/data\")\nThis will write the Spark DataFrame referenced by the tbl R variable to the given HDFS path. You can use the spark_read_parquet function to read the same table back into a subsequent Spark session:\ntbl <- spark_read_parquet(sc, \"data\", \"hdfs://hdfs.company.org:9000/hdfs-path/data\")\nYou can also write data as CSV or JSON using the spark_write_csv and spark_write_json functions."
  },
  {
    "href": "dplyr.html#hive-functions",
    "title": "sparklyr",
    "section": "Hive Functions",
    "text": "Many of Hive’s built-in functions (UDF) and built-in aggregate functions (UDAF) can be called inside dplyr’s mutate and summarize. The Languange Reference UDF page provides the list of available functions.\nThe following example uses the datediff and current_date Hive UDFs to figure the difference between the flight_date and the current system date:\nflights %>% \n  mutate(flight_date = paste(year,month,day,sep=\"-\"),\n         days_since = datediff(current_date(), flight_date)) %>%\n  group_by(flight_date,days_since) %>%\n  tally() %>%\n  arrange(-days_since)\n## # Source: lazy query [?? x 3]\n## # Database: spark_connection\n## # Groups: flight_date\n## # Ordered by: -days_since\n##    flight_date days_since     n\n##    <chr>            <int> <dbl>\n##  1 2013-1-1          1844   842\n##  2 2013-1-2          1843   943\n##  3 2013-1-3          1842   914\n##  4 2013-1-4          1841   915\n##  5 2013-1-5          1840   720\n##  6 2013-1-6          1839   832\n##  7 2013-1-7          1838   933\n##  8 2013-1-8          1837   899\n##  9 2013-1-9          1836   902\n## 10 2013-1-10         1835   932\n## # ... with more rows"
  },
  {
    "href": "graphframes.html#highlights",
    "title": "R interface for GraphFrames",
    "section": "Highlights",
    "text": "Support for GraphFrames which aims to provide the functionality of GraphX.\nPerform graph algorithms like: PageRank, ShortestPaths and many others.\nDesigned to work with sparklyr and the sparklyr extensions."
  },
  {
    "href": "graphframes.html#installation",
    "title": "R interface for GraphFrames",
    "section": "Installation",
    "text": "To install from CRAN, run:\ninstall.packages(\"graphframes\")\nFor the development version, run:\ndevtools::install_github(\"rstudio/graphframes\")"
  },
  {
    "href": "graphframes.html#examples",
    "title": "R interface for GraphFrames",
    "section": "Examples",
    "text": "The examples make use of the highschool dataset from the ggplot package.\n\nCreate a GraphFrame\nThe base for graph analyses in Spark, using sparklyr, will be a GraphFrame.\nOpen a new Spark connection using sparklyr, and copy the highschool data set\nlibrary(graphframes)\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\", version = \"2.1.0\")\n\nhighschool_tbl <- copy_to(sc, ggraph::highschool, \"highschool\")\n\nhead(highschool_tbl)\n## # Source: spark<?> [?? x 3]\n##    from    to  year\n##   <dbl> <dbl> <dbl>\n## 1     1    14  1957\n## 2     1    15  1957\n## 3     1    21  1957\n## 4     1    54  1957\n## 5     1    55  1957\n## 6     2    21  1957\nThe vertices table is be constructed using dplyr. The variable name expected by the GraphFrame is id.\nfrom_tbl <- highschool_tbl %>%\n  distinct(from) %>%\n  transmute(id = from)\n\nto_tbl <- highschool_tbl %>%\n  distinct(to) %>%\n  transmute(id = to)\n\n\nvertices_tbl <- from_tbl %>%\n  sdf_bind_rows(to_tbl)\n\nhead(vertices_tbl)\n## # Source: spark<?> [?? x 1]\n##      id\n##   <dbl>\n## 1     1\n## 2     3\n## 3     4\n## 4     6\n## 5     7\n## 6    12\nThe edges table can also be created using dplyr. In order for the GraphFrame to work, the from variable needs be renamed src, and the to variable dst.\n# Create a table with <source, destination> edges\nedges_tbl <- highschool_tbl %>%\n  transmute(src = from, dst = to)\nThe gf_graphframe() function creates a new GraphFrame\ngf_graphframe(vertices_tbl, edges_tbl)\n## GraphFrame\n## Vertices:\n##   Database: spark_connection\n##   $ id <dbl> 1, 3, 4, 6, 7, 12, 13, 14, 16, 17, 19, 20, 22, 27, 31, 32, 33, 35,…\n## Edges:\n##   Database: spark_connection\n##   $ src <dbl> 1, 1, 1, 1, 1, 2, 2, 3, 3, 4, 4, 4, 4, 5, 5, 6, 6, 6, 7, 8, 8, 9,…\n##   $ dst <dbl> 14, 15, 21, 54, 55, 21, 22, 9, 15, 5, 18, 19, 43, 19, 43, 13, 20,…\n\n\nBasic Page Rank\nWe will calculate PageRank over this dataset. The gf_graphframe() command can easily be piped into the gf_pagerank() function to execute the Page Rank.\ngf_graphframe(vertices_tbl, edges_tbl) %>%\n  gf_pagerank(reset_prob = 0.15, max_iter = 10L, source_id = \"1\")\n## GraphFrame\n## Vertices:\n##   Database: spark_connection\n##   $ id       <dbl> 4, 4, 7, 7, 12, 12, 16, 16, 22, 22, 31, 31, 32, 32, 33, 33, …\n##   $ pagerank <dbl> 8.568153e-04, 8.568153e-04, 3.609726e-04, 3.609726e-04, 1.21…\n## Edges:\n##   Database: spark_connection\n##   $ src    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n##   $ dst    <dbl> 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22…\n##   $ weight <dbl> 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125,…\nAdditionaly, one can calculate the degrees of vertices using gf_degrees as follows:\ngf_graphframe(vertices_tbl, edges_tbl) %>%\n  gf_degrees()\n## # Source: spark<?> [?? x 2]\n##       id degree\n##    <dbl>  <int>\n##  1     1     10\n##  2    14      7\n##  3    54     23\n##  4    55     25\n##  5    22     30\n##  6     3      2\n##  7     4     13\n##  8    19     20\n##  9     6     10\n## 10    13     16\n## # … with more rows\n\n\nVisualizations\nIn order to visualize large graphframes, one can use sample_n and then use ggraph with igraph to visualize the graph as follows:\nlibrary(ggraph)\nlibrary(igraph)\n\ngraph <- highschool_tbl %>%\n  sample_n(20) %>%\n  collect() %>%\n  graph_from_data_frame()\n\nggraph(graph, layout = 'kk') +\n    geom_edge_link(aes(colour = factor(year))) +\n    geom_node_point() +\n    ggtitle('An example')\n\n\n\nplot of chunk unnamed-chunk-9"
  },
  {
    "href": "graphframes.html#additional-functions",
    "title": "R interface for GraphFrames",
    "section": "Additional functions",
    "text": "Apart from calculating PageRank using gf_pagerank, the following functions are available:\n\ngf_bfs(): Breadth-first search (BFS).\ngf_connected_components(): Connected components.\ngf_shortest_paths(): Shortest paths algorithm.\ngf_scc(): Strongly connected components.\ngf_triangle_count: Computes the number of triangles passing through each vertex and others."
  },
  {
    "href": "index.html#installation",
    "title": "sparklyr",
    "section": "Installation",
    "text": "You can install the sparklyr package from CRAN as follows:\ninstall.packages(\"sparklyr\")\nYou should also install a local version of Spark for development purposes:\nlibrary(sparklyr)\nspark_install(version = \"2.1.0\")\nTo upgrade to the latest version of sparklyr, run the following command and restart your r session:\ndevtools::install_github(\"rstudio/sparklyr\")\nIf you use the RStudio IDE, you should also download the latest preview release of the IDE which includes several enhancements for interacting with Spark (see the RStudio IDE section below for more details)."
  },
  {
    "href": "index.html#connecting-to-spark",
    "title": "sparklyr",
    "section": "Connecting to Spark",
    "text": "You can connect to both local instances of Spark as well as remote Spark clusters. Here we’ll connect to a local instance of Spark via the spark_connect function:\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nThe returned Spark connection (sc) provides a remote dplyr data source to the Spark cluster.\nFor more information on connecting to remote Spark clusters see the Deployment section of the sparklyr website."
  },
  {
    "href": "index.html#using-dplyr",
    "title": "sparklyr",
    "section": "Using dplyr",
    "text": "We can now use all of the available dplyr verbs against the tables within the cluster.\nWe’ll start by copying some datasets from R into the Spark cluster (note that you may need to install the nycflights13 and Lahman packages in order to execute this code):\ninstall.packages(c(\"nycflights13\", \"Lahman\"))\nlibrary(dplyr)\niris_tbl <- copy_to(sc, iris)\nflights_tbl <- copy_to(sc, nycflights13::flights, \"flights\")\nbatting_tbl <- copy_to(sc, Lahman::Batting, \"batting\")\ndplyr::src_tbls(sc)\n## [1] \"batting\" \"flights\" \"iris\"\nTo start with here’s a simple filtering example:\n# filter by departure delay and print the first few records\nflights_tbl %>% filter(dep_delay == 2)\n## # Source:   lazy query [?? x 19]\n## # Database: spark_connection\n##     year month   day dep_time sched_dep_time dep_delay arr_time\n##    <int> <int> <int>    <int>          <int>     <dbl>    <int>\n##  1  2013     1     1      517            515         2      830\n##  2  2013     1     1      542            540         2      923\n##  3  2013     1     1      702            700         2     1058\n##  4  2013     1     1      715            713         2      911\n##  5  2013     1     1      752            750         2     1025\n##  6  2013     1     1      917            915         2     1206\n##  7  2013     1     1      932            930         2     1219\n##  8  2013     1     1     1028           1026         2     1350\n##  9  2013     1     1     1042           1040         2     1325\n## 10  2013     1     1     1231           1229         2     1523\n## # ... with more rows, and 12 more variables: sched_arr_time <int>,\n## #   arr_delay <dbl>, carrier <chr>, flight <int>, tailnum <chr>,\n## #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n## #   minute <dbl>, time_hour <dbl>\nIntroduction to dplyr provides additional dplyr examples you can try. For example, consider the last example from the tutorial which plots data on flight delays:\ndelay <- flights_tbl %>%\n  group_by(tailnum) %>%\n  summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %>%\n  filter(count > 20, dist < 2000, !is.na(delay)) %>%\n  collect\n\n# plot delays\nlibrary(ggplot2)\nggplot(delay, aes(dist, delay)) +\n  geom_point(aes(size = count), alpha = 1/2) +\n  geom_smooth() +\n  scale_size_area(max_size = 2)\n## `geom_smooth()` using method = 'gam'\n\n\nWindow Functions\ndplyr window functions are also supported, for example:\nbatting_tbl %>%\n  select(playerID, yearID, teamID, G, AB:H) %>%\n  arrange(playerID, yearID, teamID) %>%\n  group_by(playerID) %>%\n  filter(min_rank(desc(H)) <= 2 & H > 0)\n## # Source:     lazy query [?? x 7]\n## # Database:   spark_connection\n## # Groups:     playerID\n## # Ordered by: playerID, yearID, teamID\n##     playerID yearID teamID     G    AB     R     H\n##        <chr>  <int>  <chr> <int> <int> <int> <int>\n##  1 aaronha01   1959    ML1   154   629   116   223\n##  2 aaronha01   1963    ML1   161   631   121   201\n##  3 abbotji01   1999    MIL    20    21     0     2\n##  4 abnersh01   1992    CHA    97   208    21    58\n##  5 abnersh01   1990    SDN    91   184    17    45\n##  6 acklefr01   1963    CHA     2     5     0     1\n##  7 acklefr01   1964    CHA     3     1     0     1\n##  8 adamecr01   2016    COL   121   225    25    49\n##  9 adamecr01   2015    COL    26    53     4    13\n## 10 adamsac01   1943    NY1    70    32     3     4\n## # ... with more rows\nFor additional documentation on using dplyr with Spark see the dplyr section of the sparklyr website."
  },
  {
    "href": "index.html#using-sql",
    "title": "sparklyr",
    "section": "Using SQL",
    "text": "It’s also possible to execute SQL queries directly against tables within a Spark cluster. The spark_connection object implements a DBI interface for Spark, so you can use dbGetQuery to execute SQL and return the result as an R data frame:\nlibrary(DBI)\niris_preview <- dbGetQuery(sc, \"SELECT * FROM iris LIMIT 10\")\niris_preview\n##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa"
  },
  {
    "href": "index.html#machine-learning",
    "title": "sparklyr",
    "section": "Machine Learning",
    "text": "You can orchestrate machine learning algorithms in a Spark cluster via the machine learning functions within sparklyr. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning workflows.\nHere’s an example where we use ml_linear_regression to fit a linear regression model. We’ll use the built-in mtcars dataset, and see if we can predict a car’s fuel consumption (mpg) based on its weight (wt), and the number of cylinders the engine contains (cyl). We’ll assume in each case that the relationship between mpg and each of our features is linear.\n# copy mtcars into spark\nmtcars_tbl <- copy_to(sc, mtcars)\n\n# transform our data set, and then partition into 'training', 'test'\npartitions <- mtcars_tbl %>%\n  filter(hp >= 100) %>%\n  mutate(cyl8 = cyl == 8) %>%\n  sdf_partition(training = 0.5, test = 0.5, seed = 1099)\n\n# fit a linear model to the training dataset\nfit <- partitions$training %>%\n  ml_linear_regression(response = \"mpg\", features = c(\"wt\", \"cyl\"))\nfit\n## Call: ml_linear_regression.tbl_spark(., response = \"mpg\", features = c(\"wt\", \"cyl\"))\n##\n## Formula: mpg ~ wt + cyl\n##\n## Coefficients:\n## (Intercept)          wt         cyl\n##   33.499452   -2.818463   -0.923187\nFor linear regression models produced by Spark, we can use summary() to learn a bit more about the quality of our fit, and the statistical significance of each of our predictors.\nsummary(fit)\n## Call: ml_linear_regression.tbl_spark(., response = \"mpg\", features = c(\"wt\", \"cyl\"))\n##\n## Deviance Residuals:\n##    Min     1Q Median     3Q    Max\n## -1.752 -1.134 -0.499  1.296  2.282\n##\n## Coefficients:\n## (Intercept)          wt         cyl\n##   33.499452   -2.818463   -0.923187\n##\n## R-Squared: 0.8274\n## Root Mean Squared Error: 1.422\nSpark machine learning supports a wide array of algorithms and feature transformations and as illustrated above it’s easy to chain these functions together with dplyr pipelines. To learn more see the machine learning section."
  },
  {
    "href": "index.html#reading-and-writing-data",
    "title": "sparklyr",
    "section": "Reading and Writing Data",
    "text": "You can read and write data in CSV, JSON, and Parquet formats. Data can be stored in HDFS, S3, or on the local filesystem of cluster nodes.\ntemp_csv <- tempfile(fileext = \".csv\")\ntemp_parquet <- tempfile(fileext = \".parquet\")\ntemp_json <- tempfile(fileext = \".json\")\n\nspark_write_csv(iris_tbl, temp_csv)\niris_csv_tbl <- spark_read_csv(sc, \"iris_csv\", temp_csv)\n\nspark_write_parquet(iris_tbl, temp_parquet)\niris_parquet_tbl <- spark_read_parquet(sc, \"iris_parquet\", temp_parquet)\n\nspark_write_json(iris_tbl, temp_json)\niris_json_tbl <- spark_read_json(sc, \"iris_json\", temp_json)\n\ndplyr::src_tbls(sc)\n## [1] \"batting\"      \"flights\"      \"iris\"         \"iris_csv\"\n## [5] \"iris_json\"    \"iris_parquet\" \"mtcars\""
  },
  {
    "href": "index.html#distributed-r",
    "title": "sparklyr",
    "section": "Distributed R",
    "text": "You can execute arbitrary r code across your cluster using spark_apply. For example, we can apply rgamma over iris as follows:\nspark_apply(iris_tbl, function(data) {\n  data[1:4] + rgamma(1,2)\n})\n## # Source:   table<sparklyr_tmp_115c74acb6510> [?? x 4]\n## # Database: spark_connection\n##    Sepal_Length Sepal_Width Petal_Length Petal_Width\n##           <dbl>       <dbl>        <dbl>       <dbl>\n##  1     5.336757    3.736757     1.636757   0.4367573\n##  2     5.136757    3.236757     1.636757   0.4367573\n##  3     4.936757    3.436757     1.536757   0.4367573\n##  4     4.836757    3.336757     1.736757   0.4367573\n##  5     5.236757    3.836757     1.636757   0.4367573\n##  6     5.636757    4.136757     1.936757   0.6367573\n##  7     4.836757    3.636757     1.636757   0.5367573\n##  8     5.236757    3.636757     1.736757   0.4367573\n##  9     4.636757    3.136757     1.636757   0.4367573\n## 10     5.136757    3.336757     1.736757   0.3367573\n## # ... with more rows\nYou can also group by columns to perform an operation over each group of rows and make use of any package within the closure:\nspark_apply(\n  iris_tbl,\n  function(e) broom::tidy(lm(Petal_Width ~ Petal_Length, e)),\n  names = c(\"term\", \"estimate\", \"std.error\", \"statistic\", \"p.value\"),\n  group_by = \"Species\"\n)\n## # Source:   table<sparklyr_tmp_115c73965f30> [?? x 6]\n## # Database: spark_connection\n##      Species         term    estimate  std.error  statistic      p.value\n##        <chr>        <chr>       <dbl>      <dbl>      <dbl>        <dbl>\n## 1 versicolor  (Intercept) -0.08428835 0.16070140 -0.5245029 6.023428e-01\n## 2 versicolor Petal_Length  0.33105360 0.03750041  8.8279995 1.271916e-11\n## 3  virginica  (Intercept)  1.13603130 0.37936622  2.9945505 4.336312e-03\n## 4  virginica Petal_Length  0.16029696 0.06800119  2.3572668 2.253577e-02\n## 5     setosa  (Intercept) -0.04822033 0.12164115 -0.3964146 6.935561e-01\n## 6     setosa Petal_Length  0.20124509 0.08263253  2.4354220 1.863892e-02"
  },
  {
    "href": "index.html#extensions",
    "title": "sparklyr",
    "section": "Extensions",
    "text": "The facilities used internally by sparklyr for its dplyr and machine learning interfaces are available to extension packages. Since Spark is a general purpose cluster computing system there are many potential applications for extensions (e.g. interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).\nHere’s a simple example that wraps a Spark text file line counting function with an R function:\n# write a CSV\ntempfile <- tempfile(fileext = \".csv\")\nwrite.csv(nycflights13::flights, tempfile, row.names = FALSE, na = \"\")\n\n# define an R interface to Spark line counting\ncount_lines <- function(sc, path) {\n  spark_context(sc) %>%\n    invoke(\"textFile\", path, 1L) %>%\n      invoke(\"count\")\n}\n\n# call spark to count the lines of the CSV\ncount_lines(sc, tempfile)\n## [1] 336777\nTo learn more about creating extensions see the Extensions section of the sparklyr website."
  },
  {
    "href": "index.html#table-utilities",
    "title": "sparklyr",
    "section": "Table Utilities",
    "text": "You can cache a table into memory with:\ntbl_cache(sc, \"batting\")\nand unload from memory using:\ntbl_uncache(sc, \"batting\")"
  },
  {
    "href": "index.html#connection-utilities",
    "title": "sparklyr",
    "section": "Connection Utilities",
    "text": "You can view the Spark web console using the spark_web function:\nspark_web(sc)\nYou can show the log using the spark_log function:\nspark_log(sc, n = 10)\n## 17/11/09 15:55:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 69 (/var/folders/fz/v6wfsg2x1fb1rw4f6r0x4jwm0000gn/T//RtmpyR8oP9/file115c74b94924.csv MapPartitionsRDD[258] at textFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n## 17/11/09 15:55:18 INFO TaskSchedulerImpl: Adding task set 69.0 with 1 tasks\n## 17/11/09 15:55:18 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 140, localhost, executor driver, partition 0, PROCESS_LOCAL, 4904 bytes)\n## 17/11/09 15:55:18 INFO Executor: Running task 0.0 in stage 69.0 (TID 140)\n## 17/11/09 15:55:18 INFO HadoopRDD: Input split: file:/var/folders/fz/v6wfsg2x1fb1rw4f6r0x4jwm0000gn/T/RtmpyR8oP9/file115c74b94924.csv:0+33313106\n## 17/11/09 15:55:18 INFO Executor: Finished task 0.0 in stage 69.0 (TID 140). 832 bytes result sent to driver\n## 17/11/09 15:55:18 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 140) in 126 ms on localhost (executor driver) (1/1)\n## 17/11/09 15:55:18 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool\n## 17/11/09 15:55:18 INFO DAGScheduler: ResultStage 69 (count at NativeMethodAccessorImpl.java:0) finished in 0.126 s\n## 17/11/09 15:55:18 INFO DAGScheduler: Job 47 finished: count at NativeMethodAccessorImpl.java:0, took 0.131380 s\nFinally, we disconnect from Spark:\nspark_disconnect(sc)"
  },
  {
    "href": "index.html#rstudio-ide",
    "title": "sparklyr",
    "section": "RStudio IDE",
    "text": "The latest RStudio Preview Release of the RStudio IDE includes integrated support for Spark and the sparklyr package, including tools for:\n\nCreating and managing Spark connections\nBrowsing the tables and columns of Spark DataFrames\nPreviewing the first 1,000 rows of Spark DataFrames\n\nOnce you’ve installed the sparklyr package, you should find a new Spark pane within the IDE. This pane includes a New Connection dialog which can be used to make connections to local or remote Spark instances:\n\nOnce you’ve connected to Spark you’ll be able to browse the tables contained within the Spark cluster and preview Spark DataFrames using the standard RStudio data viewer:\n\nYou can also connect to Spark through Livy through a new connection dialog:\n\nThe RStudio IDE features for sparklyr are available now as part of the RStudio Preview Release."
  },
  {
    "href": "index.html#using-h2o",
    "title": "sparklyr",
    "section": "Using H2O",
    "text": "rsparkling is a CRAN package from H2O that extends sparklyr to provide an interface into Sparkling Water. For instance, the following example installs, configures and runs h2o.glm:\noptions(rsparkling.sparklingwater.version = \"2.1.14\")\n\nlibrary(rsparkling)\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(h2o)\n\nsc <- spark_connect(master = \"local\", version = \"2.1.0\")\nmtcars_tbl <- copy_to(sc, mtcars, \"mtcars\")\n\nmtcars_h2o <- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)\n\nmtcars_glm <- h2o.glm(x = c(\"wt\", \"cyl\"),\n                      y = \"mpg\",\n                      training_frame = mtcars_h2o,\n                      lambda_search = TRUE)\nmtcars_glm\n## Model Details:\n## ==============\n##\n## H2ORegressionModel: glm\n## Model ID:  GLM_model_R_1510271749678_1\n## GLM Model: summary\n##     family     link                              regularization\n## 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.1013 )\n##                                                                lambda_search\n## 1 nlambda = 100, lambda.max = 10.132, lambda.min = 0.1013, lambda.1se = -1.0\n##   number_of_predictors_total number_of_active_predictors\n## 1                          2                           2\n##   number_of_iterations                                training_frame\n## 1                  100 frame_rdd_29_b907d4915799eac74fb1ea60ad594bbf\n##\n## Coefficients: glm coefficients\n##       names coefficients standardized_coefficients\n## 1 Intercept    38.941654                 20.090625\n## 2       cyl    -1.468783                 -2.623132\n## 3        wt    -3.034558                 -2.969186\n##\n## H2ORegressionMetrics: glm\n## ** Reported on training data. **\n##\n## MSE:  6.017684\n## RMSE:  2.453097\n## MAE:  1.940985\n## RMSLE:  0.1114801\n## Mean Residual Deviance :  6.017684\n## R^2 :  0.8289895\n## Null Deviance :1126.047\n## Null D.o.F. :31\n## Residual Deviance :192.5659\n## Residual D.o.F. :29\n## AIC :156.2425\nspark_disconnect(sc)"
  },
  {
    "href": "index.html#connecting-through-livy",
    "title": "sparklyr",
    "section": "Connecting through Livy",
    "text": "Livy enables remote connections to Apache Spark clusters. Before connecting to Livy, you will need the connection information to an existing service running Livy. Otherwise, to test livy in your local environment, you can install it and run it locally as follows:\nlivy_install()\nlivy_service_start()\nTo connect, use the Livy service address as master and method = \"livy\" in spark_connect. Once connection completes, use sparklyr as usual, for instance:\nsc <- spark_connect(master = \"http://localhost:8998\", method = \"livy\")\ncopy_to(sc, iris)\n## # Source:   table<iris> [?? x 5]\n## # Database: spark_connection\n##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl>   <chr>\n##  1          5.1         3.5          1.4         0.2  setosa\n##  2          4.9         3.0          1.4         0.2  setosa\n##  3          4.7         3.2          1.3         0.2  setosa\n##  4          4.6         3.1          1.5         0.2  setosa\n##  5          5.0         3.6          1.4         0.2  setosa\n##  6          5.4         3.9          1.7         0.4  setosa\n##  7          4.6         3.4          1.4         0.3  setosa\n##  8          5.0         3.4          1.5         0.2  setosa\n##  9          4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## # ... with more rows\nspark_disconnect(sc)\nOnce you are done using livy locally, you should stop this service with:\nlivy_service_stop()\nTo connect to remote livy clusters that support basic authentication connect as:\nconfig <- livy_config(username=\"<username>\", password=\"<password\">)\nsc <- spark_connect(master = \"<address>\", method = \"livy\", config = config)\nspark_disconnect(sc)"
  },
  {
    "href": "guides/textmining.html#data-import",
    "title": "sparklyr",
    "section": "Data Import",
    "text": "Connect to Spark\nAn additional goal of this article is to encourage the reader to try it out, so a simple Spark local mode session is used.\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\", version = \"2.1.0\")\n\n\nspark_read_text()\nThe spark_read_text() is a new function which works like readLines() but for sparklyr. It comes in handy when non-structured data, such as lines in a book, is what is available for analysis.\n# Imports Mark Twain's file\n\n# Setting up the path to the file in a Windows OS laptop\ntwain_path <- paste0(\"file:///\", getwd(), \"/mark_twain.txt\")\ntwain <-  spark_read_text(sc, \"twain\", twain_path)\n# Imports Sir Arthur Conan Doyle's file\ndoyle_path <- paste0(\"file:///\", getwd(), \"/arthur_doyle.txt\")\ndoyle <-  spark_read_text(sc, \"doyle\", doyle_path)"
  },
  {
    "href": "guides/textmining.html#data-transformation",
    "title": "sparklyr",
    "section": "Data transformation",
    "text": "The objective is to end up with a tidy table inside Spark with one row per word used. The steps will be:\n\nThe needed data transformations apply to the data from both authors. The data sets will be appended to one another\nPunctuation will be removed\nThe words inside each line will be separated, or tokenized\nFor a cleaner analysis, stop words will be removed\nTo tidy the data, each word in a line will become its own row\nThe results will be saved to Spark memory\n\n\nsdf_bind_rows()\n\nsdf_bind_rows() appends the doyle Spark Dataframe to the twain Spark Dataframe. This function can be used in lieu of a dplyr::bind_rows() wrapper function. For this exercise, the column author is added to differentiate between the two bodies of work.\n\nall_words <- doyle %>%\n  mutate(author = \"doyle\") %>%\n  sdf_bind_rows({\n    twain %>%\n      mutate(author = \"twain\")}) %>%\n  filter(nchar(line) > 0)\n\n\nregexp_replace\n\nThe Hive UDF, regexp_replace, is used as a sort of gsub() that works inside Spark. In this case it is used to remove punctuation. The usual [:punct:] regular expression did not work well during development, so a custom list is provided. For more information, see the Hive Functions section in the dplyr page.\n\nall_words <- all_words %>%\n  mutate(line = regexp_replace(line, \"[_\\\"\\'():;,.!?\\\\-]\", \" \"))\n\n\nft_tokenizer()\n\nft_tokenizer() uses the Spark API to separate each word. It creates a new list column with the results.\n\nall_words <- all_words %>%\n    ft_tokenizer(input.col = \"line\",\n               output.col = \"word_list\")\n\nhead(all_words, 4)\n## # Source:   lazy query [?? x 3]\n## # Database: spark_connection\n##   author                              line  word_list\n##    <chr>                             <chr>     <list>\n## 1  doyle    THE RETURN OF SHERLOCK HOLMES  <list [5]>\n## 2  doyle A Collection of Holmes Adventures <list [5]>\n## 3  doyle         by Sir Arthur Conan Doyle <list [5]>\n## 4  doyle                         CONTENTS  <list [1]>\n\n\nft_stop_words_remover()\n\nft_stop_words_remover() is a new function that, as its name suggests, takes care of removing stop words from the previous transformation. It expects a list column, so it is important to sequence it correctly after a ft_tokenizer() command. In the sample results, notice that the new wo_stop_words column contains less items than word_list.\n\nall_words <- all_words %>%\n  ft_stop_words_remover(input.col = \"word_list\",\n                        output.col = \"wo_stop_words\")\n\nhead(all_words, 4)\n## # Source:   lazy query [?? x 4]\n## # Database: spark_connection\n##   author                              line  word_list wo_stop_words\n##    <chr>                             <chr>     <list>        <list>\n## 1  doyle    THE RETURN OF SHERLOCK HOLMES  <list [5]>    <list [3]>\n## 2  doyle A Collection of Holmes Adventures <list [5]>    <list [3]>\n## 3  doyle         by Sir Arthur Conan Doyle <list [5]>    <list [4]>\n## 4  doyle                         CONTENTS  <list [1]>    <list [1]>\n\n\nexplode\n\nThe Hive UDF explode performs the job of unnesting the tokens into their own row. Some further filtering and field selection is done to reduce the size of the dataset.\n\nall_words <- all_words %>%\n  mutate(word = explode(wo_stop_words)) %>%\n  select(word, author) %>%\n  filter(nchar(word) > 2)\n\nhead(all_words, 4)\n## # Source:   lazy query [?? x 2]\n## # Database: spark_connection\n##         word author\n##        <chr>  <chr>\n## 1     return  doyle\n## 2   sherlock  doyle\n## 3     holmes  doyle\n## 4 collection  doyle\n\n\ncompute()\n\ncompute() will operate this transformation and cache the results in Spark memory. It is a good idea to pass a name to compute() to make it easier to identify it inside the Spark environment. In this case the name will be all_words\n\nall_words <- all_words %>%\n  compute(\"all_words\")\n\n\nFull code\nThis is what the code would look like on an actual analysis:\nall_words <- doyle %>%\n  mutate(author = \"doyle\") %>%\n  sdf_bind_rows({\n    twain %>%\n      mutate(author = \"twain\")}) %>%\n  filter(nchar(line) > 0) %>%\n  mutate(line = regexp_replace(line, \"[_\\\"\\'():;,.!?\\\\-]\", \" \")) %>%\n  ft_tokenizer(input.col = \"line\",\n               output.col = \"word_list\") %>%\n  ft_stop_words_remover(input.col = \"word_list\",\n                        output.col = \"wo_stop_words\") %>%\n  mutate(word = explode(wo_stop_words)) %>%\n  select(word, author) %>%\n  filter(nchar(word) > 2) %>%\n  compute(\"all_words\")"
  },
  {
    "href": "guides/textmining.html#data-analysis",
    "title": "sparklyr",
    "section": "Data Analysis",
    "text": "Words used the most\nword_count <- all_words %>%\n  group_by(author, word) %>%\n  tally() %>%\n  arrange(desc(n))\n\nword_count\n## # Source:     lazy query [?? x 3]\n## # Database:   spark_connection\n## # Groups:     author\n## # Ordered by: desc(n)\n##    author  word     n\n##     <chr> <chr> <dbl>\n##  1  twain   one 20028\n##  2  doyle  upon 16482\n##  3  twain would 15735\n##  4  doyle   one 14534\n##  5  doyle  said 13716\n##  6  twain  said 13204\n##  7  twain could 11301\n##  8  doyle would 11300\n##  9  twain  time 10502\n## 10  doyle   man 10478\n## # ... with more rows\n\n\nWords used by Doyle and not Twain\ndoyle_unique <- filter(word_count, author == \"doyle\") %>%\n  anti_join(filter(word_count, author == \"twain\"), by = \"word\") %>%\n  arrange(desc(n)) %>%\n  compute(\"doyle_unique\")\n\ndoyle_unique\n## # Source:     lazy query [?? x 3]\n## # Database:   spark_connection\n## # Groups:     author\n## # Ordered by: desc(n), desc(n)\n##    author      word     n\n##     <chr>     <chr> <dbl>\n##  1  doyle     nigel   972\n##  2  doyle   alleyne   500\n##  3  doyle      ezra   421\n##  4  doyle     maude   337\n##  5  doyle   aylward   336\n##  6  doyle   catinat   301\n##  7  doyle   sharkey   281\n##  8  doyle  lestrade   280\n##  9  doyle summerlee   248\n## 10  doyle     congo   211\n## # ... with more rows\ndoyle_unique %>%\n  head(100) %>%\n  collect() %>%\n  with(wordcloud::wordcloud(\n    word,\n    n,\n    colors = c(\"#999999\", \"#E69F00\", \"#56B4E9\",\"#56B4E9\")))\n\n\n\nTwain and Sherlock\nThe word cloud highlighted something interesting. The word lestrade is listed as one of the words used by Doyle but not Twain. Lestrade is the last name of a major character in the Sherlock Holmes books. It makes sense that the word “sherlock” appears considerably more times than “lestrade” in Doyle’s books, so why is Sherlock not in the word cloud? Did Mark Twain use the word “sherlock” in his writings?\nall_words %>%\n  filter(author == \"twain\",\n         word == \"sherlock\") %>%\n  tally()\n## # Source:   lazy query [?? x 1]\n## # Database: spark_connection\n##       n\n##   <dbl>\n## 1    16\nThe all_words table contains 16 instances of the word sherlock in the words used by Twain in his works. The instr Hive UDF is used to extract the lines that contain that word in the twain table. This Hive function works can be used instead of base::grep() or stringr::str_detect(). To account for any word capitalization, the lower command will be used in mutate() to make all words in the full text lower cap.\n\n\ninstr & lower\nMost of these lines are in a short story by Mark Twain called A Double Barrelled Detective Story. As per the Wikipedia page about this story, this is a satire by Twain on the mystery novel genre, published in 1902.\ntwain %>%\n  mutate(line = lower(line)) %>%\n  filter(instr(line, \"sherlock\") > 0) %>%\n  pull(line)\n##  [1] \"late sherlock holmes, and yet discernible by a member of a race charged\"\n##  [2] \"sherlock holmes.\"\n##  [3] \"\\\"uncle sherlock! the mean luck of it!--that he should come just\"\n##  [4] \"another trouble presented itself. \\\"uncle sherlock 'll be wanting to talk\"\n##  [5] \"flint buckner's cabin in the frosty gloom. they were sherlock holmes and\"\n##  [6] \"\\\"uncle sherlock's got some work to do, gentlemen, that 'll keep him till\"\n##  [7] \"\\\"by george, he's just a duke, boys! three cheers for sherlock holmes,\"\n##  [8] \"he brought sherlock holmes to the billiard-room, which was jammed with\"\n##  [9] \"of interest was there--sherlock holmes. the miners stood silent and\"\n## [10] \"the room; the chair was on it; sherlock holmes, stately, imposing,\"\n## [11] \"\\\"you have hunted me around the world, sherlock holmes, yet god is my\"\n## [12] \"\\\"if it's only sherlock holmes that's troubling you, you needn't worry\"\n## [13] \"they sighed; then one said: \\\"we must bring sherlock holmes. he can be\"\n## [14] \"i had small desire that sherlock holmes should hang for my deeds, as you\"\n## [15] \"\\\"my name is sherlock holmes, and i have not been doing anything.\\\"\"\n## [16] \"late sherlock holmes, and yet discernible by a member of a race charged\"\nspark_disconnect(sc)"
  },
  {
    "href": "guides/textmining.html#appendix",
    "title": "sparklyr",
    "section": "Appendix",
    "text": "gutenbergr package\nThis is an example of how the data for this article was pulled from the Gutenberg site:\nlibrary(gutenbergr)\n\ngutenberg_works()  %>%\n  filter(author == \"Twain, Mark\") %>%\n  pull(gutenberg_id) %>%\n  gutenberg_download() %>%\n  pull(text) %>%\n  writeLines(\"mark_twain.txt\")"
  },
  {
    "href": "guides/h2o.html#overview",
    "title": "sparklyr",
    "section": "Overview",
    "text": "The rsparkling extension package provides bindings to H2O’s distributed machine learning algorithms via sparklyr. In particular, rsparkling allows you to access the machine learning routines provided by the Sparkling Water Spark package.\nTogether with sparklyr’s dplyr interface, you can easily create and tune H2O machine learning workflows on Spark, orchestrated entirely within R.\nrsparkling provides a few simple conversion functions that allow the user to transfer data between Spark DataFrames and H2O Frames. Once the Spark DataFrames are available as H2O Frames, the h2o R interface can be used to train H2O machine learning algorithms on the data.\nA typical machine learning pipeline with rsparkling might be composed of the following stages. To fit a model, you might need to:\n\nPerform SQL queries through the sparklyr dplyr interface,\nUse the sdf_* and ft_* family of functions to generate new columns, or partition your data set,\nConvert your training, validation and/or test data frames into H2O Frames using the as_h2o_frame function,\nChoose an appropriate H2O machine learning algorithm to model your data,\nInspect the quality of your model fit, and use it to make predictions with new data."
  },
  {
    "href": "guides/h2o.html#installation",
    "title": "sparklyr",
    "section": "Installation",
    "text": "You can install the rsparkling package from CRAN as follows:\ninstall.packages(\"rsparkling\")\nThen set the Sparkling Water version for rsparkling.:\noptions(rsparkling.sparklingwater.version = \"2.1.14\")\nFor Spark 2.0.x set rsparkling.sparklingwater.version to 2.0.3 instead, for Spark 1.6.2 use 1.6.8."
  },
  {
    "href": "guides/h2o.html#using-h2o",
    "title": "sparklyr",
    "section": "Using H2O",
    "text": "Now let’s walk through a simple example to demonstrate the use of H2O’s machine learning algorithms within R. We’ll use h2o.glm to fit a linear regression model. Using the built-in mtcars dataset, we’ll try to predict a car’s fuel consumption (mpg) based on its weight (wt), and the number of cylinders the engine contains (cyl).\nFirst, we will initialize a local Spark connection, and copy the mtcars dataset into Spark.\nlibrary(rsparkling)\nlibrary(sparklyr)\nlibrary(h2o)\nlibrary(dplyr)\n\nsc <- spark_connect(\"local\", version = \"2.1.0\")\n\nmtcars_tbl <- copy_to(sc, mtcars, \"mtcars\")\nNow, let’s perform some simple transformations – we’ll\n\nRemove all cars with horsepower less than 100,\nProduce a column encoding whether a car has 8 cylinders or not,\nPartition the data into separate training and test data sets,\nFit a model to our training data set,\nEvaluate our predictive performance on our test dataset.\n\n# transform our data set, and then partition into 'training', 'test'\npartitions <- mtcars_tbl %>%\n  filter(hp >= 100) %>%\n  mutate(cyl8 = cyl == 8) %>%\n  sdf_partition(training = 0.5, test = 0.5, seed = 1099)\nNow, we convert our training and test sets into H2O Frames using rsparkling conversion functions. We have already split the data into training and test frames using dplyr.\ntraining <- as_h2o_frame(sc, partitions$training, strict_version_check = FALSE)\ntest <- as_h2o_frame(sc, partitions$test, strict_version_check = FALSE)\nAlternatively, we can use the h2o.splitFrame() function instead of sdf_partition() to partition the data within H2O instead of Spark (e.g. partitions <- h2o.splitFrame(as_h2o_frame(mtcars_tbl), 0.5))\n# fit a linear model to the training dataset\nglm_model <- h2o.glm(x = c(\"wt\", \"cyl\"), \n                     y = \"mpg\", \n                     training_frame = training,\n                     lambda_search = TRUE)\nFor linear regression models produced by H2O, we can use either print() or summary() to learn a bit more about the quality of our fit. The summary() method returns some extra information about scoring history and variable importance.\nglm_model\n## Model Details:\n## ==============\n## \n## H2ORegressionModel: glm\n## Model ID:  GLM_model_R_1510348062048_1 \n## GLM Model: summary\n##     family     link                               regularization\n## 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.05468 )\n##                                                                 lambda_search\n## 1 nlambda = 100, lambda.max = 5.4682, lambda.min = 0.05468, lambda.1se = -1.0\n##   number_of_predictors_total number_of_active_predictors\n## 1                          2                           2\n##   number_of_iterations                                training_frame\n## 1                  100 frame_rdd_32_929e407384e0082416acd4c9897144a0\n## \n## Coefficients: glm coefficients\n##       names coefficients standardized_coefficients\n## 1 Intercept    32.997281                 16.625000\n## 2       cyl    -0.906688                 -1.349195\n## 3        wt    -2.712562                 -2.282649\n## \n## H2ORegressionMetrics: glm\n## ** Reported on training data. **\n## \n## MSE:  2.03293\n## RMSE:  1.425808\n## MAE:  1.306314\n## RMSLE:  0.08238032\n## Mean Residual Deviance :  2.03293\n## R^2 :  0.8265696\n## Null Deviance :93.775\n## Null D.o.F. :7\n## Residual Deviance :16.26344\n## Residual D.o.F. :5\n## AIC :36.37884\nThe output suggests that our model is a fairly good fit, and that both a cars weight, as well as the number of cylinders in its engine, will be powerful predictors of its average fuel consumption. (The model suggests that, on average, heavier cars consume more fuel.)\nLet’s use our H2O model fit to predict the average fuel consumption on our test data set, and compare the predicted response with the true measured fuel consumption. We’ll build a simple ggplot2 plot that will allow us to inspect the quality of our predictions.\nlibrary(ggplot2)\n\n# compute predicted values on our test dataset\npred <- h2o.predict(glm_model, newdata = test)\n# convert from H2O Frame to Spark DataFrame\npredicted <- as_spark_dataframe(sc, pred, strict_version_check = FALSE)\n\n# extract the true 'mpg' values from our test dataset\nactual <- partitions$test %>%\n  select(mpg) %>%\n  collect() %>%\n  `[[`(\"mpg\")\n\n# produce a data.frame housing our predicted + actual 'mpg' values\ndata <- data.frame(\n  predicted = predicted,\n  actual    = actual\n)\n# a bug in data.frame does not set colnames properly; reset here \nnames(data) <- c(\"predicted\", \"actual\")\n\n# plot predicted vs. actual values\nggplot(data, aes(x = actual, y = predicted)) +\n  geom_abline(lty = \"dashed\", col = \"red\") +\n  geom_point() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  coord_fixed(ratio = 1) +\n  labs(\n    x = \"Actual Fuel Consumption\",\n    y = \"Predicted Fuel Consumption\",\n    title = \"Predicted vs. Actual Fuel Consumption\"\n  )\n\nAlthough simple, our model appears to do a fairly good job of predicting a car’s average fuel consumption.\nAs you can see, we can easily and effectively combine dplyr data transformation pipelines with the machine learning algorithms provided by H2O’s Sparkling Water."
  },
  {
    "href": "guides/h2o.html#algorithms",
    "title": "sparklyr",
    "section": "Algorithms",
    "text": "Once the H2OContext is made available to Spark (as demonstrated below), all of the functions in the standard h2o R interface can be used with H2O Frames (converted from Spark DataFrames). Here is a table of the available algorithms:\n\n\n\nFunction\nDescription\n\n\n\n\nh2o.glm\nGeneralized Linear Model\n\n\nh2o.deeplearning\nMultilayer Perceptron\n\n\nh2o.randomForest\nRandom Forest\n\n\nh2o.gbm\nGradient Boosting Machine\n\n\nh2o.naiveBayes\nNaive-Bayes\n\n\nh2o.prcomp\nPrincipal Components Analysis\n\n\nh2o.svd\nSingular Value Decomposition\n\n\nh2o.glrm\nGeneralized Low Rank Model\n\n\nh2o.kmeans\nK-Means Clustering\n\n\nh2o.anomaly\nAnomaly Detection via Deep Learning Autoencoder\n\n\n\nAdditionally, the h2oEnsemble R package can be used to generate Super Learner ensembles of H2O algorithms:\n\n\n\nFunction\nDescription\n\n\n\n\nh2o.ensemble\nSuper Learner / Stacking\n\n\nh2o.stack\nSuper Learner / Stacking"
  },
  {
    "href": "guides/h2o.html#transformers",
    "title": "sparklyr",
    "section": "Transformers",
    "text": "A model is often fit not on a dataset as-is, but instead on some transformation of that dataset. Spark provides feature transformers, facilitating many common transformations of data within a Spark DataFrame, and sparklyr exposes these within the ft_* family of functions. Transformers can be used on Spark DataFrames, and the final training set can be sent to the H2O cluster for machine learning.\n\n\n\n\n\n\n\n\nFunction\n\n\nDescription\n\n\n\n\n\n\nft_binarizer\n\n\nThreshold numerical features to binary (0/1) feature\n\n\n\n\nft_bucketizer\n\n\nBucketizer transforms a column of continuous features to a column of feature buckets\n\n\n\n\nft_discrete_cosine_transform\n\n\nTransforms a length NN real-valued sequence in the time domain into another length NN real-valued sequence in the frequency domain\n\n\n\n\nft_elementwise_product\n\n\nMultiplies each input vector by a provided weight vector, using element-wise multiplication.\n\n\n\n\nft_index_to_string\n\n\nMaps a column of label indices back to a column containing the original labels as strings\n\n\n\n\nft_quantile_discretizer\n\n\nTakes a column with continuous features and outputs a column with binned categorical features\n\n\n\n\nft_sql_transformer\n\n\nImplements the transformations which are defined by a SQL statement\n\n\n\n\nft_string_indexer\n\n\nEncodes a string column of labels to a column of label indices\n\n\n\n\nft_vector_assembler\n\n\nCombines a given list of columns into a single vector column"
  },
  {
    "href": "guides/h2o.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "We will use the iris data set to examine a handful of learning algorithms and transformers. The iris data set measures attributes for 150 flowers in 3 different species of iris.\niris_tbl <- copy_to(sc, iris, \"iris\", overwrite = TRUE)\niris_tbl\n## # Source:   table<iris> [?? x 5]\n## # Database: spark_connection\n##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl>   <chr>\n##  1          5.1         3.5          1.4         0.2  setosa\n##  2          4.9         3.0          1.4         0.2  setosa\n##  3          4.7         3.2          1.3         0.2  setosa\n##  4          4.6         3.1          1.5         0.2  setosa\n##  5          5.0         3.6          1.4         0.2  setosa\n##  6          5.4         3.9          1.7         0.4  setosa\n##  7          4.6         3.4          1.4         0.3  setosa\n##  8          5.0         3.4          1.5         0.2  setosa\n##  9          4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## # ... with more rows\nConvert to an H2O Frame:\niris_hf <- as_h2o_frame(sc, iris_tbl, strict_version_check = FALSE)\n\nK-Means Clustering\nUse H2O’s K-means clustering to partition a dataset into groups. K-means clustering partitions points into k groups, such that the sum of squares from points to the assigned cluster centers is minimized.\nkmeans_model <- h2o.kmeans(training_frame = iris_hf, \n                           x = 3:4,\n                           k = 3,\n                           seed = 1)\nTo look at particular metrics of the K-means model, we can use h2o.centroid_stats() and h2o.centers() or simply print out all the model metrics using print(kmeans_model).\n# print the cluster centers\nh2o.centers(kmeans_model)\n##   petal_length petal_width\n## 1     1.462000     0.24600\n## 2     5.566667     2.05625\n## 3     4.296154     1.32500\n# print the centroid statistics\nh2o.centroid_stats(kmeans_model)\n## Centroid Statistics: \n##   centroid     size within_cluster_sum_of_squares\n## 1        1 50.00000                       1.41087\n## 2        2 48.00000                       9.29317\n## 3        3 52.00000                       7.20274\n\n\nPCA\nUse H2O’s Principal Components Analysis (PCA) to perform dimensionality reduction. PCA is a statistical method to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate in turn has the largest variance possible.\npca_model <- h2o.prcomp(training_frame = iris_hf,\n                        x = 1:4,\n                        k = 4,\n                        seed = 1)\n## Warning in doTryCatch(return(expr), name, parentenv, handler): _train:\n## Dataset used may contain fewer number of rows due to removal of rows with\n## NA/missing values. If this is not desirable, set impute_missing argument in\n## pca call to TRUE/True/true/... depending on the client language.\npca_model\n## Model Details:\n## ==============\n## \n## H2ODimReductionModel: pca\n## Model ID:  PCA_model_R_1510348062048_3 \n## Importance of components: \n##                             pc1      pc2      pc3      pc4\n## Standard deviation     7.861342 1.455041 0.283531 0.154411\n## Proportion of Variance 0.965303 0.033069 0.001256 0.000372\n## Cumulative Proportion  0.965303 0.998372 0.999628 1.000000\n## \n## \n## H2ODimReductionMetrics: pca\n## \n## No model metrics available for PCA\n\n\nRandom Forest\nUse H2O’s Random Forest to perform regression or classification on a dataset. We will continue to use the iris dataset as an example for this problem.\nAs usual, we define the response and predictor variables using the x and y arguments. Since we’d like to do a classification, we need to ensure that the response column is encoded as a factor (enum) column.\ny <- \"Species\"\nx <- setdiff(names(iris_hf), y)\niris_hf[,y] <- as.factor(iris_hf[,y])\nWe can split the iris_hf H2O Frame into a train and test set (the split defaults to 75/25 train/test).\nsplits <- h2o.splitFrame(iris_hf, seed = 1)\nThen we can train a Random Forest model:\nrf_model <- h2o.randomForest(x = x, \n                             y = y,\n                             training_frame = splits[[1]],\n                             validation_frame = splits[[2]],\n                             nbins = 32,\n                             max_depth = 5,\n                             ntrees = 20,\n                             seed = 1)\nSince we passed a validation frame, the validation metrics will be calculated. We can retrieve individual metrics using functions such as h2o.mse(rf_model, valid = TRUE). The confusion matrix can be printed using the following:\nh2o.confusionMatrix(rf_model, valid = TRUE)\n## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n##            setosa versicolor virginica  Error     Rate\n## setosa          7          0         0 0.0000 =  0 / 7\n## versicolor      0         13         0 0.0000 = 0 / 13\n## virginica       0          1        10 0.0909 = 1 / 11\n## Totals          7         14        10 0.0323 = 1 / 31\nTo view the variable importance computed from an H2O model, you can use either the h2o.varimp() or h2o.varimp_plot() functions:\nh2o.varimp_plot(rf_model)\n\n\n\nGradient Boosting Machine\nThe Gradient Boosting Machine (GBM) is one of H2O’s most popular algorithms, as it works well on many types of data. We will continue to use the iris dataset as an example for this problem.\nUsing the same dataset and x and y from above, we can train a GBM:\ngbm_model <- h2o.gbm(x = x, \n                     y = y,\n                     training_frame = splits[[1]],\n                     validation_frame = splits[[2]],                     \n                     ntrees = 20,\n                     max_depth = 3,\n                     learn_rate = 0.01,\n                     col_sample_rate = 0.7,\n                     seed = 1)\nSince this is a multi-class problem, we may be interested in inspecting the confusion matrix on a hold-out set. Since we passed along a validatin_frame at train time, the validation metrics are already computed and we just need to retreive them from the model object.\nh2o.confusionMatrix(gbm_model, valid = TRUE)\n## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n##            setosa versicolor virginica  Error     Rate\n## setosa          7          0         0 0.0000 =  0 / 7\n## versicolor      0         13         0 0.0000 = 0 / 13\n## virginica       0          1        10 0.0909 = 1 / 11\n## Totals          7         14        10 0.0323 = 1 / 31\n\n\nDeep Learning\nUse H2O’s Deep Learning to perform regression or classification on a dataset, extact non-linear features generated by the deep neural network, and/or detect anomalies using a deep learning model with auto-encoding.\nIn this example, we will use the prostate dataset available within the h2o package:\npath <- system.file(\"extdata\", \"prostate.csv\", package = \"h2o\")\nprostate_df <- spark_read_csv(sc, \"prostate\", path)\nhead(prostate_df)\n## # Source:   lazy query [?? x 9]\n## # Database: spark_connection\n##      ID CAPSULE   AGE  RACE DPROS DCAPS   PSA   VOL GLEASON\n##   <int>   <int> <int> <int> <int> <int> <dbl> <dbl>   <int>\n## 1     1       0    65     1     2     1   1.4   0.0       6\n## 2     2       0    72     1     3     2   6.7   0.0       7\n## 3     3       0    70     1     1     2   4.9   0.0       6\n## 4     4       0    76     2     2     1  51.2  20.0       7\n## 5     5       0    69     1     1     1  12.3  55.9       6\n## 6     6       1    71     1     3     2   3.3   0.0       8\nOnce we’ve done whatever data manipulation is required to run our model we’ll get a reference to it as an h2o frame then split it into training and test sets using the h2o.splitFrame function:\nprostate_hf <- as_h2o_frame(sc, prostate_df, strict_version_check = FALSE)\nsplits <- h2o.splitFrame(prostate_hf, seed = 1)\nNext we define the response and predictor columns.\ny <- \"VOL\"\n#remove response and ID cols\nx <- setdiff(names(prostate_hf), c(\"ID\", y))\nNow we can train a deep neural net.\ndl_fit <- h2o.deeplearning(x = x, y = y,\n                           training_frame = splits[[1]],\n                           epochs = 15,\n                           activation = \"Rectifier\",\n                           hidden = c(10, 5, 10),\n                           input_dropout_ratio = 0.7)\nEvaluate performance on a test set:\nh2o.performance(dl_fit, newdata = splits[[2]])\n## H2ORegressionMetrics: deeplearning\n## \n## MSE:  253.7022\n## RMSE:  15.92803\n## MAE:  12.90077\n## RMSLE:  1.885052\n## Mean Residual Deviance :  253.7022\nNote that the above metrics are not reproducible when H2O’s Deep Learning is run on multiple cores, however, the metrics should be fairly stable across repeat runs.\n\n\nGrid Search\nH2O’s grid search capabilities currently supports traditional (Cartesian) grid search and random grid search. Grid search in R provides the following capabilities:\n\nH2OGrid class: Represents the results of the grid search\nh2o.getGrid(<grid_id>, sort_by, decreasing): Display the specified grid\nh2o.grid: Start a new grid search parameterized by\n\nmodel builder name (e.g., algorithm = \"gbm\")\nmodel parameters (e.g., ntrees = 100)\nhyper_parameters: attribute for passing a list of hyper parameters (e.g., list(ntrees=c(1,100), learn_rate=c(0.1,0.001)))\nsearch_criteria: optional attribute for specifying more a advanced search strategy\n\n\n\nCartesian Grid Search\nBy default, h2o.grid() will train a Cartesian grid search – meaning, all possible models in the specified grid. In this example, we will re-use the prostate data as an example dataset for a regression problem.\nsplits <- h2o.splitFrame(prostate_hf, seed = 1)\n\ny <- \"VOL\"\n#remove response and ID cols\nx <- setdiff(names(prostate_hf), c(\"ID\", y))\nAfter prepping the data, we define a grid and execute the grid search.\n# GBM hyperparamters\ngbm_params1 <- list(learn_rate = c(0.01, 0.1),\n                    max_depth = c(3, 5, 9),\n                    sample_rate = c(0.8, 1.0),\n                    col_sample_rate = c(0.2, 0.5, 1.0))\n\n# Train and validate a grid of GBMs\ngbm_grid1 <- h2o.grid(\"gbm\", x = x, y = y,\n                      grid_id = \"gbm_grid1\",\n                      training_frame = splits[[1]],\n                      validation_frame = splits[[1]],\n                      ntrees = 100,\n                      seed = 1,\n                      hyper_params = gbm_params1)\n\n# Get the grid results, sorted by validation MSE\ngbm_gridperf1 <- h2o.getGrid(grid_id = \"gbm_grid1\", \n                             sort_by = \"mse\", \n                             decreasing = FALSE)\ngbm_gridperf1\n## H2O Grid Details\n## ================\n## \n## Grid ID: gbm_grid1 \n## Used hyper parameters: \n##   -  col_sample_rate \n##   -  learn_rate \n##   -  max_depth \n##   -  sample_rate \n## Number of models: 36 \n## Number of failed models: 0 \n## \n## Hyper-Parameter Search Summary: ordered by increasing mse\n##   col_sample_rate learn_rate max_depth sample_rate          model_ids\n## 1             1.0        0.1         9         1.0 gbm_grid1_model_35\n## 2             0.5        0.1         9         1.0 gbm_grid1_model_34\n## 3             1.0        0.1         9         0.8 gbm_grid1_model_17\n## 4             0.5        0.1         9         0.8 gbm_grid1_model_16\n## 5             1.0        0.1         5         0.8 gbm_grid1_model_11\n##                  mse\n## 1  88.10947523138782\n## 2  102.3118989994892\n## 3 102.78632321923726\n## 4  126.4217260351778\n## 5  149.6066650109763\n## \n## ---\n##    col_sample_rate learn_rate max_depth sample_rate          model_ids\n## 31             0.5       0.01         3         0.8  gbm_grid1_model_1\n## 32             0.2       0.01         5         1.0 gbm_grid1_model_24\n## 33             0.5       0.01         3         1.0 gbm_grid1_model_19\n## 34             0.2       0.01         5         0.8  gbm_grid1_model_6\n## 35             0.2       0.01         3         1.0 gbm_grid1_model_18\n## 36             0.2       0.01         3         0.8  gbm_grid1_model_0\n##                   mse\n## 31  324.8117304723162\n## 32 325.10992525687294\n## 33 325.27898443785045\n## 34 329.36983845305735\n## 35 338.54411936919456\n## 36  339.7744828617712\n\n\nRandom Grid Search\nH2O’s Random Grid Search samples from the given parameter space until a set of constraints is met. The user can specify the total number of desired models using (e.g. max_models = 40), the amount of time (e.g. max_runtime_secs = 1000), or tell the grid to stop after performance stops improving by a specified amount. Random Grid Search is a practical way to arrive at a good model without too much effort.\nThe example below is set to run fairly quickly – increase max_runtime_secs or max_models to cover more of the hyperparameter space in your grid search. Also, you can expand the hyperparameter space of each of the algorithms by modifying the definition of hyper_param below.\n# GBM hyperparamters\ngbm_params2 <- list(learn_rate = seq(0.01, 0.1, 0.01),\n                    max_depth = seq(2, 10, 1),\n                    sample_rate = seq(0.5, 1.0, 0.1),\n                    col_sample_rate = seq(0.1, 1.0, 0.1))\nsearch_criteria2 <- list(strategy = \"RandomDiscrete\", \n                         max_models = 50)\n\n# Train and validate a grid of GBMs\ngbm_grid2 <- h2o.grid(\"gbm\", x = x, y = y,\n                      grid_id = \"gbm_grid2\",\n                      training_frame = splits[[1]],\n                      validation_frame = splits[[2]],\n                      ntrees = 100,\n                      seed = 1,\n                      hyper_params = gbm_params2,\n                      search_criteria = search_criteria2)\n\n# Get the grid results, sorted by validation MSE\ngbm_gridperf2 <- h2o.getGrid(grid_id = \"gbm_grid2\", \n                             sort_by = \"mse\", \n                             decreasing = FALSE)\nTo get the best model, as measured by validation MSE, we simply grab the first row of the gbm_gridperf2@summary_table object, since this table is already sorted such that the lowest MSE model is on top.\ngbm_gridperf2@summary_table[1,]\n## Hyper-Parameter Search Summary: ordered by increasing mse\n##   col_sample_rate learn_rate max_depth sample_rate          model_ids\n## 1             0.8       0.01         2         0.7 gbm_grid2_model_35\n##                  mse\n## 1 244.61196951586288\nIn the examples above, we generated two different grids, specified by grid_id. The first grid was called grid_id = \"gbm_grid1\" and the second was called grid_id = \"gbm_grid2\". However, if we are using the same dataset & algorithm in two grid searches, it probably makes more sense just to add the results of the second grid search to the first. If you want to add models to an existing grid, rather than create a new one, you simply re-use the same grid_id."
  },
  {
    "href": "guides/h2o.html#exporting-models",
    "title": "sparklyr",
    "section": "Exporting Models",
    "text": "There are two ways of exporting models from H2O – saving models as a binary file, or saving models as pure Java code.\n\nBinary Models\nThe more traditional method is to save a binary model file to disk using the h2o.saveModel() function. To load the models using h2o.loadModel(), the same version of H2O that generated the models is required. This method is commonly used when H2O is being used in a non-production setting.\nA binary model can be saved as follows:\nh2o.saveModel(my_model, path = \"/Users/me/h2omodels\")\n\n\nJava (POJO) Models\nOne of the most valuable features of H2O is it’s ability to export models as pure Java code, or rather, a “Plain Old Java Object” (POJO). You can learn more about H2O POJO models in this POJO quickstart guide. The POJO method is used most commonly when a model is deployed in a production setting. POJO models are ideal for when you need very fast prediction response times, and minimal requirements – the POJO is a standalone Java class with no dependencies on the full H2O stack.\nTo generate the POJO for your model, use the following command:\nh2o.download_pojo(my_model, path = \"/Users/me/h2omodels\")\nFinally, disconnect with:\nspark_disconnect_all()\n## [1] 1\nYou can learn more about how to take H2O models to production in the productionizing H2O models section of the H2O docs."
  },
  {
    "href": "guides/h2o.html#additional-resources",
    "title": "sparklyr",
    "section": "Additional Resources",
    "text": "Main documentation site for Sparkling Water (and all H2O software projects)\nH2O.ai website\n\nIf you are new to H2O for machine learning, we recommend you start with the Intro to H2O Tutorial, followed by the H2O Grid Search & Model Selection Tutorial. There are a number of other H2O R tutorials and demos available, as well as the H2O World 2015 Training Gitbook, and the Machine Learning with R and H2O Booklet (pdf)."
  },
  {
    "href": "guides/pipelines.html#introduction-to-ml-pipelines",
    "title": "sparklyr",
    "section": "Introduction to ML Pipelines",
    "text": "The official Apache Spark site contains a more complete overview of ML Pipelines. This article will focus in introducing the basic concepts and steps to work with ML Pipelines via sparklyr.\nThere are two important stages in building an ML Pipeline. The first one is creating a Pipeline. A good way to look at it, or call it, is as an “empty” pipeline. This step just builds the steps that the data will go through. This is the somewhat equivalent of doing this in R:\nr_pipeline <-  . %>% mutate(cyl = paste0(\"c\", cyl)) %>% lm(am ~ cyl + mpg, data = .)\nr_pipeline\n\n## Functional sequence with the following components:\n## \n##  1. mutate(., cyl = paste0(\"c\", cyl))\n##  2. lm(am ~ cyl + mpg, data = .)\n## \n## Use 'functions' to extract the individual functions.\nThe r_pipeline object has all the steps needed to transform and fit the model, but it has not yet transformed any data.\nThe second step, is to pass data through the pipeline, which in turn will output a fitted model. That is called a PipelineModel. The PipelineModel can then be used to produce predictions.\nr_model <- r_pipeline(mtcars)\nr_model\n\n## \n## Call:\n## lm(formula = am ~ cyl + mpg, data = .)\n## \n## Coefficients:\n## (Intercept)        cylc6        cylc8          mpg  \n##    -0.54388      0.03124     -0.03313      0.04767\n\nTaking advantage of Pipelines and PipelineModels\nThe two stage ML Pipeline approach produces two final data products:\n\nA PipelineModel that can be added to the daily Spark jobs which will produce new predictions for the incoming data, and again, with no R dependencies.\nA Pipeline that can be easily re-fitted on a regular interval, say every month. All that is needed is to pass a new sample to obtain the new coefficients."
  },
  {
    "href": "guides/pipelines.html#pipeline",
    "title": "sparklyr",
    "section": "Pipeline",
    "text": "An additional goal of this article is that the reader can follow along, so the data, transformations and Spark connection in this example will be kept as easy to reproduce as possible.\nlibrary(nycflights13)\nlibrary(sparklyr)\nlibrary(dplyr)\nsc <- spark_connect(master = \"local\", spark_version = \"2.2.0\")\n\n## * Using Spark: 2.2.0\n\nspark_flights <- sdf_copy_to(sc, flights)\n\nFeature Transformers\nPipelines make heavy use of Feature Transformers. If new to Spark, and sparklyr, it would be good to review what these transformers do. These functions use the Spark API directly to transform the data, and may be faster at making the data manipulations that a dplyr (SQL) transformation.\nIn sparklyr the ft functions are essentially are wrappers to original Spark feature transformer.\n\n\nft_dplyr_transformer\nThis example will start with dplyr transformations, which are ultimately SQL transformations, loaded into the df variable.\nIn sparklyr, there is one feature transformer that is not available in Spark, ft_dplyr_transformer(). The goal of this function is to convert the dplyr code to a SQL Feature Transformer that can then be used in a Pipeline.\ndf <- spark_flights %>%\n  filter(!is.na(dep_delay)) %>%\n  mutate(\n    month = paste0(\"m\", month),\n    day = paste0(\"d\", day)\n  ) %>%\n  select(dep_delay, sched_dep_time, month, day, distance) \nThis is the resulting pipeline stage produced from the dplyr code:\nft_dplyr_transformer(sc, df)\nUse the ml_param() function to extract the “statement” attribute. That attribute contains the finalized SQL statement. Notice that the flights table name has been replace with __THIS__. This allows the pipeline to accept different table names as its source, making the pipeline very modular.\nft_dplyr_transformer(sc, df) %>%\n  ml_param(\"statement\")\n\n## [1] \"SELECT `dep_delay`, `sched_dep_time`, `month`, `day`, `distance`\\nFROM (SELECT `year`, CONCAT(\\\"m\\\", `month`) AS `month`, CONCAT(\\\"d\\\", `day`) AS `day`, `dep_time`, `sched_dep_time`, `dep_delay`, `arr_time`, `sched_arr_time`, `arr_delay`, `carrier`, `flight`, `tailnum`, `origin`, `dest`, `air_time`, `distance`, `hour`, `minute`, `time_hour`\\nFROM (SELECT *\\nFROM `__THIS__`\\nWHERE (NOT(((`dep_delay`) IS NULL)))) `bjbujfpqzq`) `axbwotqnbr`\"\n\n\nCreating the Pipeline\nThe following step will create a 5 stage pipeline:\n\nSQL transformer - Resulting from the ft_dplyr_transformer() transformation\nBinarizer - To determine if the flight should be considered delay. The eventual outcome variable.\nBucketizer - To split the day into specific hour buckets\nR Formula - To define the model’s formula\nLogistic Model\n\n\nflights_pipeline <- ml_pipeline(sc) %>%\n  ft_dplyr_transformer(\n    tbl = df\n    ) %>%\n  ft_binarizer(\n    input.col = \"dep_delay\",\n    output.col = \"delayed\",\n    threshold = 15\n  ) %>%\n  ft_bucketizer(\n    input.col = \"sched_dep_time\",\n    output.col = \"hours\",\n    splits = c(400, 800, 1200, 1600, 2000, 2400)\n  )  %>%\n  ft_r_formula(delayed ~ month + day + hours + distance) %>% \n  ml_logistic_regression()\nAnother nice feature for ML Pipelines in sparklyr, is the print-out. It makes it really easy to how each stage is setup:\nflights_pipeline\n\n## Pipeline (Estimator) with 5 stages\n## <pipeline_24044e4f2e21> \n##   Stages \n##   |--1 SQLTransformer (Transformer)\n##   |    <dplyr_transformer_2404e6a1b8e> \n##   |     (Parameters -- Column Names)\n##   |--2 Binarizer (Transformer)\n##   |    <binarizer_24045c9227f2> \n##   |     (Parameters -- Column Names)\n##   |      input_col: dep_delay\n##   |      output_col: delayed\n##   |--3 Bucketizer (Transformer)\n##   |    <bucketizer_240412366b1e> \n##   |     (Parameters -- Column Names)\n##   |      input_col: sched_dep_time\n##   |      output_col: hours\n##   |--4 RFormula (Estimator)\n##   |    <r_formula_240442d75f00> \n##   |     (Parameters -- Column Names)\n##   |      features_col: features\n##   |      label_col: label\n##   |     (Parameters)\n##   |      force_index_label: FALSE\n##   |      formula: delayed ~ month + day + hours + distance\n##   |--5 LogisticRegression (Estimator)\n##   |    <logistic_regression_24044321ad0> \n##   |     (Parameters -- Column Names)\n##   |      features_col: features\n##   |      label_col: label\n##   |      prediction_col: prediction\n##   |      probability_col: probability\n##   |      raw_prediction_col: rawPrediction\n##   |     (Parameters)\n##   |      aggregation_depth: 2\n##   |      elastic_net_param: 0\n##   |      family: auto\n##   |      fit_intercept: TRUE\n##   |      max_iter: 100\n##   |      reg_param: 0\n##   |      standardization: TRUE\n##   |      threshold: 0.5\n##   |      tol: 1e-06\nNotice that there are no coefficients defined yet. That’s because no data has been actually processed. Even though df uses spark_flights(), recall that the final SQL transformer makes that name, so there’s no data to process yet."
  },
  {
    "href": "guides/pipelines.html#pipelinemodel",
    "title": "sparklyr",
    "section": "PipelineModel",
    "text": "A quick partition of the data is created for this exercise.\npartitioned_flights <- sdf_partition(\n  spark_flights,\n  training = 0.01,\n  testing = 0.01,\n  rest = 0.98\n)\nThe ml_fit() function produces the PipelineModel. The training partition of the partitioned_flights data is used to train the model:\nfitted_pipeline <- ml_fit(\n  flights_pipeline,\n  partitioned_flights$training\n)\nfitted_pipeline\n\n## PipelineModel (Transformer) with 5 stages\n## <pipeline_24044e4f2e21> \n##   Stages \n##   |--1 SQLTransformer (Transformer)\n##   |    <dplyr_transformer_2404e6a1b8e> \n##   |     (Parameters -- Column Names)\n##   |--2 Binarizer (Transformer)\n##   |    <binarizer_24045c9227f2> \n##   |     (Parameters -- Column Names)\n##   |      input_col: dep_delay\n##   |      output_col: delayed\n##   |--3 Bucketizer (Transformer)\n##   |    <bucketizer_240412366b1e> \n##   |     (Parameters -- Column Names)\n##   |      input_col: sched_dep_time\n##   |      output_col: hours\n##   |--4 RFormulaModel (Transformer)\n##   |    <r_formula_240442d75f00> \n##   |     (Parameters -- Column Names)\n##   |      features_col: features\n##   |      label_col: label\n##   |     (Transformer Info)\n##   |      formula:  chr \"delayed ~ month + day + hours + distance\" \n##   |--5 LogisticRegressionModel (Transformer)\n##   |    <logistic_regression_24044321ad0> \n##   |     (Parameters -- Column Names)\n##   |      features_col: features\n##   |      label_col: label\n##   |      prediction_col: prediction\n##   |      probability_col: probability\n##   |      raw_prediction_col: rawPrediction\n##   |     (Transformer Info)\n##   |      coefficient_matrix:  num [1, 1:43] 0.709 -0.3401 -0.0328 0.0543 -0.4774 ... \n##   |      coefficients:  num [1:43] 0.709 -0.3401 -0.0328 0.0543 -0.4774 ... \n##   |      intercept:  num -3.04 \n##   |      intercept_vector:  num -3.04 \n##   |      num_classes:  int 2 \n##   |      num_features:  int 43 \n##   |      threshold:  num 0.5\nNotice that the print-out for the fitted pipeline now displays the model’s coefficients.\nThe ml_transform() function can be used to run predictions, in other words it is used instead of predict() or sdf_predict().\npredictions <- ml_transform(\n  fitted_pipeline,\n  partitioned_flights$testing\n)\n\npredictions %>%\n  group_by(delayed, prediction) %>%\n  tally()\n\n## # Source:   lazy query [?? x 3]\n## # Database: spark_connection\n## # Groups:   delayed\n##   delayed prediction     n\n##     <dbl>      <dbl> <dbl>\n## 1      0.         1.   51.\n## 2      0.         0. 2599.\n## 3      1.         0.  666.\n## 4      1.         1.   69."
  },
  {
    "href": "guides/pipelines.html#save-the-pipelines-to-disk",
    "title": "sparklyr",
    "section": "Save the pipelines to disk",
    "text": "The ml_save() command can be used to save the Pipeline and PipelineModel to disk. The resulting output is a folder with the selected name, which contains all of the necessary Scala scripts:\nml_save(\n  flights_pipeline,\n  \"flights_pipeline\",\n  overwrite = TRUE\n)\n\n## NULL\n\nml_save(\n  fitted_pipeline,\n  \"flights_model\",\n  overwrite = TRUE\n)\n\n## NULL"
  },
  {
    "href": "guides/pipelines.html#use-an-existing-pipelinemodel",
    "title": "sparklyr",
    "section": "Use an existing PipelineModel",
    "text": "The ml_load() command can be used to re-load Pipelines and PipelineModels. The saved ML Pipeline files can only be loaded into an open Spark session.\nreloaded_model <- ml_load(sc, \"flights_model\")\nA simple query can be used as the table that will be used to make the new predictions. This of course, does not have to done in R, at this time the “flights_model” can be loaded into an independent Spark session outside of R.\nnew_df <- spark_flights %>%\n  filter(\n    month == 7,\n    day == 5\n  )\n\nml_transform(reloaded_model, new_df) \n\n## # Source:   table<sparklyr_tmp_24041e052b5> [?? x 12]\n## # Database: spark_connection\n##    dep_delay sched_dep_time month day   distance delayed hours features  \n##        <dbl>          <int> <chr> <chr>    <dbl>   <dbl> <dbl> <list>    \n##  1       39.           2359 m7    d5       1617.      1.    4. <dbl [43]>\n##  2      141.           2245 m7    d5       2475.      1.    4. <dbl [43]>\n##  3        0.            500 m7    d5        529.      0.    0. <dbl [43]>\n##  4       -5.            536 m7    d5       1400.      0.    0. <dbl [43]>\n##  5       -2.            540 m7    d5       1089.      0.    0. <dbl [43]>\n##  6       -7.            545 m7    d5       1416.      0.    0. <dbl [43]>\n##  7       -3.            545 m7    d5       1576.      0.    0. <dbl [43]>\n##  8       -7.            600 m7    d5       1076.      0.    0. <dbl [43]>\n##  9       -7.            600 m7    d5         96.      0.    0. <dbl [43]>\n## 10       -6.            600 m7    d5        937.      0.    0. <dbl [43]>\n## # ... with more rows, and 4 more variables: label <dbl>,\n## #   rawPrediction <list>, probability <list>, prediction <dbl>"
  },
  {
    "href": "guides/pipelines.html#re-fit-an-existing-pipeline",
    "title": "sparklyr",
    "section": "Re-fit an existing Pipeline",
    "text": "First, reload the pipeline into an open Spark session:\nreloaded_pipeline <- ml_load(sc, \"flights_pipeline\")\nUse ml_fit() again to pass new data, in this case, sample_frac() is used instead of sdf_partition() to provide the new data. The idea being that the re-fitting would happen at a later date than when the model was initially fitted.\nnew_model <-  ml_fit(reloaded_pipeline, sample_frac(spark_flights, 0.01))\n\nnew_model\n\n## PipelineModel (Transformer) with 5 stages\n## <pipeline_24044e4f2e21> \n##   Stages \n##   |--1 SQLTransformer (Transformer)\n##   |    <dplyr_transformer_2404e6a1b8e> \n##   |     (Parameters -- Column Names)\n##   |--2 Binarizer (Transformer)\n##   |    <binarizer_24045c9227f2> \n##   |     (Parameters -- Column Names)\n##   |      input_col: dep_delay\n##   |      output_col: delayed\n##   |--3 Bucketizer (Transformer)\n##   |    <bucketizer_240412366b1e> \n##   |     (Parameters -- Column Names)\n##   |      input_col: sched_dep_time\n##   |      output_col: hours\n##   |--4 RFormulaModel (Transformer)\n##   |    <r_formula_240442d75f00> \n##   |     (Parameters -- Column Names)\n##   |      features_col: features\n##   |      label_col: label\n##   |     (Transformer Info)\n##   |      formula:  chr \"delayed ~ month + day + hours + distance\" \n##   |--5 LogisticRegressionModel (Transformer)\n##   |    <logistic_regression_24044321ad0> \n##   |     (Parameters -- Column Names)\n##   |      features_col: features\n##   |      label_col: label\n##   |      prediction_col: prediction\n##   |      probability_col: probability\n##   |      raw_prediction_col: rawPrediction\n##   |     (Transformer Info)\n##   |      coefficient_matrix:  num [1, 1:43] 0.258 0.648 -0.317 0.36 -0.279 ... \n##   |      coefficients:  num [1:43] 0.258 0.648 -0.317 0.36 -0.279 ... \n##   |      intercept:  num -3.77 \n##   |      intercept_vector:  num -3.77 \n##   |      num_classes:  int 2 \n##   |      num_features:  int 43 \n##   |      threshold:  num 0.5\nThe new model can be saved using ml_save(). A new name is used in this case, but the same name as the existing PipelineModel to replace it.\nml_save(new_model, \"new_flights_model\", overwrite = TRUE)\n\n## NULL\nFinally, this example is complete by closing the Spark session.\nspark_disconnect(sc)"
  },
  {
    "href": "guides/connections.html#local-mode",
    "title": "Configuring Spark Connections",
    "section": "Local mode",
    "text": "Local mode is an excellent way to learn and experiment with Spark. Local mode also provides a convenient development environment for analyses, reports, and applications that you plan to eventually deploy to a multi-node Spark cluster.\nTo work in local mode, you should first install a version of Spark for local use. You can do this using the spark_install() function, for example:\n\nRecommended properties\nThe following are the recommended Spark properties to set when connecting via R:\n\nsparklyr.cores.local - It defaults to using all of the available cores. Not a necessary property to set, unless there’s a reason to use less cores than available for a given Spark session.\nsparklyr.shell.driver-memory - The limit is the amount of RAM available in the computer minus what would be needed for OS operations.\nspark.memory.fraction - The default is set to 60% of the requested memory per executor. For more information, please see this Memory Management Overview page in the official Spark website.\n\n\n\nConnection example\nconf$`sparklyr.cores.local` <- 4\nconf$`sparklyr.shell.driver-memory` <- \"16G\"\nconf$spark.memory.fraction <- 0.9\n\nsc <- spark_connect(master = \"local\", \n                    version = \"2.1.0\",\n                    config = conf)\n\nExecutors page\nTo see how the requested configuration affected the Spark connection, go to the Executors page in the Spark Web UI available in http://localhost:4040/storage/"
  },
  {
    "href": "guides/connections.html#customizing-connections",
    "title": "Configuring Spark Connections",
    "section": "Customizing connections",
    "text": "A connection to Spark can be customized by setting the values of certain Spark properties. In sparklyr, Spark properties can be set by using the config argument in the spark_connect() function.\nBy default, spark_connect() uses spark_config() as the default configuration. But that can be customized as shown in the example code below. Because of the unending number of possible combinations, spark_config() contains only a basic configuration, so it will be very likely that additional settings will be needed to properly connect to the cluster.\nconf <- spark_config()   # Load variable with spark_config()\n\nconf$spark.executor.memory <- \"16G\" # Use `$` to add or set values\n\nsc <- spark_connect(master = \"yarn-client\", \n                    config = conf)  # Pass the conf variable \n\nSpark definitions\nIt may be useful to provide some simple definitions for the Spark nomenclature:\n\nNode: A server\nWorker Node: A server that is part of the cluster and are available to run Spark jobs\nMaster Node: The server that coordinates the Worker nodes.\nExecutor: A sort of virtual machine inside a node. One Node can have multiple Executors.\nDriver Node: The Node that initiates the Spark session. Typically, this will be the server where sparklyr is located.\nDriver (Executor): The Driver Node will also show up in the Executor list.\n\n\n\nUseful concepts\n\nSpark configuration properties passed by R are just requests - In most cases, the cluster has the final say regarding the resources apportioned to a given Spark session.\nThe cluster overrides ‘silently’ - Many times, no errors are returned when more resources than allowed are requested, or if an attempt is made to change a setting fixed by the cluster."
  },
  {
    "href": "guides/connections.html#yarn",
    "title": "Configuring Spark Connections",
    "section": "YARN",
    "text": "Background\nUsing Spark and R inside a Hadoop based Data Lake is becoming a common practice at companies. Currently, there is no good way to manage user connections to the Spark service centrally. There are some caps and settings that can be applied, but in most cases there are configurations that the R user will need to customize.\nThe Running on YARN page in Spark’s official website is the best place to start for configuration settings reference, please bookmark it. Cluster administrators and users can benefit from this document. If Spark is new to the company, the YARN tunning article, courtesy of Cloudera, does a great job at explaining how the Spark/YARN architecture works.\n\n\nRecommended properties\nThe following are the recommended Spark properties to set when connecting via R:\n\nspark.executor.memory - The maximum possible is managed by the YARN cluster. See the Executor Memory Error\nspark.executor.cores - Number of cores assigned per Executor.\nspark.executor.instances - Number of executors to start. This property is acknowledged by the cluster if spark.dynamicAllocation.enabled is set to “false”.\nspark.dynamicAllocation.enabled - Overrides the mechanism that Spark provides to dynamically adjust resources. Disabling it provides more control over the number of the Executors that can be started, which in turn impact the amount of storage available for the session. For more information, please see the Dynamic Resource Allocation page in the official Spark website.\n\n\n\nClient mode\nUsing yarn-client as the value for the master argument in spark_connect() will make the server in which R is running to be the Spark’s session driver. Here is a sample connection:\nconf <- spark_config()\n\nconf$spark.executor.memory <- \"300M\"\nconf$spark.executor.cores <- 2\nconf$spark.executor.instances <- 3\nconf$spark.dynamicAllocation.enabled <- \"false\"\n\nsc <- spark_connect(master = \"yarn-client\", \n                    spark_home = \"/usr/lib/spark/\",\n                    version = \"1.6.0\",\n                    config = conf)\n\nExecutors page\nTo see how the requested configuration affected the Spark connection, go to the Executors page in the Spark Web UI. Typically, the Spark Web UI can be found using the exact same URL used for RStudio but on port 4040.\nNotice that 155.3MB per executor are assigned instead of the 300MB requested. This is because the spark.memory.fraction has been fixed by the cluster, plus, there is fixed amount of memory designated for overhead.\n\n\n\n\nCluster mode\nRunning in cluster mode means that YARN will choose where the driver of the Spark session will run. This means that the server where R is running may not necessarily be the driver for that session. Here is a good write-up explaining how running Spark applications work: Running Spark on YARN\nThe server will need to have copies of at least two files: yarn-site.xml and hive-site.xml. There may be other files needed based on your cluster’s individual setup.\nThis is an example of connecting to a Cloudera cluster:\nlibrary(sparklyr)\n\nSys.setenv(JAVA_HOME=\"/usr/lib/jvm/java-7-oracle-cloudera/\")\nSys.setenv(SPARK_HOME = '/opt/cloudera/parcels/CDH/lib/spark')\nSys.setenv(YARN_CONF_DIR = '/opt/cloudera/parcels/CDH/lib/spark/conf/yarn-conf')\n\nconf$spark.executor.memory <- \"300M\"\nconf$spark.executor.cores <- 2\nconf$spark.executor.instances <- 3\nconf$spark.dynamicAllocation.enabled <- \"false\"\nconf <- spark_config()\n\nsc <- spark_connect(master = \"yarn-cluster\", \n                    config = conf)\n\n\nExecutor memory error\nRequesting more memory or CPUs for Executors than allowed will return an error. This is one of the exceptions to the cluster’s ‘silent’ overrides. It will return a message similar to this:\n    Failed during initialize_connection: java.lang.IllegalArgumentException: Required executor memory (16384+1638 MB) is above the max threshold (8192 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'\nA cluster’s administrator is the only person who can make changes to the settings mentioned in the error. If the cluster is supported by a vendor, like Cloudera or Hortonworks, then the change can be made using the cluster’s web UI. Otherwise, changes to those settings are done directly in the yarn-default.xml file.\n\n\nKerberos\nThere are two options to access a “kerberized” data lake:\n\nUse kinit to get and cache the ticket. After kinit is installed and configured. After kinit is setup, it can used in R via a system() call prior to connecting to the cluster:\n\nsystem(\"echo '<password>' | kinit <username>\")\nFor more information visit this site: Apache - Authenticate with kinit\n\nA preferred option may be to use the out-of-the-box integration with Kerberos that the commercial version of RStudio Server offers."
  },
  {
    "href": "guides/connections.html#standalone-mode",
    "title": "Configuring Spark Connections",
    "section": "Standalone mode",
    "text": "Recommended properties\nThe following are the recommended Spark properties to set when connecting via R:\nThe default behavior in Standalone mode is to create one executor per worker. So in a 3 worker node cluster, there will be 3 executors setup. The basic properties that can be set are:\n\nspark.executor.memory - The requested memory cannot exceed the actual RAM available.\nspark.memory.fraction - The default is set to 60% of the requested memory per executor. For more information, please see this Memory Management Overview page in the official Spark website.\nspark.executor.cores - The requested cores cannot be higher than the cores available in each worker.\n\n\nDynamic Allocation\nIf dynamic allocation is disabled, then Spark will attempt to assign all of the available cores evenly across the cluster. The property used is spark.dynamicAllocation.enabled.\nFor example, the Standalone cluster used for this article has 3 worker nodes. Each node has 14.7GB in RAM and 4 cores. This means that there are a total of 12 cores (3 workers with 4 cores) and 44.1GB in RAM (3 workers with 14.7GB in RAM each).\nIf the spark.executor.cores property is set to 2, and dynamic allocation is disabled, then Spark will spawn 6 executors. The spark.executor.memory property should be set to a level that when the value is multiplied by 6 (number of executors) it will not be over total available RAM. In this case, the value can be safely set to 7GB so that the total memory requested will be 42GB, which is under the available 44.1GB.\n\n\n\nConnection example\nconf <- spark_config()\nconf$spark.executor.memory <- \"7GB\"\nconf$spark.memory.fraction <- 0.9\nconf$spark.executor.cores <- 2\nconf$spark.dynamicAllocation.enabled <- \"false\"\n\nsc <- spark_connect(master=\"spark://master-url:7077\", \n              version = \"2.1.0\",\n              config = conf,\n              spark_home = \"/home/ubuntu/spark-2.1.0-bin-hadoop2.7/\")\n\nExecutors page\nTo see how the requested configuration affected the Spark connection, go to the Executors page in the Spark Web UI. Typically, the Spark Web UI can be found using the exact same URL used for RStudio but on port 4040:"
  },
  {
    "href": "guides/streaming.html#the-sparklyr-interface",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "The sparklyr interface",
    "text": "As stated in the Spark’s official site, Spark Streaming makes it easy to build scalable fault-tolerant streaming applications. Because is part of the Spark API, it is possible to re-use query code that queries the current state of the stream, as well as joining the streaming data with historical data. Please see Spark’s official documentation for a deeper look into Spark Streaming.\nThe sparklyr interface provides the following:\n\nAbility to run dplyr, SQL, spark_apply(), and PipelineModels against a stream\nRead in multiple formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc\nWrite stream results to Spark memory and the following file formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc\nAn out-of-the box graph visualization to monitor the stream\nA new reactiveSpark() function, that allows Shiny apps to poll the contents of the stream create Shiny apps that are able to read the contents of the stream"
  },
  {
    "href": "guides/streaming.html#interacting-with-a-stream",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Interacting with a stream",
    "text": "A good way of looking at the way how Spark streams update is as a three stage operation:\n\nInput - Spark reads the data inside a given folder. The folder is expected to contain multiple data files, with new files being created containing the most current stream data.\nProcessing - Spark applies the desired operations on top of the data. These operations could be data manipulations (dplyr, SQL), data transformations (sdf operations, PipelineModel predictions), or native R manipulations (spark_apply()).\nOutput - The results of processing the input files are saved in a different folder.\n\nIn the same way all of the read and write operations in sparklyr for Spark Standalone, or in sparklyr’s local mode, the input and output folders are actual OS file system folders. For Hadoop clusters, these will be folder locations inside the HDFS."
  },
  {
    "href": "guides/streaming.html#example-1---inputoutput",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 1 - Input/Output",
    "text": "The first intro example is a small script that can be used with a local master. The result should be to see the stream_view() app showing live the number of records processed for each iteration of test data being sent to the stream.\n\nlibrary(future)\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\", spark_version = \"2.3.0\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\nstream_generate_test(iterations = 1)\nread_folder <- stream_read_csv(sc, \"source\") \nwrite_output <- stream_write_csv(read_folder, \"source-out\")\ninvisible(future(stream_generate_test(interval = 0.5)))\n\nstream_view(write_output)\n\n\n\n\n\n\n\nstream_stop(write_output)\nspark_disconnect(sc)\n\n\nCode breakdown\n\nOpen the Spark connection ::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", spark_version = \"2.3.0\")\n:::\nOptional step. This resets the input and output folders. It makes it easier to run the code multiple times in a clean manner. ::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n:::\nProduces a single test file inside the “source” folder. This allows the “read” function to infer CSV file definition. ::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nstream_generate_test(iterations = 1)\nlist.files(\"source\")\n:::\n[1] \"stream_1.csv\"\nPoints the stream reader to the folder where the streaming files will be placed. Since it is primed with a single CSV file, it will use as the expected layout of subsequent files. By default, stream_read_csv() creates a single integer variable data frame. ::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nread_folder <- stream_read_csv(sc, \"source\")\n:::\nThe output writer is what starts the streaming job. It will start monitoring the input folder, and then write the new results in the “source-out” folder. So as new records stream in, new files will be created in the “source-out” folder. Since there are no operations on the incoming data at this time, the output files will have the same exact raw data as the input files. The only difference is that the files and sub folders within “source-out” will be structured how Spark structures data folders. ::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nwrite_output <- stream_write_csv(read_folder, \"source-out\")\nlist.files(\"source-out\")\n::: [1] \"_spark_metadata\"                                     \"checkpoint\"  [3] \"part-00000-1f29719a-2314-40e1-b93d-a647a3d57154-c000.csv\"\nThe test generation function will run 100 files every 0.2 seconds. To run the tests “out-of-sync” with the current R session, the future package is used. ::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nlibrary(future)\ninvisible(future(stream_generate_test(interval = 0.2, iterations = 100)))\n:::\nThe stream_view() function can be used before the 50 tests are complete because of the use of the future package. It will monitor the status of the job that write_output is pointing to and provide information on the amount of data coming into the “source” folder and going out into the “source-out” folder. ::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nstream_view(write_output)\n:::\nThe monitor will continue to run even after the tests are complete. To end the experiment, stop the Shiny app and then use the following to stop the stream and close the Spark session. ::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nstream_stop(write_output)\nspark_disconnect(sc)\n:::"
  },
  {
    "href": "guides/streaming.html#example-2---processing",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 2 - Processing",
    "text": "The second example builds on the first. It adds a processing step that manipulates the input data before saving it to the output folder. In this case, a new binary field is added indicating if the value from x is over 400 or not. This time, while run the second code chunk in this example a few times during the stream tests to see the aggregated values change.\n\nlibrary(future)\nlibrary(sparklyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nsc <- spark_connect(master = \"local\", spark_version = \"2.3.0\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\nstream_generate_test(iterations = 1)\nread_folder <- stream_read_csv(sc, \"source\") \n\nprocess_stream <- read_folder %>%\n  mutate(x = as.double(x)) %>%\n  ft_binarizer(\n    input_col = \"x\",\n    output_col = \"over\",\n    threshold = 400\n  )\n\nwrite_output <- stream_write_csv(process_stream, \"source-out\")\ninvisible(future(stream_generate_test(interval = 0.2, iterations = 100)))\n\nRun this code a few times during the experiment: ::: {.cell original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nspark_read_csv(sc, \"stream\", \"source-out\", memory = FALSE) %>%\n  group_by(over) %>%\n  tally()\n:::\nThe results would look similar to this. The n totals will increase as the experiment progresses.\n# Source:   lazy query [?? x 2]\n# Database: spark_connection\n   over     n\n  <dbl> <dbl>\n1     0 40215\n2     1 60006\nClean up after the experiment ::: {.cell original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nstream_stop(write_output)\nspark_disconnect(sc)\n:::\n\nCode breakdown\n\nThe processing starts with the read_folder variable that contains the input stream. It coerces the integer field x, into a type double. This is because the next function, ft_binarizer() does not accept integers. The binarizer determines if x is over 400 or not. This is a good illustration of how dplyr can help simplify the manipulation needed during the processing stage. ::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nprocess_stream <- read_folder %>%\n  mutate(x = as.double(x)) %>%\n  ft_binarizer(\n    input_col = \"x\",\n    output_col = \"over\",\n    threshold = 400\n  )\n:::\nThe output now needs to write-out the processed data instead of the raw input data. Swap read_folder with process_stream. ::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nwrite_output <- stream_write_csv(process_stream, \"source-out\")\n:::\nThe “source-out” folder can be treated as a if it was a single table within Spark. Using spark_read_csv(), the data can be mapped, but not brought into memory (memory = FALSE). This allows the current results to be further analyzed using regular dplyr commands. ::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nspark_read_csv(sc, \"stream\", \"source-out\", memory = FALSE) %>%\n  group_by(over) %>%\n  tally()\n:::"
  },
  {
    "href": "guides/streaming.html#example-3---aggregate-in-process-and-output-to-memory",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 3 - Aggregate in process and output to memory",
    "text": "Another option is to save the results of the processing into a in-memory Spark table. Unless intentionally saving it to disk, the table and its data will only exist while the Spark session is active.\nThe biggest advantage of using Spark memory as the target, is that it will allow for aggregation to happen during processing. This is an advantage because aggregation is not allowed for any file output, expect Kafka, on the input/process stage.\nUsing example 2 as the base, this example code will perform some aggregations to the current stream input and save only those summarized results into Spark memory:\n\nlibrary(future)\nlibrary(sparklyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nsc <- spark_connect(master = \"local\", spark_version = \"2.3.0\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\n\nstream_generate_test(iterations = 1)\nread_folder <- stream_read_csv(sc, \"source\") \n\nprocess_stream <- read_folder %>%\n  stream_watermark() %>%\n  group_by(timestamp) %>%\n  summarise(\n    max_x = max(x, na.rm = TRUE),\n    min_x = min(x, na.rm = TRUE),\n    count = n()\n  )\n\nwrite_output <- stream_write_memory(process_stream, name = \"stream\")\n\ninvisible(future(stream_generate_test()))\n\nRun this command a different times while the experiment is running: ::: {.cell original.params.src=“r, eval = FALSE” chunk.echo=“false”}\ntbl(sc, \"stream\") \n:::\nClean up after the experiment ::: {.cell original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nstream_stop(write_output)\nspark_disconnect(sc)\n:::\n\nCode breakdown\n\nThe stream_watermark() functions add a new timestamp variable that is then used in the group_by() command. This is required by Spark Stream to accept summarized results as output of the stream. The second step is to simply decide what kinds of aggregations we need to perform. In this case, a simply max, min and count are performed.\n::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nprocess_stream <- read_folder %>%\n  stream_watermark() %>%\n  group_by(timestamp) %>%\n  summarise(\n    max_x = max(x, na.rm = TRUE),\n    min_x = min(x, na.rm = TRUE),\n    count = n()\n  )\n:::\nThe spark_write_memory() function is used to write the output to Spark memory. The results will appear as a table of the Spark session with the name assigned in the name argument, in this case the name selected is: “stream”.\n::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nwrite_output <- stream_write_memory(process_stream, name = \"stream\")\n:::\nTo query the current data in the “stream” table can be queried by using the dplyr tbl() command. ::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\ntbl(sc, \"stream\") \n:::"
  },
  {
    "href": "guides/streaming.html#example-4---shiny-integration",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 4 - Shiny integration",
    "text": "sparklyr provides a new Shiny function called reactiveSpark(). It can take a Spark data frame, in this case the one created as a result of the stream processing, and then creates a Spark memory stream table, the same way a table is created in example 3.\n\nlibrary(future)\nlibrary(sparklyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(ggplot2)\n\nsc <- spark_connect(master = \"local\", spark_version = \"2.3.0\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\nstream_generate_test(iterations = 1)\nread_folder <- stream_read_csv(sc, \"source\") \n\nprocess_stream <- read_folder %>%\n  stream_watermark() %>%\n  group_by(timestamp) %>%\n  summarise(\n    max_x = max(x, na.rm = TRUE),\n    min_x = min(x, na.rm = TRUE),\n    count = n()\n  )\n\ninvisible(future(stream_generate_test(interval = 0.2, iterations = 100)))\n\nlibrary(shiny)\nui <- function(){\n  tableOutput(\"table\")\n}\nserver <- function(input, output, session){\n  \n  ps <- reactiveSpark(process_stream)\n  \n  output$table <- renderTable({\n    ps() %>%\n      mutate(timestamp = as.character(timestamp)) \n    })\n}\nrunGadget(ui, server)\n\n\n\n\n\n\n\nCode breakdown\n\nNotice that there is no stream_write_... command. The reason is that reactiveSpark() function contains the stream_write_memory() function.\nThis very basic Shiny app simply displays the output of a table in the ui section ::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nlibrary(shiny)\n\nui <- function(){\n  tableOutput(\"table\")\n}\n:::\nIn the server section, the reactiveSpark() function will update every time there’s a change to the stream and return a data frame. The results are saved to a variable called ps() in this script. Treat the ps() variable as a regular table that can be piped from, as shown in the example. In this case, the timestamp variable is converted to string for to make it easier to read. ::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nserver <- function(input, output, session){\n\n  ps <- reactiveSpark(process_stream)\n\n  output$table <- renderTable({\n    ps() %>%\n      mutate(timestamp = as.character(timestamp)) \n  })\n}\n:::\nUse runGadget() to display the Shiny app in the Viewer pane. This is optional, the app can be run using normal Shiny run functions.\n\n::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\n```{.r .cell-code}\nrunGadget(ui, server)\n```\n:::"
  },
  {
    "href": "guides/streaming.html#example-5---ml-pipeline-model",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 5 - ML Pipeline Model",
    "text": "This example uses a fitted Pipeline Model to process the input, and saves the predictions to the output. This approach would be used to apply Machine Learning on top of streaming data.\n\nlibrary(sparklyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nsc <- spark_connect(master = \"local\", spark_version = \"2.3.0\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\ndf <- data.frame(x = rep(1:1000), y = rep(2:1001))\n\nstream_generate_test(df = df, iteration = 1)\n\nmodel_sample <- spark_read_csv(sc, \"sample\", \"source\")\n\npipeline <- sc %>%\n  ml_pipeline() %>%\n  ft_r_formula(x ~ y) %>%\n  ml_linear_regression()\n\nfitted_pipeline <- ml_fit(pipeline, model_sample)\n\nml_stream <- stream_read_csv(\n    sc = sc, \n    path = \"source\", \n    columns = c(x = \"integer\", y = \"integer\")\n  )  %>%\n  ml_transform(fitted_pipeline, .)  %>%\n  select(- features) %>%\n  stream_write_csv(\"source-out\")\n\nstream_generate_test(df = df, interval = 0.5)\n\n\nspark_read_csv(sc, \"stream\", \"source-out\", memory = FALSE) \n\n### Source: spark<stream> [?? x 4]\n##       x     y label prediction\n## * <int> <int> <dbl>      <dbl>\n## 1   276   277   276       276.\n## 2   277   278   277       277.\n## 3   278   279   278       278.\n## 4   279   280   279       279.\n## 5   280   281   280       280.\n## 6   281   282   281       281.\n## 7   282   283   282       282.\n## 8   283   284   283       283.\n## 9   284   285   284       284.\n##10   285   286   285       285.\n### ... with more rows\n\nstream_stop(ml_stream)\nspark_disconnect(sc)\n\n\nCode Breakdown\n\nCreates and fits a pipeline ::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\ndf <- data.frame(x = rep(1:1000), y = rep(2:1001))\nstream_generate_test(df = df, iteration = 1)\nmodel_sample <- spark_read_csv(sc, \"sample\", \"source\")\n\npipeline <- sc %>%\n  ml_pipeline() %>%\n  ft_r_formula(x ~ y) %>%\n  ml_linear_regression()\n\nfitted_pipeline <- ml_fit(pipeline, model_sample)\n:::\nThis example pipelines the input, process and output in a single code segment. The ml_transform() function is used to create the predictions. Because the CSV format does not support list type fields, the features column is removed before the results are sent to the output. ::: {.cell indent=” ” original.params.src=“r, eval = FALSE” chunk.echo=“false”}\nml_stream <- stream_read_csv(\n    sc = sc, \n    path = \"source\", \n    columns = c(x = \"integer\", y = \"integer\")\n  )  %>%\n  ml_transform(fitted_pipeline, .)  %>%\n  select(- features) %>%\n  stream_write_csv(\"source-out\")\n:::"
  },
  {
    "href": "guides/arrow.html#introduction",
    "title": "Using Apache Arrow",
    "section": "Introduction",
    "text": "Apache Arrow is a cross-language development platform for in-memory data. Arrow is supported starting with sparklyr 1.0.0 to improve performance when transferring data between Spark and R. You can find some performance benchmarks under:\n\nsparklyr 1.0: Arrow, XGBoost, Broom and TFRecords.\nSpeeding up R and Apache Spark using Apache Arrow."
  },
  {
    "href": "guides/arrow.html#installation",
    "title": "Using Apache Arrow",
    "section": "Installation",
    "text": "Install the latest release of arrow from CRAN with\ninstall.packages(\"arrow\")\nPlease see https://arrow.apache.org/docs/r if you have any question about using arrow from R."
  },
  {
    "href": "guides/arrow.html#use-cases",
    "title": "Using Apache Arrow",
    "section": "Use Cases",
    "text": "There are three main use cases for arrow in sparklyr:\n\nData Copying: When copying data with copy_to(), Arrow will be used.\nData Collection: Also, when collecting either, implicitly by printing datasets or explicitly calling collect.\nR Transformations: When using spark_apply(), data will be transferred using Arrow when possible.\n\nTo use arrow in sparklyr one simply needs to import this library:\n\nlibrary(arrow)\n\nAttaching package: ‘arrow’\n\nThe following object is masked from ‘package:utils’:\n\n    timestamp\n\nThe following objects are masked from ‘package:base’:\n\n    array, table"
  },
  {
    "href": "guides/arrow.html#considerations",
    "title": "Using Apache Arrow",
    "section": "Considerations",
    "text": "Types\nSome data types are mapped to slightly different, one can argue more correct, types when using Arrow. For instance, consider collecting 64 bit integers in sparklyr:\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\ninteger64 <- sdf_len(sc, 2, type = \"integer64\")\ninteger64\n\n# Source: spark<?> [?? x 1]\n     id\n  <dbl>\n1     1\n2     2\nNotice that sparklyr collects 64 bit integers as double; however, using arrow:\n\nlibrary(arrow)\ninteger64\n\n# Source: spark<?> [?? x 1]\n  id             \n  <S3: integer64>\n1 1              \n2 2 \n64 bit integers are now being collected as proper 64 bit integer using the bit64 package.\n\n\nFallback\nThe Arrow R package supports many data types; however, in cases where a type is unsupported, sparklyr will fallback to not using arrow and print a warning.\n\nlibrary(sparklyr.nested)\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(arrow)\n\nsc <- spark_connect(master = \"local\")\ncars <- copy_to(sc, mtcars)\n\nsdf_nest(cars, hp) %>%\n  group_by(cyl) %>%\n  summarize(data = collect_list(data))\n\n# Source: spark<?> [?? x 2]\n    cyl data       \n  <dbl> <list>     \n1     6 <list [7]> \n2     4 <list [11]>\n3     8 <list [14]>\nWarning message:\nIn arrow_enabled_object.spark_jobj(sdf) :\n  Arrow disabled due to columns: data"
  },
  {
    "href": "guides/distributed-r.html#overview",
    "title": "sparklyr",
    "section": "Overview",
    "text": "sparklyr provides support to run arbitrary R code at scale within your Spark Cluster through spark_apply(). This is especially useful where there is a need to use functionality available only in R or R packages that is not available in Apache Spark nor Spark Packages.\nspark_apply() applies an R function to a Spark object (typically, a Spark DataFrame). Spark objects are partitioned so they can be distributed across a cluster. You can use spark_apply with the default partitions or you can define your own partitions with the group_by argument. Your R function must return another Spark DataFrame. spark_apply will run your R function on each partition and output a single Spark DataFrame.\n\nApply an R function to a Spark Object\nLets run a simple example. We will apply the identify function, I(), over a list of numbers we created with the sdf_len function.\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nsdf_len(sc, 5, repartition = 1) %>%\n  spark_apply(function(e) I(e))\n## # Source:   table<sparklyr_tmp_378c2e4fb50> [?? x 1]\n## # Database: spark_connection\n##      id\n##   <dbl>\n## 1     1\n## 2     2\n## 3     3\n## 4     4\n## 5     5\nYour R function should be designed to operate on an R data frame. The R function passed to spark_apply expects a DataFrame and will return an object that can be cast as a DataFrame. We can use the class function to verify the class of the data.\nsdf_len(sc, 10, repartition = 1) %>%\n  spark_apply(function(e) class(e))\n## # Source:   table<sparklyr_tmp_378c7ce7618d> [?? x 1]\n## # Database: spark_connection\n##           id\n##        <chr>\n## 1 data.frame\nSpark will partition your data by hash or range so it can be distributed across a cluster. In the following example we create two partitions and count the number of rows in each partition. Then we print the first record in each partition.\ntrees_tbl <- sdf_copy_to(sc, trees, repartition = 2)\n\ntrees_tbl %>%\n  spark_apply(function(e) nrow(e), names = \"n\")\n## # Source:   table<sparklyr_tmp_378c15c45eb1> [?? x 1]\n## # Database: spark_connection\n##       n\n##   <int>\n## 1    16\n## 2    15\ntrees_tbl %>%\n  spark_apply(function(e) head(e, 1))\n## # Source:   table<sparklyr_tmp_378c29215418> [?? x 3]\n## # Database: spark_connection\n##   Girth Height Volume\n##   <dbl>  <dbl>  <dbl>\n## 1   8.3     70   10.3\n## 2   8.6     65   10.3\nWe can apply any arbitrary function to the partitions in the Spark DataFrame. For instance, we can scale or jitter the columns. Notice that spark_apply applies the R function to all partitions and returns a single DataFrame.\ntrees_tbl %>%\n  spark_apply(function(e) scale(e))\n## # Source:   table<sparklyr_tmp_378c8922ba8> [?? x 3]\n## # Database: spark_connection\n##         Girth      Height     Volume\n##         <dbl>       <dbl>      <dbl>\n##  1 -1.4482330 -0.99510521 -1.1503645\n##  2 -1.3021313 -2.06675697 -1.1558670\n##  3 -0.7469449  0.68891899 -0.6826528\n##  4 -0.6592839 -1.60747764 -0.8587325\n##  5 -0.6300635  0.53582588 -0.4735581\n##  6 -0.5716229  0.38273277 -0.3855183\n##  7 -0.5424025 -0.07654655 -0.5395880\n##  8 -0.3670805 -0.22963966 -0.6661453\n##  9 -0.1040975  1.30129143  0.1427209\n## 10  0.1296653 -0.84201210 -0.3029809\n## # ... with more rows\ntrees_tbl %>%\n  spark_apply(function(e) lapply(e, jitter))\n## # Source:   table<sparklyr_tmp_378c43237574> [?? x 3]\n## # Database: spark_connection\n##        Girth   Height   Volume\n##        <dbl>    <dbl>    <dbl>\n##  1  8.319392 70.04321 10.30556\n##  2  8.801237 62.85795 10.21751\n##  3 10.719805 81.15618 18.78076\n##  4 11.009892 65.98926 15.58448\n##  5 11.089322 80.14661 22.58749\n##  6 11.309682 79.01360 24.18158\n##  7 11.418486 75.88748 21.38380\n##  8 11.982421 74.85612 19.09375\n##  9 12.907616 84.81742 33.80591\n## 10 13.691892 71.05309 25.70321\n## # ... with more rows\nBy default spark_apply() derives the column names from the input Spark data frame. Use the names argument to rename or add new columns.\ntrees_tbl %>%\n  spark_apply(\n    function(e) data.frame(2.54 * e$Girth, e),\n    names = c(\"Girth(cm)\", colnames(trees)))\n## # Source:   table<sparklyr_tmp_378c14e015b5> [?? x 4]\n## # Database: spark_connection\n##    `Girth(cm)` Girth Height Volume\n##          <dbl> <dbl>  <dbl>  <dbl>\n##  1      21.082   8.3     70   10.3\n##  2      22.352   8.8     63   10.2\n##  3      27.178  10.7     81   18.8\n##  4      27.940  11.0     66   15.6\n##  5      28.194  11.1     80   22.6\n##  6      28.702  11.3     79   24.2\n##  7      28.956  11.4     76   21.4\n##  8      30.480  12.0     75   19.1\n##  9      32.766  12.9     85   33.8\n## 10      34.798  13.7     71   25.7\n## # ... with more rows\n\n\nGroup By\nIn some cases you may want to apply your R function to specific groups in your data. For example, suppose you want to compute regression models against specific subgroups. To solve this, you can specify a group_by argument. This example counts the number of rows in iris by species and then fits a simple linear model for each species.\niris_tbl <- sdf_copy_to(sc, iris)\n\niris_tbl %>%\n  spark_apply(nrow, group_by = \"Species\")\n## # Source:   table<sparklyr_tmp_378c1b8155f3> [?? x 2]\n## # Database: spark_connection\n##      Species Sepal_Length\n##        <chr>        <int>\n## 1 versicolor           50\n## 2  virginica           50\n## 3     setosa           50\niris_tbl %>%\n  spark_apply(\n    function(e) summary(lm(Petal_Length ~ Petal_Width, e))$r.squared,\n    names = \"r.squared\",\n    group_by = \"Species\")\n## # Source:   table<sparklyr_tmp_378c30e6155> [?? x 2]\n## # Database: spark_connection\n##      Species r.squared\n##        <chr>     <dbl>\n## 1 versicolor 0.6188467\n## 2  virginica 0.1037537\n## 3     setosa 0.1099785"
  },
  {
    "href": "guides/distributed-r.html#distributing-packages",
    "title": "sparklyr",
    "section": "Distributing Packages",
    "text": "With spark_apply() you can use any R package inside Spark. For instance, you can use the broom package to create a tidy data frame from linear regression output.\nspark_apply(\n  iris_tbl,\n  function(e) broom::tidy(lm(Petal_Length ~ Petal_Width, e)),\n  names = c(\"term\", \"estimate\", \"std.error\", \"statistic\", \"p.value\"),\n  group_by = \"Species\")\n## # Source:   table<sparklyr_tmp_378c5502500b> [?? x 6]\n## # Database: spark_connection\n##      Species        term  estimate std.error statistic      p.value\n##        <chr>       <chr>     <dbl>     <dbl>     <dbl>        <dbl>\n## 1 versicolor (Intercept) 1.7812754 0.2838234  6.276000 9.484134e-08\n## 2 versicolor Petal_Width 1.8693247 0.2117495  8.827999 1.271916e-11\n## 3  virginica (Intercept) 4.2406526 0.5612870  7.555230 1.041600e-09\n## 4  virginica Petal_Width 0.6472593 0.2745804  2.357267 2.253577e-02\n## 5     setosa (Intercept) 1.3275634 0.0599594 22.141037 7.676120e-27\n## 6     setosa Petal_Width 0.5464903 0.2243924  2.435422 1.863892e-02\nTo use R packages inside Spark, your packages must be installed on the worker nodes. The first time you call spark_apply all of the contents in your local .libPaths() will be copied into each Spark worker node via the SparkConf.addFile() function. Packages will only be copied once and will persist as long as the connection remains open. It’s not uncommon for R libraries to be several gigabytes in size, so be prepared for a one-time tax while the R packages are copied over to your Spark cluster. You can disable package distribution by setting packages = FALSE. Note: packages are not copied in local mode (master=\"local\") because the packages already exist on the system."
  },
  {
    "href": "guides/distributed-r.html#handling-errors",
    "title": "sparklyr",
    "section": "Handling Errors",
    "text": "It can be more difficult to troubleshoot R issues in a cluster than in local mode. For instance, the following R code causes the distributed execution to fail and suggests you check the logs for details.\nspark_apply(iris_tbl, function(e) stop(\"Make this fail\"))\n Error in force(code) : \n  sparklyr worker rscript failure, check worker logs for details\nIn local mode, sparklyr will retrieve the logs for you. The logs point out the real failure as ERROR sparklyr: RScript (4190) Make this fail as you might expect.\n---- Output Log ----\n(17/07/27 21:24:18 ERROR sparklyr: Worker (2427) is shutting down with exception ,java.net.SocketException: Socket closed)\n17/07/27 21:24:18 WARN TaskSetManager: Lost task 0.0 in stage 389.0 (TID 429, localhost, executor driver): 17/07/27 21:27:21 INFO sparklyr: RScript (4190) retrieved 150 rows \n17/07/27 21:27:21 INFO sparklyr: RScript (4190) computing closure \n17/07/27 21:27:21 ERROR sparklyr: RScript (4190) Make this fail \nIt is worth mentioning that different cluster providers and platforms expose worker logs in different ways. Specific documentation for your environment will point out how to retrieve these logs."
  },
  {
    "href": "guides/distributed-r.html#requirements",
    "title": "sparklyr",
    "section": "Requirements",
    "text": "The R Runtime is expected to be pre-installed in the cluster for spark_apply to function. Failure to install the cluster will trigger a Cannot run program, no such file or directory error while attempting to use spark_apply(). Contact your cluster administrator to consider making the R runtime available throughout the entire cluster.\nA Homogeneous Cluster is required since the driver node distributes, and potentially compiles, packages to the workers. For instance, the driver and workers must have the same processor architecture, system libraries, etc."
  },
  {
    "href": "guides/distributed-r.html#configuration",
    "title": "sparklyr",
    "section": "Configuration",
    "text": "The following table describes relevant parameters while making use of spark_apply.\n\n\n\n\n\n\n\n\nValue\n\n\nDescription\n\n\n\n\n\n\nspark.r.command\n\n\nThe path to the R binary. Useful to select from multiple R versions.\n\n\n\n\nsparklyr.worker.gateway.address\n\n\nThe gateway address to use under each worker node. Defaults to sparklyr.gateway.address.\n\n\n\n\nsparklyr.worker.gateway.port\n\n\nThe gateway port to use under each worker node. Defaults to sparklyr.gateway.port.\n\n\n\n\nFor example, one could make use of an specific R version by running:\nconfig <- spark_config()\nconfig[[\"spark.r.command\"]] <- \"<path-to-r-version>\"\n\nsc <- spark_connect(master = \"local\", config = config)\nsdf_len(sc, 10) %>% spark_apply(function(e) e)"
  },
  {
    "href": "guides/distributed-r.html#limitations",
    "title": "sparklyr",
    "section": "Limitations",
    "text": "Closures\nClosures are serialized using serialize, which is described as “A simple low-level interface for serializing to connections.”. One of the current limitations of serialize is that it wont serialize objects being referenced outside of it’s environment. For instance, the following function will error out since the closures references external_value:\nexternal_value <- 1\nspark_apply(iris_tbl, function(e) e + external_value)\n\n\nLivy\nCurrently, Livy connections do not support distributing packages since the client machine where the libraries are precompiled might not have the same processor architecture, not operating systems that the cluster machines.\n\n\nComputing over Groups\nWhile performing computations over groups, spark_apply() will provide partitions over the selected column; however, this implies that each partition can fit into a worker node, if this is not the case an exception will be thrown. To perform operations over groups that exceed the resources of a single node, one can consider partitioning to smaller units or use dplyr::do which is currently optimized for large partitions.\n\n\nPackage Installation\nSince packages are copied only once for the duration of the spark_connect() connection, installing additional packages is not supported while the connection is active. Therefore, if a new package needs to be installed, spark_disconnect() the connection, modify packages and reconnect."
  },
  {
    "href": "guides/aws-s3.html#aws-access-keys",
    "title": "Using Spark with AWS S3 buckets",
    "section": "AWS Access Keys",
    "text": "AWS Access Keys are needed to access S3 data. To learn how to setup a new keys, please review the AWS documentation: http://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html .We then pass the keys to R via Environment Variables:\nSys.setenv(AWS_ACCESS_KEY_ID=\"[Your access key]\")\nSys.setenv(AWS_SECRET_ACCESS_KEY=\"[Your secret access key]\")"
  },
  {
    "href": "guides/aws-s3.html#connecting-to-spark",
    "title": "Using Spark with AWS S3 buckets",
    "section": "Connecting to Spark",
    "text": "There are four key settings needed to connect to Spark and use S3:\n\nA Hadoop-AWS package\nExecutor memory (key but not critical)\nThe master URL\nThe Spark Home\n\nTo connect to Spark, we first need to initialize a variable with the contents of sparklyr default config (spark_config) which we will then customize for our needs\nlibrary(sparklyr)\n\nconf <- spark_config()\n\nHadoop-AWS package:\nA Spark connection can be enhanced by using packages, please note that these are not R packages. For example, there are packages that tells Spark how to read CSV files, Hadoop or Hadoop in AWS.\nIn order to read S3 buckets, our Spark connection will need a package called hadoop-aws. If needed, multiple packages can be used. We experimented with many combinations of packages, and determined that for reading data in S3 we only need the one. The version we used, 2.7.3, refers to the latest Hadoop version, so as this article ages, please make sure to check this site to ensure that you are using the latest version: https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws\nconf$sparklyr.defaultPackages <- \"org.apache.hadoop:hadoop-aws:2.7.3\"\n\n\nExecutor Memory\nAs mentioned above this setting key but not critical. There are two points worth highlighting about it is:\n\nThe only performance related setting in a Spark Stand Alone cluster that can be tweaked, and in most cases because Spark defaults to a fraction of what is available, we then need to increase it by manually passing a value to that setting.\nIf more than the available RAM is requested, then Spark will set the Cores to 0, thus rendering the session unusable.\n\nconf$spark.executor.memory <- \"14g\"\n\n\nMaster URL and Spark home\nThere are three important points to mention when executing the spark_connect command:\n\nThe master will be the Spark Master’s URL. To find the URL, please see the Spark Cluster section.\nPoint the Spark Home to the location where Spark was installed in this node\nMake sure to the conf variable as the value for the config argument\n\nsc <- spark_connect(master = \"spark://ip-172-30-1-5.us-west-2.compute.internal:7077\", \n                    spark_home = \"/home/ubuntu/spark-2.1.0-bin-hadoop2.7/\",\n                    config =  conf)"
  },
  {
    "href": "guides/aws-s3.html#data-importwrangle-approach",
    "title": "Using Spark with AWS S3 buckets",
    "section": "Data Import/Wrangle approach",
    "text": "We experimented with multiple approaches. Most of the factors for settling on a recommended approach were made based on the speed of each step. The premise is that we would rather wait longer during Data Import, if it meant that we can much faster Register and Cache our data subsets during Data Wrangling, especially since we would expect to end up with many subsets as we explore and model.The selected combination was the second slowest during the Import stage, but the fastest when caching a subset, by a lot.\nIn our tests, it took 72 seconds to read and cache the 29 columns of the 41 million rows of data, the slowest was 77 seconds. But when it comes to registering and caching a considerably sizable subset of 3 columns and almost all of the 41 million records, this approach was 17X faster than the second fastest approach. It took 1/3 of a second to register and cache the subset, the second fastest was 5 seconds.\nTo implement this approach, we need to set three arguments in the spark_csv_read() step:\n\nmemory\ninfer_schema\ncolumns\n\nAgain, this is a recommended approach. The columns argument is needed only if infer_schema is set to FALSE. When memory is set to TRUE it makes Spark load the entire dataset into memory, and setting infer_schema to FALSE prevents Spark from trying to figure out what the schema of the files are. By trying different combinations the memory and infer_schema arguments you may be able to find an approach that may better fits your needs.\n\nReading the schema\nSurprisingly, another critical detail that can easily be overlooked is choosing the right s3 URI scheme. There are two options: s3n and s3a. In most examples and tutorials I found, there was no reason give of why or when to use which one. The article the finally clarified it was this one: https://wiki.apache.org/hadoop/AmazonS3\nThe gist of it is that s3a is the recommended one going forward, especially for Hadoop versions 2.7 and above. This means that if we copy from older examples that used Hadoop 2.6 we would more likely also used s3n thus making data import much, much slower."
  },
  {
    "href": "guides/aws-s3.html#data-import",
    "title": "Using Spark with AWS S3 buckets",
    "section": "Data Import",
    "text": "After the long introduction in the previous section, there is only one point to add about the following code chunk. If there are any NA values in numeric fields, then define the column as character and then convert it on later subsets using dplyr. The data import will fail if it finds any NA values on numeric fields. This is a small trade off in this approach because the next fastest one does not have this issue but is 17X slower at caching subsets.\nflights <- spark_read_csv(sc, \"flights_spark\", \n                          path =  \"s3a://flights-data/full\", \n                          memory = TRUE, \n                          columns = list(\n                            Year = \"character\",\n                            Month = \"character\",\n                            DayofMonth = \"character\",\n                            DayOfWeek = \"character\",\n                            DepTime = \"character\",\n                            CRSDepTime = \"character\",\n                            ArrTime = \"character\",\n                            CRSArrTime = \"character\",\n                            UniqueCarrier = \"character\",\n                            FlightNum = \"character\",\n                            TailNum = \"character\",\n                            ActualElapsedTime = \"character\",\n                            CRSElapsedTime = \"character\",\n                            AirTime = \"character\",\n                            ArrDelay = \"character\",\n                            DepDelay = \"character\",\n                            Origin = \"character\",\n                            Dest = \"character\",\n                            Distance = \"character\",\n                            TaxiIn = \"character\",\n                            TaxiOut = \"character\",\n                            Cancelled = \"character\",\n                            CancellationCode = \"character\",\n                            Diverted = \"character\",\n                            CarrierDelay = \"character\",\n                            WeatherDelay = \"character\",\n                            NASDelay = \"character\",\n                            SecurityDelay = \"character\",\n                            LateAircraftDelay = \"character\"), \n                         infer_schema = FALSE)"
  },
  {
    "href": "guides/aws-s3.html#data-wrangle",
    "title": "Using Spark with AWS S3 buckets",
    "section": "Data Wrangle",
    "text": "There are a few points we need to highlight about the following simple dyplr code:\nBecause there were NAs in the original fields, we have to mutate them to a number. Try coercing any variable as integer instead of numeric, this will save a lot of space when cached to Spark memory. The sdf_register command can be piped at the end of the code. After running the code, a new table will appear in the RStudio IDE’s Spark tab\ntidy_flights <- tbl(sc, \"flights_spark\") %>%\n  mutate(ArrDelay = as.integer(ArrDelay),\n         DepDelay = as.integer(DepDelay),\n         Distance = as.integer(Distance)) %>%\n  filter(!is.na(ArrDelay)) %>%\n  select(DepDelay, ArrDelay, Distance) %>%\n  sdf_register(\"tidy_spark\")\nAfter we use tbl_cache() to load the tidy_spark table into Spark memory. We can see the new table in the Storage page of our Spark session.\ntbl_cache(sc, \"tidy_spark\")"
  },
  {
    "href": "guides/mleap.html#install-mleap",
    "title": "R interface for MLeap",
    "section": "Install mleap",
    "text": "mleap can be installed from CRAN via\n\ninstall.packages(\"mleap\")\n\nor, for the latest development version from GitHub, using\n\ndevtools::install_github(\"rstudio/mleap\")"
  },
  {
    "href": "guides/mleap.html#setup",
    "title": "R interface for MLeap",
    "section": "Setup",
    "text": "Once mleap has been installed, we can install the external dependencies using:\n\nlibrary(mleap)\ninstall_mleap()\n\nAnother dependency of mleap is Maven. If it is already installed, just point mleap to its location:\noptions(maven.home = \"path/to/maven\")\nIf Maven is not yet installed, which is the most likely case, use the following to install it:\ninstall_maven()"
  },
  {
    "href": "guides/mleap.html#create-an-mleap-bundle",
    "title": "R interface for MLeap",
    "section": "Create an MLeap Bundle",
    "text": "Start Spark session using sparklyr ::: {.cell layout.align=“center” indent=” ” original.params.src=“r, message = FALSE” chunk.echo=“false” hash=“mleap_cache/html/unnamed-chunk-10_1333a7d01e60752aa9c1dc08c17edcdb”}\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.2.0\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\n:::\nCreate a fit an ML Pipeline\n::: {.cell layout.align=“center” indent=” ” original.params.src=“r, message = FALSE” chunk.echo=“false” hash=“mleap_cache/html/unnamed-chunk-12_8b4b2652d91ffedf2dc592e621abd0f7”}\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\n\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\n:::\nA transformed data frame with the appropriate schema is required for exporting the Pipeline model ::: {.cell layout.align=“center” indent=” ” original.params.src=“r” chunk.echo=“false” hash=“mleap_cache/html/unnamed-chunk-14_c08209be649dae090975e1898d0e5cfb”}\ntransformed_tbl <- ml_transform(pipeline_model, mtcars_tbl)\n:::\nExport the model using the ml_write_bundle() function from mleap ::: {.cell layout.align=“center” indent=” ” original.params.src=“r” chunk.echo=“false” hash=“mleap_cache/html/unnamed-chunk-16_5b711629a2ca1324be81c1a0670bbf36”}\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, transformed_tbl, model_path)\n:::\nClose Spark session ::: {.cell layout.align=“center” indent=” ” original.params.src=“r” chunk.echo=“false” hash=“mleap_cache/html/unnamed-chunk-18_f881e6a01a95776c75440d404ea196b8”}\nspark_disconnect(sc)\n:::\n\nAt this point, we can share mtcars_model.zip with the deployment/implementation engineers, and they would be able to embed the model in another application. See the MLeap docs for details."
  },
  {
    "href": "guides/mleap.html#test-the-mleap-bundle",
    "title": "R interface for MLeap",
    "section": "Test the mleap bundle",
    "text": "The mleap package also provides R functions for testing that the saved models behave as expected. Here we load the previously saved model:\n\nmodel <- mleap_load_bundle(model_path)\nmodel\n\nTo retrieve the schema associated with the model use the mleap_model_schema() function\n\nmleap_model_schema(model)\n\nThen, we create a new data frame to be scored, and make predictions using the model:\n\nnewdata <- tibble::tribble(\n  ~qsec, ~hp, ~wt,\n  16.2,  101, 2.68,\n  18.1,  99,  3.08\n)\n\n# Transform the data frame\ntransformed_df <- mleap_transform(model, newdata)\ndplyr::glimpse(transformed_df)"
  },
  {
    "href": "guides/caching.html#introduction",
    "title": "Understanding Spark Caching",
    "section": "Introduction",
    "text": "Spark also supports pulling data sets into a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small dataset or when running an iterative algorithm like random forests. Since operations in Spark are lazy, caching can help force computation. Sparklyr tools can be used to cache and uncache DataFrames. The Spark UI will tell you which DataFrames and what percentages are in memory.\nBy using a reproducible example, we will review some of the main configuration settings, commands and command arguments that can be used that can help you get the best out of Spark’s memory management options."
  },
  {
    "href": "guides/caching.html#preparation",
    "title": "Understanding Spark Caching",
    "section": "Preparation",
    "text": "Download Test Data\nThe 2008 and 2007 Flights data from the Statistical Computing site will be used for this exercise. The spark_read_csv supports reading compressed CSV files in a bz2 format, so no additional file preparation is needed.\n\nif(!file.exists(\"2008.csv.bz2\"))\n  {download.file(\"http://stat-computing.org/dataexpo/2009/2008.csv.bz2\", \"2008.csv.bz2\")}\nif(!file.exists(\"2007.csv.bz2\"))\n  {download.file(\"http://stat-computing.org/dataexpo/2009/2007.csv.bz2\", \"2007.csv.bz2\")}\n\n\n\nStart a Spark session\nA local deployment will be used for this example.\n\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Install Spark version 2\nspark_install(version = \"2.0.0\")\n\n# Customize the connection configuration\nconf <- spark_config()\nconf$`sparklyr.shell.driver-memory` <- \"16G\"\n\n# Connect to Spark\nsc <- spark_connect(master = \"local\", config = conf, version = \"2.0.0\")"
  },
  {
    "href": "guides/caching.html#the-memory-argument",
    "title": "Understanding Spark Caching",
    "section": "The Memory Argument",
    "text": "In the spark_read_… functions, the memory argument controls if the data will be loaded into memory as an RDD. Setting it to FALSE means that Spark will essentially map the file, but not make a copy of it in memory. This makes the spark_read_csv command run faster, but the trade off is that any data transformation operations will take much longer.\n\nspark_read_csv(sc, \"flights_spark_2008\", \"2008.csv.bz2\", memory = FALSE)\n\nIn the RStudio IDE, the flights_spark_2008 table now shows up in the Spark tab.\n\n  \n\nTo access the Spark Web UI, click the SparkUI button in the RStudio Spark Tab. As expected, the Storage page shows no tables loaded into memory."
  },
  {
    "href": "guides/caching.html#loading-less-data-into-memory",
    "title": "Understanding Spark Caching",
    "section": "Loading Less Data into Memory",
    "text": "Using the pre-processing capabilities of Spark, the data will be transformed before being loaded into memory. In this section, we will continue to build on the example started in the Spark Read section\n\nLazy Transform\nThe following dplyr script will not be immediately run, so the code is processed quickly. There are some check-ups made, but for the most part it is building a Spark SQL statement in the background.\n\nflights_table <- tbl(sc,\"flights_spark_2008\") %>%\n  mutate(DepDelay = as.numeric(DepDelay),\n         ArrDelay = as.numeric(ArrDelay),\n         DepDelay > 15 , DepDelay < 240,\n         ArrDelay > -60 , ArrDelay < 360, \n         Gain = DepDelay - ArrDelay) %>%\n  filter(ArrDelay > 0) %>%\n  select(Origin, Dest, UniqueCarrier, Distance, DepDelay, ArrDelay, Gain)\n\n\n\nRegister in Spark\nsdf_register will register the resulting Spark SQL in Spark. The results will show up as a table called flights_spark. But a table of the same name is still not loaded into memory in Spark.\n\nsdf_register(flights_table, \"flights_spark\")\n\n\n  \n\n\n\nCache into Memory\nThe tbl_cache command loads the results into an Spark RDD in memory, so any analysis from there on will not need to re-read and re-transform the original file. The resulting Spark RDD is smaller than the original file because the transformations created a smaller data set than the original file.\n\ntbl_cache(sc, \"flights_spark\")\n\n\n  \n\n\n\nDriver Memory\nIn the Executors page of the Spark Web UI, we can see that the Storage Memory is at about half of the 16 gigabytes requested. This is mainly because of a Spark setting called spark.memory.fraction, which reserves by default 40% of the memory requested."
  },
  {
    "href": "guides/caching.html#process-on-the-fly",
    "title": "Understanding Spark Caching",
    "section": "Process on the fly",
    "text": "The plan is to read the Flights 2007 file, combine it with the 2008 file and summarize the data without bringing either file fully into memory.\n\nspark_read_csv(sc, \"flights_spark_2007\" , \"2007.csv.bz2\", memory = FALSE)\n\n\nUnion and Transform\nThe union command is akin to the bind_rows dyplyr command. It will allow us to append the 2007 file to the 2008 file, and as with the previous transform, this script will be evaluated lazily.\n\nall_flights <- tbl(sc, \"flights_spark_2008\") %>%\n  union(tbl(sc, \"flights_spark_2007\")) %>%\n  group_by(Year, Month) %>%\n  tally()\n\n\n\nCollect into R\nWhen receiving a collect command, Spark will execute the SQL statement and send the results back to R in a data frame. In this case, R only loads 24 observations into a data frame called all_flights.\n\nall_flights <- all_flights %>%\n  collect()\n\n\n  \n\n\n\nPlot in R\nNow the smaller data set can be plotted\n\nggplot(data = all_flights, aes(x = Month, y = n/1000, fill = factor(Year))) +\n  geom_area(position = \"dodge\", alpha = 0.5) +\n  geom_line(alpha = 0.4) +\n  scale_fill_brewer(palette = \"Dark2\", name = \"Year\") +\n  scale_x_continuous(breaks = 1:12, labels = c(\"J\",\"F\",\"M\",\"A\",\"M\",\"J\",\"J\",\"A\",\"S\",\"O\",\"N\",\"D\")) +\n  theme_light() +\n  labs(y=\"Number of Flights (Thousands)\", title = \"Number of Flights Year-Over-Year\")"
  },
  {
    "href": "guides/data-lakes.html#audience",
    "title": "Data Science using a Data Lake",
    "section": "Audience",
    "text": "This article aims explain how to take advantage of Apache Spark inside organizations that have already implemented, or are in the process of implementing, a Hadoop based Big Data Lake."
  },
  {
    "href": "guides/data-lakes.html#introduction",
    "title": "Data Science using a Data Lake",
    "section": "Introduction",
    "text": "We have noticed that the types of questions we field after a demo of sparklyr to our customers were more about high-level architecture than how the package works. To answer those questions, we put together a set of slides that illustrate and discuss important concepts, to help customers see where Spark, R, and sparklyr fit in a Big Data Platform implementation. In this article, we’ll review those slides and provide a narrative that will help you better envision how you can take advantage of our products."
  },
  {
    "href": "guides/data-lakes.html#r-for-data-science",
    "title": "Data Science using a Data Lake",
    "section": "R for Data Science",
    "text": "It is very important to preface the Use Case review with some background information about where RStudio focuses its efforts when developing packages and products. Many vendors offer R integration, but in most cases, what this means is that they will add a model built in R to their pipeline or interface, and pass new inputs to that model to generate outputs that can be used in the next step in the pipeline, or in a calculation for the interface.\nIn contrast, our focus is on the process that happens before that: the discipline that produces the model, meaning Data Science.\n\n\n\n\n\nIn their R for Data Science book, Hadley Wickham and Garrett Grolemund provide a great diagram that nicely illustrates the Data Science process: We import data into memory with R and clean and tidy the data. Then we go into a cyclical process called understand, which helps us to get to know our data, and hopefully find the answer to the question we started with. This cycle typically involves making transformations to our tidied data, using the transformed data to fit models, and visualizing results. Once we find an answer to our question, we then communicate the results.\nData Scientists like using R because it allows them to complete a Data Science project from beginning to end inside the R environment, and in memory."
  },
  {
    "href": "guides/data-lakes.html#hadoop-as-a-data-source",
    "title": "Data Science using a Data Lake",
    "section": "Hadoop as a Data Source",
    "text": "What happens when the data that needs to be analyzed is very large, like the data sets found in a Hadoop cluster? It would be impossible to fit these in memory, so workarounds are normally used. Possible workarounds include using a comparatively minuscule data sample, or download as much data as possible. This becomes disruptive to Data Scientists because either the small sample may not be representative, or they have to wait a long time in every iteration of importing a lot of data, exploring a lot of data, and modeling a lot of data."
  },
  {
    "href": "guides/data-lakes.html#spark-as-an-analysis-engine",
    "title": "Data Science using a Data Lake",
    "section": "Spark as an Analysis Engine",
    "text": "We noticed that a very important mental leap to make is to see Spark not just as a gateway to Hadoop (or worse, as an additional data source), but as a computing engine. As such, it is an excellent vehicle to scale our analytics. Spark has many capabilities that makes it ideal for Data Science in a data lake, such as close integration with Hadoop and Hive, the ability to cache data into memory across multiple nodes, data transformers, and its Machine Learning libraries.\nThe approach, then, is to push as much compute to the cluster as possible, using R primarily as an interface to Spark for the Data Scientist, which will then collect as few results as possible back into R memory, mostly to visualize and communicate. As shown in the slide, the more import, tidy, transform and modeling work we can push to Spark, the faster we can analyze very large data sets."
  },
  {
    "href": "guides/data-lakes.html#cluster-setup",
    "title": "Data Science using a Data Lake",
    "section": "Cluster Setup",
    "text": "Here is an illustration of how R, RStudio, and sparklyr can be added to the YARN managed cluster. The highlights are:\n\nR, RStudio, and sparklyr need to be installed on one node only, typically an edge node\nThe Data Scientist can access R, Spark, and the cluster via a web browser by navigating to the RStudio IDE inside the edge node"
  },
  {
    "href": "guides/data-lakes.html#considerations",
    "title": "Data Science using a Data Lake",
    "section": "Considerations",
    "text": "There are some important considerations to keep in mind when combining your Data Lake and R for large scale analytics:\n\nSpark’s Machine Learning libraries may not contain specific models that a Data Scientist needs. For those cases, workarounds would include using a sparklyr extension like H2O, or collecting a sample of the data into R memory for modeling.\nSpark does not have visualization functionality; currently, the best approach is to collect pre-calculated data into R for plotting. A good way to drastically reduce the number of rows being brought back into memory is to push as much computation as possible to Spark, and return just the results to be plotted. For example, the bins of a Histogram can be calculated in Spark, so that only the final bucket values would be returned to R for visualization. Here is sample code for such a scenario: sparkDemos/Histogram\nA particular use case may require a different way of scaling analytics. We have published an article that provides a very good overview of the options that are available: R for Enterprise: How to Scale Your Analytics Using R"
  },
  {
    "href": "guides/data-lakes.html#r-for-data-science-toolchain-with-spark",
    "title": "Data Science using a Data Lake",
    "section": "R for Data Science Toolchain with Spark",
    "text": "With sparklyr, the Data Scientist will be able to access the Data Lake’s data, and also gain an additional, very powerful understand layer via Spark. sparklyr, along with the RStudio IDE and the tidyverse packages, provides the Data Scientist with an excellent toolbox to analyze data, big and small."
  },
  {
    "href": "examples/cloudera-aws.html#hive-data",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Hive data",
    "text": "For this demo, we have created and populated 3 tables in Hive. The table names are: flights, airlines and airports. Using Hue, we can see the loaded tables. For the links to the data files and their Hive import scripts please see Appendix A."
  },
  {
    "href": "examples/cloudera-aws.html#cache-the-tables-into-memory",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Cache the tables into memory",
    "text": "Use tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame.\n\n# Cache flights Hive table into Spark\ntbl_cache(sc, 'flights')\nflights_tbl <- tbl(sc, 'flights')\n\n# Cache airlines Hive table into Spark\ntbl_cache(sc, 'airlines')\nairlines_tbl <- tbl(sc, 'airlines')\n\n# Cache airports Hive table into Spark\ntbl_cache(sc, 'airports')\nairports_tbl <- tbl(sc, 'airports')"
  },
  {
    "href": "examples/cloudera-aws.html#create-a-model-data-set",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Create a model data set",
    "text": "Filter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight.\n\n# Filter records and create target variable 'gain'\nmodel_data <- flights_tbl %>%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>%\n  filter(depdelay > 15 & depdelay < 240) %>%\n  filter(arrdelay > -60 & arrdelay < 360) %>%\n  filter(year >= 2003 & year <= 2007) %>%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>%\n  mutate(gain = depdelay - arrdelay) %>%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain)\n\n# Summarize data by carrier\nmodel_data %>%\n  group_by(uniquecarrier) %>%\n  summarize(description = min(description), gain=mean(gain), \n            distance=mean(distance), depdelay=mean(depdelay)) %>%\n  select(description, gain, distance, depdelay) %>%\n  arrange(gain)\n\nSource:   query [?? x 4]\nDatabase: spark connection master=yarn-client app=sparklyr local=FALSE\n\n                    description       gain  distance depdelay\n                          <chr>      <dbl>     <dbl>    <dbl>\n1        ATA Airlines d/b/a ATA -5.5679651 1240.7219 61.84391\n2       Northwest Airlines Inc. -3.1134556  779.1926 48.84979\n3                     Envoy Air -2.2056576  437.0883 54.54923\n4             PSA Airlines Inc. -1.9267647  500.6955 55.60335\n5  ExpressJet Airlines Inc. (1) -1.5886314  537.3077 61.58386\n6               JetBlue Airways -1.3742524 1087.2337 59.80750\n7         SkyWest Airlines Inc. -1.1265678  419.6489 54.04198\n8          Delta Air Lines Inc. -0.9829374  956.9576 50.19338\n9        American Airlines Inc. -0.9631200 1066.8396 56.78222\n10  AirTran Airways Corporation -0.9411572  665.6574 53.38363\n# ... with more rows"
  },
  {
    "href": "examples/cloudera-aws.html#train-a-linear-model",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Train a linear model",
    "text": "Predict time gained or lost in flight as a function of distance, departure delay, and airline carrier.\n\n# Partition the data into training and validation sets\nmodel_partition <- model_data %>% \n  sdf_partition(train = 0.8, valid = 0.2, seed = 5555)\n\n# Fit a linear model\nml1 <- model_partition$train %>%\n  ml_linear_regression(gain ~ distance + depdelay + uniquecarrier)\n\n# Summarize the linear model\nsummary(ml1)\n\nCall: ml_linear_regression(., gain ~ distance + depdelay + uniquecarrier)\n\nDeviance Residuals: (approximate):\n     Min       1Q   Median       3Q      Max \n-302.343   -5.669    2.714    9.832  104.130 \n\nCoefficients:\n                    Estimate  Std. Error  t value  Pr(>|t|)    \n(Intercept)      -1.26566581  0.10385870 -12.1864 < 2.2e-16 ***\ndistance          0.00308711  0.00002404 128.4155 < 2.2e-16 ***\ndepdelay         -0.01397013  0.00028816 -48.4812 < 2.2e-16 ***\nuniquecarrier_AA -2.18483090  0.10985406 -19.8885 < 2.2e-16 ***\nuniquecarrier_AQ  3.14330242  0.29114487  10.7964 < 2.2e-16 ***\nuniquecarrier_AS  0.09210380  0.12825003   0.7182 0.4726598    \nuniquecarrier_B6 -2.66988794  0.12682192 -21.0523 < 2.2e-16 ***\nuniquecarrier_CO -1.11611186  0.11795564  -9.4621 < 2.2e-16 ***\nuniquecarrier_DL -1.95206198  0.11431110 -17.0767 < 2.2e-16 ***\nuniquecarrier_EV  1.70420830  0.11337215  15.0320 < 2.2e-16 ***\nuniquecarrier_F9 -1.03178176  0.15384863  -6.7065 1.994e-11 ***\nuniquecarrier_FL -0.99574060  0.12034738  -8.2739 2.220e-16 ***\nuniquecarrier_HA -1.16970713  0.34894788  -3.3521 0.0008020 ***\nuniquecarrier_MQ -1.55569040  0.10975613 -14.1741 < 2.2e-16 ***\nuniquecarrier_NW -3.58502418  0.11534938 -31.0797 < 2.2e-16 ***\nuniquecarrier_OH -1.40654797  0.12034858 -11.6873 < 2.2e-16 ***\nuniquecarrier_OO -0.39069404  0.11132164  -3.5096 0.0004488 ***\nuniquecarrier_TZ -7.26285217  0.34428509 -21.0955 < 2.2e-16 ***\nuniquecarrier_UA -0.56995737  0.11186757  -5.0949 3.489e-07 ***\nuniquecarrier_US -0.52000028  0.11218498  -4.6352 3.566e-06 ***\nuniquecarrier_WN  4.22838982  0.10629405  39.7801 < 2.2e-16 ***\nuniquecarrier_XE -1.13836940  0.11332176 -10.0455 < 2.2e-16 ***\nuniquecarrier_YV  3.17149538  0.11709253  27.0854 < 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nR-Squared: 0.02301\nRoot Mean Squared Error: 17.83"
  },
  {
    "href": "examples/cloudera-aws.html#assess-model-performance",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Assess model performance",
    "text": "Compare the model performance using the validation data.\n\n# Calculate average gains by predicted decile\nmodel_deciles <- lapply(model_partition, function(x) {\n  sdf_predict(ml1, x) %>%\n    mutate(decile = ntile(desc(prediction), 10)) %>%\n    group_by(decile) %>%\n    summarize(gain = mean(gain)) %>%\n    select(decile, gain) %>%\n    collect()\n})\n\n# Create a summary dataset for plotting\ndeciles <- rbind(\n  data.frame(data = 'train', model_deciles$train),\n  data.frame(data = 'valid', model_deciles$valid),\n  make.row.names = FALSE\n)\n\n# Plot average gains by predicted decile\ndeciles %>%\n  ggplot(aes(factor(decile), gain, fill = data)) +\n  geom_bar(stat = 'identity', position = 'dodge') +\n  labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes')"
  },
  {
    "href": "examples/cloudera-aws.html#visualize-predictions",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Visualize predictions",
    "text": "Compare actual gains to predicted gains for an out of time sample.\n\n# Select data from an out of time sample\ndata_2008 <- flights_tbl %>%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>%\n  filter(depdelay > 15 & depdelay < 240) %>%\n  filter(arrdelay > -60 & arrdelay < 360) %>%\n  filter(year == 2008) %>%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>%\n  mutate(gain = depdelay - arrdelay) %>%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest)\n\n# Summarize data by carrier\ncarrier <- sdf_predict(ml1, data_2008) %>%\n  group_by(description) %>%\n  summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>%\n  filter(freq > 10000) %>%\n  collect\n\n# Plot actual gains and predicted gains by airline carrier\nggplot(carrier, aes(gain, prediction)) + \n  geom_point(alpha = 0.75, color = 'red', shape = 3) +\n  geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') +\n  geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) +\n  labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted')\n\n\n\n\nSome carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights."
  },
  {
    "href": "examples/cloudera-aws.html#build-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Build dashboard",
    "text": "Aggregate the scored data by origin, destination, and airline. Save the aggregated data.\n\n# Summarize by origin, destination, and carrier\nsummary_2008 <- sdf_predict(ml1, data_2008) %>%\n  rename(carrier = uniquecarrier, airline = description) %>%\n  group_by(origin, dest, carrier, airline) %>%\n  summarize(\n    flights = n(),\n    distance = mean(distance),\n    avg_dep_delay = mean(depdelay),\n    avg_arr_delay = mean(arrdelay),\n    avg_gain = mean(gain),\n    pred_gain = mean(prediction)\n    )\n\n# Collect and save objects\npred_data <- collect(summary_2008)\nairports <- collect(select(airports_tbl, name, faa, lat, lon))\nml1_summary <- capture.output(summary(ml1))\nsave(pred_data, airports, ml1_summary, file = 'flights_pred_2008.RData')"
  },
  {
    "href": "examples/cloudera-aws.html#publish-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Publish dashboard",
    "text": "Use the saved data to build an R Markdown flexdashboard. Publish the flexdashboard\n\n#Appendix\n\nAppendix A - Data files\nRun the following script to download data from the web onto your master node. Download the yearly flight data and the airlines lookup table.\n\n# Make download directory\nmkdir /tmp/flights\n\n# Download flight data by year\nfor i in {2006..2008}\n  do\n    echo \"$(date) $i Download\"\n    fnam=$i.csv.bz2\n    wget -O /tmp/flights/$fnam http://stat-computing.org/dataexpo/2009/$fnam\n    echo \"$(date) $i Unzip\"\n    bunzip2 /tmp/flights/$fnam\n  done\n\n# Download airline carrier data\nwget -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS\n\n# Download airports data\nwget -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\n\n\n\nHive tables\nWe used the Hue interface, logged in as ‘admin’ to load the data into HDFS and then into Hive.\nCREATE EXTERNAL TABLE IF NOT EXISTS flights\n(\nyear int,\nmonth int,\ndayofmonth int,\ndayofweek int,\ndeptime int,\ncrsdeptime int,\narrtime int, \ncrsarrtime int,\nuniquecarrier string,\nflightnum int,\ntailnum string, \nactualelapsedtime int,\ncrselapsedtime int,\nairtime string,\narrdelay int,\ndepdelay int, \norigin string,\ndest string,\ndistance int,\ntaxiin string,\ntaxiout string,\ncancelled int,\ncancellationcode string,\ndiverted int,\ncarrierdelay string,\nweatherdelay string,\nnasdelay string,\nsecuritydelay string,\nlateaircraftdelay string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLINES TERMINATED BY '\\n'\nTBLPROPERTIES(\"skip.header.line.count\"=\"1\");\nLOAD DATA INPATH '/user/admin/flights/2006.csv/' INTO TABLE flights;\nLOAD DATA INPATH '/user/admin/flights/2007.csv/' INTO TABLE flights;\nLOAD DATA INPATH '/user/admin/flights/2008.csv/' INTO TABLE flights;\n# Create metadata for airlines\nCREATE EXTERNAL TABLE IF NOT EXISTS airlines\n(\nCode string,\nDescription string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE\ntblproperties(\"skip.header.line.count\"=\"1\");\nLOAD DATA INPATH '/user/admin/L_UNIQUE_CARRIERS.csv' INTO TABLE airlines;\nCREATE EXTERNAL TABLE IF NOT EXISTS airports\n(\nid string,\nname string,\ncity string,\ncountry string,\nfaa string,\nicao string,\nlat double,\nlon double,\nalt int,\ntz_offset double,\ndst string,\ntz_name string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE;\nLOAD DATA INPATH '/user/admin/airports.dat' INTO TABLE airports;"
  },
  {
    "href": "examples/databricks-cluster-odbc.html#overview",
    "title": "Using an ODBC connection with Databricks",
    "section": "Overview",
    "text": "This configuration details how to connect to Databricks using an ODBC connection. With this setup, R can connect to Databricks using the odbc and DBI R packages. This type of configuration is the recommended approach for connecting to Databricks from RStudio Connect and can also be used from RStudio Workbench."
  },
  {
    "href": "examples/databricks-cluster-odbc.html#advantages-and-limitations",
    "title": "Using an ODBC connection with Databricks",
    "section": "Advantages and limitations",
    "text": "Advantages:\n\nODBC connections tend to be more stable than Spark connections. This is especially beneficial for content published to RStudio Connect.\nIf code is developed using a Spark connection and sparklyr, it is easy to swap out the connection type for an ODBC connection and the remaining code will still run.\nThe Spark ODBC driver provided by Databricks was benchmarked against a native Spark connection and the performance of the two is very comparable.\n\nLimitations: - Not all Spark features and functions are available through an ODBC connection."
  },
  {
    "href": "examples/databricks-cluster-odbc.html#driver-installation",
    "title": "Using an ODBC connection with Databricks",
    "section": "Driver installation",
    "text": "Download and install the Spark ODBC driver from Databricks"
  },
  {
    "href": "examples/databricks-cluster-odbc.html#driver-configuration",
    "title": "Using an ODBC connection with Databricks",
    "section": "Driver configuration",
    "text": "Create a DSN for Databricks.\n[Databricks-Spark]\nDriver=Simba\nServer=<server-hostname>\nHOST=<server-hostname>\nPORT=<port>\nSparkServerType=3\nSchema=default\nThriftTransport=2\nSSL=1\nAuthMech=3\nUID=token\nPWD=<personal-access-token>\nHTTPPath=<http-path>"
  },
  {
    "href": "examples/databricks-cluster-odbc.html#connect-to-databricks",
    "title": "Using an ODBC connection with Databricks",
    "section": "Connect to Databricks",
    "text": "The connection can be tested from the command line using isql -v Databricks-Spark where Databricks-Spark is the DSN name for the connection. If that connects successfully, then the following code can be used to create a connection from an R session:\nlibrary(DBI)\nlibrary(odbc)\n\ncon <- dbConnect(odbc(), \"Databricks-Spark\")"
  },
  {
    "href": "examples/databricks-cluster-odbc.html#additional-information",
    "title": "Using an ODBC connection with Databricks",
    "section": "Additional information",
    "text": "For more information about ODBC connections from R, please visit db.rstudio.com."
  },
  {
    "href": "examples/databricks-cluster-local.html#overview",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Overview",
    "text": "If the recommended path of connecting to Spark remotely with Databricks Connect does not apply to your use case, then you can install RStudio Workbench directly within a Databricks cluster as described in the sections below.\nWith this configuration, RStudio Workbench is installed on the Spark driver node and allows users to work locally with Spark using sparklyr.\n\nThis configuration can result in increased complexity, limited connectivity to other storage and compute resources, resource contention between RStudio Workbench and Databricks, and maintenance concerns due to the ephemeral nature of Databricks clusters.\nFor additional details, refer to the FAQ for RStudio in the Databricks Documentation."
  },
  {
    "href": "examples/databricks-cluster-local.html#advantages-and-limitations",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Advantages and limitations",
    "text": "Advantages:\n\nAbility for users to connect sparklyr to Spark without configuring remote connectivity\nProvides a high-bandwidth connection between R and the Spark JVM processes because they are running on the same machine\nCan load data from the cluster directly into an R session since RStudio Workbench is installed within the Databricks cluster\n\nLimitations:\n\nIf the Databricks cluster is restarted or terminated, then the instance of RStudio Workbench will be terminated and its configuration will be lost\nIf users do not persist their code through version control or the Databricks File System, then you risk losing user’s work if the cluster is restarted or terminated\nRStudio Workbench (and other RStudio products) installed within a Databricks cluster will be limited to the compute resources and lifecycle of that particular Spark cluster\nNon-Spark jobs will use CPU and RAM resources within the Databricks cluster\nNeed to install one instance of RStudio Workbench per Spark cluster that you want to run jobs on"
  },
  {
    "href": "examples/databricks-cluster-local.html#requirements",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Requirements",
    "text": "A running Databricks cluster with a runtime version 4.1 or above\nThe cluster must not have “table access control” or “automatic termination” enabled\nYou must have “Can Attach To” permission for the Databricks cluster"
  },
  {
    "href": "examples/databricks-cluster-local.html#preparation",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Preparation",
    "text": "The following steps walk through the process to install RStudio Workbench on the Spark driver node within your Databricks cluster.\nThe recommended method for installing RStudio Workbench to the Spark driver node is via SSH. However, an alternative method is available if you are not able to access the Spark driver node via SSH."
  },
  {
    "href": "examples/databricks-cluster-local.html#configure-ssh-access-to-the-spark-driver-node",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Configure SSH access to the Spark driver node",
    "text": "Configure SSH access to the Spark driver node in Databricks by following the steps in the SSH access to clusters section of the Databricks Cluster configurations documentation.\nNote: If you are unable to configure SSH access or connect to the Spark driver node via SSH, then you can follow the steps in the Get started with RStudio Workbench section of the RStudio on Databricks documentation to install RStudio Workbench from a Databricks notebook, then skip to the access RStudio Workbench section of this documentation."
  },
  {
    "href": "examples/databricks-cluster-local.html#connect-to-the-spark-driver-node-via-ssh",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Connect to the Spark driver node via SSH",
    "text": "Connect to the Spark driver node via SSH on port 2200 by using the following command on your local machine:\nssh ubuntu@<spark-driver-node-address> -p 2200 -i <path-to-private-SSH-key>\nReplace <spark-driver-node-address> with the DNS name or IP address of the Spark driver node, and <path-to-private-SSH-key> with the path to your private SSH key on your local machine."
  },
  {
    "href": "examples/databricks-cluster-local.html#install-rstudio-workbench-on-the-spark-driver-node",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Install RStudio Workbench on the Spark driver node",
    "text": "After you SSH into the Spark driver node, then you can follow the typical steps to install RStudio Workbench in the RStudio documentation. In the installation steps, you can select Ubuntu as the target Linux distribution."
  },
  {
    "href": "examples/databricks-cluster-local.html#configure-rstudio-workbench",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Configure RStudio Workbench",
    "text": "The following configuration steps are required to be able to use RStudio Workbench with Databricks.\nAdd the following configuration lines to /etc/rstudio/rserver.conf to use proxied authentication with Databricks and enable the administrator dashboard:\nauth-proxy=1\nauth-proxy-user-header-rewrite=^(.*)$ $1\nauth-proxy-sign-in-url=<domain>/login.html\nadmin-enabled=1\nAdd the following configuration line to /etc/rstudio/rsession-profile to set the PATH to be used with RStudio Workbench:\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:$PATH\nAdd the following configuration lines to /etc/rstudio/rsession.conf to configure sessions in RStudio Workbench to work with Databricks:\nsession-rprofile-on-resume-default=1\nallow-terminal-websockets=0\nRestart RStudio Workbench:\nsudo rstudio-server restart"
  },
  {
    "href": "examples/databricks-cluster-local.html#access-rstudio-workbench",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Access RStudio Workbench",
    "text": "From the Databricks console, click on the Databricks cluster that you want to work with:\n\nFrom within the Databricks cluster, click on the Apps tab:\n\nClick on the Set up RStudio button:\n\nTo access RStudio Workbench, click on the link to Open RStudio:\n\nIf you configured proxied authentication in RStudio Workbench as described in the previous section, then you do not need to use the username or password that is displayed. Instead, RStudio Workbench will automatically login and start a new RStudio session as your logged-in Databricks user:\n\nOther users can access RStudio Workbench from the Databricks console by following the same steps described above. You do not need to create those users in RStudio Workbench or their home directory beforehand."
  },
  {
    "href": "examples/databricks-cluster-local.html#configure-sparklyr",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Configure sparklyr",
    "text": "Use the following R code to establish a connection from sparklyr to the Databricks cluster:\nSparkR::sparkR.session()\nlibrary(sparklyr)\nsc <- spark_connect(method = \"databricks\")"
  },
  {
    "href": "examples/databricks-cluster-local.html#additional-information",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Additional information",
    "text": "For more information on using RStudio Workbench inside of Databricks, refer to the sections on RStudio on Databricks (AWS) or RStudio on Databricks (Azure) in the Databricks documentation."
  },
  {
    "href": "examples/qubole-cluster.html#overview",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Overview",
    "text": "Qubole users can request access to RStudio Server Pro. This allows users to use sparklyr to interact directly with Spark from within the Qubole cluster."
  },
  {
    "href": "examples/qubole-cluster.html#advantages-and-limitations",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Advantages and limitations",
    "text": "Advantages:\n\nAbility for users to connect sparklyr directly to Spark within Qubole\nProvides a high-bandwidth connection between R and the Spark JVM processes because they are running on the same machine\nCan load data from the cluster directly into an R session since RStudio Workbench is installed within the Qubole cluster\nA unique, persistent home directory for each user\n\nLimitations:\n\nPersistent packages must be managed using Qubole Environments, not directly from within RStudio\nRStudio Workbench installed within a Qubole cluster will be limited to the compute resources and lifecycle of that particular Spark cluster\nNon-Spark jobs will use CPU and RAM resources within the Qubole cluster"
  },
  {
    "href": "examples/qubole-cluster.html#access-rstudio-workbench",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Access RStudio Workbench",
    "text": "RStudio Workbench can be accessed from the cluster resources menu:"
  },
  {
    "href": "examples/qubole-cluster.html#use-sparklyr",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Use sparklyr",
    "text": "Use the following R code to establish a connection from sparklyr to the Qubole cluster:\nlibrary(sparklyr)\nsc <- spark_connect(method = \"qubole\")"
  },
  {
    "href": "examples/qubole-cluster.html#additional-information",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Additional information",
    "text": "For more information on using RStudio Workbench inside of Qubole, refer to the Qubole documentation."
  },
  {
    "href": "examples/databricks-cluster-remote.html#overview",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Overview",
    "text": "With this configuration, RStudio Workbench is installed outside of the Spark cluster and allows users to connect to Spark remotely using sparklyr with Databricks Connect.\n\nThis is the recommended configuration because it targets separate environments, involves a typical configuration process, avoids resource contention, and allows RStudio Workbench to connect to Databricks as well as other remote storage and compute resources."
  },
  {
    "href": "examples/databricks-cluster-remote.html#advantages-and-limitations",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Advantages and limitations",
    "text": "Advantages:\n\nRStudio Workbench will remain functional if Databricks clusters are terminated\nProvides the ability to communicate with one or more Databricks clusters as a remote compute resource\nAvoids resource contention between RStudio Workbench and Databricks\n\nLimitations:\n\nDatabricks Connect does not currently support the following APIs from sparklyr: Broom APIs, Streaming APIs, Broadcast APIs, Most MLlib APIs, csv_file serialization mode, and the spark_submit API\nDatabricks Connect does not support structured streaming\nDatabricks Connect does not support running arbitrary code that is not a part of a Spark job on the remote cluster\nDatabricks Connect does not support Scala, Python, and R APIs for Delta table operations\nDatabricks Connect does not support most utilities in Databricks Utilities. However, dbutils.fs and dbutils.secrets are supported\n\nFor more information on the limitations of Databricks Connect, refer to the Limitation section of the Databricks Connect documentation."
  },
  {
    "href": "examples/databricks-cluster-remote.html#requirements",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Requirements",
    "text": "RStudio Workbench installed outside of the Databricks cluster\nJava 8 installed on the machine with RStudio Workbench\nA running Databricks cluster with a runtime version 5.5 or above"
  },
  {
    "href": "examples/databricks-cluster-remote.html#install-python",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Install Python",
    "text": "The Databricks Connect client is provided as a Python library. The minor version of your Python installation must be the same as the minor Python version of your Databricks cluster.\nRefer to the steps in the install Python section of the RStudio Documentation to install Python on the same server where RStudio Workbench is installed.\nNote that you can either install Python for all users in a global location (as an administrator) or in a home directory (as an end user)."
  },
  {
    "href": "examples/databricks-cluster-remote.html#install-databricks-connect",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Install Databricks Connect",
    "text": "Run the following command to install Databricks Connect on the server with RStudio Workbench:\npip install -U databricks-connect==6.3.*  # or a different version to match your Databricks cluster\nNote that you can either install this library for all users in a global Python environment (as an administrator) or for an individual user in their Python environment (e.g., using the pip --user option or installing into a conda environment or virtual environment)."
  },
  {
    "href": "examples/databricks-cluster-remote.html#configure-databricks-connect",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Configure Databricks Connect",
    "text": "To configure the Databricks Connect client, you can run the following command in a terminal when logged in as a user in RStudio Workbench:\ndatabricks-connect configure\nIn the prompts that follow, enter the following information:\n\n\n\n\n\n\n\n\nParameter\nDescription\nExample Value\n\n\n\n\nDatabricks Host\nBase address of your Databricks console URL\nhttps://dbc-01234567-89ab.cloud.databricks.com\n\n\nDatabricks Token\nUser token generated from the Databricks Console under your “User Settings”\ndapi24g06bdd96f2700b09dd336d5444c1yz\n\n\nCluster ID\nCluster ID in the Databricks console under Advanced Options > Tags > ClusterId\n0308-033548-colt989\n\n\nOrg ID\nFound in the ?o=orgId portion of your Databricks Console URL\n8498623428173033\n\n\nPort\nThe port that Databricks Connect connects to\n15001\n\n\n\nAfter you’ve completed the configuration process for Databricks Connect, you can run the following command in a terminal to test the connectivity of Databricks Connect to your Databricks cluster:\ndatabricks-connect test"
  },
  {
    "href": "examples/databricks-cluster-remote.html#install-sparklyr",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Install sparklyr",
    "text": "The integration of sparklyr with Databricks Connect is currently being added to the development version of sparklyr. To use this functionality now, you’ll need to install the development version of sparklyr by running the following command in an R console:\ndevtools::install_github(\"sparklyr/sparklyr\")"
  },
  {
    "href": "examples/databricks-cluster-remote.html#install-spark",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Install Spark",
    "text": "To work with a remote Databricks cluster, you need to have a local installation of Spark that matches the version of Spark on the Databricks Cluster.\nYou can install Spark by running the following command in an R console:\nlibrary(sparklyr)\nsparklyr::spark_install()\nYou can specify the version of Spark to install along with other options. Refer to the spark_install() options in the sparklyr reference documentation for more information."
  },
  {
    "href": "examples/databricks-cluster-remote.html#use-sparklyr",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Use sparklyr",
    "text": "In order to connect to Databricks using sparklyr and databricks-connect, SPARK_HOME must be set to the output of the databricks-connect get-spark-home command.\nYou can set SPARK_HOME as an environment variable or directly within spark_connect(). The following R code demonstrates connecting to Databricks, copying some data into the cluster, summarizing that data using sparklyr, and disconnecting:\nlibrary(sparklyr)\nlibrary(dplyr)\n\ndatabricks_connect_spark_home <- system(\"databricks-connect get-spark-home\", intern = TRUE)\nsc <- spark_connect(method = \"databricks\", spark_home = databricks_connect_spark_home)\n\ncars_tbl <- copy_to(sc, mtcars, overwrite = TRUE)\n\ncars_tbl %>% \n  group_by(cyl) %>% \n  summarise(mean_mpg = mean(mpg, na.rm = TRUE),\n            mean_hp  = mean(hp, na.rm = TRUE))\n\nspark_disconnect(sc)"
  },
  {
    "href": "examples/databricks-cluster-remote.html#additional-information",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Additional information",
    "text": "For more information on the setup, configuration, troubleshooting, and limitations of Databricks Connect, refer to the Databricks Connect section of the Databricks documentation."
  },
  {
    "href": "examples/yarn-cluster-emr.html#set-up-the-cluster",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Set up the cluster",
    "text": "This demonstration uses Amazon Web Services (AWS), but it could just as easily use Microsot, Google, or any other provider. We will use Elastic Map Reduce (EMR) to easily set up a cluster with two core nodes and one master node. Nodes use virtual servers from the Elastic Compute Cloud (EC2). Note: There is no free tier for EMR, charges will apply.\nBefore beginning this setup we assume you have:\n\nFamiliarity with and access to an AWS account\nFamiliarity with basic linux commands\nSudo privileges in order to install software from the command line\n\n\n\n\n\nBuild an EMR cluster\nBefore beginning the EMR wizard setup, make sure you create the following in AWS:\n\nAn AWS key pair (.pem key) so you can SSH into the EC2 master node\nA security group that gives you access to port 22 on your IP and port 8787 from anywhere\n\n\n\n\nStep 1: Select software\nMake sure to select Hive and Spark as part of the install. Note that by choosing Spark, R will also be installed on the master node as part of the distribution.\n\n\n\n\nStep 2: Select hardware\nInstall 2 core nodes and one master node with m3.xlarge 80 GiB storage per node. You can easily increase the number of nodes later.\n\n\n\n\nStep 3: Select general cluster settings\nClick next on the general cluster settings.\n\n\n\n\nStep 4: Select security\nEnter your EC2 key pair and security group. Make sure the security group has ports 22 and 8787 open.\n\n\n\n\n\nConnect to EMR\nThe cluster page will give you details about your EMR cluster and instructions on connecting.\n\nConnect to the master node via SSH using your key pair. Once you connect you will see the EMR welcome. ::: {.cell original.params.src=“bash eval=FALSE” chunk.echo=“false”}\n# Log in to master node\nssh -i ~/spark-demo.pem hadoop@ec2-52-10-102-11.us-west-2.compute.amazonaws.com\n:::\n\n\n\nInstall RStudio Server\nEMR uses Amazon Linux which is based on Centos. Update your master node and install dependencies that will be used by R packages.\n\n# Update\nsudo yum update\nsudo yum install libcurl-devel openssl-devel # used for devtools\n\nThe installation of RStudio Server is easy. Download the preview version of RStudio and install on the master node.\n\n# Install RStudio Server\nwget -P /tmp https://s3.amazonaws.com/rstudio-dailybuilds/rstudio-server-rhel-0.99.1266-x86_64.rpm\nsudo yum install --nogpgcheck /tmp/rstudio-server-rhel-0.99.1266-x86_64.rpm\n\n\n\nCreate a User\nCreate a user called rstudio-user that will perform the data analysis. Create a user directory for rstudio-user on HDFS with the hadoop fs command.\n\n# Make User\nsudo useradd -m rstudio-user\nsudo passwd rstudio-user\n\n# Create new directory in hdfs\nhadoop fs -mkdir /user/rstudio-user\nhadoop fs -chmod 777 /user/rstudio-user"
  },
  {
    "href": "examples/yarn-cluster-emr.html#download-flights-data",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Download flights data",
    "text": "The flights data is a well known data source representing 123 million flights over 22 years. It consumes roughly 12 GiB of storage in uncompressed CSV format in yearly files.\n\nSwitch User\nFor data loading and analysis, make sure you are logged in as regular user.\n\n# create directories on hdfs for new user\nhadoop fs -mkdir /user/rstudio-user\nhadoop fs -chmod 777 /user/rstudio-user\n\n# switch user\nsu rstudio-user\n\n\n\nDownload data\nRun the following script to download data from the web onto your master node. Download the yearly flight data and the airlines lookup table.\n\n# Make download directory\nmkdir /tmp/flights\n\n# Download flight data by year\nfor i in {1987..2008}\n  do\n    echo \"$(date) $i Download\"\n    fnam=$i.csv.bz2\n    wget -O /tmp/flights/$fnam http://stat-computing.org/dataexpo/2009/$fnam\n    echo \"$(date) $i Unzip\"\n    bunzip2 /tmp/flights/$fnam\n  done\n\n# Download airline carrier data\nwget -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS\n\n# Download airports data\nwget -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\n\n\n\nDistribute into HDFS\nCopy data into HDFS using the hadoop fs command.\n\n# Copy flight data to HDFS\nhadoop fs -mkdir /user/rstudio-user/flights/\nhadoop fs -put /tmp/flights /user/rstudio-user/\n\n# Copy airline data to HDFS\nhadoop fs -mkdir /user/rstudio-user/airlines/\nhadoop fs -put /tmp/airlines.csv /user/rstudio-user/airlines\n\n# Copy airport data to HDFS\nhadoop fs -mkdir /user/rstudio-user/airports/\nhadoop fs -put /tmp/airports.csv /user/rstudio-user/airports"
  },
  {
    "href": "examples/yarn-cluster-emr.html#create-hive-tables",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Create Hive tables",
    "text": "Launch Hive from the command line.\n\n# Open Hive prompt\nhive\n\nCreate the metadata that will structure the flights table. Load data into the Hive table.\n# Create metadata for flights\nCREATE EXTERNAL TABLE IF NOT EXISTS flights\n(\nyear int,\nmonth int,\ndayofmonth int,\ndayofweek int,\ndeptime int,\ncrsdeptime int,\narrtime int, \ncrsarrtime int,\nuniquecarrier string,\nflightnum int,\ntailnum string, \nactualelapsedtime int,\ncrselapsedtime int,\nairtime string,\narrdelay int,\ndepdelay int, \norigin string,\ndest string,\ndistance int,\ntaxiin string,\ntaxiout string,\ncancelled int,\ncancellationcode string,\ndiverted int,\ncarrierdelay string,\nweatherdelay string,\nnasdelay string,\nsecuritydelay string,\nlateaircraftdelay string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLINES TERMINATED BY '\\n'\nTBLPROPERTIES(\"skip.header.line.count\"=\"1\");\n\n# Load data into table\nLOAD DATA INPATH '/user/rstudio-user/flights' INTO TABLE flights;\nCreate the metadata that will structure the airlines table. Load data into the Hive table.\n# Create metadata for airlines\nCREATE EXTERNAL TABLE IF NOT EXISTS airlines\n(\nCode string,\nDescription string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE\ntblproperties(\"skip.header.line.count\"=\"1\");\n\n# Load data into table\nLOAD DATA INPATH '/user/rstudio-user/airlines' INTO TABLE airlines;\nCreate the metadata that will structure the airports table. Load data into the Hive table.\n# Create metadata for airports\nCREATE EXTERNAL TABLE IF NOT EXISTS airports\n(\nid string,\nname string,\ncity string,\ncountry string,\nfaa string,\nicao string,\nlat double,\nlon double,\nalt int,\ntz_offset double,\ndst string,\ntz_name string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE;\n\n# Load data into table\nLOAD DATA INPATH '/user/rstudio-user/airports' INTO TABLE airports;"
  },
  {
    "href": "examples/yarn-cluster-emr.html#connect-to-spark",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Connect to Spark",
    "text": "Log in to RStudio Server by pointing a browser at your master node IP:8787.\n\n\n\nSet the environment variable SPARK_HOME and then run spark_connect. After connecting you will be able to browse the Hive metadata in the RStudio Server Spark pane.\n\n# Connect to Spark\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nSys.setenv(SPARK_HOME=\"/usr/lib/spark\")\nconfig <- spark_config()\nsc <- spark_connect(master = \"yarn-client\", config = config, version = '1.6.2')\n\nOnce you are connected, you will see the Spark pane appear along with your hive tables.\n\n\n\nYou can inspect your tables by clicking on the data icon."
  },
  {
    "href": "examples/yarn-cluster-emr.html#cache-the-tables-into-memory",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Cache the tables into memory",
    "text": "Use tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame.\n\n# Cache flights Hive table into Spark\ntbl_cache(sc, 'flights')\nflights_tbl <- tbl(sc, 'flights')\n\n# Cache airlines Hive table into Spark\ntbl_cache(sc, 'airlines')\nairlines_tbl <- tbl(sc, 'airlines')\n\n# Cache airports Hive table into Spark\ntbl_cache(sc, 'airports')\nairports_tbl <- tbl(sc, 'airports')"
  },
  {
    "href": "examples/yarn-cluster-emr.html#create-a-model-data-set",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Create a model data set",
    "text": "Filter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight.\n\n# Filter records and create target variable 'gain'\nmodel_data <- flights_tbl %>%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>%\n  filter(depdelay > 15 & depdelay < 240) %>%\n  filter(arrdelay > -60 & arrdelay < 360) %>%\n  filter(year >= 2003 & year <= 2007) %>%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>%\n  mutate(gain = depdelay - arrdelay) %>%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain)\n\n# Summarize data by carrier\nmodel_data %>%\n  group_by(uniquecarrier) %>%\n  summarize(description = min(description), gain=mean(gain), \n            distance=mean(distance), depdelay=mean(depdelay)) %>%\n  select(description, gain, distance, depdelay) %>%\n  arrange(gain)\n\nSource:   query [?? x 4]\nDatabase: spark connection master=yarn-client app=sparklyr local=FALSE\n\n                    description       gain  distance depdelay\n                          <chr>      <dbl>     <dbl>    <dbl>\n1        ATA Airlines d/b/a ATA -3.3480120 1134.7084 56.06583\n2  ExpressJet Airlines Inc. (1) -3.0326180  519.7125 59.41659\n3                     Envoy Air -2.5434415  416.3716 53.12529\n4       Northwest Airlines Inc. -2.2030586  779.2342 48.52828\n5          Delta Air Lines Inc. -1.8248026  868.3997 50.77174\n6   AirTran Airways Corporation -1.4331555  641.8318 54.96702\n7    Continental Air Lines Inc. -0.9617003 1116.6668 57.00553\n8        American Airlines Inc. -0.8860262 1074.4388 55.45045\n9             Endeavor Air Inc. -0.6392733  467.1951 58.47395\n10              JetBlue Airways -0.3262134 1139.0443 54.06156\n# ... with more rows"
  },
  {
    "href": "examples/yarn-cluster-emr.html#train-a-linear-model",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Train a linear model",
    "text": "Predict time gained or lost in flight as a function of distance, departure delay, and airline carrier.\n\n# Partition the data into training and validation sets\nmodel_partition <- model_data %>% \n  sdf_partition(train = 0.8, valid = 0.2, seed = 5555)\n\n# Fit a linear model\nml1 <- model_partition$train %>%\n  ml_linear_regression(gain ~ distance + depdelay + uniquecarrier)\n\n# Summarize the linear model\nsummary(ml1)\n\nDeviance Residuals: (approximate):\n     Min       1Q   Median       3Q      Max \n-305.422   -5.593    2.699    9.750  147.871 \n\nCoefficients:\n                    Estimate  Std. Error  t value  Pr(>|t|)    \n(Intercept)      -1.24342576  0.10248281 -12.1330 < 2.2e-16 ***\ndistance          0.00326600  0.00001670 195.5709 < 2.2e-16 ***\ndepdelay         -0.01466233  0.00020337 -72.0977 < 2.2e-16 ***\nuniquecarrier_AA -2.32650517  0.10522524 -22.1098 < 2.2e-16 ***\nuniquecarrier_AQ  2.98773637  0.28798507  10.3746 < 2.2e-16 ***\nuniquecarrier_AS  0.92054894  0.11298561   8.1475 4.441e-16 ***\nuniquecarrier_B6 -1.95784698  0.11728289 -16.6934 < 2.2e-16 ***\nuniquecarrier_CO -2.52618081  0.11006631 -22.9514 < 2.2e-16 ***\nuniquecarrier_DH  2.23287189  0.11608798  19.2343 < 2.2e-16 ***\nuniquecarrier_DL -2.68848119  0.10621977 -25.3106 < 2.2e-16 ***\nuniquecarrier_EV  1.93484736  0.10724290  18.0417 < 2.2e-16 ***\nuniquecarrier_F9 -0.89788137  0.14422281  -6.2257 4.796e-10 ***\nuniquecarrier_FL -1.46706706  0.11085354 -13.2343 < 2.2e-16 ***\nuniquecarrier_HA -0.14506644  0.25031456  -0.5795    0.5622    \nuniquecarrier_HP  2.09354855  0.12337515  16.9690 < 2.2e-16 ***\nuniquecarrier_MQ -1.88297535  0.10550507 -17.8473 < 2.2e-16 ***\nuniquecarrier_NW -2.79538927  0.10752182 -25.9983 < 2.2e-16 ***\nuniquecarrier_OH  0.83520117  0.11032997   7.5700 3.730e-14 ***\nuniquecarrier_OO  0.61993842  0.10679884   5.8047 6.447e-09 ***\nuniquecarrier_TZ -4.99830389  0.15912629 -31.4109 < 2.2e-16 ***\nuniquecarrier_UA -0.68294396  0.10638099  -6.4198 1.365e-10 ***\nuniquecarrier_US -0.61589284  0.10669583  -5.7724 7.815e-09 ***\nuniquecarrier_WN  3.86386059  0.10362275  37.2878 < 2.2e-16 ***\nuniquecarrier_XE -2.59658123  0.10775736 -24.0966 < 2.2e-16 ***\nuniquecarrier_YV  3.11113140  0.11659679  26.6828 < 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nR-Squared: 0.02385\nRoot Mean Squared Error: 17.74"
  },
  {
    "href": "examples/yarn-cluster-emr.html#assess-model-performance",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Assess model performance",
    "text": "Compare the model performance using the validation data.\n\n# Calculate average gains by predicted decile\nmodel_deciles <- lapply(model_partition, function(x) {\n  sdf_predict(ml1, x) %>%\n    mutate(decile = ntile(desc(prediction), 10)) %>%\n    group_by(decile) %>%\n    summarize(gain = mean(gain)) %>%\n    select(decile, gain) %>%\n    collect()\n})\n\n# Create a summary dataset for plotting\ndeciles <- rbind(\n  data.frame(data = 'train', model_deciles$train),\n  data.frame(data = 'valid', model_deciles$valid),\n  make.row.names = FALSE\n)\n\n# Plot average gains by predicted decile\ndeciles %>%\n  ggplot(aes(factor(decile), gain, fill = data)) +\n  geom_bar(stat = 'identity', position = 'dodge') +\n  labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes')"
  },
  {
    "href": "examples/yarn-cluster-emr.html#visualize-predictions",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Visualize predictions",
    "text": "Compare actual gains to predicted gains for an out of time sample.\n\n# Select data from an out of time sample\ndata_2008 <- flights_tbl %>%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>%\n  filter(depdelay > 15 & depdelay < 240) %>%\n  filter(arrdelay > -60 & arrdelay < 360) %>%\n  filter(year == 2008) %>%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>%\n  mutate(gain = depdelay - arrdelay) %>%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest)\n\n# Summarize data by carrier\ncarrier <- sdf_predict(ml1, data_2008) %>%\n  group_by(description) %>%\n  summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>%\n  filter(freq > 10000) %>%\n  collect\n\n# Plot actual gains and predicted gains by airline carrier\nggplot(carrier, aes(gain, prediction)) + \n  geom_point(alpha = 0.75, color = 'red', shape = 3) +\n  geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') +\n  geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) +\n  labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted')\n\n\n\n\nSome carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights."
  },
  {
    "href": "examples/yarn-cluster-emr.html#build-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Build dashboard",
    "text": "Aggregate the scored data by origin, destination, and airline. Save the aggregated data.\n\n# Summarize by origin, destination, and carrier\nsummary_2008 <- sdf_predict(ml1, data_2008) %>%\n  rename(carrier = uniquecarrier, airline = description) %>%\n  group_by(origin, dest, carrier, airline) %>%\n  summarize(\n    flights = n(),\n    distance = mean(distance),\n    avg_dep_delay = mean(depdelay),\n    avg_arr_delay = mean(arrdelay),\n    avg_gain = mean(gain),\n    pred_gain = mean(prediction)\n    )\n\n# Collect and save objects\npred_data <- collect(summary_2008)\nairports <- collect(select(airports_tbl, name, faa, lat, lon))\nml1_summary <- capture.output(summary(ml1))\nsave(pred_data, airports, ml1_summary, file = 'flights_pred_2008.RData')"
  },
  {
    "href": "examples/yarn-cluster-emr.html#publish-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Publish dashboard",
    "text": "Use the saved data to build an R Markdown flexdashboard. Publish the flexdashboard to Shiny Server, Shinyapps.io or RStudio Connect."
  },
  {
    "href": "examples/qubole-overview.html#overview",
    "title": "Using sparklyr with Qubole",
    "section": "Overview",
    "text": "This documentation demonstrates how to use sparklyr with Apache Spark in Qubole along with RStudio Server Pro and RStudio Connect."
  },
  {
    "href": "examples/qubole-overview.html#best-practices-for-working-with-qubole",
    "title": "Using sparklyr with Qubole",
    "section": "Best practices for working with Qubole",
    "text": "Manage packages via Qubole Environments - Packages installed via install.packages() are not available on cluster restart. Packages managed through Qubole Environments are persistent.\nRestrict workloads to interactive analysis - Only perform workloads related to exploratory or interactive analysis with Spark, then write the results to a database, file system, or cloud storage for more efficient retrieval in apps, reports, and APIs.\nLoad and query results efficiently - Because of the nature of Spark computations and the associated overhead, Shiny apps that use Spark on the backend tend to have performance and runtime issues; consider reading the results from a database, file system, or cloud storage instead."
  },
  {
    "href": "examples/qubole-overview.html#using-rstudio-workbench-with-qubole",
    "title": "Using sparklyr with Qubole",
    "section": "Using RStudio Workbench with Qubole",
    "text": "The Qubole platform includes RStudio Workbench. More details about how to request RStudio Workbench and access it from within a Qubole cluster are available from Qubole.\n\n\nView steps for running RStudio Workbench inside Qubole"
  },
  {
    "href": "examples/qubole-overview.html#using-rstudio-connect-with-qubole",
    "title": "Using sparklyr with Qubole",
    "section": "Using RStudio Connect with Qubole",
    "text": "The best configuration for working with Qubole and RStudio Connect is to install RStudio Connect outside of the Qubole cluster and connect to Qubole remotely. This is accomplished using the Qubole ODBC Driver."
  },
  {
    "href": "examples/databricks-cluster.html#overview",
    "title": "Using sparklyr with Databricks",
    "section": "Overview",
    "text": "This documentation demonstrates how to use sparklyr with Apache Spark in Databricks along with RStudio Team, RStudio Workbench, RStudio Connect, and RStudio Package Manager."
  },
  {
    "href": "examples/databricks-cluster.html#using-rstudio-team-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Using RStudio Team with Databricks",
    "text": "RStudio Team is a bundle of our popular professional software for developing data science projects, publishing data products, and managing packages.\nRStudio Team and sparklyr can be used with Databricks to work with large datasets and distributed computations with Apache Spark. The most common use case is to perform interactive analysis and exploratory development with RStudio Workbench and sparklyr; write out the results to a database, file system, or cloud storage; then publish apps, reports, and APIs to RStudio Connect that query and access the results.\n\nThe sections below describe best practices and different options for configuring specific RStudio products to work with Databricks."
  },
  {
    "href": "examples/databricks-cluster.html#best-practices-for-working-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Best practices for working with Databricks",
    "text": "Maintain separate installation environments - Install RStudio Workbench, RStudio Connect, and RStudio Package Manager outside of the Databricks cluster so that they are not limited to the compute resources or ephemeral nature of Databricks clusters.\nConnect to Databricks remotely - Work with Databricks as a remote compute resource, similar to how you would connect remotely to external databases, data sources, and storage systems. This can be accomplished using Databricks Connect (as described in the Connecting to Databricks remotely section below) or by performing SQL queries with JDBC/ODBC using the Databricks Spark SQL Driver on AWS or Azure.\nRestrict workloads to interactive analysis - Only perform workloads related to exploratory or interactive analysis with Spark, then write the results to a database, file system, or cloud storage for more efficient retrieval in apps, reports, and APIs.\nLoad and query results efficiently - Because of the nature of Spark computations and the associated overhead, Shiny apps that use Spark on the backend tend to have performance and runtime issues; consider reading the results from a database, file system, or cloud storage instead."
  },
  {
    "href": "examples/databricks-cluster.html#using-rstudio-workbench-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Using RStudio Workbench with Databricks",
    "text": "There are two options for using sparklyr and RStudio Workbench with Databricks:\n\nOption 1: Connecting to Databricks remotely (Recommended Option)\nOption 2: Working inside of Databricks (Alternative Option)\n\n\nOption 1 - Connecting to Databricks remotely\nWith this configuration, RStudio Workbench is installed outside of the Spark cluster and allows users to connect to Spark remotely using sparklyr with Databricks Connect.\nThis is the recommended configuration because it targets separate environments, involves a typical configuration process, avoids resource contention, and allows RStudio Workbench to connect to Databricks as well as other remote storage and compute resources.\n\n\nView steps for connecting to Databricks remotely\n\n\n  \n\n\nOption 2 - Working inside of Databricks\nIf you cannot work with Spark remotely, you should install RStudio Workbench on the Driver node of a long-running, persistent Databricks cluster as opposed to a worker node or an ephemeral cluster.\nWith this configuration, RStudio Workbench is installed on the Spark driver node and allows users to connect to Spark locally using sparklyr.\nThis configuration can result in increased complexity, limited connectivity to other storage and compute resources, resource contention between RStudio Workbench and Databricks, and maintenance concerns due to the ephemeral nature of Databricks clusters.\n\n\nView steps for working inside of Databricks"
  },
  {
    "href": "examples/databricks-cluster.html#using-rstudio-connect-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Using RStudio Connect with Databricks",
    "text": "The server environment within Databricks clusters is not permissive enough to support RStudio Connect or the process sandboxing mechanisms that it uses to isolate published content.\nTherefore, the only supported configuration is to install RStudio Connect outside of the Databricks cluster and connect to Databricks remotely.\nWhether RStudio Workbench is installed outside of the Databricks cluster (Recommended Option) or within the Databricks cluster (Alternative Option), you can publish content to RStudio Connect as long as HTTP/HTTPS network traffic is allowed from RStudio Workbench to RStudio Connect.\nThere are two options for using RStudio Connect with Databricks:\n\nPerforming SQL queries with ODBC using the Databricks Spark SQL Driver (Recommended Option).\nAdding calls in your R code to create and run Databricks jobs with bricksteR and the Databricks Jobs API (Alternative Option)"
  },
  {
    "href": "examples/databricks-cluster.html#using-rstudio-package-manager-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Using RStudio Package Manager with Databricks",
    "text": "Whether RStudio Workbench is installed outside of the Databricks cluster (Recommended Option) or within the Databricks cluster (Alternative Option), you can install packages from repositories in RStudio Package Manager as long as HTTP/HTTPS network traffic is allowed from RStudio Workbench to RStudio Package Manager."
  },
  {
    "href": "examples/stand-alone-aws.html#overview",
    "title": "Spark Standalone Deployment in AWS",
    "section": "Overview",
    "text": "The plan is to launch 4 identical EC2 server instances. One server will be the Master node and the other 3 the worker nodes. In one of the worker nodes, we will install RStudio server.\nWhat makes a server the Master node is only the fact that it is running the master service, while the other machines are running the slave service and are pointed to that first master. This simple setup, allows us to install the same Spark components on all 4 servers and then just add RStudio to one of them.\nThe topology will look something like this:"
  },
  {
    "href": "examples/stand-alone-aws.html#aws-ec-instances",
    "title": "Spark Standalone Deployment in AWS",
    "section": "AWS EC Instances",
    "text": "Here are the details of the EC2 instance, just deploy one at this point:\n\nType: t2.medium\nOS: Ubuntu 16.04 LTS\nDisk space: At least 20GB\nSecurity group: Open the following ports: 8080 (Spark UI), 4040 (Spark Worker UI), 8088 (sparklyr UI) and 8787 (RStudio). Also open All TCP ports for the machines inside the security group."
  },
  {
    "href": "examples/stand-alone-aws.html#spark",
    "title": "Spark Standalone Deployment in AWS",
    "section": "Spark",
    "text": "Perform the steps in this section on all of the servers that will be part of the cluster.\n\nInstall Java 8\n\nWe will add the Java 8 repository, install it and set it as default\n\nsudo apt-add-repository ppa:webupd8team/java\nsudo apt-get update\nsudo apt-get install oracle-java8-installer\nsudo apt-get install oracle-java8-set-default\nsudo apt-get update\nor alternatively, run\nsudo apt install openjdk-8-jdk\nto install Open JDK version 8.\n\n\nDownload Spark\n\nDownload and unpack a pre-compiled version of Spark. Here’s is the link to the official Spark download page\n\nwget http://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-hadoop2.7.tgz\ntar -xvzf spark-2.1.0-bin-hadoop2.7.tgz\ncd spark-2.1.0-bin-hadoop2.7\n\n\nCreate and launch AMI\n\nWe will create an image of the server. In Amazon, these are called AMIs, for information please see the User Guide.\nLaunch 3 instances of the AMI"
  },
  {
    "href": "examples/stand-alone-aws.html#rstudio-server",
    "title": "Spark Standalone Deployment in AWS",
    "section": "RStudio Server",
    "text": "Select one of the nodes to execute this section. Please check the RStudio download page for the latest version\n\nInstall R\n\nIn order to get the latest R core, we will need to update the source list in Ubuntu.\n\nsudo sh -c 'echo \"deb http://cran.rstudio.com/bin/linux/ubuntu xenial/\" >> /etc/apt/sources.list'\ngpg --keyserver keyserver.ubuntu.com --recv-key 0x517166190x51716619e084dab9\ngpg -a --export 0x517166190x51716619e084dab9 | sudo apt-key add -\nsudo apt-get update\n\nNow we can install R\n\nsudo apt-get install r-base\nsudo apt-get install gdebi-core\n\n\nInstall RStudio\n\nWe will download and install 1.044 of RStudio Server. To find the latest version, please visit the RStudio website. In order to get the enhanced integration with Spark, RStudio version 1.044 or later will be needed.\n\nwget https://download2.rstudio.org/rstudio-server-1.0.153-amd64.deb\nsudo gdebi rstudio-server-1.0.153-amd64.deb\n\n\nInstall dependencies\n\nRun the following commands\n\nsudo apt-get -y install libcurl4-gnutls-dev\nsudo apt-get -y install libssl-dev\nsudo apt-get -y install libxml2-dev\n\n\nAdd default user\n\nRun the following command to add a default user\n\nsudo adduser rstudio-user\n\n\nStart the Master node\n\nSelect one of the servers to become your Master node\nRun the command that starts the master service\n\nsudo spark-2.1.0-bin-hadoop2.7/sbin/start-master.sh\n\nClose the terminal connection (optional)\n\n\n\nStart Worker nodes\n\nStart the slave service. Important: Use dots not dashes as separators for the Spark Master node’s address\n\nsudo spark-2.1.0-bin-hadoop2.7/sbin/start-slave.sh spark://[Master node's IP address]:7077\nsudo spark-2.1.0-bin-hadoop2.7/sbin/start-slave.sh spark://ip-172-30-1-94.us-west-2.compute.internal:7077 - Close the terminal connection (optional)\n\n\nPre-load pacakges\n\nLog into RStudio (port 8787)\nUse ‘rstudio-user’\n\ninstall.packages(\"sparklyr\")\n\n\nConnect to the Spark Master\n\nNavigate to the Spark Master’s UI, typically on port 8080 \nNote the Spark Master URL\nLogon to RStudio\nRun the following code\n\n\nlibrary(sparklyr)\n\nconf <- spark_config()\nconf$spark.executor.memory <- \"2GB\"\nconf$spark.memory.fraction <- 0.9\n\nsc <- spark_connect(master=\"[Spark Master URL]\",\n              version = \"2.1.0\",\n              config = conf,\n              spark_home = \"/home/ubuntu/spark-2.1.0-bin-hadoop2.7/\")"
  },
  {
    "href": "mlib.html#overview",
    "title": "sparklyr",
    "section": "Overview",
    "text": "sparklyr provides bindings to Spark’s distributed machine learning library. In particular, sparklyr allows you to access the machine learning routines provided by the spark.ml package. Together with sparklyr’s dplyr interface, you can easily create and tune machine learning workflows on Spark, orchestrated entirely within R.\nsparklyr provides three families of functions that you can use with Spark machine learning:\n\nMachine learning algorithms for analyzing data (ml_*)\nFeature transformers for manipulating individual features (ft_*)\nFunctions for manipulating Spark DataFrames (sdf_*)\n\nAn analytic workflow with sparklyr might be composed of the following stages. For an example see Example Workflow.\n\nPerform SQL queries through the sparklyr dplyr interface,\nUse the sdf_* and ft_* family of functions to generate new columns, or partition your data set,\nChoose an appropriate machine learning algorithm from the ml_* family of functions to model your data,\nInspect the quality of your model fit, and use it to make predictions with new data.\nCollect the results for visualization and further analysis in R"
  },
  {
    "href": "mlib.html#algorithms",
    "title": "sparklyr",
    "section": "Algorithms",
    "text": "Spark’s machine learning library can be accessed from sparklyr through the ml_* set of functions:\n\n\n\nFunction\nDescription\n\n\n\n\nml_kmeans\nK-Means Clustering\n\n\nml_linear_regression\nLinear Regression\n\n\nml_logistic_regression\nLogistic Regression\n\n\nml_survival_regression\nSurvival Regression\n\n\nml_generalized_linear_regression\nGeneralized Linear Regression\n\n\nml_decision_tree\nDecision Trees\n\n\nml_random_forest\nRandom Forests\n\n\nml_gradient_boosted_trees\nGradient-Boosted Trees\n\n\nml_pca\nPrincipal Components Analysis\n\n\nml_naive_bayes\nNaive-Bayes\n\n\nml_multilayer_perceptron\nMultilayer Perceptron\n\n\nml_lda\nLatent Dirichlet Allocation\n\n\nml_one_vs_rest\nOne vs Rest\n\n\n\n\nFormulas\nThe ml_* functions take the arguments response and features. But features can also be a formula with main effects (it currently does not accept interaction terms). The intercept term can be omitted by using -1.\n# Equivalent statements\nml_linear_regression(z ~ -1 + x + y)\nml_linear_regression(intercept = FALSE, response = \"z\", features = c(\"x\", \"y\"))\n\n\nOptions\nThe Spark model output can be modified with the ml_options argument in the ml_* functions. The ml_options is an experts only interface for tweaking the model output. For example, model.transform can be used to mutate the Spark model object before the fit is performed."
  },
  {
    "href": "mlib.html#transformers",
    "title": "sparklyr",
    "section": "Transformers",
    "text": "A model is often fit not on a dataset as-is, but instead on some transformation of that dataset. Spark provides feature transformers, facilitating many common transformations of data within a Spark DataFrame, and sparklyr exposes these within the ft_* family of functions. These routines generally take one or more input columns, and generate a new output column formed as a transformation of those columns.\n\n\n\n\n\n\n\n\nFunction\n\n\nDescription\n\n\n\n\n\n\nft_binarizer\n\n\nThreshold numerical features to binary (0/1) feature\n\n\n\n\nft_bucketizer\n\n\nBucketizer transforms a column of continuous features to a column of feature buckets\n\n\n\n\nft_discrete_cosine_transform\n\n\nTransforms a length NN real-valued sequence in the time domain into another length NN real-valued sequence in the frequency domain\n\n\n\n\nft_elementwise_product\n\n\nMultiplies each input vector by a provided weight vector, using element-wise multiplication.\n\n\n\n\nft_index_to_string\n\n\nMaps a column of label indices back to a column containing the original labels as strings\n\n\n\n\nft_quantile_discretizer\n\n\nTakes a column with continuous features and outputs a column with binned categorical features\n\n\n\n\nsql_transformer\n\n\nImplements the transformations which are defined by a SQL statement\n\n\n\n\nft_string_indexer\n\n\nEncodes a string column of labels to a column of label indices\n\n\n\n\nft_vector_assembler\n\n\nCombines a given list of columns into a single vector column"
  },
  {
    "href": "mlib.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "We will use the iris data set to examine a handful of learning algorithms and transformers. The iris data set measures attributes for 150 flowers in 3 different species of iris.\nlibrary(sparklyr)\n## Warning: package 'sparklyr' was built under R version 3.4.3\nlibrary(ggplot2)\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nsc <- spark_connect(master = \"local\")\n## * Using Spark: 2.1.0\niris_tbl <- copy_to(sc, iris, \"iris\", overwrite = TRUE)\niris_tbl\n## # Source:   table<iris> [?? x 5]\n## # Database: spark_connection\n##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl>   <chr>\n##  1          5.1         3.5          1.4         0.2  setosa\n##  2          4.9         3.0          1.4         0.2  setosa\n##  3          4.7         3.2          1.3         0.2  setosa\n##  4          4.6         3.1          1.5         0.2  setosa\n##  5          5.0         3.6          1.4         0.2  setosa\n##  6          5.4         3.9          1.7         0.4  setosa\n##  7          4.6         3.4          1.4         0.3  setosa\n##  8          5.0         3.4          1.5         0.2  setosa\n##  9          4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## # ... with more rows\n\nK-Means Clustering\nUse Spark’s K-means clustering to partition a dataset into groups. K-means clustering partitions points into k groups, such that the sum of squares from points to the assigned cluster centers is minimized.\nkmeans_model <- iris_tbl %>%\n  ml_kmeans(k = 3, features = c(\"Petal_Length\", \"Petal_Width\"))\n## * No rows dropped by 'na.omit' call\n# print our model fit\nkmeans_model\n## K-means clustering with 3 clusters\n## \n## Cluster centers:\n##   Petal_Width Petal_Length\n## 1    1.359259     4.292593\n## 2    0.246000     1.462000\n## 3    2.047826     5.626087\n## \n## Within Set Sum of Squared Errors =  31.41289\n# predict the associated class\npredicted <- ml_predict(kmeans_model, iris_tbl) %>%\n  collect\ntable(predicted$Species, predicted$prediction)\n##             \n##               0  1  2\n##   setosa      0 50  0\n##   versicolor 48  0  2\n##   virginica   6  0 44\n# plot cluster membership\nml_predict(kmeans_model) %>%\n  collect() %>%\n  ggplot(aes(Petal_Length, Petal_Width)) +\n  geom_point(aes(Petal_Width, Petal_Length, col = factor(prediction + 1)),\n             size = 2, alpha = 0.5) + \n  geom_point(data = kmeans_model$centers, aes(Petal_Width, Petal_Length),\n             col = scales::muted(c(\"red\", \"green\", \"blue\")),\n             pch = 'x', size = 12) +\n  scale_color_discrete(name = \"Predicted Cluster\",\n                       labels = paste(\"Cluster\", 1:3)) +\n  labs(\n    x = \"Petal Length\",\n    y = \"Petal Width\",\n    title = \"K-Means Clustering\",\n    subtitle = \"Use Spark.ML to predict cluster membership with the iris dataset.\"\n  )\n\n\n\nLinear Regression\nUse Spark’s linear regression to model the linear relationship between a response variable and one or more explanatory variables.\nlm_model <- iris_tbl %>%\n  select(Petal_Width, Petal_Length) %>%\n  ml_linear_regression(Petal_Length ~ Petal_Width)\n## * No rows dropped by 'na.omit' call\niris_tbl %>%\n  select(Petal_Width, Petal_Length) %>%\n  collect %>%\n  ggplot(aes(Petal_Length, Petal_Width)) +\n    geom_point(aes(Petal_Width, Petal_Length), size = 2, alpha = 0.5) +\n    geom_abline(aes(slope = coef(lm_model)[[\"Petal_Width\"]],\n                    intercept = coef(lm_model)[[\"(Intercept)\"]]),\n                color = \"red\") +\n  labs(\n    x = \"Petal Width\",\n    y = \"Petal Length\",\n    title = \"Linear Regression: Petal Length ~ Petal Width\",\n    subtitle = \"Use Spark.ML linear regression to predict petal length as a function of petal width.\"\n  )\n\n\n\nLogistic Regression\nUse Spark’s logistic regression to perform logistic regression, modeling a binary outcome as a function of one or more explanatory variables.\n# Prepare beaver dataset\nbeaver <- beaver2\nbeaver$activ <- factor(beaver$activ, labels = c(\"Non-Active\", \"Active\"))\ncopy_to(sc, beaver, \"beaver\")\n## # Source:   table<beaver> [?? x 4]\n## # Database: spark_connection\n##      day  time  temp      activ\n##    <dbl> <dbl> <dbl>      <chr>\n##  1   307   930 36.58 Non-Active\n##  2   307   940 36.73 Non-Active\n##  3   307   950 36.93 Non-Active\n##  4   307  1000 37.15 Non-Active\n##  5   307  1010 37.23 Non-Active\n##  6   307  1020 37.24 Non-Active\n##  7   307  1030 37.24 Non-Active\n##  8   307  1040 36.90 Non-Active\n##  9   307  1050 36.95 Non-Active\n## 10   307  1100 36.89 Non-Active\n## # ... with more rows\nbeaver_tbl <- tbl(sc, \"beaver\")\n\nglm_model <- beaver_tbl %>%\n  mutate(binary_response = as.numeric(activ == \"Active\")) %>%\n  ml_logistic_regression(binary_response ~ temp)\n## * No rows dropped by 'na.omit' call\nglm_model\n## Call: binary_response ~ temp\n## \n## Coefficients:\n## (Intercept)        temp \n##  -550.52331    14.69184\n\n\nPCA\nUse Spark’s Principal Components Analysis (PCA) to perform dimensionality reduction. PCA is a statistical method to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate in turn has the largest variance possible.\npca_model <- tbl(sc, \"iris\") %>%\n  select(-Species) %>%\n  ml_pca()\n## * No rows dropped by 'na.omit' call\nprint(pca_model)\n## Explained variance:\n## \n##         PC1         PC2         PC3         PC4 \n## 0.924618723 0.053066483 0.017102610 0.005212184 \n## \n## Rotation:\n##                      PC1         PC2         PC3        PC4\n## Sepal_Length -0.36138659 -0.65658877  0.58202985  0.3154872\n## Sepal_Width   0.08452251 -0.73016143 -0.59791083 -0.3197231\n## Petal_Length -0.85667061  0.17337266 -0.07623608 -0.4798390\n## Petal_Width  -0.35828920  0.07548102 -0.54583143  0.7536574\n\n\nRandom Forest\nUse Spark’s Random Forest to perform regression or multiclass classification.\nrf_model <- iris_tbl %>%\n  ml_random_forest(Species ~ Petal_Length + Petal_Width, type = \"classification\")\n## * No rows dropped by 'na.omit' call\nrf_predict <- sdf_predict(rf_model, iris_tbl) %>%\n  ft_string_indexer(\"Species\", \"Species_idx\") %>%\n  collect\n\ntable(rf_predict$Species_idx, rf_predict$prediction)\n##    \n##      0  1  2\n##   0 49  1  0\n##   1  0 50  0\n##   2  0  0 50\n\n\nSDF Partitioning\nSplit a Spark DataFrame into training, test datasets.\npartitions <- tbl(sc, \"iris\") %>%\n  sdf_partition(training = 0.75, test = 0.25, seed = 1099)\n\nfit <- partitions$training %>%\n  ml_linear_regression(Petal_Length ~ Petal_Width)\n## * No rows dropped by 'na.omit' call\nestimate_mse <- function(df){\n  sdf_predict(fit, df) %>%\n  mutate(resid = Petal_Length - prediction) %>%\n  summarize(mse = mean(resid ^ 2)) %>%\n  collect\n}\n\nsapply(partitions, estimate_mse)\n## $training.mse\n## [1] 0.2374596\n## \n## $test.mse\n## [1] 0.1898848\n\n\nFT String Indexing\nUse ft_string_indexer and ft_index_to_string to convert a character column into a numeric column and back again.\nft_string2idx <- iris_tbl %>%\n  ft_string_indexer(\"Species\", \"Species_idx\") %>%\n  ft_index_to_string(\"Species_idx\", \"Species_remap\") %>%\n  collect\n\ntable(ft_string2idx$Species, ft_string2idx$Species_remap)\n##             \n##              setosa versicolor virginica\n##   setosa         50          0         0\n##   versicolor      0         50         0\n##   virginica       0          0        50\n\n\nSDF Mutate\nsdf_mutate is provided as a helper function, to allow you to use feature transformers. For example, the previous code snippet could have been written as:\nft_string2idx <- iris_tbl %>%\n  sdf_mutate(Species_idx = ft_string_indexer(Species)) %>%\n  sdf_mutate(Species_remap = ft_index_to_string(Species_idx)) %>%\n  collect\n  \nft_string2idx %>%\n  select(Species, Species_idx, Species_remap) %>%\n  distinct\n## # A tibble: 3 x 3\n##      Species Species_idx Species_remap\n##        <chr>       <dbl>         <chr>\n## 1     setosa           2        setosa\n## 2 versicolor           0    versicolor\n## 3  virginica           1     virginica\n\n\nExample Workflow\nLet’s walk through a simple example to demonstrate the use of Spark’s machine learning algorithms within R. We’ll use ml_linear_regression to fit a linear regression model. Using the built-in mtcars dataset, we’ll try to predict a car’s fuel consumption (mpg) based on its weight (wt), and the number of cylinders the engine contains (cyl).\nFirst, we will copy the mtcars dataset into Spark.\nmtcars_tbl <- copy_to(sc, mtcars, \"mtcars\")\nTransform the data with Spark SQL, feature transformers, and DataFrame functions.\n\nUse Spark SQL to remove all cars with horsepower less than 100\nUse Spark feature transformers to bucket cars into two groups based on cylinders\nUse Spark DataFrame functions to partition the data into test and training\n\nThen fit a linear model using spark ML. Model MPG as a function of weight and cylinders.\n# transform our data set, and then partition into 'training', 'test'\npartitions <- mtcars_tbl %>%\n  filter(hp >= 100) %>%\n  sdf_mutate(cyl8 = ft_bucketizer(cyl, c(0,8,12))) %>%\n  sdf_partition(training = 0.5, test = 0.5, seed = 888)\n\n# fit a linear mdoel to the training dataset\nfit <- partitions$training %>%\n  ml_linear_regression(mpg ~ wt + cyl)\n## * No rows dropped by 'na.omit' call\n# summarize the model\nsummary(fit)\n## Call: ml_linear_regression(., mpg ~ wt + cyl)\n## \n## Deviance Residuals::\n##     Min      1Q  Median      3Q     Max \n## -2.0947 -1.2747 -0.1129  1.0876  2.2185 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 33.79558    2.67240 12.6462 4.92e-07 ***\n## wt          -1.59625    0.73729 -2.1650  0.05859 .  \n## cyl         -1.58036    0.49670 -3.1817  0.01115 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-Squared: 0.8267\n## Root Mean Squared Error: 1.437\nThe summary() suggests that our model is a fairly good fit, and that both a cars weight, as well as the number of cylinders in its engine, will be powerful predictors of its average fuel consumption. (The model suggests that, on average, heavier cars consume more fuel.)\nLet’s use our Spark model fit to predict the average fuel consumption on our test data set, and compare the predicted response with the true measured fuel consumption. We’ll build a simple ggplot2 plot that will allow us to inspect the quality of our predictions.\n# Score the data\npred <- sdf_predict(fit, partitions$test) %>%\n  collect\n\n# Plot the predicted versus actual mpg\nggplot(pred, aes(x = mpg, y = prediction)) +\n  geom_abline(lty = \"dashed\", col = \"red\") +\n  geom_point() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  coord_fixed(ratio = 1) +\n  labs(\n    x = \"Actual Fuel Consumption\",\n    y = \"Predicted Fuel Consumption\",\n    title = \"Predicted vs. Actual Fuel Consumption\"\n  )\n\nAlthough simple, our model appears to do a fairly good job of predicting a car’s average fuel consumption.\nAs you can see, we can easily and effectively combine feature transformers, machine learning algorithms, and Spark DataFrame functions into a complete analysis with Spark and R."
  },
  {
    "href": "extensions.html#introduction",
    "title": "Creating Extensions for sparklyr",
    "section": "Introduction",
    "text": "The sparklyr package provides a dplyr interface to Spark DataFrames as well as an R interface to Spark’s distributed machine learning pipelines. However, since Spark is a general-purpose cluster computing system there are many other R interfaces that could be built (e.g. interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).\nThe facilities used internally by sparklyr for its dplyr and machine learning interfaces are available to extension packages. This guide describes how you can use these tools to create your own custom R interfaces to Spark.\n\nExamples\nHere’s an example of an extension function that calls the text file line counting function available via the SparkContext:\n\nlibrary(sparklyr)\ncount_lines <- function(sc, file) {\n  spark_context(sc) %>% \n    invoke(\"textFile\", file, 1L) %>% \n    invoke(\"count\")\n}\n\nThe count_lines function takes a spark_connection (sc) argument which enables it to obtain a reference to the SparkContext object, and in turn call the textFile().count() method.\nYou can use this function with an existing sparklyr connection as follows:\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\ncount_lines(sc, \"hdfs://path/data.csv\")\n\nHere are links to some additional examples of extension packages:\n\n\n\n\n\n\n\nPackage\nDescription\n\n\n\n\nspark.sas7bdat\nRead in SAS data in parallel into Apache Spark.\n\n\nrsparkling\nExtension for using H2O machine learning algorithms against Spark Data Frames.\n\n\nsparkhello\nSimple example of including a custom JAR file within an extension package.\n\n\nrddlist\nImplements some methods of an R list as a Spark RDD (resilient distributed dataset).\n\n\nsparkwarc\nLoad WARC files into Apache Spark with sparklyr.\n\n\nsparkavro\nLoad Avro data into Spark with sparklyr. It is a wrapper of spark-avro\n\n\ncrassy\nConnect to Cassandra with sparklyr using the Spark-Cassandra-Connector.\n\n\nsparklygraphs\nR interface for GraphFrames which aims to provide the functionality of GraphX.\n\n\nsparklyr.nested\nExtension for working with nested data.\n\n\nsparklyudf\nSimple example registering an Scala UDF within an extension package.\n\n\nmleap\nR Interface to MLeap.\n\n\nsparkbq\nSparklyr extension package to connect to Google BigQuery.\n\n\nsparkgeo\nSparklyr extension package providing geospatial analytics capabilities.\n\n\nsparklytd\nSpaklyr plugin for td-spark to connect TD from R.\n\n\nsparkts\nExtensions for the spark-timeseries framework.\n\n\nsparkxgb\nR interface for XGBoost on Spark.\n\n\nsparktf\nR interface to Spark TensorFlow Connector.\n\n\ngeospark\nR interface to GeoSpark to perform spatial analysis in Spark.\n\n\nmmlspark\nMicrosoft Machine Learning for Apache Spark."
  },
  {
    "href": "extensions.html#core-types",
    "title": "Creating Extensions for sparklyr",
    "section": "Core Types",
    "text": "Three classes are defined for representing the fundamental types of the R to Java bridge:\n\n\n\nFunction\nDescription\n\n\n\n\nspark_connection\nConnection between R and the Spark shell process\n\n\nspark_jobj\nInstance of a remote Spark object\n\n\nspark_dataframe\nInstance of a remote Spark DataFrame object\n\n\n\nS3 methods are defined for each of these classes so they can be easily converted to or from objects that contain or wrap them. Note that for any given spark_jobj it’s possible to discover the underlying spark_connection."
  },
  {
    "href": "extensions.html#calling-spark-from-r",
    "title": "Creating Extensions for sparklyr",
    "section": "Calling Spark from R",
    "text": "There are several functions available for calling the methods of Java objects and static methods of Java classes:\n\n\n\nFunction\nDescription\n\n\n\n\ninvoke\nCall a method on an object\n\n\ninvoke_new\nCreate a new object by invoking a constructor\n\n\ninvoke_static\nCall a static method on an object\n\n\n\nFor example, to create a new instance of the java.math.BigInteger class and then call the longValue() method on it you would use code like this:\n\nbillionBigInteger <- invoke_new(sc, \"java.math.BigInteger\", \"1000000000\")\nbillion <- invoke(billionBigInteger, \"longValue\")\n\nNote the sc argument: that’s the spark_connection object which is provided by the front-end package (e.g. sparklyr).\nThe previous example can be re-written to be more compact and clear using magrittr pipes:\n\nbillion <- sc %>% \n  invoke_new(\"java.math.BigInteger\", \"1000000000\") %>%\n    invoke(\"longValue\")\n\nThis syntax is similar to the method-chaining syntax often used with Scala code so is generally preferred.\nCalling a static method of a class is also straightforward. For example, to call the Math::hypot() static function you would use this code:\n\nhypot <- sc %>% \n  invoke_static(\"java.lang.Math\", \"hypot\", 10, 20)"
  },
  {
    "href": "extensions.html#wrapper-functions",
    "title": "Creating Extensions for sparklyr",
    "section": "Wrapper Functions",
    "text": "Creating an extension typically consists of writing R wrapper functions for a set of Spark services. In this section we’ll describe the typical form of these functions as well as how to handle special types like Spark DataFrames.\nHere’s the wrapper function for textFile().count() which we defined earlier:\n\ncount_lines <- function(sc, file) {\n  spark_context(sc) %>% \n    invoke(\"textFile\", file, 1L) %>% \n      invoke(\"count\")\n}\n\nThe count_lines function takes a spark_connection (sc) argument which enables it to obtain a reference to the SparkContext object, and in turn call the textFile().count() method.\nThe following functions are useful for implementing wrapper functions of various kinds:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nspark_connection\nGet the Spark connection associated with an object (S3)\n\n\nspark_jobj\nGet the Spark jobj associated with an object (S3)\n\n\nspark_dataframe\nGet the Spark DataFrame associated with an object (S3)\n\n\nspark_context\nGet the SparkContext for a spark_connection\n\n\nhive_context\nGet the HiveContext for a spark_connection\n\n\nspark_version\nGet the version of Spark (as a numeric_version) for a spark_connection\n\n\n\nThe use of these functions is illustrated in this simple example:\n\nanalyze <- function(x, features) {\n  \n  # normalize whatever we were passed (e.g. a dplyr tbl) into a DataFrame\n  df <- spark_dataframe(x)\n  \n  # get the underlying connection so we can create new objects\n  sc <- spark_connection(df)\n  \n  # create an object to do the analysis and call its `analyze` and `summary`\n  # methods (note that the df and features are passed to the analyze function)\n  summary <- sc %>%  \n    invoke_new(\"com.example.tools.Analyzer\") %>% \n      invoke(\"analyze\", df, features) %>% \n      invoke(\"summary\")\n\n  # return the results\n  summary\n}\n\nThe first argument is an object that can be accessed using the Spark DataFrame API (this might be an actual reference to a DataFrame or could rather be a dplyr tbl which has a DataFrame reference inside it).\nAfter using the spark_dataframe function to normalize the reference, we extract the underlying Spark connection associated with the data frame using the spark_connection function. Finally, we create a new Analyzer object, call it’s analyze method with the DataFrame and list of features, and then call the summary method on the results of the analysis.\nAccepting a spark_jobj or spark_dataframe as the first argument of a function makes it very easy to incorporate into magrittr pipelines so this pattern is highly recommended when possible."
  },
  {
    "href": "extensions.html#dependencies",
    "title": "Creating Extensions for sparklyr",
    "section": "Dependencies",
    "text": "When creating R packages which implement interfaces to Spark you may need to include additional dependencies. Your dependencies might be a set of Spark Packages or might be a custom JAR file. In either case, you’ll need a way to specify that these dependencies should be included during the initialization of a Spark session. A Spark dependency is defined using the spark_dependency function:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nspark_dependency\nDefine a Spark dependency consisting of JAR files and Spark packages\n\n\n\nYour extension package can specify it’s dependencies by implementing a function named spark_dependencies within the package (this function should not be publicly exported). For example, let’s say you were creating an extension package named sparkds that needs to include a custom JAR as well as the Redshift and Apache Avro packages:\n\nspark_dependencies <- function(spark_version, scala_version, ...) {\n  spark_dependency(\n    jars = c(\n      system.file(\n        sprintf(\"java/sparkds-%s-%s.jar\", spark_version, scala_version), \n        package = \"sparkds\"\n      )\n    ),\n    packages = c(\n      sprintf(\"com.databricks:spark-redshift_%s:0.6.0\", scala_version),\n      sprintf(\"com.databricks:spark-avro_%s:2.0.1\", scala_version)\n    )\n  )\n}\n\n.onLoad <- function(libname, pkgname) {\n  sparklyr::register_extension(pkgname)\n}\n\nThe spark_version argument is provided so that a package can support multiple Spark versions for it’s JARs. Note that the argument will include just the major and minor versions (e.g. 1.6 or 2.0) and will not include the patch level (as JARs built for a given major/minor version are expected to work for all patch levels).\nThe scala_version argument is provided so that a single package can support multiple Scala compiler versions for it’s JARs and packages (currently Scala 1.6 downloadable binaries are compiled with Scala 2.10 and Scala 2.0 downloadable binaries are compiled with Scala 2.11).\nThe ... argument is unused but nevertheless should be included to ensure compatibility if new arguments are added to spark_dependencies in the future.\nThe .onLoad function registers your extension package so that it’s spark_dependencies function will be automatically called when new connections to Spark are made via spark_connect:\n\nlibrary(sparklyr)\nlibrary(sparkds)\nsc <- spark_connect(master = \"local\")\n\n\nCompiling JARs\nThe sparklyr package includes a utility function (compile_package_jars) that will automatically compile a JAR file from your Scala source code for the required permutations of Spark and Scala compiler versions. To use the function just invoke it from the root directory of your R package as follows:\n\nsparklyr::compile_package_jars()\n\nNote that a prerequisite to calling compile_package_jars is the installation of the Scala 2.10 and 2.11 compilers to one of the following paths:\n\n/opt/scala\n/opt/local/scala\n/usr/local/scala\n~/scala (Windows-only)\n\nSee the sparkhello repository for a complete example of including a custom JAR within an extension package.\n\nCRAN\nWhen including a JAR file within an R package distributed on CRAN, you should follow the guidelines provided in Writing R Extensions:\n\nJava code is a special case: except for very small programs, .java files should be byte-compiled (to a .class file) and distributed as part of a .jar file: the conventional location for the .jar file(s) is inst/java. It is desirable (and required under an Open Source license) to make the Java source files available: this is best done in a top-level java directory in the package – the source files should not be installed."
  },
  {
    "href": "extensions.html#data-types",
    "title": "Creating Extensions for sparklyr",
    "section": "Data Types",
    "text": "The ensure_* family of functions can be used to enforce specific data types that are passed to a Spark routine. For example, Spark routines that require an integer will not accept an R numeric element. Use these functions ensure certain parameters are scalar integers, or scalar doubles, and so on.\n\nensure_scalar_integer\nensure_scalar_double\nensure_scalar_boolean\nensure_scalar_character\n\nIn order to match the correct data types while calling Scala code from R, or retrieving results from Scala back to R, consider the following types mapping:\n\n\n\nFrom R\nScala\nTo R\n\n\n\n\nNULL\nvoid\nNULL\n\n\ninteger\nInt\ninteger\n\n\ncharacter\nString\ncharacter\n\n\nlogical\nBoolean\nlogical\n\n\ndouble\nDouble\ndouble\n\n\nnumeric\nDouble\ndouble\n\n\n\nFloat\ndouble\n\n\n\nDecimal\ndouble\n\n\n\nLong\ndouble\n\n\nraw\nArray[Byte]\nraw\n\n\nDate\nDate\nDate\n\n\nPOSIXlt\nTime\n\n\n\nPOSIXct\nTime\nPOSIXct\n\n\nlist\nArray[T]\nlist\n\n\nenvironment\nMap[String, T]\n\n\n\njobj\nObject\njobj"
  },
  {
    "href": "extensions.html#compiling",
    "title": "Creating Extensions for sparklyr",
    "section": "Compiling",
    "text": "Most Spark extensions won’t need to define their own compilation specification, and can instead rely on the default behavior of compile_package_jars. For users who would like to take more control over where the scalac compilers should be looked up, use the spark_compilation_spec fucnction. The Spark compilation specification is used when compiling Spark extension Java Archives, and defines which versions of Spark, as well as which versions of Scala, should be used for compilation."
  },
  {
    "href": "deployment.html#deployment",
    "title": "Deployment and Configuration",
    "section": "Deployment",
    "text": "There are two well supported deployment modes for sparklyr:\n\nLocal — Working on a local desktop typically with smaller/sampled datasets\nCluster — Working directly within or alongside a Spark cluster (standalone, YARN, Mesos, etc.)\n\n\nLocal Deployment\nLocal mode is an excellent way to learn and experiment with Spark. Local mode also provides a convenient development environment for analyses, reports, and applications that you plan to eventually deploy to a multi-node Spark cluster.\nTo work in local mode you should first install a version of Spark for local use. You can do this using the spark_install function, for example:\n\nsparklyr::spark_install(version = \"2.1.0\")\n\nTo connect to the local Spark instance you pass “local” as the value of the Spark master node to spark_connect:\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\n\nFor the local development scenario, see the Configuration section below for details on how to have the same code work seamlessly in both development and production environments.\n\n\nCluster Deployment\nA common deployment strategy is to submit your application from a gateway machine that is physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster). In this setup, client mode is appropriate. In client mode, the driver is launched directly within the spark-submit process which acts as a client to the cluster. The input and output of the application is attached to the console. Thus, this mode is especially suitable for applications that involve the REPL (e.g. Spark shell). For more information see Submitting Applications.\nTo use spaklyr with a Spark cluster you should locate your R session on a machine that is either directly on one of the cluster nodes or is close to the cluster (for networking performance). In the case where R is not running directly on the cluster you should also ensure that the machine has a Spark version and configuration identical to that of the cluster nodes.\nThe most straightforward way to run R within or near to the cluster is either a remote SSH session or via RStudio Server.\nIn cluster mode you use the version of Spark already deployed on the cluster node. This version is located via the SPARK_HOME environment variable, so you should be sure that this variable is correctly defined on your server before attempting a connection. This would typically be done within the Renviron.site configuration file. For example:\nSPARK_HOME=/opt/spark/spark-2.0.0-bin-hadoop2.6\nTo connect, pass the address of the master node to spark_connect, for example:\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"spark://local:7077\")\n\nFor a Hadoop YARN cluster, you can connect using the YARN master, for example:\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"yarn-client\")\n\nIf you are running on EC2 using the Spark EC2 deployment scripts then you can read the master from /root/spark-ec2/cluster-url, for example:\n\nlibrary(sparklyr)\ncluster_url <- system('cat /root/spark-ec2/cluster-url', intern=TRUE)\nsc <- spark_connect(master = cluster_url)\n\n\n\nLivy Connections\nLivy, “An Open Source REST Service for Apache Spark (Apache License)” , is available starting in sparklyr 0.5 as an experimental feature. Among many scenarios, this enables connections from the RStudio desktop to Apache Spark when Livy is available and correctly configured in the remote cluster.\nTo work with Livy locally, sparklyr supports livy_install() which installs Livy in your local environment, this is similar to spark_install(). Since Livy is a service to enable remote connections into Apache Spark, the service needs to be started with livy_service_start(). Once the service is running, spark_connect() needs to reference the running service and use method = \"Livy\", then sparklyr can be used as usual. A short example follows:\n\nlivy_install()\nlivy_service_start()\n\nsc <- spark_connect(master = \"http://localhost:8998\", method = \"livy\")\ncopy_to(sc, iris)\n\nspark_disconnect(sc)\nlivy_service_stop()\n\n\n\nConnection Tools\nYou can view the Spark web UI via the spark_web function, and view the Spark log via the spark_log function:\n\nspark_web(sc)\nspark_log(sc)\n\nYou can disconnect from Spark using the spark_disconnect function:\n\nspark_disconnect(sc)\n\n\n\nCollect\nThe collect function transfers data from Spark into R. The data are collected from a cluster environment and transfered into local R memory. In the process, all data is first transfered from executor nodes to the driver node. Therefore, the driver node must have enough memory to collect all the data.\nCollecting data on the driver node is relatively slow. The process also inflates the data as it moves from the executor nodes to the driver node. Caution should be used when collecting large data.\nThe following parameters could be adjusted to avoid OutOfMemory and Timeout errors:\n\nspark.executor.heartbeatInterval\nspark.network.timeout\nspark.driver.extraJavaOptions\nspark.driver.memory\nspark.yarn.driver.memoryOverhead\nspark.driver.maxResultSize"
  },
  {
    "href": "deployment.html#configuration",
    "title": "Deployment and Configuration",
    "section": "Configuration",
    "text": "This section describes the various options available for configuring both the behavior of the sparklyr package as well as the underlying Spark cluster. Creating multiple configuration profiles (e.g. development, test, production) is also covered.\n\nConfig Files\nThe configuration for a Spark connection is specified via the config parameter of the spark_connect function. By default the configuration is established by calling the spark_config function. This code represents the default behavior:\n\nspark_connect(master = \"local\", config = spark_config())\n\nBy default the spark_config function reads configuration data from a file named config.yml located in the current working directory (or in parent directories if not located in the working directory). This file is not required and only need be provided for overriding default behavior. You can also specify an alternate config file name and/or location.\nThe config.yml file is in turn processed using the config package, which enables support for multiple named configuration profiles.\n\n\nPackage Options\nThere are a number of options available to configure the behavior of the sparklyr package:\nFor example, this configuration file sets the number of local cores to 4 and the amount of memory allocated for the Spark driver to 4G:\ndefault:\n  sparklyr.cores.local: 4\n  sparklyr.shell.driver-memory: 4G\nNote that the use of default will be explained below in Multiple Profiles.\n\nSpark\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nsparklyr.shell.*\nCommand line parameters to pass to spark-submit. For example, sparklyr.shell.executor-memory: 20G configures --executor-memory 20G (see the Spark documentation for details on supported options).\n\n\n\n\n\nRuntime\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nsparklyr.cores.local\nNumber of cores to use when running in local mode (defaults to parallel::detectCores).\n\n\nsparklyr.sparkui.url\nConfigures the url to the Spark UI web interface when calling spark_web.\n\n\nsparklyr.defaultPackages\nList of default Spark packages to install in the cluster (defaults to “com.databricks:spark-csv_2.11:1.3.0” and “com.amazonaws:aws-java-sdk-pom:1.10.34”).\n\n\nsparklyr.sanitize.column.names\nAllows Spark to automatically rename column names to conform to Spark naming restrictions.\n\n\n\n\n\nDiagnostics\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nsparklyr.backend.threads\nNumber of threads to use in the sparklyr backend to process incoming connections form the sparklyr client.\n\n\nsparklyr.app.jar\nThe application jar to be submitted in Spark submit.\n\n\nsparklyr.ports.file\nPath to the ports file used to share connection information to the sparklyr backend.\n\n\nsparklyr.ports.wait.seconds\nNumber of seconds to wait while for the Spark connection to initialize.\n\n\nsparklyr.verbose\nProvide additional feedback while performing operations. Currently used to communicate which column names are being sanitized in sparklyr.sanitize.column.names.\n\n\n\n\n\n\nSpark Options\nYou can also use config.yml to specify arbitrary Spark configuration properties:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nspark.*\nConfiguration settings for the Spark context (applied by creating a SparkConf containing the specified properties). For example, spark.executor.memory: 1g configures the memory available in each executor (see Spark Configuration for additional options.)\n\n\nspark.sql.*\nConfiguration settings for the Spark SQL context (applied using SET). For instance, spark.sql.shuffle.partitions configures number of partitions to use while shuffling (see SQL Programming Guide for additional options).\n\n\n\nFor example, this configuration file sets a custom scratch directory for Spark and specifies 100 as the number of partitions to use when shuffling data for joins or aggregations:\ndefault:\n  spark.local.dir: /tmp/spark-scratch\n  spark.sql.shuffle.partitions: 100\n\n\nUser Options\nYou can also include arbitrary custom user options within the config.yml file. These can be named anything you like so long as they do not use either spark or sparklyr as a prefix. For example, this configuration file defines dataset and sample-size options:\ndefault:\n  dataset: \"observations.parquet\"\n  sample-size: 10000\n\n\nMultiple Profiles\nThe config package enables the definition of multiple named configuration profiles for different environments (e.g. default, test, production). All environments automatically inherit from the default environment and can optionally also inherit from each other.\nFor example, you might want to use a distinct datasets for development and testing or might want to use custom Spark configuration properties that are only applied when running on a production cluster. Here’s how that would be expressed in config.yml:\ndefault:\n  dataset: \"observations-dev.parquet\"\n  sample-size: 10000\n\nproduction:\n  spark.memory.fraction: 0.9\n  spark.rdd.compress: true\n  dataset: \"observations.parquet\"\n  sample-size: null\nYou can also use this feature to specify distinct Spark master nodes for different environments, for example:\ndefault:\n  spark.master: \"local\"\n\nproduction:\n  spark.master: \"spark://local:7077\"\nWith this configuration, you can omit the master argument entirely from the call to spark_connect:\n\nsc <- spark_connect()\n\nNote that the currently active configuration is determined via the value of R_CONFIG_ACTIVE environment variable. See the config package documentation for additional details.\n\n\nTuning\nIn general, you will need to tune a Spark cluster for it to perform well. Spark applications tend to consume a lot of resources. There are many knobs to control the performance of Yarn and executor (i.e. worker) nodes in a cluster. Some of the parameters to pay attention to are as follows:\n\nspark.executor.heartbeatInterval\nspark.network.timeout\nspark.executor.extraJavaOptions\nspark.executor.memory\nspark.yarn.executor.memoryOverhead\nspark.executor.cores\nspark.executor.instances (if is not enabled)\n\n\nExample Config\nHere is an example spark configuration for an EMR cluster on AWS with 1 master and 2 worker nodes. Eache node has 8 vCPU and 61 GiB of memory.\n\n\n\nParameter\nValue\n\n\n\n\nspark.driver.extraJavaOptions\nappend -XX:MaxPermSize=30G\n\n\nspark.driver.maxResultSize\n0\n\n\nspark.driver.memory\n30G\n\n\nspark.yarn.driver.memoryOverhead\n4096\n\n\nspark.yarn.executor.memoryOverhead\n4096\n\n\nspark.executor.memory\n4G\n\n\nspark.executor.cores\n2\n\n\nspark.dynamicAllocation.maxExecutors\n15\n\n\n\nConfiguration parameters can be set in the config R object or can be set in the config.yml. Alternatively, they can be set in the spark-defaults.conf.\n\nConfiguration in R script\n\nconfig <- spark_config()\nconfig$spark.executor.cores <- 2\nconfig$spark.executor.memory <- \"4G\"\nsc <- spark_connect(master = \"yarn-client\", config = config, version = '2.0.0')\n\n\n\nConfiguration in YAML script\ndefault:\n  spark.executor.cores: 2\n  spark.executor.memory: 4G"
  },
  {
    "href": "deployment.html#rstudio-server",
    "title": "Deployment and Configuration",
    "section": "RStudio Server",
    "text": "RStudio Server provides a web-based IDE interface to a remote R session, making it ideal for use as a front-end to a Spark cluster. This section covers some additional configuration options that are useful for RStudio Server.\n\nConnection Options\nThe RStudio IDE Spark pane provides a New Connection dialog to assist in connecting with both local instances of Spark and Spark clusters:\n\nYou can configure which connection choices are presented using the rstudio.spark.connections option. By default, users are presented with possibility of both local and cluster connections, however, you can modify this behavior to present only one of these, or even a specific Spark master URL. Some commonly used combinations of connection choices include:\n\n\n\n\n\n\n\nValue\nDescription\n\n\n\n\nc(\"local\", \"cluster\")\nDefault. Present connections to both local and cluster Spark instances.\n\n\n\"local\"\nPresent only connections to local Spark instances.\n\n\n\"spark://local:7077\"\nPresent only a connection to a specific Spark cluster.\n\n\nc(\"spark://local:7077\", \"cluster\")\nPresent a connection to a specific Spark cluster and other clusters.\n\n\n\nThis option should generally be set within Rprofile.site. For example:\n\noptions(rstudio.spark.connections = \"spark://local:7077\")\n\n\n\nSpark Installations\nIf you are running within local mode (as opposed to cluster mode) you may want to provide pre-installed Spark version(s) to be shared by all users of the server. You can do this by installing Spark versions within a shared directory (e.g. /opt/spark) then designating it as the Spark installation directory.\nFor example, after installing one or more versions of Spark to /opt/spark you would add the following to Rprofile.site:\n\noptions(spark.install.dir = \"/opt/spark\")\n\nIf this directory is read-only for ordinary users then RStudio will not offer installation of additional versions, which will help guide users to a version that is known to be compatible with versions of Spark deployed on clusters in the same organization."
  },
  {
    "href": "reference/sdf_pivot.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Construct a pivot table over a Spark Dataframe, using a syntax similar to that from reshape2::dcast ."
  },
  {
    "href": "reference/sdf_pivot.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_pivot(x, formula, fun.aggregate = \"count\")"
  },
  {
    "href": "reference/sdf_pivot.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nA two-sided list() formula of the form x_1 + x_2 + ... ~ y_1 . The left-hand side of the formula indicates which variables are used for grouping, and the right-hand side indicates which variable is used for pivoting. Currently, only a single pivot column is supported.\n\n\nfun.aggregate\nHow should the grouped dataset be aggregated? Can be a length-one character vector, giving the name of a Spark aggregation function to be called; a named list() list mapping column names to an aggregation method, or an list() function that is invoked on the grouped dataset."
  },
  {
    "href": "reference/sdf_pivot.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\n# aggregating by mean\niris_tbl %>%\nmutate(Petal_Width = ifelse(Petal_Width > 1.5, \"High\", \"Low\")) %>%\nsdf_pivot(Petal_Width ~ Species,\nfun.aggregate = list(Petal_Length = \"mean\")\n)\n\n# aggregating all observations in a list\niris_tbl %>%\nmutate(Petal_Width = ifelse(Petal_Width > 1.5, \"High\", \"Low\")) %>%\nsdf_pivot(Petal_Width ~ Species,\nfun.aggregate = list(Petal_Length = \"collect_list\")\n)"
  },
  {
    "href": "reference/hof_exists.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Determines whether an element satisfying the given predicate exists in each array from an array column (this is essentially a dplyr wrapper for the exists(array<T>, function<T, Boolean>): Boolean built-in Spark SQL function)"
  },
  {
    "href": "reference/hof_exists.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "hof_exists(x, pred, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "href": "reference/hof_exists.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nThe Spark data frame to search\n\n\npred\nA boolean predicate\n\n\nexpr\nThe array being searched (could be any SQL expression evaluating to an array)\n\n\ndest_col\nColumn to store the search result\n\n\n...\nAdditional params to dplyr::mutate"
  },
  {
    "href": "reference/sdf_unnest_longer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Expand a struct column or an array column within a Spark dataframe into one or more rows, similar what to tidyr::unnest_longer does to an R dataframe. An index column, if included, will be 1-based if col is an array column."
  },
  {
    "href": "reference/sdf_unnest_longer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_unnest_longer(\n  data,\n  col,\n  values_to = NULL,\n  indices_to = NULL,\n  include_indices = NULL,\n  names_repair = \"check_unique\",\n  ptype = list(),\n  transform = list()\n)"
  },
  {
    "href": "reference/sdf_unnest_longer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\ndata\nThe Spark dataframe to be unnested\n\n\ncol\nThe struct column to extract components from\n\n\nvalues_to\nName of column to store vector values. Defaults to col.\n\n\nindices_to\nA string giving the name of column which will contain the inner names or position (if not named) of the values. Defaults to col with _id suffix\n\n\ninclude_indices\nWhether to include an index column. An index column will be included by default if col is a struct column. It will also be included if indices_to is not NULL.\n\n\nnames_repair\nStrategy for fixing duplicate column names (the semantic will be exactly identical to that of .name_repair option in tibble )\n\n\nptype\nOptionally, supply an R data frame prototype for the output. Each column of the unnested result will be casted based on the Spark equivalent of the type of the column with the same name within ptype, e.g., if ptype has a column x of type character, then column x of the unnested result will be casted from its original SQL type to StringType.\n\n\ntransform\nOptionally, a named list of transformation functions applied"
  },
  {
    "href": "reference/sdf_unnest_longer.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.4.0\")\n\n# unnesting a struct column\nsdf <- copy_to(\nsc,\ntibble::tibble(\nx = 1:3,\ny = list(list(a = 1, b = 2), list(a = 3, b = 4), list(a = 5, b = 6))\n)\n)\n\nunnested <- sdf %>% sdf_unnest_longer(y, indices_to = \"attr\")\n\n# unnesting an array column\nsdf <- copy_to(\nsc,\ntibble::tibble(\nx = 1:3,\ny = list(1:10, 1:5, 1:2)\n)\n)\n\nunnested <- sdf %>% sdf_unnest_longer(y, indices_to = \"array_idx\")"
  },
  {
    "href": "reference/sdf_rlnorm.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a log normal distribution."
  },
  {
    "href": "reference/sdf_rlnorm.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_rlnorm(\n  sc,\n  n,\n  meanlog = 0,\n  sdlog = 1,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "href": "reference/sdf_rlnorm.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nmeanlog\nThe mean of the normally distributed natural logarithm of this distribution.\n\n\nsdlog\nThe Standard deviation of the normally distributed natural logarithm of this distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "href": "reference/sdf_rlnorm.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark statistical routines: sdf_rbeta , sdf_rbinom , sdf_rcauchy , sdf_rchisq , sdf_rexp , sdf_rgamma , sdf_rgeom , sdf_rhyper , sdf_rnorm , sdf_rpois , sdf_rt , sdf_runif , sdf_rweibull"
  },
  {
    "href": "reference/sdf_persist.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Persist a Spark DataFrame, forcing any pending computations and (optionally) serializing the results to disk."
  },
  {
    "href": "reference/sdf_persist.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_persist(x, storage.level = \"MEMORY_AND_DISK\", name = NULL)"
  },
  {
    "href": "reference/sdf_persist.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nstorage.level\nThe storage level to be used. Please view the Spark Documentation for information on what storage levels are accepted.\n\n\nname\nA name to assign this table. Passed to [sdf_register()]."
  },
  {
    "href": "reference/sdf_persist.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Spark DataFrames invoke their operations lazily – pending operations are deferred until their results are actually needed. Persisting a Spark DataFrame effectively ‘forces’ any pending computations, and then persists the generated Spark DataFrame as requested (to memory, to disk, or otherwise).\nUsers of Spark should be careful to persist the results of any computations which are non-deterministic – otherwise, one might see that the values within a column seem to ‘change’ as new operations are performed on that data set."
  },
  {
    "href": "reference/stream_write_parquet.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Writes a Spark dataframe stream into a parquet stream."
  },
  {
    "href": "reference/stream_write_parquet.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_write_parquet(\n  x,\n  path,\n  mode = c(\"append\", \"complete\", \"update\"),\n  trigger = stream_trigger_interval(),\n  checkpoint = file.path(path, \"checkpoints\", random_string(\"\")),\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/stream_write_parquet.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe destination path. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\" , \"complete\" or \"update\" .\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous .\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/stream_write_parquet.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark stream serialization: stream_read_csv , stream_read_delta , stream_read_json , stream_read_kafka , stream_read_orc , stream_read_parquet , stream_read_socket , stream_read_text , stream_write_console , stream_write_csv , stream_write_delta , stream_write_json , stream_write_kafka , stream_write_memory , stream_write_orc , stream_write_text"
  },
  {
    "href": "reference/stream_write_parquet.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\n\nsdf_len(sc, 10) %>% spark_write_parquet(\"parquet-in\")\n\nstream <- stream_read_parquet(sc, \"parquet-in\") %>% stream_write_parquet(\"parquet-out\")\n\nstream_stop(stream)"
  },
  {
    "href": "reference/sdf_rt.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a t-distribution."
  },
  {
    "href": "reference/sdf_rt.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_rt(sc, n, df, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "href": "reference/sdf_rt.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\ndf\nDegrees of freedom (> 0, maybe non-integer).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "href": "reference/sdf_rt.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark statistical routines: sdf_rbeta , sdf_rbinom , sdf_rcauchy , sdf_rchisq , sdf_rexp , sdf_rgamma , sdf_rgeom , sdf_rhyper , sdf_rlnorm , sdf_rnorm , sdf_rpois , sdf_runif , sdf_rweibull"
  },
  {
    "href": "reference/jfloat_array.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Instantiate an Array[Float] object with the value specified. NOTE: this method is useful when one has to invoke a Java/Scala method requiring an Array[Float] as one of its parameters."
  },
  {
    "href": "reference/jfloat_array.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "jfloat_array(sc, x)"
  },
  {
    "href": "reference/jfloat_array.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nx\nA numeric vector in R."
  },
  {
    "href": "reference/jfloat_array.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"spark://HOST:PORT\")\n\njflt_arr <- jfloat_array(sc, c(-1.23e-8, 0, -1.23e-8))\n# jflt_arr is now a reference an array of java.lang.Float"
  },
  {
    "href": "reference/spark_write_parquet.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Serialize a Spark DataFrame to the Parquet format."
  },
  {
    "href": "reference/spark_write_parquet.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_write_parquet(\n  x,\n  path,\n  mode = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/spark_write_parquet.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure. For more details see also http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options. See http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration .\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_write_parquet.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/spark_adaptive_query_execution.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Retrieves or sets whether Spark adaptive query execution is enabled"
  },
  {
    "href": "reference/spark_adaptive_query_execution.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_adaptive_query_execution(sc, enable = NULL)"
  },
  {
    "href": "reference/spark_adaptive_query_execution.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nenable\nWhether to enable Spark adaptive query execution. Defaults to NULL to retrieve configuration entries."
  },
  {
    "href": "reference/spark_adaptive_query_execution.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark runtime configuration: spark_advisory_shuffle_partition_size , spark_auto_broadcast_join_threshold , spark_coalesce_initial_num_partitions , spark_coalesce_min_num_partitions , spark_coalesce_shuffle_partitions , spark_session_config"
  },
  {
    "href": "reference/ft_one_hot_encoder.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "One-hot encoding maps a column of label indices to a column of binary vectors, with at most a single one-value. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features. Typically, used with ft_string_indexer() to index a column first."
  },
  {
    "href": "reference/ft_one_hot_encoder.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_one_hot_encoder(\n  x,\n  input_cols = NULL,\n  output_cols = NULL,\n  handle_invalid = NULL,\n  drop_last = TRUE,\n  uid = random_string(\"one_hot_encoder_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_one_hot_encoder.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_cols\nThe name of the input columns.\n\n\noutput_cols\nThe name of the output columns.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error”\n\n\ndrop_last\nWhether to drop the last category. Defaults to TRUE .\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_one_hot_encoder.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_one_hot_encoder.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/spark_connection.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Retrieve the spark_connection associated with an list() object."
  },
  {
    "href": "reference/spark_connection.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_connection(x, ...)"
  },
  {
    "href": "reference/spark_connection.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn list() object from which a spark_connection can be obtained.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/get_spark_sql_catalog_implementation.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Retrieve the Spark connection’s SQL catalog implementation property"
  },
  {
    "href": "reference/get_spark_sql_catalog_implementation.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "get_spark_sql_catalog_implementation(sc)"
  },
  {
    "href": "reference/get_spark_sql_catalog_implementation.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nspark_connection"
  },
  {
    "href": "reference/get_spark_sql_catalog_implementation.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "spark.sql.catalogImplementation property from the connection’s runtime configuration"
  },
  {
    "href": "reference/stream_stop.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Stops processing data from a Spark stream."
  },
  {
    "href": "reference/stream_stop.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_stop(stream)"
  },
  {
    "href": "reference/stream_stop.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nstream\nThe spark stream object to be stopped."
  },
  {
    "href": "reference/spark_write_rds.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Write Spark dataframe to RDS files. Each partition of the dataframe will be exported to a separate RDS file so that all partitions can be processed in parallel."
  },
  {
    "href": "reference/spark_write_rds.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_write_rds(x, dest_uri)"
  },
  {
    "href": "reference/spark_write_rds.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame to be exported\n\n\ndest_uri\nCan be a URI template containing ” partitionId ” (e.g., “hdfs://my_data_part_ partitionId .rds”) where ” partitionId ” will be substituted with ID of each partition using glue, or a list of URIs to be assigned to RDS output from all partitions (e.g., “hdfs://my_data_part_0.rds”, “hdfs://my_data_part_1.rds”, and so on) If working with a Spark instance running locally, then all URIs should be in “file://” form. Otherwise the scheme of the URI should reflect the underlying file system the Spark instance is working with (e.g., “hdfs://”). If the resulting list of URI(s) does not contain unique values, then it will be post-processed with make.unique() to ensure uniqueness."
  },
  {
    "href": "reference/spark_write_rds.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "A tibble containing partition ID and RDS file location for each partition of the input Spark dataframe."
  },
  {
    "href": "reference/stream_read_json.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Reads a JSON stream as a Spark dataframe stream."
  },
  {
    "href": "reference/stream_read_json.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_read_json(sc, path, name = NULL, columns = NULL, options = list(), ...)"
  },
  {
    "href": "reference/stream_read_json.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType , \"boolean\" for BooleanType , \"byte\" for ByteType , \"integer\" for IntegerType , \"integer64\" for LongType , \"double\" for DoubleType , \"character\" for StringType , \"timestamp\" for TimestampType and \"date\" for DateType .\n\n\noptions\nA list of strings with additional options.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/stream_read_json.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark stream serialization: stream_read_csv , stream_read_delta , stream_read_kafka , stream_read_orc , stream_read_parquet , stream_read_socket , stream_read_text , stream_write_console , stream_write_csv , stream_write_delta , stream_write_json , stream_write_kafka , stream_write_memory , stream_write_orc , stream_write_parquet , stream_write_text"
  },
  {
    "href": "reference/stream_read_json.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\n\ndir.create(\"json-in\")\njsonlite::write_json(list(a = c(1, 2), b = c(10, 20)), \"json-in/data.json\")\n\njson_path <- file.path(\"file://\", getwd(), \"json-in\")\n\nstream <- stream_read_json(sc, json_path) %>% stream_write_json(\"json-out\")\n\nstream_stop(stream)"
  },
  {
    "href": "reference/spark_read_source.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Read from a generic source into a Spark DataFrame."
  },
  {
    "href": "reference/spark_read_source.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_read_source(\n  sc,\n  name = NULL,\n  path = name,\n  source,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  columns = NULL,\n  ...\n)"
  },
  {
    "href": "reference/spark_read_source.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nsource\nA data source capable of reading data.\n\n\noptions\nA list of strings with additional options. See http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration .\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType , \"boolean\" for BooleanType , \"byte\" for ByteType , \"integer\" for IntegerType , \"integer64\" for LongType , \"double\" for DoubleType , \"character\" for StringType , \"timestamp\" for TimestampType and \"date\" for DateType .\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_read_source.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/spark_pipeline_stage.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Helper function to create pipeline stage objects with common parameter setters."
  },
  {
    "href": "reference/spark_pipeline_stage.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_pipeline_stage(\n  sc,\n  class,\n  uid,\n  features_col = NULL,\n  label_col = NULL,\n  prediction_col = NULL,\n  probability_col = NULL,\n  raw_prediction_col = NULL,\n  k = NULL,\n  max_iter = NULL,\n  seed = NULL,\n  input_col = NULL,\n  input_cols = NULL,\n  output_col = NULL,\n  output_cols = NULL\n)"
  },
  {
    "href": "reference/spark_pipeline_stage.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection object.\n\n\nclass\nClass name for the pipeline stage.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula .\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nk\nThe number of clusters to create\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\ninput_col\nThe name of the input column.\n\n\ninput_cols\nNames of output columns.\n\n\noutput_col\nThe name of the output column.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold."
  },
  {
    "href": "reference/compile_package_jars.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Compile the scala source files contained within an list() package into a Java Archive ( jar ) file that can be loaded and used within a Spark environment."
  },
  {
    "href": "reference/compile_package_jars.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "compile_package_jars(..., spec = NULL)"
  },
  {
    "href": "reference/compile_package_jars.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\n...\nOptional compilation specifications, as generated by spark_compilation_spec . When no arguments are passed, spark_default_compilation_spec is used instead.\n\n\nspec\nAn optional list of compilation specifications. When set, this option takes precedence over arguments passed to ... ."
  },
  {
    "href": "reference/ml_gaussian_mixture.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "This class performs expectation maximization for multivariate Gaussian Mixture Models (GMMs). A GMM represents a composite distribution of independent Gaussian distributions with associated “mixing” weights specifying each’s contribution to the composite. Given a set of sample points, this class will maximize the log-likelihood for a mixture of k Gaussians, iterating until the log-likelihood changes by less than tol , or until it has reached the max number of iterations. While this process is generally guaranteed to converge, it is not guaranteed to find a global optimum."
  },
  {
    "href": "reference/ml_gaussian_mixture.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_gaussian_mixture(\n  x,\n  formula = NULL,\n  k = 2,\n  max_iter = 100,\n  tol = 0.01,\n  seed = NULL,\n  features_col = \"features\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  uid = random_string(\"gaussian_mixture_\"),\n  ...\n)"
  },
  {
    "href": "reference/ml_gaussian_mixture.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nUsed when x is a tbl_spark . R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nk\nThe number of clusters to create\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments, see Details."
  },
  {
    "href": "reference/ml_gaussian_mixture.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the clustering estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , an estimator is constructed then immediately fit with the input tbl_spark , returning a clustering model.\ntbl_spark , with formula or features specified: When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the estimator. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model . This signature does not apply to ml_lda() ."
  },
  {
    "href": "reference/ml_gaussian_mixture.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-clustering.html for more information on the set of clustering algorithms.\nOther ml clustering algorithms: ml_bisecting_kmeans , ml_kmeans , ml_lda"
  },
  {
    "href": "reference/ml_gaussian_mixture.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\ngmm_model <- ml_gaussian_mixture(iris_tbl, Species ~ .)\npred <- sdf_predict(iris_tbl, gmm_model)\nml_clustering_evaluator(pred)"
  },
  {
    "href": "reference/copy_to.spark_connection.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Copy an R data.frame to Spark, and return a reference to the generated Spark DataFrame as a tbl_spark . The returned object will act as a dplyr -compatible interface to the underlying Spark table."
  },
  {
    "href": "reference/copy_to.spark_connection.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "list(list(\"copy_to\"), list(\"spark_connection\"))(\n  dest,\n  df,\n  name = spark_table_name(substitute(df)),\n  overwrite = FALSE,\n  memory = TRUE,\n  repartition = 0L,\n  ...\n)"
  },
  {
    "href": "reference/copy_to.spark_connection.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\ndest\nA spark_connection .\n\n\ndf\nAn list() data.frame .\n\n\nname\nThe name to assign to the copied table in Spark.\n\n\noverwrite\nBoolean; overwrite a pre-existing table with the name name if one already exists?\n\n\nmemory\nBoolean; should the table be cached into memory?\n\n\nrepartition\nThe number of partitions to use when distributing the table across the Spark cluster. The default (0) can be used to avoid partitioning.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/copy_to.spark_connection.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "A tbl_spark , representing a dplyr -compatible interface to a Spark DataFrame."
  },
  {
    "href": "reference/ft_lsh_utils.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Utility functions for LSH models"
  },
  {
    "href": "reference/ft_lsh_utils.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_approx_nearest_neighbors(\n  model,\n  dataset,\n  key,\n  num_nearest_neighbors,\n  dist_col = \"distCol\"\n)\nml_approx_similarity_join(\n  model,\n  dataset_a,\n  dataset_b,\n  threshold,\n  dist_col = \"distCol\"\n)"
  },
  {
    "href": "reference/ft_lsh_utils.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nmodel\nA fitted LSH model, returned by either ft_minhash_lsh() or ft_bucketed_random_projection_lsh() .\n\n\ndataset\nThe dataset to search for nearest neighbors of the key.\n\n\nkey\nFeature vector representing the item to search for.\n\n\nnum_nearest_neighbors\nThe maximum number of nearest neighbors.\n\n\ndist_col\nOutput column for storing the distance between each result row and the key.\n\n\ndataset_a\nOne of the datasets to join.\n\n\ndataset_b\nAnother dataset to join.\n\n\nthreshold\nThe threshold for the distance of row pairs."
  },
  {
    "href": "reference/spark_connection-class.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "spark_connection class"
  },
  {
    "href": "reference/right_join.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See right_join for more details."
  },
  {
    "href": "reference/mutate.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See mutate for more details."
  },
  {
    "href": "reference/sdf_runif.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generator method for creating a single-column Spark dataframes comprised of i.i.d. samples from the uniform distribution U(0, 1)."
  },
  {
    "href": "reference/sdf_runif.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_runif(\n  sc,\n  n,\n  min = 0,\n  max = 1,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "href": "reference/sdf_runif.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nmin\nThe lower limit of the distribution.\n\n\nmax\nThe upper limit of the distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "href": "reference/sdf_runif.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark statistical routines: sdf_rbeta , sdf_rbinom , sdf_rcauchy , sdf_rchisq , sdf_rexp , sdf_rgamma , sdf_rgeom , sdf_rhyper , sdf_rlnorm , sdf_rnorm , sdf_rpois , sdf_rt , sdf_rweibull"
  },
  {
    "href": "reference/stream_write_memory.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Writes a Spark dataframe stream into a memory stream."
  },
  {
    "href": "reference/stream_write_memory.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_write_memory(\n  x,\n  name = random_string(\"sparklyr_tmp_\"),\n  mode = c(\"append\", \"complete\", \"update\"),\n  trigger = stream_trigger_interval(),\n  checkpoint = file.path(\"checkpoints\", name, random_string(\"\")),\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/stream_write_memory.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nname\nThe name to assign to the newly generated stream.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\" , \"complete\" or \"update\" .\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous .\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/stream_write_memory.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark stream serialization: stream_read_csv , stream_read_delta , stream_read_json , stream_read_kafka , stream_read_orc , stream_read_parquet , stream_read_socket , stream_read_text , stream_write_console , stream_write_csv , stream_write_delta , stream_write_json , stream_write_kafka , stream_write_orc , stream_write_parquet , stream_write_text"
  },
  {
    "href": "reference/stream_write_memory.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\n\ndir.create(\"csv-in\")\nwrite.csv(iris, \"csv-in/data.csv\", row.names = FALSE)\n\ncsv_path <- file.path(\"file://\", getwd(), \"csv-in\")\n\nstream <- stream_read_csv(sc, csv_path) %>% stream_write_memory(\"csv-out\")\n\nstream_stop(stream)"
  },
  {
    "href": "reference/ft_normalizer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Normalize a vector to have unit norm using the given p-norm."
  },
  {
    "href": "reference/ft_normalizer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_normalizer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  p = 2,\n  uid = random_string(\"normalizer_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_normalizer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\np\nNormalization in L^p space. Must be >= 1. Defaults to 2.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_normalizer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_normalizer.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/filter.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See filter for more details."
  },
  {
    "href": "reference/ml_als_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "These methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "href": "reference/ml_als_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "list(list(\"tidy\"), list(\"ml_model_als\"))(x, ...)\nlist(list(\"augment\"), list(\"ml_model_als\"))(x, newdata = NULL, ...)\nlist(list(\"glance\"), list(\"ml_model_als\"))(x, ...)"
  },
  {
    "href": "reference/ml_als_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n...\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "href": "reference/spark_auto_broadcast_join_threshold.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command ANALYZE TABLE <tableName> COMPUTE STATISTICS noscan has been run, and file-based data source tables where the statistics are computed directly on the files of data."
  },
  {
    "href": "reference/spark_auto_broadcast_join_threshold.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_auto_broadcast_join_threshold(sc, threshold = NULL)"
  },
  {
    "href": "reference/spark_auto_broadcast_join_threshold.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nthreshold\nMaximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. Defaults to NULL to retrieve configuration entries."
  },
  {
    "href": "reference/spark_auto_broadcast_join_threshold.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark runtime configuration: spark_adaptive_query_execution , spark_advisory_shuffle_partition_size , spark_coalesce_initial_num_partitions , spark_coalesce_min_num_partitions , spark_coalesce_shuffle_partitions , spark_session_config"
  },
  {
    "href": "reference/spark_install_find.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Find a given Spark installation by version."
  },
  {
    "href": "reference/spark_install_find.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_install_find(\n  version = NULL,\n  hadoop_version = NULL,\n  installed_only = TRUE,\n  latest = FALSE,\n  hint = FALSE\n)"
  },
  {
    "href": "reference/spark_install_find.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nversion\nVersion of Spark to install. See spark_available_versions for a list of supported versions\n\n\nhadoop_version\nVersion of Hadoop to install. See spark_available_versions for a list of supported versions\n\n\ninstalled_only\nSearch only the locally installed versions?\n\n\nlatest\nCheck for latest version?\n\n\nhint\nOn failure should the installation code be provided?"
  },
  {
    "href": "reference/spark_read_parquet.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Read a Parquet file into a Spark DataFrame."
  },
  {
    "href": "reference/spark_read_parquet.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_read_parquet(\n  sc,\n  name = NULL,\n  path = name,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  columns = NULL,\n  schema = NULL,\n  ...\n)"
  },
  {
    "href": "reference/spark_read_parquet.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\noptions\nA list of strings with additional options. See http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration .\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType , \"boolean\" for BooleanType , \"byte\" for ByteType , \"integer\" for IntegerType , \"integer64\" for LongType , \"double\" for DoubleType , \"character\" for StringType , \"timestamp\" for TimestampType and \"date\" for DateType .\n\n\nschema\nA (java) read schema. Useful for optimizing read operation on nested data.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_read_parquet.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "You can read data from HDFS ( hdfs:// ), S3 ( s3a:// ), as well as the local file system ( file:// ).\nIf you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf spark.hadoop.fs.s3a.access.key , spark.hadoop.fs.s3a.secret.key or any of the methods outlined in the aws-sdk documentation Working with AWS credentials In order to work with the newer s3a:// protocol also set the values for spark.hadoop.fs.s3a.impl and spark.hadoop.fs.s3a.endpoint . In addition, to support v4 of the S3 api be sure to pass the -Dcom.amazonaws.services.s3.enableV4 driver options for the config key spark.driver.extraJavaOptions For instructions on how to configure s3n:// check the hadoop documentation: s3n authentication properties"
  },
  {
    "href": "reference/spark_read_parquet.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/livy_config.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Create a Spark Configuration for Livy"
  },
  {
    "href": "reference/livy_config.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "livy_config(\n  config = spark_config(),\n  username = NULL,\n  password = NULL,\n  negotiate = FALSE,\n  custom_headers = list(`X-Requested-By` = \"sparklyr\"),\n  proxy = NULL,\n  curl_opts = NULL,\n  ...\n)"
  },
  {
    "href": "reference/livy_config.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nconfig\nOptional base configuration\n\n\nusername\nThe username to use in the Authorization header\n\n\npassword\nThe password to use in the Authorization header\n\n\nnegotiate\nWhether to use gssnegotiate method or not\n\n\ncustom_headers\nList of custom headers to append to http requests. Defaults to list(\"X-Requested-By\" = \"sparklyr\") .\n\n\nproxy\nEither NULL or a proxy specified by httr::use_proxy(). Defaults to NULL.\n\n\ncurl_opts\nList of CURL options (e.g., verbose, connecttimeout, dns_cache_timeout, etc, see httr::httr_options() for a list of valid options) – NOTE: these configurations are for libcurl only and separate from HTTP headers or Livy session parameters.\n\n\n...\nadditional Livy session parameters"
  },
  {
    "href": "reference/livy_config.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Extends a Spark spark_config() configuration with settings for Livy. For instance, username and password define the basic authentication settings for a Livy session.\nThe default value of \"custom_headers\" is set to list(\"X-Requested-By\" = \"sparklyr\") in order to facilitate connection to Livy servers with CSRF protection enabled.\nAdditional parameters for Livy sessions are: list(“”, ” “, list(list(list(”proxy_user”)), list(“User to impersonate when starting the session”)), “”, ” “, list(list(list(”jars”)), list(“jars to be used in this session”)), “”, ” “, list(list(list(”py_files”)), list(“Python files to be used in this session”)), “”, ” “, list(list(list(”files”)), list(“files to be used in this session”)), “”, ” “, list(list(list(”driver_memory”)), list(“Amount of memory to use for the driver process”)), “”, ” “, list(list(list(”driver_cores”)), list(“Number of cores to use for the driver process”)), “”, ” “, list(list(list(”executor_memory”)), list(“Amount of memory to use per executor process”)), “”, ” “, list(list(list(”executor_cores”)), list(“Number of cores to use for each executor”)), “”, ” “, list(list(list(”num_executors”)), list(“Number of executors to launch for this session”)), “”, ” “, list(list(list(”archives”)), list(“Archives to be used in this session”)), “”, ” “, list(list(list(”queue”)), list(“The name of the YARN queue to which submitted”)), “”, ” “, list(list(list(”name”)), list(“The name of this session”)), “”, ” “, list(list(list(”heartbeat_timeout”)), list(“Timeout in seconds to which session be orphaned”)), “”, ” “, list(list(list(”conf”)), list(“Spark configuration properties (Map of key=value)”)), “”)\nNote that queue is supported only by version 0.4.0 of Livy or newer. If you are using the older one, specify queue via config (e.g. config = spark_config(spark.yarn.queue = \"my_queue\") )."
  },
  {
    "href": "reference/livy_config.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Named list with configuration data"
  },
  {
    "href": "reference/connection_config.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Read configuration values for a connection"
  },
  {
    "href": "reference/connection_config.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "connection_config(sc, prefix, not_prefix = list())"
  },
  {
    "href": "reference/connection_config.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nspark_connection\n\n\nprefix\nPrefix to read parameters for (e.g. spark.context. , spark.sql. , etc.)\n\n\nnot_prefix\nPrefix to not include."
  },
  {
    "href": "reference/connection_config.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Named list of config parameters (note that if a prefix was specified then the names will not include the prefix)"
  },
  {
    "href": "reference/ml_generalized_linear_regression.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Perform regression using Generalized Linear Model (GLM)."
  },
  {
    "href": "reference/ml_generalized_linear_regression.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_generalized_linear_regression(\n  x,\n  formula = NULL,\n  family = \"gaussian\",\n  link = NULL,\n  fit_intercept = TRUE,\n  offset_col = NULL,\n  link_power = NULL,\n  link_prediction_col = NULL,\n  reg_param = 0,\n  max_iter = 25,\n  weight_col = NULL,\n  solver = \"irls\",\n  tol = 1e-06,\n  variance_power = 0,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"generalized_linear_regression_\"),\n  ...\n)"
  },
  {
    "href": "reference/ml_generalized_linear_regression.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nUsed when x is a tbl_spark . R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nfamily\nName of family which is a description of the error distribution to be used in the model. Supported options: “gaussian”, “binomial”, “poisson”, “gamma” and “tweedie”. Default is “gaussian”.\n\n\nlink\nName of link function which provides the relationship between the linear predictor and the mean of the distribution function. See for supported link functions.\n\n\nfit_intercept\nBoolean; should the model be fit with an intercept term?\n\n\noffset_col\nOffset column name. If this is not set, we treat all instance offsets as 0.0. The feature specified as offset has a constant coefficient of 1.0.\n\n\nlink_power\nIndex in the power link function. Only applicable to the Tweedie family. Note that link power 0, 1, -1 or 0.5 corresponds to the Log, Identity, Inverse or Sqrt link, respectively. When not set, this value defaults to 1 - variancePower, which matches the R “statmod” package.\n\n\nlink_prediction_col\nLink prediction (linear predictor) column name. Default is not set, which means we do not output link prediction.\n\n\nreg_param\nRegularization parameter (aka lambda)\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nweight_col\nThe name of the column to use as weights for the model fit.\n\n\nsolver\nSolver algorithm for optimization.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nvariance_power\nPower in the variance function of the Tweedie distribution which provides the relationship between the variance and mean of the distribution. Only applicable to the Tweedie family. (see Tweedie Distribution (Wikipedia) ) Supported values: 0 and [1, Inf). Note that variance power 0, 1, or 2 corresponds to the Gaussian, Poisson or Gamma family, respectively.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula .\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; see Details."
  },
  {
    "href": "reference/ml_generalized_linear_regression.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "When x is a tbl_spark and formula (alternatively, response and features ) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\" ) can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model , ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows.\nValid link functions for each family is listed below. The first link function of each family is the default one.\n\ngaussian: “identity”, “log”, “inverse”\nbinomial: “logit”, “probit”, “loglog”\npoisson: “log”, “identity”, “sqrt”\ngamma: “inverse”, “identity”, “log”\ntweedie: power link function specified through link_power . The default link power in the tweedie family is 1 - variance_power ."
  },
  {
    "href": "reference/ml_generalized_linear_regression.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a predictor is constructed then immediately fit with the input tbl_spark , returning a prediction model.\ntbl_spark , with formula : specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model ."
  },
  {
    "href": "reference/ml_generalized_linear_regression.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression , ml_decision_tree_classifier , ml_gbt_classifier , ml_isotonic_regression , ml_linear_regression , ml_linear_svc , ml_logistic_regression , ml_multilayer_perceptron_classifier , ml_naive_bayes , ml_one_vs_rest , ml_random_forest_classifier"
  },
  {
    "href": "reference/ml_generalized_linear_regression.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\n\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, name = \"mtcars_tbl\", overwrite = TRUE)\n\npartitions <- mtcars_tbl %>%\nsdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\nmtcars_training <- partitions$training\nmtcars_test <- partitions$test\n\n# Specify the grid\nfamily <- c(\"gaussian\", \"gamma\", \"poisson\")\nlink <- c(\"identity\", \"log\")\nfamily_link <- expand.grid(family = family, link = link, stringsAsFactors = FALSE)\nfamily_link <- data.frame(family_link, rmse = 0)\n\n# Train the models\nfor (i in seq_len(nrow(family_link))) {\nglm_model <- mtcars_training %>%\nml_generalized_linear_regression(mpg ~ .,\nfamily = family_link[i, 1],\nlink = family_link[i, 2]\n)\n\npred <- ml_predict(glm_model, mtcars_test)\nfamily_link[i, 3] <- ml_regression_evaluator(pred, label_col = \"mpg\")\n}\n\nfamily_link"
  },
  {
    "href": "reference/ml_lda.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Latent Dirichlet Allocation (LDA), a topic model designed for text documents."
  },
  {
    "href": "reference/ml_lda.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_lda(\n  x,\n  formula = NULL,\n  k = 10,\n  max_iter = 20,\n  doc_concentration = NULL,\n  topic_concentration = NULL,\n  subsampling_rate = 0.05,\n  optimizer = \"online\",\n  checkpoint_interval = 10,\n  keep_last_checkpoint = TRUE,\n  learning_decay = 0.51,\n  learning_offset = 1024,\n  optimize_doc_concentration = TRUE,\n  seed = NULL,\n  features_col = \"features\",\n  topic_distribution_col = \"topicDistribution\",\n  uid = random_string(\"lda_\"),\n  ...\n)\nml_describe_topics(model, max_terms_per_topic = 10)\nml_log_likelihood(model, dataset)\nml_log_perplexity(model, dataset)\nml_topics_matrix(model)"
  },
  {
    "href": "reference/ml_lda.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nUsed when x is a tbl_spark . R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nk\nThe number of clusters to create\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\ndoc_concentration\nConcentration parameter (commonly named “alpha”) for the prior placed on documents’ distributions over topics (“theta”). See details.\n\n\ntopic_concentration\nConcentration parameter (commonly named “beta” or “eta”) for the prior placed on topics’ distributions over terms.\n\n\nsubsampling_rate\n(For Online optimizer only) Fraction of the corpus to be sampled and used in each iteration of mini-batch gradient descent, in range (0, 1]. Note that this should be adjusted in synch with max_iter so the entire corpus is used. Specifically, set both so that maxIterations * miniBatchFraction greater than or equal to 1.\n\n\noptimizer\nOptimizer or inference algorithm used to estimate the LDA model. Supported: “online” for Online Variational Bayes (default) and “em” for Expectation-Maximization.\n\n\ncheckpoint_interval\nSet checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.\n\n\nkeep_last_checkpoint\n(Spark 2.0.0+) (For EM optimizer only) If using checkpointing, this indicates whether to keep the last checkpoint. If FALSE , then the checkpoint will be deleted. Deleting the checkpoint can cause failures if a data partition is lost, so set this bit with care. Note that checkpoints will be cleaned up via reference counting, regardless.\n\n\nlearning_decay\n(For Online optimizer only) Learning rate, set as an exponential decay rate. This should be between (0.5, 1.0] to guarantee asymptotic convergence. This is called “kappa” in the Online LDA paper (Hoffman et al., 2010). Default: 0.51, based on Hoffman et al.\n\n\nlearning_offset\n(For Online optimizer only) A (positive) learning parameter that downweights early iterations. Larger values make early iterations count less. This is called “tau0” in the Online LDA paper (Hoffman et al., 2010) Default: 1024, following Hoffman et al.\n\n\noptimize_doc_concentration\n(For Online optimizer only) Indicates whether the doc_concentration (Dirichlet parameter for document-topic distribution) will be optimized during training. Setting this to true will make the model more expressive and fit the training data better. Default: FALSE\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\ntopic_distribution_col\nOutput column with estimates of the topic mixture distribution for each document (often called “theta” in the literature). Returns a vector of zeros for an empty document.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments, see Details.\n\n\nmodel\nA fitted LDA model returned by ml_lda() .\n\n\nmax_terms_per_topic\nMaximum number of terms to collect for each topic. Default value of 10.\n\n\ndataset\ntest corpus to use for calculating log likelihood or log perplexity"
  },
  {
    "href": "reference/ml_lda.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "For ml_lda.tbl_spark with the formula interface, you can specify named arguments in ... that will be passed ft_regex_tokenizer(), ft_stop_words_remover(), and ft_count_vectorizer(). For example, to increase the default min_token_length, you can use ml_lda(dataset, ~ text, min_token_length = 4).\nTerminology for LDA:\n\n“term” = “word”: an element of the vocabulary\n“token”: instance of a term appearing in a document\n“topic”: multinomial distribution over terms representing some concept\n“document”: one piece of text, corresponding to one row in the input data\n\nOriginal LDA paper (journal version): Blei, Ng, and Jordan. “Latent Dirichlet Allocation.” JMLR, 2003.\nInput data ( features_col ): LDA is given a collection of documents as input data, via the features_col parameter. Each document is specified as a Vector of length vocab_size , where each entry is the count for the corresponding term (word) in the document. Feature transformers such as ft_tokenizer and ft_count_vectorizer can be useful for converting text to word count vectors"
  },
  {
    "href": "reference/ml_lda.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the clustering estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , an estimator is constructed then immediately fit with the input tbl_spark , returning a clustering model.\ntbl_spark , with formula or features specified: When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the estimator. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model . This signature does not apply to ml_lda() .\n\nml_describe_topics returns a DataFrame with topics and their top-weighted terms.\nml_log_likelihood calculates a lower bound on the log likelihood of the entire corpus"
  },
  {
    "href": "reference/ml_lda.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-clustering.html for more information on the set of clustering algorithms.\nOther ml clustering algorithms: ml_bisecting_kmeans , ml_gaussian_mixture , ml_kmeans"
  },
  {
    "href": "reference/ml_lda.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(janeaustenr)\nlibrary(dplyr)\nsc <- spark_connect(master = \"local\")\n\nlines_tbl <- sdf_copy_to(sc,\nausten_books()[c(1:30), ],\nname = \"lines_tbl\",\noverwrite = TRUE\n)\n\n# transform the data in a tidy form\nlines_tbl_tidy <- lines_tbl %>%\nft_tokenizer(\ninput_col = \"text\",\noutput_col = \"word_list\"\n) %>%\nft_stop_words_remover(\ninput_col = \"word_list\",\noutput_col = \"wo_stop_words\"\n) %>%\nmutate(text = explode(wo_stop_words)) %>%\nfilter(text != \"\") %>%\nselect(text, book)\n\nlda_model <- lines_tbl_tidy %>%\nml_lda(~text, k = 4)\n\n# vocabulary and topics\ntidy(lda_model)"
  },
  {
    "href": "reference/stream_render.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Collects streaming statistics to render the stream as an ‘htmlwidget’."
  },
  {
    "href": "reference/stream_render.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_render(stream = NULL, collect = 10, stats = NULL, ...)"
  },
  {
    "href": "reference/stream_render.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nstream\nThe stream to render\n\n\ncollect\nThe interval in seconds to collect data before rendering the ‘htmlwidget’.\n\n\nstats\nOptional stream statistics collected using stream_stats() , when specified, stream should be omitted.\n\n\n...\nAdditional optional arguments."
  },
  {
    "href": "reference/stream_render.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\")\n\ndir.create(\"iris-in\")\nwrite.csv(iris, \"iris-in/iris.csv\", row.names = FALSE)\n\nstream <- stream_read_csv(sc, \"iris-in/\") %>%\nstream_write_csv(\"iris-out/\")\n\nstream_render(stream)\nstream_stop(stream)"
  },
  {
    "href": "reference/ml_clustering_evaluator.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Evaluator for clustering results. The metric computes the Silhouette measure using the squared Euclidean distance. The Silhouette is a measure for the validation of the consistency within clusters. It ranges between 1 and -1, where a value close to 1 means that the points in a cluster are close to the other points in the same cluster and far from the points of the other clusters."
  },
  {
    "href": "reference/ml_clustering_evaluator.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_clustering_evaluator(\n  x,\n  features_col = \"features\",\n  prediction_col = \"prediction\",\n  metric_name = \"silhouette\",\n  uid = random_string(\"clustering_evaluator_\"),\n  ...\n)"
  },
  {
    "href": "reference/ml_clustering_evaluator.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection object or a tbl_spark containing label and prediction columns. The latter should be the output of sdf_predict .\n\n\nfeatures_col\nName of features column.\n\n\nprediction_col\nName of the prediction column.\n\n\nmetric_name\nThe performance metric. Currently supports “silhouette”.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ml_clustering_evaluator.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The calculated performance metric"
  },
  {
    "href": "reference/ml_clustering_evaluator.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\npartitions <- iris_tbl %>%\nsdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\niris_training <- partitions$training\niris_test <- partitions$test\n\nformula <- Species ~ .\n\n# Train the models\nkmeans_model <- ml_kmeans(iris_training, formula = formula)\nb_kmeans_model <- ml_bisecting_kmeans(iris_training, formula = formula)\ngmm_model <- ml_gaussian_mixture(iris_training, formula = formula)\n\n# Predict\npred_kmeans <- ml_predict(kmeans_model, iris_test)\npred_b_kmeans <- ml_predict(b_kmeans_model, iris_test)\npred_gmm <- ml_predict(gmm_model, iris_test)\n\n# Evaluate\nml_clustering_evaluator(pred_kmeans)\nml_clustering_evaluator(pred_b_kmeans)\nml_clustering_evaluator(pred_gmm)"
  },
  {
    "href": "reference/ft_interaction.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Implements the feature interaction transform. This transformer takes in Double and Vector type columns and outputs a flattened vector of their feature interactions. To handle interaction, we first one-hot encode any nominal features. Then, a vector of the feature cross-products is produced."
  },
  {
    "href": "reference/ft_interaction.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_interaction(\n  x,\n  input_cols = NULL,\n  output_col = NULL,\n  uid = random_string(\"interaction_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_interaction.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_cols\nThe names of the input columns\n\n\noutput_col\nThe name of the output column.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_interaction.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_interaction.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/left_join.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See left_join for more details."
  },
  {
    "href": "reference/spark_read_jdbc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Read from JDBC connection into a Spark DataFrame."
  },
  {
    "href": "reference/spark_read_jdbc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_read_jdbc(\n  sc,\n  name,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  columns = NULL,\n  ...\n)"
  },
  {
    "href": "reference/spark_read_jdbc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe name to assign to the newly generated table.\n\n\noptions\nA list of strings with additional options. See http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration .\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType , \"boolean\" for BooleanType , \"byte\" for ByteType , \"integer\" for IntegerType , \"integer64\" for LongType , \"double\" for DoubleType , \"character\" for StringType , \"timestamp\" for TimestampType and \"date\" for DateType .\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_read_jdbc.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/spark_read_jdbc.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(\nmaster = \"local\",\nconfig = list(\n`sparklyr.shell.driver-class-path` = \"/usr/share/java/mysql-connector-java-8.0.25.jar\"\n)\n)\nspark_read_jdbc(\nsc,\nname = \"my_sql_table\",\noptions = list(\nurl = \"jdbc:mysql://localhost:3306/my_sql_schema\",\ndriver = \"com.mysql.jdbc.Driver\",\nuser = \"me\",\npassword = \"******\",\ndbtable = \"my_sql_table\"\n)\n)"
  },
  {
    "href": "reference/spark_config_value.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "A helper function to retrieve values from spark_config()"
  },
  {
    "href": "reference/spark_config_value.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_config_value(config, name, default = NULL)"
  },
  {
    "href": "reference/spark_config_value.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nconfig\nThe configuration list from spark_config()\n\n\nname\nThe name of the configuration entry\n\n\ndefault\nThe default value to use when entry is not present"
  },
  {
    "href": "reference/ft_word2vec.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Word2Vec transforms a word into a code for further natural language processing or machine learning process."
  },
  {
    "href": "reference/ft_word2vec.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_word2vec(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  vector_size = 100,\n  min_count = 5,\n  max_sentence_length = 1000,\n  num_partitions = 1,\n  step_size = 0.025,\n  max_iter = 1,\n  seed = NULL,\n  uid = random_string(\"word2vec_\"),\n  ...\n)\nml_find_synonyms(model, word, num)"
  },
  {
    "href": "reference/ft_word2vec.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nvector_size\nThe dimension of the code that you want to transform from words. Default: 100\n\n\nmin_count\nThe minimum number of times a token must appear to be included in the word2vec model’s vocabulary. Default: 5\n\n\nmax_sentence_length\n(Spark 2.0.0+) Sets the maximum length (in words) of each sentence in the input data. Any sentence longer than this threshold will be divided into chunks of up to max_sentence_length size. Default: 1000\n\n\nnum_partitions\nNumber of partitions for sentences of words. Default: 1\n\n\nstep_size\nParam for Step size to be used for each iteration of optimization (> 0).\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused.\n\n\nmodel\nA fitted Word2Vec model, returned by ft_word2vec() .\n\n\nword\nA word, as a length-one character vector.\n\n\nnum\nNumber of words closest in similarity to the given word to find."
  },
  {
    "href": "reference/ft_word2vec.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "In the case where x is a tbl_spark , the estimator fits against x to obtain a transformer, which is then immediately used to transform x , returning a tbl_spark ."
  },
  {
    "href": "reference/ft_word2vec.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark\n\nml_find_synonyms() returns a DataFrame of synonyms and cosine similarities"
  },
  {
    "href": "reference/ft_word2vec.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer"
  },
  {
    "href": "reference/ft_stop_words_remover.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "A feature transformer that filters out stop words from input."
  },
  {
    "href": "reference/ft_stop_words_remover.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_stop_words_remover(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  case_sensitive = FALSE,\n  stop_words = ml_default_stop_words(spark_connection(x), \"english\"),\n  uid = random_string(\"stop_words_remover_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_stop_words_remover.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\ncase_sensitive\nWhether to do a case sensitive comparison over the stop words.\n\n\nstop_words\nThe words to be filtered out.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_stop_words_remover.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_stop_words_remover.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nml_default_stop_words\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/spark_table_name.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Attempts to generate a table name from an expression; otherwise, assigns an auto-generated generic name with “sparklyr_” prefix."
  },
  {
    "href": "reference/spark_table_name.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_table_name(expr)"
  },
  {
    "href": "reference/spark_table_name.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nexpr\nThe expression to attempt to use as name"
  },
  {
    "href": "reference/spark_write_orc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Serialize a Spark DataFrame to the ORC format."
  },
  {
    "href": "reference/spark_write_orc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_write_orc(\n  x,\n  path,\n  mode = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/spark_write_orc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure. For more details see also http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options. See http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration .\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_write_orc.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/spark_home_set.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Set the SPARK_HOME environment variable. This slightly speeds up some operations, including the connection time."
  },
  {
    "href": "reference/spark_home_set.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_home_set(path = NULL, ...)"
  },
  {
    "href": "reference/spark_home_set.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\npath\nA string containing the path to the installation location of Spark. If NULL , the path to the most latest Spark/Hadoop versions is used.\n\n\n...\nAdditional parameters not currently used."
  },
  {
    "href": "reference/spark_home_set.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The function is mostly invoked for the side-effect of setting the SPARK_HOME environment variable. It also returns TRUE if the environment was successfully set, and FALSE otherwise."
  },
  {
    "href": "reference/spark_home_set.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "# Not run due to side-effects\nspark_home_set()"
  },
  {
    "href": "reference/spark_write_delta.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Writes a Spark DataFrame into Delta Lake."
  },
  {
    "href": "reference/spark_write_delta.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_write_delta(\n  x,\n  path,\n  mode = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/spark_write_delta.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure. For more details see also http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_write_delta.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/ml_kmeans.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "K-means clustering with support for k-means|| initialization proposed by Bahmani et al. Using ml_kmeans() with the formula interface requires Spark 2.0+."
  },
  {
    "href": "reference/ml_kmeans.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_kmeans(\n  x,\n  formula = NULL,\n  k = 2,\n  max_iter = 20,\n  tol = 1e-04,\n  init_steps = 2,\n  init_mode = \"k-means||\",\n  seed = NULL,\n  features_col = \"features\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"kmeans_\"),\n  ...\n)\nml_compute_cost(model, dataset)\nml_compute_silhouette_measure(\n  model,\n  dataset,\n  distance_measure = c(\"squaredEuclidean\", \"cosine\")\n)"
  },
  {
    "href": "reference/ml_kmeans.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nUsed when x is a tbl_spark . R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nk\nThe number of clusters to create\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\ninit_steps\nNumber of steps for the k-means\n\n\ninit_mode\nInitialization algorithm. This can be either “random” to choose random points as initial cluster centers, or “k-means\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments, see Details.\n\n\nmodel\nA fitted K-means model returned by ml_kmeans()\n\n\ndataset\nDataset on which to calculate K-means cost\n\n\ndistance_measure\nDistance measure to apply when computing the Silhouette measure."
  },
  {
    "href": "reference/ml_kmeans.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the clustering estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , an estimator is constructed then immediately fit with the input tbl_spark , returning a clustering model.\ntbl_spark , with formula or features specified: When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the estimator. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model . This signature does not apply to ml_lda() .\n\nml_compute_cost() returns the K-means cost (sum of squared distances of points to their nearest center) for the model on the given data.\nml_compute_silhouette_measure() returns the Silhouette measure of the clustering on the given data."
  },
  {
    "href": "reference/ml_kmeans.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-clustering.html for more information on the set of clustering algorithms.\nOther ml clustering algorithms: ml_bisecting_kmeans , ml_gaussian_mixture , ml_lda"
  },
  {
    "href": "reference/ml_kmeans.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\nml_kmeans(iris_tbl, Species ~ .)"
  },
  {
    "href": "reference/reactiveSpark.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Given a spark object, returns a reactive data source for the contents of the spark object. This function is most useful to read Spark streams."
  },
  {
    "href": "reference/reactiveSpark.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "reactiveSpark(x, intervalMillis = 1000, session = NULL)"
  },
  {
    "href": "reference/reactiveSpark.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\nintervalMillis\nApproximate number of milliseconds to wait to retrieve updated data frame. This can be a numeric value, or a function that returns a numeric value.\n\n\nsession\nThe user session to associate this file reader with, or NULL if none. If non-null, the reader will automatically stop when the session ends."
  },
  {
    "href": "reference/spark_read_table.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Reads from a Spark Table into a Spark DataFrame."
  },
  {
    "href": "reference/spark_read_table.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_read_table(\n  sc,\n  name,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  columns = NULL,\n  ...\n)"
  },
  {
    "href": "reference/spark_read_table.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe name to assign to the newly generated table.\n\n\noptions\nA list of strings with additional options. See http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration .\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType , \"boolean\" for BooleanType , \"byte\" for ByteType , \"integer\" for IntegerType , \"integer64\" for LongType , \"double\" for DoubleType , \"character\" for StringType , \"timestamp\" for TimestampType and \"date\" for DateType .\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_read_table.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/sdf_with_unique_id.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Add a unique ID column to a Spark DataFrame. The Spark monotonicallyIncreasingId function is used to produce these and is guaranteed to produce unique, monotonically increasing ids; however, there is no guarantee that these IDs will be sequential. The table is persisted immediately after the column is generated, to ensure that the column is stable – otherwise, it can differ across new computations."
  },
  {
    "href": "reference/sdf_with_unique_id.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_with_unique_id(x, id = \"id\")"
  },
  {
    "href": "reference/sdf_with_unique_id.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nid\nThe name of the column to host the generated IDs."
  },
  {
    "href": "reference/ft_elementwise_product.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Outputs the Hadamard product (i.e., the element-wise product) of each input vector with a provided “weight” vector. In other words, it scales each column of the dataset by a scalar multiplier."
  },
  {
    "href": "reference/ft_elementwise_product.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_elementwise_product(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  scaling_vec = NULL,\n  uid = random_string(\"elementwise_product_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_elementwise_product.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nscaling_vec\nthe vector to multiply with input vectors\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_elementwise_product.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_elementwise_product.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/pivot_longer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See pivot_longer for more details."
  },
  {
    "href": "reference/spark_install.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Install versions of Spark for use with local Spark connections (i.e. spark_connect(master = \"local\" )"
  },
  {
    "href": "reference/spark_install.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_install(\n  version = NULL,\n  hadoop_version = NULL,\n  reset = TRUE,\n  logging = \"INFO\",\n  verbose = interactive()\n)\nspark_uninstall(version, hadoop_version)\nspark_install_dir()\nspark_install_tar(tarfile)\nspark_installed_versions()\nspark_available_versions(\n  show_hadoop = FALSE,\n  show_minor = FALSE,\n  show_future = FALSE\n)"
  },
  {
    "href": "reference/spark_install.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nversion\nVersion of Spark to install. See spark_available_versions for a list of supported versions\n\n\nhadoop_version\nVersion of Hadoop to install. See spark_available_versions for a list of supported versions\n\n\nreset\nAttempts to reset settings to defaults.\n\n\nlogging\nLogging level to configure install. Supported options: “WARN”, “INFO”\n\n\nverbose\nReport information as Spark is downloaded / installed\n\n\ntarfile\nPath to TAR file conforming to the pattern spark-###-bin-(hadoop)?### where ### reference spark and hadoop versions respectively.\n\n\nshow_hadoop\nShow Hadoop distributions?\n\n\nshow_minor\nShow minor Spark versions?\n\n\nshow_future\nShould future versions which have not been released be shown?"
  },
  {
    "href": "reference/spark_install.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "List with information about the installed version."
  },
  {
    "href": "reference/stream_read_delta.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Reads a Delta Lake table as a Spark dataframe stream."
  },
  {
    "href": "reference/stream_read_delta.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_read_delta(sc, path, name = NULL, options = list(), ...)"
  },
  {
    "href": "reference/stream_read_delta.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\noptions\nA list of strings with additional options.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/stream_read_delta.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Please note that Delta Lake requires installing the appropriate package by setting the packages parameter to \"delta\" in spark_connect()"
  },
  {
    "href": "reference/stream_read_delta.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark stream serialization: stream_read_csv , stream_read_json , stream_read_kafka , stream_read_orc , stream_read_parquet , stream_read_socket , stream_read_text , stream_write_console , stream_write_csv , stream_write_delta , stream_write_json , stream_write_kafka , stream_write_memory , stream_write_orc , stream_write_parquet , stream_write_text"
  },
  {
    "href": "reference/stream_read_delta.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.4.0\", packages = \"delta\")\n\nsdf_len(sc, 5) %>% spark_write_delta(path = \"delta-test\")\n\nstream <- stream_read_delta(sc, \"delta-test\") %>%\nstream_write_json(\"json-out\")\n\nstream_stop(stream)"
  },
  {
    "href": "reference/spark_apply.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Applies an R function to a Spark object (typically, a Spark DataFrame)."
  },
  {
    "href": "reference/spark_apply.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_apply(\n  x,\n  f,\n  columns = NULL,\n  memory = TRUE,\n  group_by = NULL,\n  packages = NULL,\n  context = NULL,\n  name = NULL,\n  barrier = NULL,\n  fetch_result_as_sdf = TRUE,\n  partition_index_param = \"\",\n  arrow_max_records_per_batch = NULL,\n  auto_deps = FALSE,\n  ...\n)"
  },
  {
    "href": "reference/spark_apply.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn object (usually a spark_tbl ) coercable to a Spark DataFrame.\n\n\nf\nA function that transforms a data frame partition into a data frame. The function f has signature f(df, context, group1, group2, ...) where df is a data frame with the data to be processed, context is an optional object passed as the context parameter and group1 to groupN contain the values of the group_by values. When group_by is not specified, f takes only one argument. Can also be an rlang anonymous function. For example, as ~ .x + 1 to define an expression that adds one to the given .x data frame.\n\n\ncolumns\nA vector of column names or a named vector of column types for the transformed object. When not specified, a sample of 10 rows is taken to infer out the output columns automatically, to avoid this performance penalty, specify the column types. The sample size is configurable using the sparklyr.apply.schema.infer configuration option.\n\n\nmemory\nBoolean; should the table be cached into memory?\n\n\ngroup_by\nColumn name used to group by data frame partitions.\n\n\npackages\nBoolean to distribute .libPaths() packages to each node, a list of packages to distribute, or a package bundle created with spark_apply_bundle() . Defaults to TRUE or the sparklyr.apply.packages value set in spark_config() . For clusters using Yarn cluster mode, packages can point to a package bundle created using spark_apply_bundle() and made available as a Spark file using config$sparklyr.shell.files . For clusters using Livy, packages can be manually installed on the driver node. For offline clusters where available.packages() is not available, manually download the packages database from https://cran.r-project.org/web/packages/packages.rds and set Sys.setenv(sparklyr.apply.packagesdb = \"<pathl-to-rds>\") . Otherwise, all packages will be used by default. For clusters where R packages already installed in every worker node, the spark.r.libpaths config entry can be set in spark_config() to the local packages library. To specify multiple paths collapse them (without spaces) with a comma delimiter (e.g., \"/lib/path/one,/lib/path/two\" ).\n\n\ncontext\nOptional object to be serialized and passed back to f() .\n\n\nname\nOptional table name while registering the resulting data frame.\n\n\nbarrier\nOptional to support Barrier Execution Mode in the scheduler.\n\n\nfetch_result_as_sdf\nWhether to return the transformed results in a Spark Dataframe (defaults to TRUE ). When set to FALSE , results will be returned as a list of R objects instead. NOTE: fetch_result_as_sdf must be set to FALSE when the transformation function being applied is returning R objects that cannot be stored in a Spark Dataframe (e.g., complex numbers or any other R data type that does not have an equivalent representation among Spark SQL data types).\n\n\npartition_index_param\nOptional if non-empty, then f also receives the index of the partition being processed as a named argument with this name, in addition to all positional argument(s) it will receive NOTE: when fetch_result_as_sdf is set to FALSE , object returned from the transformation function also must be serializable by the base::serialize function in R.\n\n\narrow_max_records_per_batch\nMaximum size of each Arrow record batch, ignored if Arrow serialization is not enabled.\n\n\nauto_deps\n[Experimental] Whether to infer all required R packages by examining the closure f() and only distribute required R and their transitive dependencies to Spark worker nodes (default: FALSE). NOTE: this option will only take effect if packages is set to TRUE or is a character vector of R package names. If packages is a character vector of R package names, then both the set of packages specified by packages and the set of inferred packages will be distributed to Spark workers.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_apply.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local[3]\")\n\n# creates an Spark data frame with 10 elements then multiply times 10 in R\nsdf_len(sc, 10) %>% spark_apply(function(df) df * 10)\n\n# using barrier mode\nsdf_len(sc, 3, repartition = 3) %>%\nspark_apply(nrow, barrier = TRUE, columns = c(id = \"integer\")) %>%\ncollect()"
  },
  {
    "href": "reference/ml-model-constructors.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Functions for developers writing extensions for Spark ML. These functions are constructors for ml_model objects that are returned when using the formula interface."
  },
  {
    "href": "reference/ml-model-constructors.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_supervised_pipeline(predictor, dataset, formula, features_col, label_col)\nml_clustering_pipeline(predictor, dataset, formula, features_col)\nml_construct_model_supervised(\n  constructor,\n  predictor,\n  formula,\n  dataset,\n  features_col,\n  label_col,\n  ...\n)\nml_construct_model_clustering(\n  constructor,\n  predictor,\n  formula,\n  dataset,\n  features_col,\n  ...\n)\nnew_ml_model_prediction(\n  pipeline_model,\n  formula,\n  dataset,\n  label_col,\n  features_col,\n  ...,\n  class = character()\n)\nnew_ml_model(pipeline_model, formula, dataset, ..., class = character())\nnew_ml_model_classification(\n  pipeline_model,\n  formula,\n  dataset,\n  label_col,\n  features_col,\n  predicted_label_col,\n  ...,\n  class = character()\n)\nnew_ml_model_regression(\n  pipeline_model,\n  formula,\n  dataset,\n  label_col,\n  features_col,\n  ...,\n  class = character()\n)\nnew_ml_model_clustering(\n  pipeline_model,\n  formula,\n  dataset,\n  features_col,\n  ...,\n  class = character()\n)"
  },
  {
    "href": "reference/ml-model-constructors.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\npredictor\nThe pipeline stage corresponding to the ML algorithm.\n\n\ndataset\nThe training dataset.\n\n\nformula\nThe formula used for data preprocessing\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula .\n\n\nconstructor\nThe constructor function for the ml_model.\n\n\npipeline_model\nThe pipeline model object returned by ml_supervised_pipeline().\n\n\nclass\nName of the subclass."
  },
  {
    "href": "reference/stream_write_csv.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Writes a Spark dataframe stream into a tabular (typically, comma-separated) stream."
  },
  {
    "href": "reference/stream_write_csv.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_write_csv(\n  x,\n  path,\n  mode = c(\"append\", \"complete\", \"update\"),\n  trigger = stream_trigger_interval(),\n  checkpoint = file.path(path, \"checkpoint\"),\n  header = TRUE,\n  delimiter = \",\",\n  quote = \"\\\"\",\n  escape = \"\\\\\",\n  charset = \"UTF-8\",\n  null_value = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/stream_write_csv.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\" , \"complete\" or \"update\" .\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous .\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\nheader\nShould the first row of data be used as a header? Defaults to TRUE .\n\n\ndelimiter\nThe character used to delimit each column, defaults to , .\n\n\nquote\nThe character used as a quote. Defaults to ‘“’ .\n\n\nescape\nThe character used to escape other characters, defaults to \\ .\n\n\ncharset\nThe character set, defaults to \"UTF-8\" .\n\n\nnull_value\nThe character to use for default values, defaults to NULL .\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/stream_write_csv.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark stream serialization: stream_read_csv , stream_read_delta , stream_read_json , stream_read_kafka , stream_read_orc , stream_read_parquet , stream_read_socket , stream_read_text , stream_write_console , stream_write_delta , stream_write_json , stream_write_kafka , stream_write_memory , stream_write_orc , stream_write_parquet , stream_write_text"
  },
  {
    "href": "reference/stream_write_csv.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\n\ndir.create(\"csv-in\")\nwrite.csv(iris, \"csv-in/data.csv\", row.names = FALSE)\n\ncsv_path <- file.path(\"file://\", getwd(), \"csv-in\")\n\nstream <- stream_read_csv(sc, csv_path) %>% stream_write_csv(\"csv-out\")\n\nstream_stop(stream)"
  },
  {
    "href": "reference/ml_evaluate.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Compute performance metrics."
  },
  {
    "href": "reference/ml_evaluate.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_evaluate(x, dataset)\nlist(list(\"ml_evaluate\"), list(\"ml_model_logistic_regression\"))(x, dataset)\nlist(list(\"ml_evaluate\"), list(\"ml_logistic_regression_model\"))(x, dataset)\nlist(list(\"ml_evaluate\"), list(\"ml_model_linear_regression\"))(x, dataset)\nlist(list(\"ml_evaluate\"), list(\"ml_linear_regression_model\"))(x, dataset)\nlist(list(\"ml_evaluate\"), list(\"ml_model_generalized_linear_regression\"))(x, dataset)\nlist(list(\"ml_evaluate\"), list(\"ml_generalized_linear_regression_model\"))(x, dataset)\nlist(list(\"ml_evaluate\"), list(\"ml_model_clustering\"))(x, dataset)\nlist(list(\"ml_evaluate\"), list(\"ml_model_classification\"))(x, dataset)\nlist(list(\"ml_evaluate\"), list(\"ml_evaluator\"))(x, dataset)"
  },
  {
    "href": "reference/ml_evaluate.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn ML model object or an evaluator object.\n\n\ndataset\nThe dataset to be validate the model on."
  },
  {
    "href": "reference/ml_evaluate.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\nml_gaussian_mixture(iris_tbl, Species ~ .) %>%\nml_evaluate(iris_tbl)\n\nml_kmeans(iris_tbl, Species ~ .) %>%\nml_evaluate(iris_tbl)\n\nml_bisecting_kmeans(iris_tbl, Species ~ .) %>%\nml_evaluate(iris_tbl)"
  },
  {
    "href": "reference/arrow_enabled_object.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "If the given R object is not serializable by arrow due to some known limitations of arrow, then return FALSE, otherwise return TRUE"
  },
  {
    "href": "reference/arrow_enabled_object.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "arrow_enabled_object(object)"
  },
  {
    "href": "reference/arrow_enabled_object.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nobject\nThe object to be serialized"
  },
  {
    "href": "reference/arrow_enabled_object.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "df <- tibble::tibble(x = seq(5))\narrow_enabled_object(df)"
  },
  {
    "href": "reference/sdf_rhyper.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a hypergeometric distribution."
  },
  {
    "href": "reference/sdf_rhyper.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_rhyper(\n  sc,\n  nn,\n  m,\n  n,\n  k,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "href": "reference/sdf_rhyper.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nnn\nSample Size.\n\n\nm\nThe number of successes among the population.\n\n\nn\nThe number of failures among the population.\n\n\nk\nThe number of draws.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "href": "reference/sdf_rhyper.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark statistical routines: sdf_rbeta , sdf_rbinom , sdf_rcauchy , sdf_rchisq , sdf_rexp , sdf_rgamma , sdf_rgeom , sdf_rlnorm , sdf_rnorm , sdf_rpois , sdf_rt , sdf_runif , sdf_rweibull"
  },
  {
    "href": "reference/sdf_is_streaming.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Is the given Spark DataFrame a streaming data?"
  },
  {
    "href": "reference/sdf_is_streaming.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_is_streaming(x)"
  },
  {
    "href": "reference/sdf_is_streaming.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark ."
  },
  {
    "href": "reference/sdf_coalesce.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Coalesces a Spark DataFrame"
  },
  {
    "href": "reference/sdf_coalesce.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_coalesce(x, partitions)"
  },
  {
    "href": "reference/sdf_coalesce.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\npartitions\nnumber of partitions"
  },
  {
    "href": "reference/spark_get_java.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Finds the path to JAVA_HOME ."
  },
  {
    "href": "reference/spark_get_java.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_get_java(throws = FALSE)"
  },
  {
    "href": "reference/spark_get_java.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nthrows\nThrow an error when path not found?"
  },
  {
    "href": "reference/spark_dependency.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Define a Spark dependency consisting of a set of custom JARs, Spark packages, and customized dbplyr SQL translation env."
  },
  {
    "href": "reference/spark_dependency.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_dependency(\n  jars = NULL,\n  packages = NULL,\n  initializer = NULL,\n  catalog = NULL,\n  repositories = NULL,\n  dbplyr_sql_variant = NULL,\n  ...\n)"
  },
  {
    "href": "reference/spark_dependency.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\njars\nCharacter vector of full paths to JAR files.\n\n\npackages\nCharacter vector of Spark packages names.\n\n\ninitializer\nOptional callback function called when initializing a connection.\n\n\ncatalog\nOptional location where extension JAR files can be downloaded for Livy.\n\n\nrepositories\nCharacter vector of Spark package repositories.\n\n\ndbplyr_sql_variant\nCustomization of dbplyr SQL translation env. Must be a named list of the following form: list( scalar = list(scalar_fn1 = …, scalar_fn2 = …, ), aggregate = list(agg_fn1 = …, agg_fn2 = …, ), window = list(wnd_fn1 = …, wnd_fn2 = …, ) ) See sql_variant for details.\n\n\n...\nAdditional optional arguments."
  },
  {
    "href": "reference/spark_dependency.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "An object of type spark_dependency"
  },
  {
    "href": "reference/stream_trigger_interval.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Creates a Spark structured streaming trigger to execute over the specified interval."
  },
  {
    "href": "reference/stream_trigger_interval.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_trigger_interval(interval = 1000)"
  },
  {
    "href": "reference/stream_trigger_interval.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\ninterval\nThe execution interval specified in milliseconds."
  },
  {
    "href": "reference/stream_trigger_interval.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "stream_trigger_continuous"
  },
  {
    "href": "reference/spark_advisory_shuffle_partition_size.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Retrieves or sets advisory size in bytes of the shuffle partition during adaptive optimization"
  },
  {
    "href": "reference/spark_advisory_shuffle_partition_size.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_advisory_shuffle_partition_size(sc, size = NULL)"
  },
  {
    "href": "reference/spark_advisory_shuffle_partition_size.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nsize\nAdvisory size in bytes of the shuffle partition. Defaults to NULL to retrieve configuration entries."
  },
  {
    "href": "reference/spark_advisory_shuffle_partition_size.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark runtime configuration: spark_adaptive_query_execution , spark_auto_broadcast_join_threshold , spark_coalesce_initial_num_partitions , spark_coalesce_min_num_partitions , spark_coalesce_shuffle_partitions , spark_session_config"
  },
  {
    "href": "reference/spark_write_json.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Serialize a Spark DataFrame to the c(“JavaScript”, “Object Notation”) format."
  },
  {
    "href": "reference/spark_write_json.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_write_json(\n  x,\n  path,\n  mode = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/spark_write_json.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure. For more details see also http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_write_json.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/ml_unsupervised_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "These methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "href": "reference/ml_unsupervised_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "list(list(\"tidy\"), list(\"ml_model_kmeans\"))(x, ...)\nlist(list(\"augment\"), list(\"ml_model_kmeans\"))(x, newdata = NULL, ...)\nlist(list(\"glance\"), list(\"ml_model_kmeans\"))(x, ...)\nlist(list(\"tidy\"), list(\"ml_model_bisecting_kmeans\"))(x, ...)\nlist(list(\"augment\"), list(\"ml_model_bisecting_kmeans\"))(x, newdata = NULL, ...)\nlist(list(\"glance\"), list(\"ml_model_bisecting_kmeans\"))(x, ...)\nlist(list(\"tidy\"), list(\"ml_model_gaussian_mixture\"))(x, ...)\nlist(list(\"augment\"), list(\"ml_model_gaussian_mixture\"))(x, newdata = NULL, ...)\nlist(list(\"glance\"), list(\"ml_model_gaussian_mixture\"))(x, ...)"
  },
  {
    "href": "reference/ml_unsupervised_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n...\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "href": "reference/stream_write_text.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Writes a Spark dataframe stream into a text stream."
  },
  {
    "href": "reference/stream_write_text.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_write_text(\n  x,\n  path,\n  mode = c(\"append\", \"complete\", \"update\"),\n  trigger = stream_trigger_interval(),\n  checkpoint = file.path(path, \"checkpoints\", random_string(\"\")),\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/stream_write_text.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe destination path. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\" , \"complete\" or \"update\" .\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous .\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/stream_write_text.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark stream serialization: stream_read_csv , stream_read_delta , stream_read_json , stream_read_kafka , stream_read_orc , stream_read_parquet , stream_read_socket , stream_read_text , stream_write_console , stream_write_csv , stream_write_delta , stream_write_json , stream_write_kafka , stream_write_memory , stream_write_orc , stream_write_parquet"
  },
  {
    "href": "reference/stream_write_text.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\n\ndir.create(\"text-in\")\nwriteLines(\"A text entry\", \"text-in/text.txt\")\n\ntext_path <- file.path(\"file://\", getwd(), \"text-in\")\n\nstream <- stream_read_text(sc, text_path) %>% stream_write_text(\"text-out\")\n\nstream_stop(stream)"
  },
  {
    "href": "reference/spark_config_packages.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Creates Spark Configuration"
  },
  {
    "href": "reference/spark_config_packages.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_config_packages(config, packages, version, scala_version = NULL, ...)"
  },
  {
    "href": "reference/spark_config_packages.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nconfig\nThe Spark configuration object.\n\n\npackages\nA list of named packages or versioned packagese to add.\n\n\nversion\nThe version of Spark being used.\n\n\nscala_version\nAcceptable Scala version of packages to be loaded\n\n\n...\nAdditional configurations"
  },
  {
    "href": "reference/stream_stats.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Collects streaming statistics, usually, to be used with stream_render() to render streaming statistics."
  },
  {
    "href": "reference/stream_stats.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_stats(stream, stats = list())"
  },
  {
    "href": "reference/stream_stats.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nstream\nThe stream to collect statistics from.\n\n\nstats\nAn optional stats object generated using stream_stats() ."
  },
  {
    "href": "reference/stream_stats.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "A stats object containing streaming statistics that can be passed back to the stats parameter to continue aggregating streaming stats."
  },
  {
    "href": "reference/stream_stats.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\nsdf_len(sc, 10) %>%\nspark_write_parquet(path = \"parquet-in\")\n\nstream <- stream_read_parquet(sc, \"parquet-in\") %>%\nstream_write_parquet(\"parquet-out\")\n\nstream_stats(stream)"
  },
  {
    "href": "reference/sdf_read_column.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Read a single column from a Spark DataFrame, and return the contents of that column back to list() ."
  },
  {
    "href": "reference/sdf_read_column.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_read_column(x, column)"
  },
  {
    "href": "reference/sdf_read_column.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ncolumn\nThe name of a column within x ."
  },
  {
    "href": "reference/sdf_read_column.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "It is expected for this operation to preserve row order."
  },
  {
    "href": "reference/ml_summary.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Extracts a metric from the summary object of a Spark ML model."
  },
  {
    "href": "reference/ml_summary.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_summary(x, metric = NULL, allow_null = FALSE)"
  },
  {
    "href": "reference/ml_summary.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark ML model that has a summary.\n\n\nmetric\nThe name of the metric to extract. If not set, returns the summary object.\n\n\nallow_null\nWhether null results are allowed when the metric is not found in the summary."
  },
  {
    "href": "reference/spark_compilation_spec.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "For use with compile_package_jars . The Spark compilation specification is used when compiling Spark extension Java Archives, and defines which versions of Spark, as well as which versions of Scala, should be used for compilation."
  },
  {
    "href": "reference/spark_compilation_spec.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_compilation_spec(\n  spark_version = NULL,\n  spark_home = NULL,\n  scalac_path = NULL,\n  scala_filter = NULL,\n  jar_name = NULL,\n  jar_path = NULL,\n  jar_dep = NULL,\n  embedded_srcs = \"embedded_sources.R\"\n)"
  },
  {
    "href": "reference/spark_compilation_spec.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nspark_version\nThe Spark version to build against. This can be left unset if the path to a suitable Spark home is supplied.\n\n\nspark_home\nThe path to a Spark home installation. This can be left unset if spark_version is supplied; in such a case, sparklyr will attempt to discover the associated Spark installation using spark_home_dir .\n\n\nscalac_path\nThe path to the scalac compiler to be used during compilation of your Spark extension. Note that you should ensure the version of scalac selected matches the version of scalac used with the version of Spark you are compiling against.\n\n\nscala_filter\nAn optional list() function that can be used to filter which scala files are used during compilation. This can be useful if you have auxiliary files that should only be included with certain versions of Spark.\n\n\njar_name\nThe name to be assigned to the generated jar .\n\n\njar_path\nThe path to the jar tool to be used during compilation of your Spark extension.\n\n\njar_dep\nAn optional list of additional jar dependencies.\n\n\nembedded_srcs\nEmbedded source file(s) under <R package root>/java to be included in the root of the resulting jar file as resources"
  },
  {
    "href": "reference/spark_compilation_spec.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Most Spark extensions won’t need to define their own compilation specification, and can instead rely on the default behavior of compile_package_jars ."
  },
  {
    "href": "reference/ml_multilayer_perceptron_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "These methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "href": "reference/ml_multilayer_perceptron_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "list(list(\"tidy\"), list(\"ml_model_multilayer_perceptron_classification\"))(x, ...)\nlist(list(\"augment\"), list(\"ml_model_multilayer_perceptron_classification\"))(x, newdata = NULL, ...)\nlist(list(\"glance\"), list(\"ml_model_multilayer_perceptron_classification\"))(x, ...)"
  },
  {
    "href": "reference/ml_multilayer_perceptron_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n...\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "href": "reference/ml_glm_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "These methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "href": "reference/ml_glm_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "list(list(\"tidy\"), list(\"ml_model_generalized_linear_regression\"))(x, exponentiate = FALSE, ...)\nlist(list(\"tidy\"), list(\"ml_model_linear_regression\"))(x, ...)\nlist(list(\"augment\"), list(\"ml_model_generalized_linear_regression\"))(\n  x,\n  newdata = NULL,\n  type.residuals = c(\"working\", \"deviance\", \"pearson\", \"response\"),\n  ...\n)\nlist(list(\"augment\"), list(\"ml_model_linear_regression\"))(\n  x,\n  newdata = NULL,\n  type.residuals = c(\"working\", \"deviance\", \"pearson\", \"response\"),\n  ...\n)\nlist(list(\"glance\"), list(\"ml_model_generalized_linear_regression\"))(x, ...)\nlist(list(\"glance\"), list(\"ml_model_linear_regression\"))(x, ...)"
  },
  {
    "href": "reference/ml_glm_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\nexponentiate\nFor GLM, whether to exponentiate the coefficient estimates (typical for logistic regression.)\n\n\n...\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction.\n\n\ntype.residuals\ntype of residuals, defaults to \"working\" . Must be set to \"working\" when newdata is supplied."
  },
  {
    "href": "reference/ml_glm_tidiers.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "The residuals attached by augment are of type “working” by default, which is different from the default of “deviance” for residuals() or sdf_residuals() ."
  },
  {
    "href": "reference/DBISparkResult-class.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "DBI Spark Result."
  },
  {
    "href": "reference/spark_read_image.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Read image files within a directory and convert each file into a record within the resulting Spark dataframe. The output will be a Spark dataframe consisting of struct types containing the following attributes:\n\norigin: StringType\nheight: IntegerType\nwidth: IntegerType\nnChannels: IntegerType\nmode: IntegerType\ndata: BinaryType"
  },
  {
    "href": "reference/spark_read_image.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_read_image(\n  sc,\n  name = NULL,\n  dir = name,\n  drop_invalid = TRUE,\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE\n)"
  },
  {
    "href": "reference/spark_read_image.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe name to assign to the newly generated table.\n\n\ndir\nDirectory to read binary files from.\n\n\ndrop_invalid\nWhether to drop files that are not valid images from the result (default: TRUE).\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?"
  },
  {
    "href": "reference/spark_read_image.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/ft_idf.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Compute the Inverse Document Frequency (IDF) given a collection of documents."
  },
  {
    "href": "reference/ft_idf.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_idf(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  min_doc_freq = 0,\n  uid = random_string(\"idf_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_idf.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nmin_doc_freq\nThe minimum number of documents in which a term should appear. Default: 0\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_idf.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "In the case where x is a tbl_spark , the estimator fits against x to obtain a transformer, which is then immediately used to transform x , returning a tbl_spark ."
  },
  {
    "href": "reference/ft_idf.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_idf.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/sdf_copy_to.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Copy an object into Spark, and return an list() object wrapping the copied object (typically, a Spark DataFrame)."
  },
  {
    "href": "reference/sdf_copy_to.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_copy_to(sc, x, name, memory, repartition, overwrite, struct_columns, ...)\nsdf_import(x, sc, name, memory, repartition, overwrite, struct_columns, ...)"
  },
  {
    "href": "reference/sdf_copy_to.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nx\nAn list() object from which a Spark DataFrame can be generated.\n\n\nname\nThe name to assign to the copied table in Spark.\n\n\nmemory\nBoolean; should the table be cached into memory?\n\n\nrepartition\nThe number of partitions to use when distributing the table across the Spark cluster. The default (0) can be used to avoid partitioning.\n\n\noverwrite\nBoolean; overwrite a pre-existing table with the name name if one already exists?\n\n\nstruct_columns\n(only supported with Spark 2.4.0 or higher) A list of columns from the source data frame that should be converted to Spark SQL StructType columns. The source columns can contain either json strings or nested lists. All rows within each source column should have identical schemas (because otherwise the conversion result will contain unexpected null values or missing values as Spark currently does not support schema discovery on individual rows within a struct column).\n\n\n...\nOptional arguments, passed to implementing methods."
  },
  {
    "href": "reference/sdf_copy_to.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark data frames: sdf_distinct , sdf_random_split , sdf_register , sdf_sample , sdf_sort , sdf_weighted_sample"
  },
  {
    "href": "reference/sdf_copy_to.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"spark://HOST:PORT\")\nsdf_copy_to(sc, iris)"
  },
  {
    "href": "reference/ft_regex_tokenizer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "A regex based tokenizer that extracts tokens either by using the provided regex pattern to split the text (default) or repeatedly matching the regex (if gaps is false). Optional parameters also allow filtering tokens using a minimal length. It returns an array of strings that can be empty."
  },
  {
    "href": "reference/ft_regex_tokenizer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_regex_tokenizer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  gaps = TRUE,\n  min_token_length = 1,\n  pattern = \"\\\\s+\",\n  to_lower_case = TRUE,\n  uid = random_string(\"regex_tokenizer_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_regex_tokenizer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\ngaps\nIndicates whether regex splits on gaps (TRUE) or matches tokens (FALSE).\n\n\nmin_token_length\nMinimum token length, greater than or equal to 0.\n\n\npattern\nThe regular expression pattern to be used.\n\n\nto_lower_case\nIndicates whether to convert all characters to lowercase before tokenizing.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_regex_tokenizer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_regex_tokenizer.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/stream_write_console.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Writes a Spark dataframe stream into console logs."
  },
  {
    "href": "reference/stream_write_console.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_write_console(\n  x,\n  mode = c(\"append\", \"complete\", \"update\"),\n  options = list(),\n  trigger = stream_trigger_interval(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/stream_write_console.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\" , \"complete\" or \"update\" .\n\n\noptions\nA list of strings with additional options.\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous .\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/stream_write_console.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark stream serialization: stream_read_csv , stream_read_delta , stream_read_json , stream_read_kafka , stream_read_orc , stream_read_parquet , stream_read_socket , stream_read_text , stream_write_csv , stream_write_delta , stream_write_json , stream_write_kafka , stream_write_memory , stream_write_orc , stream_write_parquet , stream_write_text"
  },
  {
    "href": "reference/stream_write_console.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\n\nsdf_len(sc, 10) %>%\ndplyr::transmute(text = as.character(id)) %>%\nspark_write_text(\"text-in\")\n\nstream <- stream_read_text(sc, \"text-in\") %>% stream_write_console()\n\nstream_stop(stream)"
  },
  {
    "href": "reference/spark_read_orc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Read a ORC file into a Spark DataFrame."
  },
  {
    "href": "reference/spark_read_orc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_read_orc(\n  sc,\n  name = NULL,\n  path = name,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  columns = NULL,\n  schema = NULL,\n  ...\n)"
  },
  {
    "href": "reference/spark_read_orc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\noptions\nA list of strings with additional options. See http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration .\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType , \"boolean\" for BooleanType , \"byte\" for ByteType , \"integer\" for IntegerType , \"integer64\" for LongType , \"double\" for DoubleType , \"character\" for StringType , \"timestamp\" for TimestampType and \"date\" for DateType .\n\n\nschema\nA (java) read schema. Useful for optimizing read operation on nested data.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_read_orc.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "You can read data from HDFS ( hdfs:// ), S3 ( s3a:// ), as well as the local file system ( file:// )."
  },
  {
    "href": "reference/spark_read_orc.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/unnest.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See unnest for more details."
  },
  {
    "href": "reference/ft_hashing_tf.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Maps a sequence of terms to their term frequencies using the hashing trick."
  },
  {
    "href": "reference/ft_hashing_tf.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_hashing_tf(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  binary = FALSE,\n  num_features = 2^18,\n  uid = random_string(\"hashing_tf_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_hashing_tf.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nbinary\nBinary toggle to control term frequency counts. If true, all non-zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. (default = FALSE )\n\n\nnum_features\nNumber of features. Should be greater than 0. (default = 2^18 )\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_hashing_tf.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_hashing_tf.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/ml_add_stage.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Adds a stage to a pipeline."
  },
  {
    "href": "reference/ml_add_stage.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_add_stage(x, stage)"
  },
  {
    "href": "reference/ml_add_stage.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA pipeline or a pipeline stage.\n\n\nstage\nA pipeline stage."
  },
  {
    "href": "reference/spark_coalesce_initial_num_partitions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Retrieves or sets initial number of shuffle partitions before coalescing"
  },
  {
    "href": "reference/spark_coalesce_initial_num_partitions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_coalesce_initial_num_partitions(sc, num_partitions = NULL)"
  },
  {
    "href": "reference/spark_coalesce_initial_num_partitions.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nnum_partitions\nInitial number of shuffle partitions before coalescing. Defaults to NULL to retrieve configuration entries."
  },
  {
    "href": "reference/spark_coalesce_initial_num_partitions.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark runtime configuration: spark_adaptive_query_execution , spark_advisory_shuffle_partition_size , spark_auto_broadcast_join_threshold , spark_coalesce_min_num_partitions , spark_coalesce_shuffle_partitions , spark_session_config"
  },
  {
    "href": "reference/dplyr_hof.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "These methods implement dplyr grammars for Apache Spark higher order functions"
  },
  {
    "href": "reference/sdf_partition_sizes.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Compute the number of records within each partition of a Spark DataFrame"
  },
  {
    "href": "reference/sdf_partition_sizes.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_partition_sizes(x)"
  },
  {
    "href": "reference/sdf_partition_sizes.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark ."
  },
  {
    "href": "reference/sdf_partition_sizes.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"spark://HOST:PORT\")\nexample_sdf <- sdf_len(sc, 100L, repartition = 10L)\nexample_sdf %>%\nsdf_partition_sizes() %>%\nprint()"
  },
  {
    "href": "reference/stream_read_kafka.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Reads a Kafka stream as a Spark dataframe stream."
  },
  {
    "href": "reference/stream_read_kafka.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_read_kafka(sc, name = NULL, options = list(), ...)"
  },
  {
    "href": "reference/stream_read_kafka.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe name to assign to the newly generated stream.\n\n\noptions\nA list of strings with additional options.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/stream_read_kafka.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Please note that Kafka requires installing the appropriate package by setting the packages parameter to \"kafka\" in spark_connect()"
  },
  {
    "href": "reference/stream_read_kafka.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark stream serialization: stream_read_csv , stream_read_delta , stream_read_json , stream_read_orc , stream_read_parquet , stream_read_socket , stream_read_text , stream_write_console , stream_write_csv , stream_write_delta , stream_write_json , stream_write_kafka , stream_write_memory , stream_write_orc , stream_write_parquet , stream_write_text"
  },
  {
    "href": "reference/stream_read_kafka.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.3\", packages = \"kafka\")\n\nread_options <- list(kafka.bootstrap.servers = \"localhost:9092\", subscribe = \"topic1\")\nwrite_options <- list(kafka.bootstrap.servers = \"localhost:9092\", topic = \"topic2\")\n\nstream <- stream_read_kafka(sc, options = read_options) %>%\nstream_write_kafka(options = write_options)\n\nstream_stop(stream)"
  },
  {
    "href": "reference/ml_pipeline.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Create Spark ML Pipelines"
  },
  {
    "href": "reference/ml_pipeline.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_pipeline(x, ..., uid = random_string(\"pipeline_\"))"
  },
  {
    "href": "reference/ml_pipeline.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nEither a spark_connection or ml_pipeline_stage objects\n\n\n...\nml_pipeline_stage objects.\n\n\nuid\nA character string used to uniquely identify the ML estimator."
  },
  {
    "href": "reference/ml_pipeline.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "When x is a spark_connection , ml_pipeline() returns an empty pipeline object. When x is a ml_pipeline_stage , ml_pipeline() returns an ml_pipeline with the stages set to x and any transformers or estimators given in ... ."
  },
  {
    "href": "reference/ml_call_constructor.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Identifies the associated sparklyr ML constructor for the JVM object by inspecting its class and performing a lookup. The lookup table is specified by the sparkml/class_mapping.json files of sparklyr and the loaded extensions."
  },
  {
    "href": "reference/ml_call_constructor.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_call_constructor(jobj)"
  },
  {
    "href": "reference/ml_call_constructor.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\njobj\nThe jobj for the pipeline stage."
  },
  {
    "href": "reference/ft_quantile_discretizer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "ft_quantile_discretizer takes a column with continuous features and outputs a column with binned categorical features. The number of bins can be set using the num_buckets parameter. It is possible that the number of buckets used will be smaller than this value, for example, if there are too few distinct values of the input to create enough distinct quantiles."
  },
  {
    "href": "reference/ft_quantile_discretizer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_quantile_discretizer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  num_buckets = 2,\n  input_cols = NULL,\n  output_cols = NULL,\n  num_buckets_array = NULL,\n  handle_invalid = \"error\",\n  relative_error = 0.001,\n  uid = random_string(\"quantile_discretizer_\"),\n  weight_column = NULL,\n  ...\n)"
  },
  {
    "href": "reference/ft_quantile_discretizer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nnum_buckets\nNumber of buckets (quantiles, or categories) into which data points are grouped. Must be greater than or equal to 2.\n\n\ninput_cols\nNames of input columns.\n\n\noutput_cols\nNames of output columns.\n\n\nnum_buckets_array\nArray of number of buckets (quantiles, or categories) into which data points are grouped. Each value must be greater than or equal to 2.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error”\n\n\nrelative_error\n(Spark 2.0.0+) Relative error (see documentation for org.apache.spark.sql.DataFrameStatFunctions.approxQuantile here for description). Must be in the range [0, 1]. default: 0.001\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\nweight_column\nIf not NULL, then a generalized version of the Greenwald-Khanna algorithm will be run to compute weighted percentiles, with each input having a relative weight specified by the corresponding value in weight_column. The weights can be considered as relative frequencies of sample inputs.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_quantile_discretizer.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "NaN handling: null and NaN values will be ignored from the column during QuantileDiscretizer fitting. This will produce a Bucketizer model for making predictions. During the transformation, Bucketizer will raise an error when it finds NaN values in the dataset, but the user can also choose to either keep or remove NaN values within the dataset by setting handle_invalid If the user chooses to keep NaN values, they will be handled specially and placed into their own bucket, for example, if 4 buckets are used, then non-NaN data will be put into buckets[0-3], but NaNs will be counted in a special bucket[4].\nAlgorithm: The bin ranges are chosen using an approximate algorithm (see the documentation for org.apache.spark.sql.DataFrameStatFunctions.approxQuantile here for a detailed description). The precision of the approximation can be controlled with the relative_error parameter. The lower and upper bin bounds will be -Infinity and +Infinity, covering all real values.\nNote that the result may be different every time you run it, since the sample strategy behind it is non-deterministic.\nIn the case where x is a tbl_spark , the estimator fits against x to obtain a transformer, which is then immediately used to transform x , returning a tbl_spark ."
  },
  {
    "href": "reference/ft_quantile_discretizer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_quantile_discretizer.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nft_bucketizer\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/sparklyr_get_backend_port.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Retrieve the port number of the sparklyr backend associated with a Spark connection."
  },
  {
    "href": "reference/sparklyr_get_backend_port.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sparklyr_get_backend_port(sc)"
  },
  {
    "href": "reference/sparklyr_get_backend_port.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection ."
  },
  {
    "href": "reference/sparklyr_get_backend_port.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The port number of the sparklyr backend associated with sc ."
  },
  {
    "href": "reference/ft_min_max_scaler.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Rescale each feature individually to a common range [min, max] linearly using column summary statistics, which is also known as min-max normalization or Rescaling"
  },
  {
    "href": "reference/ft_min_max_scaler.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_min_max_scaler(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  min = 0,\n  max = 1,\n  uid = random_string(\"min_max_scaler_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_min_max_scaler.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nmin\nLower bound after transformation, shared by all features Default: 0.0\n\n\nmax\nUpper bound after transformation, shared by all features Default: 1.0\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_min_max_scaler.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "In the case where x is a tbl_spark , the estimator fits against x to obtain a transformer, which is then immediately used to transform x , returning a tbl_spark ."
  },
  {
    "href": "reference/ft_min_max_scaler.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_min_max_scaler.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/ft_min_max_scaler.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\nfeatures <- c(\"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\")\n\niris_tbl %>%\nft_vector_assembler(\ninput_col = features,\noutput_col = \"features_temp\"\n) %>%\nft_min_max_scaler(\ninput_col = \"features_temp\",\noutput_col = \"features\"\n)"
  },
  {
    "href": "reference/spark_compile.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Given a set of scala source files, compile them into a Java Archive ( jar )."
  },
  {
    "href": "reference/spark_compile.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_compile(\n  jar_name,\n  spark_home = NULL,\n  filter = NULL,\n  scalac = NULL,\n  jar = NULL,\n  jar_dep = NULL,\n  embedded_srcs = \"embedded_sources.R\"\n)"
  },
  {
    "href": "reference/spark_compile.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nspark_home\nThe path to the Spark sources to be used alongside compilation.\n\n\nfilter\nAn optional function, used to filter out discovered scala files during compilation. This can be used to ensure that e.g. certain files are only compiled with certain versions of Spark, and so on.\n\n\nscalac\nThe path to the scalac program to be used, for compilation of scala files.\n\n\njar\nThe path to the jar program to be used, for generating of the resulting jar .\n\n\njar_dep\nAn optional list of additional jar dependencies.\n\n\nembedded_srcs\nEmbedded source file(s) under <R package root>/java to be included in the root of the resulting jar file as resources\n\n\nname\nThe name to assign to the target jar ."
  },
  {
    "href": "reference/ml_model_data.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Extracts data associated with a Spark ML model"
  },
  {
    "href": "reference/ml_model_data.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_model_data(object)"
  },
  {
    "href": "reference/ml_model_data.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nobject\na Spark ML model"
  },
  {
    "href": "reference/ml_model_data.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "A tbl_spark"
  },
  {
    "href": "reference/pivot_wider.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See pivot_wider for more details."
  },
  {
    "href": "reference/stream_read_text.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Reads a text stream as a Spark dataframe stream."
  },
  {
    "href": "reference/stream_read_text.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_read_text(sc, path, name = NULL, options = list(), ...)"
  },
  {
    "href": "reference/stream_read_text.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\noptions\nA list of strings with additional options.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/stream_read_text.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark stream serialization: stream_read_csv , stream_read_delta , stream_read_json , stream_read_kafka , stream_read_orc , stream_read_parquet , stream_read_socket , stream_write_console , stream_write_csv , stream_write_delta , stream_write_json , stream_write_kafka , stream_write_memory , stream_write_orc , stream_write_parquet , stream_write_text"
  },
  {
    "href": "reference/stream_read_text.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\n\ndir.create(\"text-in\")\nwriteLines(\"A text entry\", \"text-in/text.txt\")\n\ntext_path <- file.path(\"file://\", getwd(), \"text-in\")\n\nstream <- stream_read_text(sc, text_path) %>% stream_write_text(\"text-out\")\n\nstream_stop(stream)"
  },
  {
    "href": "reference/sdf_register.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Registers a Spark DataFrame (giving it a table name for the Spark SQL context), and returns a tbl_spark ."
  },
  {
    "href": "reference/sdf_register.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_register(x, name = NULL)"
  },
  {
    "href": "reference/sdf_register.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame.\n\n\nname\nA name to assign this table."
  },
  {
    "href": "reference/sdf_register.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark data frames: sdf_copy_to , sdf_distinct , sdf_random_split , sdf_sample , sdf_sort , sdf_weighted_sample"
  },
  {
    "href": "reference/pipe.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See %>% for more details."
  },
  {
    "href": "reference/ml_linear_regression.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Perform regression using linear regression."
  },
  {
    "href": "reference/ml_linear_regression.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_linear_regression(\n  x,\n  formula = NULL,\n  fit_intercept = TRUE,\n  elastic_net_param = 0,\n  reg_param = 0,\n  max_iter = 100,\n  weight_col = NULL,\n  loss = \"squaredError\",\n  solver = \"auto\",\n  standardization = TRUE,\n  tol = 1e-06,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"linear_regression_\"),\n  ...\n)"
  },
  {
    "href": "reference/ml_linear_regression.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nUsed when x is a tbl_spark . R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nfit_intercept\nBoolean; should the model be fit with an intercept term?\n\n\nelastic_net_param\nElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n\n\nreg_param\nRegularization parameter (aka lambda)\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nweight_col\nThe name of the column to use as weights for the model fit.\n\n\nloss\nThe loss function to be optimized. Supported options: “squaredError” and “huber”. Default: “squaredError”\n\n\nsolver\nSolver algorithm for optimization.\n\n\nstandardization\nWhether to standardize the training features before fitting the model.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula .\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; see Details."
  },
  {
    "href": "reference/ml_linear_regression.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "When x is a tbl_spark and formula (alternatively, response and features ) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\" ) can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model , ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "href": "reference/ml_linear_regression.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a predictor is constructed then immediately fit with the input tbl_spark , returning a prediction model.\ntbl_spark , with formula : specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model ."
  },
  {
    "href": "reference/ml_linear_regression.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression , ml_decision_tree_classifier , ml_gbt_classifier , ml_generalized_linear_regression , ml_isotonic_regression , ml_linear_svc , ml_logistic_regression , ml_multilayer_perceptron_classifier , ml_naive_bayes , ml_one_vs_rest , ml_random_forest_classifier"
  },
  {
    "href": "reference/ml_linear_regression.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, name = \"mtcars_tbl\", overwrite = TRUE)\n\npartitions <- mtcars_tbl %>%\nsdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\nmtcars_training <- partitions$training\nmtcars_test <- partitions$test\n\nlm_model <- mtcars_training %>%\nml_linear_regression(mpg ~ .)\n\npred <- ml_predict(lm_model, mtcars_test)\n\nml_regression_evaluator(pred, label_col = \"mpg\")"
  },
  {
    "href": "reference/download_scalac.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "compile_package_jars requires several versions of the scala compiler to work, this is to match Spark scala versions. To help setup your environment, this function will download the required compilers under the default search path."
  },
  {
    "href": "reference/download_scalac.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "download_scalac(dest_path = NULL)"
  },
  {
    "href": "reference/download_scalac.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\ndest_path\nThe destination path where scalac will be downloaded to."
  },
  {
    "href": "reference/download_scalac.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "See find_scalac for a list of paths searched and used by this function to install the required compilers."
  },
  {
    "href": "reference/ft_polynomial_expansion.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Perform feature expansion in a polynomial space. E.g. take a 2-variable feature vector as an example: (x, y), if we want to expand it with degree 2, then we get (x, x * x, y, x * y, y * y)."
  },
  {
    "href": "reference/ft_polynomial_expansion.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_polynomial_expansion(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  degree = 2,\n  uid = random_string(\"polynomial_expansion_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_polynomial_expansion.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\ndegree\nThe polynomial degree to expand, which should be greater than equal to 1. A value of 1 means no expansion. Default: 2\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_polynomial_expansion.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_polynomial_expansion.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/sdf_seq.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Creates a DataFrame for the given range"
  },
  {
    "href": "reference/sdf_seq.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_seq(\n  sc,\n  from = 1L,\n  to = 1L,\n  by = 1L,\n  repartition = NULL,\n  type = c(\"integer\", \"integer64\")\n)"
  },
  {
    "href": "reference/sdf_seq.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nfrom, to\nThe start and end to use as a range\n\n\nby\nThe increment of the sequence.\n\n\nrepartition\nThe number of partitions to use when distributing the data across the Spark cluster. Defaults to the minimum number of partitions.\n\n\ntype\nThe data type to use for the index, either \"integer\" or \"integer64\" ."
  },
  {
    "href": "reference/hive_context_config.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Retrieves the runtime configuration interface for Hive."
  },
  {
    "href": "reference/hive_context_config.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "hive_context_config(sc)"
  },
  {
    "href": "reference/hive_context_config.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection ."
  },
  {
    "href": "reference/spark_default_compilation_spec.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "This is the default compilation specification used for Spark extensions, when used with compile_package_jars ."
  },
  {
    "href": "reference/spark_default_compilation_spec.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_default_compilation_spec(\n  pkg = infer_active_package_name(),\n  locations = NULL\n)"
  },
  {
    "href": "reference/spark_default_compilation_spec.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\npkg\nThe package containing Spark extensions to be compiled.\n\n\nlocations\nAdditional locations to scan. By default, the directories /opt/scala and /usr/local/scala will be scanned."
  },
  {
    "href": "reference/replace_na.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See replace_na for more details."
  },
  {
    "href": "reference/sdf_to_avro.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Convert column(s) to avro format"
  },
  {
    "href": "reference/sdf_to_avro.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_to_avro(x, cols = colnames(x))"
  },
  {
    "href": "reference/sdf_to_avro.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn object coercible to a Spark DataFrame\n\n\ncols\nSubset of Columns to convert into avro format"
  },
  {
    "href": "reference/stream_read_socket.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Reads a Socket stream as a Spark dataframe stream."
  },
  {
    "href": "reference/stream_read_socket.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_read_socket(sc, name = NULL, columns = NULL, options = list(), ...)"
  },
  {
    "href": "reference/stream_read_socket.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe name to assign to the newly generated stream.\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType , \"boolean\" for BooleanType , \"byte\" for ByteType , \"integer\" for IntegerType , \"integer64\" for LongType , \"double\" for DoubleType , \"character\" for StringType , \"timestamp\" for TimestampType and \"date\" for DateType .\n\n\noptions\nA list of strings with additional options.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/stream_read_socket.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark stream serialization: stream_read_csv , stream_read_delta , stream_read_json , stream_read_kafka , stream_read_orc , stream_read_parquet , stream_read_text , stream_write_console , stream_write_csv , stream_write_delta , stream_write_json , stream_write_kafka , stream_write_memory , stream_write_orc , stream_write_parquet , stream_write_text"
  },
  {
    "href": "reference/stream_read_socket.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\n\n# Start socket server from terminal, example: nc -lk 9999\nstream <- stream_read_socket(sc, options = list(host = \"localhost\", port = 9999))\nstream"
  },
  {
    "href": "reference/sdf_quantile.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Given a numeric column within a Spark DataFrame, compute approximate quantiles."
  },
  {
    "href": "reference/sdf_quantile.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_quantile(\n  x,\n  column,\n  probabilities = c(0, 0.25, 0.5, 0.75, 1),\n  relative.error = 1e-05,\n  weight.column = NULL\n)"
  },
  {
    "href": "reference/sdf_quantile.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ncolumn\nThe column(s) for which quantiles should be computed. Multiple columns are only supported in Spark 2.0+.\n\n\nprobabilities\nA numeric vector of probabilities, for which quantiles should be computed.\n\n\nrelative.error\nThe maximal possible difference between the actual percentile of a result and its expected percentile (e.g., if relative.error is 0.01 and probabilities is 0.95, then any value between the 94th and 96th percentile will be considered an acceptable approximation).\n\n\nweight.column\nIf not NULL, then a generalized version of the Greenwald- Khanna algorithm will be run to compute weighted percentiles, with each sample from column having a relative weight specified by the corresponding value in weight.column. The weights can be considered as relative frequencies of sample data points."
  },
  {
    "href": "reference/hof_aggregate.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Apply an element-wise aggregation function to an array column (this is essentially a dplyr wrapper for the aggregate(array<T>, A, function<A, T, A>[, function<A, R>]): R built-in Spark SQL functions)"
  },
  {
    "href": "reference/hof_aggregate.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "hof_aggregate(\n  x,\n  start,\n  merge,\n  finish = NULL,\n  expr = NULL,\n  dest_col = NULL,\n  ...\n)"
  },
  {
    "href": "reference/hof_aggregate.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nThe Spark data frame to run aggregation on\n\n\nstart\nThe starting value of the aggregation\n\n\nmerge\nThe aggregation function\n\n\nfinish\nOptional param specifying a transformation to apply on the final value of the aggregation\n\n\nexpr\nThe array being aggregated, could be any SQL expression evaluating to an array (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the aggregated result (default: expr)\n\n\n...\nAdditional params to dplyr::mutate"
  },
  {
    "href": "reference/hof_aggregate.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\")\n# concatenates all numbers of each array in `array_column` and add parentheses\n# around the resulting string\ncopy_to(sc, tibble::tibble(array_column = list(1:5, 21:25))) %>%\nhof_aggregate(\nstart = \"\",\nmerge = ~ CONCAT(.y, .x),\nfinish = ~ CONCAT(\"(\", .x, \")\")\n)"
  },
  {
    "href": "reference/j_invoke.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Invoke a Java function and force return value of the call to be retrieved as a Java object reference."
  },
  {
    "href": "reference/j_invoke.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "j_invoke(jobj, method, ...)\nj_invoke_static(sc, class, method, ...)\nj_invoke_new(sc, class, ...)"
  },
  {
    "href": "reference/j_invoke.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\njobj\nAn list() object acting as a Java object reference (typically, a spark_jobj ).\n\n\nmethod\nThe name of the method to be invoked.\n\n\n...\nOptional arguments, currently unused.\n\n\nsc\nA spark_connection .\n\n\nclass\nThe name of the Java class whose methods should be invoked."
  },
  {
    "href": "reference/full_join.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See full_join for more details."
  },
  {
    "href": "reference/sdf-transform-methods.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Deprecated methods for transformation, fit, and prediction. These are mirrors of the corresponding ml-transform-methods ."
  },
  {
    "href": "reference/sdf-transform-methods.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_predict(x, model, ...)\nsdf_transform(x, transformer, ...)\nsdf_fit(x, estimator, ...)\nsdf_fit_and_transform(x, estimator, ...)"
  },
  {
    "href": "reference/sdf-transform-methods.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA tbl_spark .\n\n\nmodel\nA ml_transformer or a ml_model object.\n\n\n...\nOptional arguments passed to the corresponding ml_ methods.\n\n\ntransformer\nA ml_transformer object.\n\n\nestimator\nA ml_estimator object."
  },
  {
    "href": "reference/sdf-transform-methods.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "sdf_predict() , sdf_transform() , and sdf_fit_and_transform() return a transformed dataframe whereas sdf_fit() returns a ml_transformer ."
  },
  {
    "href": "reference/quote_sql_name.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Calls dbplyr::translate_sql_ on the input character vector or symbol to obtain the corresponding SQL identifier that is escaped and quoted properly"
  },
  {
    "href": "reference/quote_sql_name.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "quote_sql_name(x, con = NULL)"
  },
  {
    "href": "reference/collect_from_rds.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Deserialize Spark data that is serialized using spark_write_rds() into a R dataframe."
  },
  {
    "href": "reference/collect_from_rds.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "collect_from_rds(path)"
  },
  {
    "href": "reference/collect_from_rds.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\npath\nPath to a local RDS file that is produced by spark_write_rds() (RDS files stored in HDFS will need to be downloaded to local filesystem first (e.g., by running hadoop fs -copyToLocal ... or similar)"
  },
  {
    "href": "reference/collect_from_rds.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/spark_apply_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Creates a bundle of packages for spark_apply() ."
  },
  {
    "href": "reference/spark_apply_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_apply_bundle(packages = TRUE, base_path = getwd(), session_id = NULL)"
  },
  {
    "href": "reference/spark_apply_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\npackages\nList of packages to pack or TRUE to pack all.\n\n\nbase_path\nBase path used to store the resulting bundle.\n\n\nsession_id\nAn optional ID string to include in the bundle file name to allow the bundle to be session-specific"
  },
  {
    "href": "reference/ml_pca_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "These methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "href": "reference/ml_pca_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "list(list(\"tidy\"), list(\"ml_model_pca\"))(x, ...)\nlist(list(\"augment\"), list(\"ml_model_pca\"))(x, newdata = NULL, ...)\nlist(list(\"glance\"), list(\"ml_model_pca\"))(x, ...)"
  },
  {
    "href": "reference/ml_pca_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n...\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "href": "reference/random_string.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generate a random string with a given prefix."
  },
  {
    "href": "reference/random_string.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "random_string(prefix = \"table\")"
  },
  {
    "href": "reference/random_string.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nprefix\nA length-one character vector."
  },
  {
    "href": "reference/tbl_change_db.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Use specific database"
  },
  {
    "href": "reference/tbl_change_db.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "tbl_change_db(sc, name)"
  },
  {
    "href": "reference/tbl_change_db.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe database name."
  },
  {
    "href": "reference/ml_evaluator.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "A set of functions to calculate performance metrics for prediction models. Also see the Spark ML Documentation https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.evaluation.package"
  },
  {
    "href": "reference/ml_evaluator.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_binary_classification_evaluator(\n  x,\n  label_col = \"label\",\n  raw_prediction_col = \"rawPrediction\",\n  metric_name = \"areaUnderROC\",\n  uid = random_string(\"binary_classification_evaluator_\"),\n  ...\n)\nml_binary_classification_eval(\n  x,\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  metric_name = \"areaUnderROC\"\n)\nml_multiclass_classification_evaluator(\n  x,\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  metric_name = \"f1\",\n  uid = random_string(\"multiclass_classification_evaluator_\"),\n  ...\n)\nml_classification_eval(\n  x,\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  metric_name = \"f1\"\n)\nml_regression_evaluator(\n  x,\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  metric_name = \"rmse\",\n  uid = random_string(\"regression_evaluator_\"),\n  ...\n)"
  },
  {
    "href": "reference/ml_evaluator.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection object or a tbl_spark containing label and prediction columns. The latter should be the output of sdf_predict .\n\n\nlabel_col\nName of column string specifying which column contains the true labels or values.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nmetric_name\nThe performance metric. See details.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; currently unused.\n\n\nprediction_col\nName of the column that contains the predicted label or value NOT the scored probability. Column should be of type Double ."
  },
  {
    "href": "reference/ml_evaluator.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "The following metrics are supported\n\nBinary Classification: areaUnderROC (default) or areaUnderPR (not available in Spark 2.X.)\nMulticlass Classification: f1 (default), precision , recall , weightedPrecision , weightedRecall or accuracy ; for Spark 2.X: f1 (default), weightedPrecision , weightedRecall or accuracy .\nRegression: rmse (root mean squared error, default), mse (mean squared error), r2 , or mae (mean absolute error.)\n\nml_binary_classification_eval() is an alias for ml_binary_classification_evaluator() for backwards compatibility.\nml_classification_eval() is an alias for ml_multiclass_classification_evaluator() for backwards compatibility."
  },
  {
    "href": "reference/ml_evaluator.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The calculated performance metric"
  },
  {
    "href": "reference/ml_evaluator.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, name = \"mtcars_tbl\", overwrite = TRUE)\n\npartitions <- mtcars_tbl %>%\nsdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\nmtcars_training <- partitions$training\nmtcars_test <- partitions$test\n\n# for multiclass classification\nrf_model <- mtcars_training %>%\nml_random_forest(cyl ~ ., type = \"classification\")\n\npred <- ml_predict(rf_model, mtcars_test)\n\nml_multiclass_classification_evaluator(pred)\n\n# for regression\nrf_model <- mtcars_training %>%\nml_random_forest(cyl ~ ., type = \"regression\")\n\npred <- ml_predict(rf_model, mtcars_test)\n\nml_regression_evaluator(pred, label_col = \"cyl\")\n\n# for binary classification\nrf_model <- mtcars_training %>%\nml_random_forest(am ~ gear + carb, type = \"classification\")\n\npred <- ml_predict(rf_model, mtcars_test)\n\nml_binary_classification_evaluator(pred)"
  },
  {
    "href": "reference/ml-params.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Helper methods for working with parameters for ML objects."
  },
  {
    "href": "reference/ml-params.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_is_set(x, param, ...)\nml_param_map(x, ...)\nml_param(x, param, allow_null = FALSE, ...)\nml_params(x, params = NULL, allow_null = FALSE, ...)"
  },
  {
    "href": "reference/ml-params.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark ML object, either a pipeline stage or an evaluator.\n\n\nparam\nThe parameter to extract or set.\n\n\n...\nOptional arguments; currently unused.\n\n\nallow_null\nWhether to allow NULL results when extracting parameters. If FALSE , an error will be thrown if the specified parameter is not found. Defaults to FALSE .\n\n\nparams\nA vector of parameters to extract."
  },
  {
    "href": "reference/ml_stage.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Extraction of stages from a Pipeline or PipelineModel object."
  },
  {
    "href": "reference/ml_stage.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_stage(x, stage)\nml_stages(x, stages = NULL)"
  },
  {
    "href": "reference/ml_stage.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA ml_pipeline or a ml_pipeline_model object\n\n\nstage\nThe UID of a stage in the pipeline.\n\n\nstages\nThe UIDs of stages in the pipeline as a character vector."
  },
  {
    "href": "reference/ml_stage.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "For ml_stage() : The stage specified.\nFor ml_stages() : A list of stages. If stages is not set, the function returns all stages of the pipeline in a list."
  },
  {
    "href": "reference/spark_read_libsvm.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Read libsvm file into a Spark DataFrame."
  },
  {
    "href": "reference/spark_read_libsvm.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_read_libsvm(\n  sc,\n  name = NULL,\n  path = name,\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  options = list(),\n  ...\n)"
  },
  {
    "href": "reference/spark_read_libsvm.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\noptions\nA list of strings with additional options.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_read_libsvm.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/spark_write_source.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Writes a Spark DataFrame into a generic source."
  },
  {
    "href": "reference/spark_write_source.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_write_source(\n  x,\n  source,\n  mode = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/spark_write_source.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nsource\nA data source capable of reading data.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure. For more details see also http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_write_source.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/sdf_collect.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Collects a Spark dataframe into R."
  },
  {
    "href": "reference/sdf_collect.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_collect(object, impl = c(\"row-wise\", \"row-wise-iter\", \"column-wise\"), ...)"
  },
  {
    "href": "reference/sdf_collect.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nobject\nSpark dataframe to collect\n\n\nimpl\nWhich implementation to use while collecting Spark dataframe - row-wise: fetch the entire dataframe into memory and then process it row-by-row - row-wise-iter: iterate through the dataframe using RDD local iterator, processing one row at a time (hence reducing memory footprint) - column-wise: fetch the entire dataframe into memory and then process it column-by-column NOTE: (1) this will not apply to streaming or arrow use cases (2) this parameter will only affect implementation detail, and will not affect result of sdf_collect, and should only be set if performance profiling indicates any particular choice will be significantly better than the default choice (“row-wise”)\n\n\n...\nAdditional options."
  },
  {
    "href": "reference/spark_read_avro.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Read Apache Avro data into a Spark DataFrame. Notice this functionality requires the Spark connection sc to be instantiated with either an explicitly specified Spark version (i.e., spark_connect(..., version = <version>, packages = c(\"avro\", <other package(s)>), ...) ) or a specific version of Spark avro package to use (e.g., spark_connect(..., packages = c(\"org.apache.spark:spark-avro_2.12:3.0.0\", <other package(s)>), ...) )."
  },
  {
    "href": "reference/spark_read_avro.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_read_avro(\n  sc,\n  name = NULL,\n  path = name,\n  avro_schema = NULL,\n  ignore_extension = TRUE,\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE\n)"
  },
  {
    "href": "reference/spark_read_avro.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\navro_schema\nOptional Avro schema in JSON format\n\n\nignore_extension\nIf enabled, all files with and without .avro extension are loaded (default: TRUE )\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?"
  },
  {
    "href": "reference/spark_read_avro.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/ft_feature_hasher.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Feature Transformation – FeatureHasher (Transformer)"
  },
  {
    "href": "reference/ft_feature_hasher.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_feature_hasher(\n  x,\n  input_cols = NULL,\n  output_col = NULL,\n  num_features = 2^18,\n  categorical_cols = NULL,\n  uid = random_string(\"feature_hasher_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_feature_hasher.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_cols\nNames of input columns.\n\n\noutput_col\nName of output column.\n\n\nnum_features\nNumber of features. Defaults to \\(2^18\\) .\n\n\ncategorical_cols\nNumeric columns to treat as categorical features. By default only string and boolean columns are treated as categorical, so this param can be used to explicitly specify the numerical columns to treat as categorical.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_feature_hasher.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Feature hashing projects a set of categorical or numerical features into a feature vector of specified dimension (typically substantially smaller than that of the original feature space). This is done using the hashing trick https://en.wikipedia.org/wiki/Feature_hashing to map features to indices in the feature vector.\nThe FeatureHasher transformer operates on multiple columns. Each column may contain either numeric or categorical features. Behavior and handling of column data types is as follows: -Numeric columns: For numeric features, the hash value of the column name is used to map the feature value to its index in the feature vector. By default, numeric features are not treated as categorical (even when they are integers). To treat them as categorical, specify the relevant columns in categoricalCols. -String columns: For categorical features, the hash value of the string “column_name=value” is used to map to the vector index, with an indicator value of 1.0. Thus, categorical features are “one-hot” encoded (similarly to using OneHotEncoder with drop_last=FALSE). -Boolean columns: Boolean values are treated in the same way as string columns. That is, boolean features are represented as “column_name=true” or “column_name=false”, with an indicator value of 1.0.\nNull (missing) values are ignored (implicitly zero in the resulting feature vector).\nThe hash function used here is also the MurmurHash 3 used in HashingTF. Since a simple modulo on the hashed value is used to determine the vector index, it is advisable to use a power of two as the num_features parameter; otherwise the features will not be mapped evenly to the vector indices."
  },
  {
    "href": "reference/ft_feature_hasher.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_feature_hasher.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/sub-.tbl_spark.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Susetting operator for Spark dataframe allowing a subset of column(s) to be selected using syntaxes similar to those supported by R dataframes"
  },
  {
    "href": "reference/sub-.tbl_spark.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "list(list(\"[\"), list(\"tbl_spark\"))(x, i)"
  },
  {
    "href": "reference/sub-.tbl_spark.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nThe Spark dataframe\n\n\ni\nExpression specifying subset of column(s) to include or exclude from the result (e.g., [\"col1\"], [c(\"col1\", \"col2\")], [1:10], [-1], [NULL], or [])"
  },
  {
    "href": "reference/sub-.tbl_spark.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"spark://HOST:PORT\")\nexample_sdf <- copy_to(sc, tibble::tibble(a = 1, b = 2))\nexample_sdf[\"a\"] %>% print()"
  },
  {
    "href": "reference/ml_default_stop_words.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Loads the default stop words for the given language."
  },
  {
    "href": "reference/ml_default_stop_words.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_default_stop_words(\n  sc,\n  language = c(\"english\", \"danish\", \"dutch\", \"finnish\", \"french\", \"german\",\n    \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"russian\", \"spanish\", \"swedish\",\n    \"turkish\"),\n  ...\n)"
  },
  {
    "href": "reference/ml_default_stop_words.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection\n\n\nlanguage\nA character string.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ml_default_stop_words.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Supported languages: danish, dutch, english, finnish, french, german, hungarian, italian, norwegian, portuguese, russian, spanish, swedish, turkish. Defaults to English. See https://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/ for more details"
  },
  {
    "href": "reference/ml_default_stop_words.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "A list of stop words."
  },
  {
    "href": "reference/ml_default_stop_words.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "ft_stop_words_remover"
  },
  {
    "href": "reference/stream_write_json.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Writes a Spark dataframe stream into a JSON stream."
  },
  {
    "href": "reference/stream_write_json.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_write_json(\n  x,\n  path,\n  mode = c(\"append\", \"complete\", \"update\"),\n  trigger = stream_trigger_interval(),\n  checkpoint = file.path(path, \"checkpoints\", random_string(\"\")),\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/stream_write_json.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe destination path. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\" , \"complete\" or \"update\" .\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous .\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/stream_write_json.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark stream serialization: stream_read_csv , stream_read_delta , stream_read_json , stream_read_kafka , stream_read_orc , stream_read_parquet , stream_read_socket , stream_read_text , stream_write_console , stream_write_csv , stream_write_delta , stream_write_kafka , stream_write_memory , stream_write_orc , stream_write_parquet , stream_write_text"
  },
  {
    "href": "reference/stream_write_json.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\n\ndir.create(\"json-in\")\njsonlite::write_json(list(a = c(1, 2), b = c(10, 20)), \"json-in/data.json\")\n\njson_path <- file.path(\"file://\", getwd(), \"json-in\")\n\nstream <- stream_read_json(sc, json_path) %>% stream_write_json(\"json-out\")\n\nstream_stop(stream)"
  },
  {
    "href": "reference/ml_one_vs_rest.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Reduction of Multiclass Classification to Binary Classification. Performs reduction using one against all strategy. For a multiclass classification with k classes, train k models (one per class). Each example is scored against all k models and the model with highest score is picked to label the example."
  },
  {
    "href": "reference/ml_one_vs_rest.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_one_vs_rest(\n  x,\n  formula = NULL,\n  classifier = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"one_vs_rest_\"),\n  ...\n)"
  },
  {
    "href": "reference/ml_one_vs_rest.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nUsed when x is a tbl_spark . R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nclassifier\nObject of class ml_estimator . Base binary classifier that we reduce multiclass classification into.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula .\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; see Details."
  },
  {
    "href": "reference/ml_one_vs_rest.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "When x is a tbl_spark and formula (alternatively, response and features ) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\" ) can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model , ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "href": "reference/ml_one_vs_rest.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a predictor is constructed then immediately fit with the input tbl_spark , returning a prediction model.\ntbl_spark , with formula : specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model ."
  },
  {
    "href": "reference/ml_one_vs_rest.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression , ml_decision_tree_classifier , ml_gbt_classifier , ml_generalized_linear_regression , ml_isotonic_regression , ml_linear_regression , ml_linear_svc , ml_logistic_regression , ml_multilayer_perceptron_classifier , ml_naive_bayes , ml_random_forest_classifier"
  },
  {
    "href": "reference/spark_write_text.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Serialize a Spark DataFrame to the plain text format."
  },
  {
    "href": "reference/spark_write_text.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_write_text(\n  x,\n  path,\n  mode = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/spark_write_text.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure. For more details see also http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_write_text.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table"
  },
  {
    "href": "reference/sdf_dim.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "sdf_dim() , sdf_nrow() and sdf_ncol() provide similar functionality to dim() , nrow() and ncol() ."
  },
  {
    "href": "reference/sdf_dim.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_dim(x)\nsdf_nrow(x)\nsdf_ncol(x)"
  },
  {
    "href": "reference/sdf_dim.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn object (usually a spark_tbl )."
  },
  {
    "href": "reference/spark_dataframe.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "This S3 generic is used to access a Spark DataFrame object (as a Java object reference) from an list() object."
  },
  {
    "href": "reference/spark_dataframe.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_dataframe(x, ...)"
  },
  {
    "href": "reference/spark_dataframe.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn list() object wrapping, or containing, a Spark DataFrame.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_dataframe.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "A spark_jobj representing a Java object reference to a Spark DataFrame."
  },
  {
    "href": "reference/hof_array_sort.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Applies a custom comparator function to sort an array (this is essentially a dplyr wrapper to the array_sort(expr, func) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "href": "reference/hof_array_sort.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "hof_array_sort(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "href": "reference/hof_array_sort.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\nfunc\nThe comparator function to apply (it should take 2 array elements as arguments and return an integer, with a return value of -1 indicating the first element is less than the second, 0 indicating equality, or 1 indicating the first element is greater than the second)\n\n\nexpr\nThe array being sorted, could be any SQL expression evaluating to an array (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the sorted result (default: expr)\n\n\n...\nAdditional params to dplyr::mutate"
  },
  {
    "href": "reference/hof_array_sort.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"3.0.0\")\ncopy_to(\nsc,\ntibble::tibble(\n# x contains 2 arrays each having elements in ascending order\nx = list(1:5, 6:10)\n)\n) %>%\n# now each array from x gets sorted in descending order\nhof_array_sort(~ as.integer(sign(.y - .x)))"
  },
  {
    "href": "reference/sdf_separate_column.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Given a vector column in a Spark DataFrame, split that into n separate columns, each column made up of the different elements in the column column ."
  },
  {
    "href": "reference/sdf_separate_column.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_separate_column(x, column, into = NULL)"
  },
  {
    "href": "reference/sdf_separate_column.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ncolumn\nThe name of a (vector-typed) column.\n\n\ninto\nA specification of the columns that should be generated from column . This can either be a vector of column names, or an list() list mapping column names to the (1-based) index at which a particular vector element should be extracted."
  },
  {
    "href": "reference/select.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See select for more details."
  },
  {
    "href": "reference/stream_write_delta.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Writes a Spark dataframe stream into a Delta Lake table."
  },
  {
    "href": "reference/stream_write_delta.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_write_delta(\n  x,\n  path,\n  mode = c(\"append\", \"complete\", \"update\"),\n  checkpoint = file.path(\"checkpoints\", random_string(\"\")),\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/stream_write_delta.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\" , \"complete\" or \"update\" .\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/stream_write_delta.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Please note that Delta Lake requires installing the appropriate package by setting the packages parameter to \"delta\" in spark_connect()"
  },
  {
    "href": "reference/stream_write_delta.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark stream serialization: stream_read_csv , stream_read_delta , stream_read_json , stream_read_kafka , stream_read_orc , stream_read_parquet , stream_read_socket , stream_read_text , stream_write_console , stream_write_csv , stream_write_json , stream_write_kafka , stream_write_memory , stream_write_orc , stream_write_parquet , stream_write_text"
  },
  {
    "href": "reference/stream_write_delta.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.4.0\", packages = \"delta\")\n\ndir.create(\"text-in\")\nwriteLines(\"A text entry\", \"text-in/text.txt\")\n\ntext_path <- file.path(\"file://\", getwd(), \"text-in\")\n\nstream <- stream_read_text(sc, text_path) %>% stream_write_delta(path = \"delta-test\")\n\nstream_stop(stream)"
  },
  {
    "href": "reference/livy_install.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Automatically download and install list(“livy”) . livy provides a REST API to Spark.\nFind the LIVY_HOME directory for a given version of Livy that was previously installed using livy_install ."
  },
  {
    "href": "reference/livy_install.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "livy_install(version = \"0.6.0\", spark_home = NULL, spark_version = NULL)\nlivy_available_versions()\nlivy_install_dir()\nlivy_installed_versions()\nlivy_home_dir(version = NULL)"
  },
  {
    "href": "reference/livy_install.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nversion\nVersion of Livy\n\n\nspark_home\nThe path to a Spark installation. The downloaded and installed version of livy will then be associated with this Spark installation. When unset ( NULL ), the value is inferred based on the value of spark_version supplied.\n\n\nspark_version\nThe version of Spark to use. When unset ( NULL ), the value is inferred based on the value of livy_version supplied. A version of Spark known to be compatible with the requested version of livy is chosen when possible."
  },
  {
    "href": "reference/livy_install.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Path to LIVY_HOME (or NULL if the specified version was not found)."
  },
  {
    "href": "reference/ft_r_formula.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Implements the transforms required for fitting a dataset against an R model formula. Currently we support a limited subset of the R operators, including ~ , . , : , + , and - . Also see the R formula docs here: http://stat.ethz.ch/R-manual/R-patched/library/stats/html/formula.html"
  },
  {
    "href": "reference/ft_r_formula.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_r_formula(\n  x,\n  formula = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  force_index_label = FALSE,\n  uid = random_string(\"r_formula_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_r_formula.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nR formula as a character string or a formula. Formula objects are converted to character strings directly and the environment is not captured.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula .\n\n\nforce_index_label\n(Spark 2.1.0+) Force to index label whether it is numeric or string type. Usually we index label only when it is string type. If the formula was used by classification algorithms, we can force to index label even it is numeric type by setting this param with true. Default: FALSE .\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_r_formula.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "The basic operators in the formula are:\n\n~ separate target and terms\n\nconcat terms, “+ 0” means removing intercept\n\n\nremove a term, “- 1” means removing intercept\n\n: interaction (multiplication for numeric values, or binarized categorical values)\n. all columns except target\n\nSuppose a and b are double columns, we use the following simple examples to illustrate the effect of RFormula:\n\ny ~ a + b means model y ~ w0 + w1 * a + w2 * b where w0 is the intercept and w1, w2 are coefficients.\ny ~ a + b + a:b - 1 means model y ~ w1 * a + w2 * b + w3 * a * b where w1, w2, w3 are coefficients.\n\nRFormula produces a vector column of features and a double or string column of label. Like when formulas are used in R for linear regression, string input columns will be one-hot encoded, and numeric columns will be cast to doubles. If the label column is of type string, it will be first transformed to double with StringIndexer. If the label column does not exist in the DataFrame, the output label column will be created from the specified response variable in the formula.\nIn the case where x is a tbl_spark , the estimator fits against x to obtain a transformer, which is then immediately used to transform x , returning a tbl_spark ."
  },
  {
    "href": "reference/ft_r_formula.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_r_formula.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/ml_chisquare_test.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Conduct Pearson’s independence test for every feature against the label. For each feature, the (feature, label) pairs are converted into a contingency matrix for which the Chi-squared statistic is computed. All label and feature values must be categorical."
  },
  {
    "href": "reference/ml_chisquare_test.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_chisquare_test(x, features, label)"
  },
  {
    "href": "reference/ml_chisquare_test.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA tbl_spark .\n\n\nfeatures\nThe name(s) of the feature columns. This can also be the name of a single vector column created using ft_vector_assembler() .\n\n\nlabel\nThe name of the label column."
  },
  {
    "href": "reference/ml_chisquare_test.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "A data frame with one row for each (feature, label) pair with p-values, degrees of freedom, and test statistics."
  },
  {
    "href": "reference/ml_chisquare_test.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\nfeatures <- c(\"Petal_Width\", \"Petal_Length\", \"Sepal_Length\", \"Sepal_Width\")\n\nml_chisquare_test(iris_tbl, features = features, label = \"Species\")"
  },
  {
    "href": "reference/spark_versions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Retrieves a dataframe available Spark versions that van be installed."
  },
  {
    "href": "reference/spark_versions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_versions(latest = TRUE)"
  },
  {
    "href": "reference/spark_versions.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nlatest\nCheck for latest version?"
  },
  {
    "href": "reference/stream_lag.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Given a streaming Spark dataframe as input, this function will return another streaming dataframe that contains all columns in the input and column(s) that are shifted behind by the offset(s) specified in ... (see example)"
  },
  {
    "href": "reference/stream_lag.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_lag(x, cols, thresholds = NULL)"
  },
  {
    "href": "reference/stream_lag.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn object coercable to a Spark Streaming DataFrame.\n\n\ncols\nA list of expressions of the form  =  ~  (e.g., prev_value = value ~ 1 will create a new column prev_value containing all values from the source column value shifted behind by 1\n\n\nthresholds\nOptional named list of timestamp column(s) and corresponding time duration(s) for deterimining whether a previous record is sufficiently recent relative to the current record. If the any of the time difference(s) between the current and a previous record is greater than the maximal duration allowed, then the previous record is discarded and will not be part of the query result. The durations can be specified with numeric types (which will be interpreted as max difference allowed in number of milliseconds between 2 UNIX timestamps) or time duration strings such as “5s”, “5sec”, “5min”, “5hour”, etc. Any timestamp column in x that is not of timestamp of date Spark SQL types will be interepreted as number of milliseconds since the UNIX epoch."
  },
  {
    "href": "reference/stream_lag.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\n\nsc <- spark_connect(master = \"local\", version = \"2.2.0\")\n\nstreaming_path <- tempfile(\"days_df_\")\ndays_df <- tibble::tibble(\ntoday = weekdays(as.Date(seq(7), origin = \"1970-01-01\"))\n)\nnum_iters <- 7\nstream_generate_test(\ndf = days_df,\npath = streaming_path,\ndistribution = rep(nrow(days_df), num_iters),\niterations = num_iters\n)\n\nstream_read_csv(sc, streaming_path) %>%\nstream_lag(cols = c(yesterday = today ~ 1, two_days_ago = today ~ 2)) %>%\ncollect() %>%\nprint(n = 10L)"
  },
  {
    "href": "reference/ml_isotonic_regression.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Currently implemented using parallelized pool adjacent violators algorithm. Only univariate (single feature) algorithm supported."
  },
  {
    "href": "reference/ml_isotonic_regression.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_isotonic_regression(\n  x,\n  formula = NULL,\n  feature_index = 0,\n  isotonic = TRUE,\n  weight_col = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"isotonic_regression_\"),\n  ...\n)"
  },
  {
    "href": "reference/ml_isotonic_regression.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nUsed when x is a tbl_spark . R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nfeature_index\nIndex of the feature if features_col is a vector column (default: 0), no effect otherwise.\n\n\nisotonic\nWhether the output sequence should be isotonic/increasing (true) or antitonic/decreasing (false). Default: true\n\n\nweight_col\nThe name of the column to use as weights for the model fit.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula .\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; see Details."
  },
  {
    "href": "reference/ml_isotonic_regression.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "When x is a tbl_spark and formula (alternatively, response and features ) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\" ) can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model , ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "href": "reference/ml_isotonic_regression.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a predictor is constructed then immediately fit with the input tbl_spark , returning a prediction model.\ntbl_spark , with formula : specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model ."
  },
  {
    "href": "reference/ml_isotonic_regression.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression , ml_decision_tree_classifier , ml_gbt_classifier , ml_generalized_linear_regression , ml_linear_regression , ml_linear_svc , ml_logistic_regression , ml_multilayer_perceptron_classifier , ml_naive_bayes , ml_one_vs_rest , ml_random_forest_classifier"
  },
  {
    "href": "reference/ml_isotonic_regression.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\npartitions <- iris_tbl %>%\nsdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\niris_training <- partitions$training\niris_test <- partitions$test\n\niso_res <- iris_tbl %>%\nml_isotonic_regression(Petal_Length ~ Petal_Width)\n\npred <- ml_predict(iso_res, iris_test)\n\npred"
  },
  {
    "href": "reference/spark_apply_log.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Writes data to log under spark_apply() ."
  },
  {
    "href": "reference/spark_apply_log.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_apply_log(..., level = \"INFO\")"
  },
  {
    "href": "reference/spark_apply_log.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\n...\nArguments to write to log.\n\n\nlevel\nSeverity level for this entry; recommended values: INFO , ERROR or WARN ."
  },
  {
    "href": "reference/invoke.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Invoke methods on Java object references. These functions provide a mechanism for invoking various Java object methods directly from list() ."
  },
  {
    "href": "reference/invoke.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "invoke(jobj, method, ...)\ninvoke_static(sc, class, method, ...)\ninvoke_new(sc, class, ...)"
  },
  {
    "href": "reference/invoke.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\njobj\nAn list() object acting as a Java object reference (typically, a spark_jobj ).\n\n\nmethod\nThe name of the method to be invoked.\n\n\n...\nOptional arguments, currently unused.\n\n\nsc\nA spark_connection .\n\n\nclass\nThe name of the Java class whose methods should be invoked."
  },
  {
    "href": "reference/invoke.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Use each of these functions in the following scenarios:\nlist(list(“lll”), list(“”, list(“invoke”), ” “, list(),” Execute a method on a Java object reference (typically, a “, list(”spark_jobj”), “).”, list(), “”, list(“invoke_static”), ” “, list(),” Execute a static method associated with a Java class. “, list(),”“, list(”invoke_new”), ” “, list(),” Invoke a constructor associated with a Java class. “, list(),”“))"
  },
  {
    "href": "reference/invoke.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"spark://HOST:PORT\")\nspark_context(sc) %>%\ninvoke(\"textFile\", \"file.csv\", 1L) %>%\ninvoke(\"count\")"
  },
  {
    "href": "reference/sdf_expand_grid.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Given one or more R vectors/factors or single-column Spark dataframes, perform an expand.grid operation on all of them and store the result in a Spark dataframe"
  },
  {
    "href": "reference/sdf_expand_grid.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_expand_grid(\n  sc,\n  ...,\n  broadcast_vars = NULL,\n  memory = TRUE,\n  repartition = NULL,\n  partition_by = NULL\n)"
  },
  {
    "href": "reference/sdf_expand_grid.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\n...\nEach input variable can be either a R vector/factor or a Spark dataframe. Unnamed inputs will assume the default names of ‘Var1’, ‘Var2’, etc in the result, similar to what expand.grid does for unnamed inputs.\n\n\nbroadcast_vars\nIndicates which input(s) should be broadcasted to all nodes of the Spark cluster during the join process (default: none).\n\n\nmemory\nBoolean; whether the resulting Spark dataframe should be cached into memory (default: TRUE)\n\n\nrepartition\nNumber of partitions the resulting Spark dataframe should have\n\n\npartition_by\nVector of column names used for partitioning the resulting Spark dataframe, only supported for Spark 2.0+"
  },
  {
    "href": "reference/sdf_expand_grid.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\ngrid_sdf <- sdf_expand_grid(sc, seq(5), rnorm(10), letters)"
  },
  {
    "href": "reference/ml_fpgrowth.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "A parallel FP-growth algorithm to mine frequent itemsets."
  },
  {
    "href": "reference/ml_fpgrowth.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_fpgrowth(\n  x,\n  items_col = \"items\",\n  min_confidence = 0.8,\n  min_support = 0.3,\n  prediction_col = \"prediction\",\n  uid = random_string(\"fpgrowth_\"),\n  ...\n)\nml_association_rules(model)\nml_freq_itemsets(model)"
  },
  {
    "href": "reference/ml_fpgrowth.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nitems_col\nItems column name. Default: “items”\n\n\nmin_confidence\nMinimal confidence for generating Association Rule. min_confidence will not affect the mining for frequent itemsets, but will affect the association rules generation. Default: 0.8\n\n\nmin_support\nMinimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears more than (min_support * size-of-the-dataset) times will be output in the frequent itemsets. Default: 0.3\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; currently unused.\n\n\nmodel\nA fitted FPGrowth model returned by ml_fpgrowth()"
  },
  {
    "href": "reference/sdf_rexp.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generator method for creating a single-column Spark dataframes comprised of i.i.d. samples from an exponential distribution."
  },
  {
    "href": "reference/sdf_rexp.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_rexp(sc, n, rate = 1, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "href": "reference/sdf_rexp.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nrate\nRate of the exponential distribution (default: 1). The exponential distribution with rate lambda has mean 1 / lambda and density f(x) = lambda e ^ - lambda x .\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "href": "reference/sdf_rexp.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark statistical routines: sdf_rbeta , sdf_rbinom , sdf_rcauchy , sdf_rchisq , sdf_rgamma , sdf_rgeom , sdf_rhyper , sdf_rlnorm , sdf_rnorm , sdf_rpois , sdf_rt , sdf_runif , sdf_rweibull"
  },
  {
    "href": "reference/sdf_repartition.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Repartition a Spark DataFrame"
  },
  {
    "href": "reference/sdf_repartition.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_repartition(x, partitions = NULL, partition_by = NULL)"
  },
  {
    "href": "reference/sdf_repartition.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\npartitions\nnumber of partitions\n\n\npartition_by\nvector of column names used for partitioning, only supported for Spark 2.0+"
  },
  {
    "href": "reference/sdf_distinct.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Invoke distinct on a Spark DataFrame"
  },
  {
    "href": "reference/sdf_distinct.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_distinct(x, ..., name)"
  },
  {
    "href": "reference/sdf_distinct.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame.\n\n\n...\nOptional variables to use when determining uniqueness. If there are multiple rows for a given combination of inputs, only the first row will be preserved. If omitted, will use all variables.\n\n\nname\nA name to assign this table. Passed to [sdf_register()]."
  },
  {
    "href": "reference/sdf_distinct.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark data frames: sdf_copy_to , sdf_random_split , sdf_register , sdf_sample , sdf_sort , sdf_weighted_sample"
  },
  {
    "href": "reference/hof_transform_values.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Applies the transformation function specified to all values of a map (this is essentially a dplyr wrapper to the transform_values(expr, func) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "href": "reference/hof_transform_values.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "hof_transform_values(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "href": "reference/hof_transform_values.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\nfunc\nThe transformation function to apply (it should take (key, value) as arguments and return a transformed value)\n\n\nexpr\nThe map being transformed, could be any SQL expression evaluating to a map (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the transformed result (default: expr)\n\n\n...\nAdditional params to dplyr::mutate"
  },
  {
    "href": "reference/hof_transform_values.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"3.0.0\")\nsdf <- sdf_len(sc, 1) %>% dplyr::mutate(m = map(\"a\", 0L, \"b\", 2L, \"c\", -1L))\ntransformed_sdf <- sdf %>% hof_transform_values(~ CONCAT(.x, \" == \", .y))"
  },
  {
    "href": "reference/spark_write_table.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Writes a Spark DataFrame into a Spark table."
  },
  {
    "href": "reference/spark_write_table.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_write_table(\n  x,\n  name,\n  mode = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/spark_write_table.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nname\nThe name to assign to the newly generated table.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure. For more details see also http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_write_table.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_text"
  },
  {
    "href": "reference/spark_statistical_routines.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generator methods for creating single-column Spark dataframes comprised of i.i.d. samples from some distribution."
  },
  {
    "href": "reference/spark_statistical_routines.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "href": "reference/spark_read_delta.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Read from Delta Lake into a Spark DataFrame."
  },
  {
    "href": "reference/spark_read_delta.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_read_delta(\n  sc,\n  path,\n  name = NULL,\n  version = NULL,\n  timestamp = NULL,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  ...\n)"
  },
  {
    "href": "reference/spark_read_delta.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nname\nThe name to assign to the newly generated table.\n\n\nversion\nThe version of the delta table to read.\n\n\ntimestamp\nThe timestamp of the delta table to read. For example, \"2019-01-01\" or \"2019-01-01'T'00:00:00.000Z\" .\n\n\noptions\nA list of strings with additional options.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_read_delta.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/stream_read_orc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Reads an ORC stream as a Spark dataframe stream."
  },
  {
    "href": "reference/stream_read_orc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_read_orc(sc, path, name = NULL, columns = NULL, options = list(), ...)"
  },
  {
    "href": "reference/stream_read_orc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType , \"boolean\" for BooleanType , \"byte\" for ByteType , \"integer\" for IntegerType , \"integer64\" for LongType , \"double\" for DoubleType , \"character\" for StringType , \"timestamp\" for TimestampType and \"date\" for DateType .\n\n\noptions\nA list of strings with additional options.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/stream_read_orc.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark stream serialization: stream_read_csv , stream_read_delta , stream_read_json , stream_read_kafka , stream_read_parquet , stream_read_socket , stream_read_text , stream_write_console , stream_write_csv , stream_write_delta , stream_write_json , stream_write_kafka , stream_write_memory , stream_write_orc , stream_write_parquet , stream_write_text"
  },
  {
    "href": "reference/stream_read_orc.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\n\nsdf_len(sc, 10) %>% spark_write_orc(\"orc-in\")\n\nstream <- stream_read_orc(sc, \"orc-in\") %>% stream_write_orc(\"orc-out\")\n\nstream_stop(stream)"
  },
  {
    "href": "reference/sdf_bind.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "sdf_bind_rows() and sdf_bind_cols() are implementation of the common pattern of do.call(rbind, sdfs) or do.call(cbind, sdfs) for binding many Spark DataFrames into one."
  },
  {
    "href": "reference/sdf_bind.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_bind_rows(..., id = NULL)\nsdf_bind_cols(...)"
  },
  {
    "href": "reference/sdf_bind.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\n...\nSpark tbls to combine. Each argument can either be a Spark DataFrame or a list of Spark DataFrames When row-binding, columns are matched by name, and any missing columns with be filled with NA. When column-binding, rows are matched by position, so all data frames must have the same number of rows.\n\n\nid\nData frame identifier. When id is supplied, a new column of identifiers is created to link each row to its original Spark DataFrame. The labels are taken from the named arguments to sdf_bind_rows() . When a list of Spark DataFrames is supplied, the labels are taken from the names of the list. If no names are found a numeric sequence is used instead."
  },
  {
    "href": "reference/sdf_bind.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "The output of sdf_bind_rows() will contain a column if that column appears in any of the inputs."
  },
  {
    "href": "reference/sdf_bind.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "sdf_bind_rows() and sdf_bind_cols() return tbl_spark"
  },
  {
    "href": "reference/sdf_sql.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Defines a Spark DataFrame from a SQL query, useful to create Spark DataFrames without collecting the results immediately."
  },
  {
    "href": "reference/sdf_sql.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_sql(sc, sql)"
  },
  {
    "href": "reference/sdf_sql.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nsql\na ‘SQL’ query used to generate a Spark DataFrame."
  },
  {
    "href": "reference/sql-transformer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Implements the transformations which are defined by SQL statement. Currently we only support SQL syntax like ‘SELECT … FROM THIS …’ where ‘THIS’ represents the underlying table of the input dataset. The select clause specifies the fields, constants, and expressions to display in the output, it can be any select clause that Spark SQL supports. Users can also use Spark SQL built-in function and UDFs to operate on these selected columns."
  },
  {
    "href": "reference/sql-transformer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_sql_transformer(\n  x,\n  statement = NULL,\n  uid = random_string(\"sql_transformer_\"),\n  ...\n)\nft_dplyr_transformer(x, tbl, uid = random_string(\"dplyr_transformer_\"), ...)"
  },
  {
    "href": "reference/sql-transformer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nstatement\nA SQL statement.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused.\n\n\ntbl\nA tbl_spark generated using dplyr transformations."
  },
  {
    "href": "reference/sql-transformer.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "ft_dplyr_transformer() is mostly a wrapper around ft_sql_transformer() that takes a tbl_spark instead of a SQL statement. Internally, the ft_dplyr_transformer() extracts the dplyr transformations used to generate tbl as a SQL statement or a sampling operation. Note that only single-table dplyr verbs are supported and that the sdf_ family of functions are not."
  },
  {
    "href": "reference/sql-transformer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/sql-transformer.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/sdf-saveload.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Routines for saving and loading Spark DataFrames."
  },
  {
    "href": "reference/sdf-saveload.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_save_table(x, name, overwrite = FALSE, append = FALSE)\nsdf_load_table(sc, name)\nsdf_save_parquet(x, path, overwrite = FALSE, append = FALSE)\nsdf_load_parquet(sc, path)"
  },
  {
    "href": "reference/sdf-saveload.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nname\nThe table name to assign to the saved Spark DataFrame.\n\n\noverwrite\nBoolean; overwrite a pre-existing table of the same name?\n\n\nappend\nBoolean; append to a pre-existing table of the same name?\n\n\nsc\nA spark_connection object.\n\n\npath\nThe path where the Spark DataFrame should be saved."
  },
  {
    "href": "reference/sdf_describe.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Compute summary statistics for columns of a data frame"
  },
  {
    "href": "reference/sdf_describe.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_describe(x, cols = colnames(x))"
  },
  {
    "href": "reference/sdf_describe.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn object coercible to a Spark DataFrame\n\n\ncols\nColumns to compute statistics for, given as a character vector"
  },
  {
    "href": "reference/sdf_last_index.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Returns the last index of a Spark DataFrame. The Spark mapPartitionsWithIndex function is used to iterate through the last nonempty partition of the RDD to find the last record."
  },
  {
    "href": "reference/sdf_last_index.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_last_index(x, id = \"id\")"
  },
  {
    "href": "reference/sdf_last_index.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nid\nThe name of the index column."
  },
  {
    "href": "reference/jfloat.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Instantiate a java.lang.Float object with the value specified. NOTE: this method is useful when one has to invoke a Java/Scala method requiring a float (instead of double) type for at least one of its parameters."
  },
  {
    "href": "reference/jfloat.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "jfloat(sc, x)"
  },
  {
    "href": "reference/jfloat.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nx\nA numeric value in R."
  },
  {
    "href": "reference/jfloat.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"spark://HOST:PORT\")\n\njflt <- jfloat(sc, 1.23e-8)\n# jflt is now a reference to a java.lang.Float object"
  },
  {
    "href": "reference/sdf_from_avro.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Convert column(s) from avro format"
  },
  {
    "href": "reference/sdf_from_avro.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_from_avro(x, cols)"
  },
  {
    "href": "reference/sdf_from_avro.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn object coercible to a Spark DataFrame\n\n\ncols\nNamed list of columns to transform from Avro format plus a valid Avro schema string for each column, where column names are keys and column schema strings are values (e.g., c(example_primitive_col = \"string\","
  },
  {
    "href": "reference/spark_version_from_home.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Retrieve the version of Spark associated with a Spark installation."
  },
  {
    "href": "reference/spark_version_from_home.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_version_from_home(spark_home, default = NULL)"
  },
  {
    "href": "reference/spark_version_from_home.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nspark_home\nThe path to a Spark installation.\n\n\ndefault\nThe default version to be inferred, in case version lookup failed, e.g. no Spark installation was found at spark_home ."
  },
  {
    "href": "reference/stream_write_kafka.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Writes a Spark dataframe stream into an kafka stream."
  },
  {
    "href": "reference/stream_write_kafka.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_write_kafka(\n  x,\n  mode = c(\"append\", \"complete\", \"update\"),\n  trigger = stream_trigger_interval(),\n  checkpoint = file.path(\"checkpoints\", random_string(\"\")),\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/stream_write_kafka.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\" , \"complete\" or \"update\" .\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous .\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/stream_write_kafka.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Please note that Kafka requires installing the appropriate package by setting the packages parameter to \"kafka\" in spark_connect()"
  },
  {
    "href": "reference/stream_write_kafka.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark stream serialization: stream_read_csv , stream_read_delta , stream_read_json , stream_read_kafka , stream_read_orc , stream_read_parquet , stream_read_socket , stream_read_text , stream_write_console , stream_write_csv , stream_write_delta , stream_write_json , stream_write_memory , stream_write_orc , stream_write_parquet , stream_write_text"
  },
  {
    "href": "reference/stream_write_kafka.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.3\", packages = \"kafka\")\n\nread_options <- list(kafka.bootstrap.servers = \"localhost:9092\", subscribe = \"topic1\")\nwrite_options <- list(kafka.bootstrap.servers = \"localhost:9092\", topic = \"topic2\")\n\nstream <- stream_read_kafka(sc, options = read_options) %>%\nstream_write_kafka(options = write_options)\n\nstream_stop(stream)"
  },
  {
    "href": "reference/spark_coalesce_min_num_partitions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Retrieves or sets the minimum number of shuffle partitions after coalescing"
  },
  {
    "href": "reference/spark_coalesce_min_num_partitions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_coalesce_min_num_partitions(sc, num_partitions = NULL)"
  },
  {
    "href": "reference/spark_coalesce_min_num_partitions.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nnum_partitions\nMinimum number of shuffle partitions after coalescing. Defaults to NULL to retrieve configuration entries."
  },
  {
    "href": "reference/spark_coalesce_min_num_partitions.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark runtime configuration: spark_adaptive_query_execution , spark_advisory_shuffle_partition_size , spark_auto_broadcast_join_threshold , spark_coalesce_initial_num_partitions , spark_coalesce_shuffle_partitions , spark_session_config"
  },
  {
    "href": "reference/ml_kmeans_cluster_eval.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Evaluate a K-mean clustering"
  },
  {
    "href": "reference/ml_kmeans_cluster_eval.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nmodel\nA fitted K-means model returned by ml_kmeans()\n\n\ndataset\nDataset on which to calculate K-means cost"
  },
  {
    "href": "reference/spark_config_exists.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "A helper function to check value exist under spark_config()"
  },
  {
    "href": "reference/spark_config_exists.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_config_exists(config, name, default = NULL)"
  },
  {
    "href": "reference/spark_config_exists.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nconfig\nThe configuration list from spark_config()\n\n\nname\nThe name of the configuration entry\n\n\ndefault\nThe default value to use when entry is not present"
  },
  {
    "href": "reference/ml_lda_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "These methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "href": "reference/ml_lda_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "list(list(\"tidy\"), list(\"ml_model_lda\"))(x, ...)\nlist(list(\"augment\"), list(\"ml_model_lda\"))(x, newdata = NULL, ...)\nlist(list(\"glance\"), list(\"ml_model_lda\"))(x, ...)"
  },
  {
    "href": "reference/ml_lda_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n...\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "href": "reference/spark_jobj-class.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "spark_jobj class"
  },
  {
    "href": "reference/ft_string_indexer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "A label indexer that maps a string column of labels to an ML column of label indices. If the input column is numeric, we cast it to string and index the string values. The indices are in [0, numLabels) , ordered by label frequencies. So the most frequent label gets index 0. This function is the inverse of ft_index_to_string ."
  },
  {
    "href": "reference/ft_string_indexer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_string_indexer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  handle_invalid = \"error\",\n  string_order_type = \"frequencyDesc\",\n  uid = random_string(\"string_indexer_\"),\n  ...\n)\nml_labels(model)\nft_string_indexer_model(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  labels,\n  handle_invalid = \"error\",\n  uid = random_string(\"string_indexer_model_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_string_indexer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error”\n\n\nstring_order_type\n(Spark 2.3+)How to order labels of string column. The first label after ordering is assigned an index of 0. Options are \"frequencyDesc\" , \"frequencyAsc\" , \"alphabetDesc\" , and \"alphabetAsc\" . Defaults to \"frequencyDesc\" .\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused.\n\n\nmodel\nA fitted StringIndexer model returned by ft_string_indexer()\n\n\nlabels\nVector of labels, corresponding to indices to be assigned."
  },
  {
    "href": "reference/ft_string_indexer.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "In the case where x is a tbl_spark , the estimator fits against x to obtain a transformer, which is then immediately used to transform x , returning a tbl_spark ."
  },
  {
    "href": "reference/ft_string_indexer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark\n\nml_labels() returns a vector of labels, corresponding to indices to be assigned."
  },
  {
    "href": "reference/ft_string_indexer.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nft_index_to_string\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/ft_dct.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "A feature transformer that takes the 1D discrete cosine transform of a real vector. No zero padding is performed on the input vector. It returns a real vector of the same length representing the DCT. The return vector is scaled such that the transform matrix is unitary (aka scaled DCT-II)."
  },
  {
    "href": "reference/ft_dct.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_dct(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  inverse = FALSE,\n  uid = random_string(\"dct_\"),\n  ...\n)\nft_discrete_cosine_transform(\n  x,\n  input_col,\n  output_col,\n  inverse = FALSE,\n  uid = random_string(\"dct_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_dct.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\ninverse\nIndicates whether to perform the inverse DCT (TRUE) or forward DCT (FALSE).\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_dct.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "ft_discrete_cosine_transform() is an alias for ft_dct for backwards compatibility."
  },
  {
    "href": "reference/ft_dct.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_dct.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/print_jobj.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generic method for print jobj for a connection type"
  },
  {
    "href": "reference/print_jobj.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "print_jobj(sc, jobj, ...)"
  },
  {
    "href": "reference/print_jobj.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nspark_connection (used for type dispatch)\n\n\njobj\nObject to print"
  },
  {
    "href": "reference/ft_chisq_selector.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Chi-Squared feature selection, which selects categorical features to use for predicting a categorical label"
  },
  {
    "href": "reference/ft_chisq_selector.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_chisq_selector(\n  x,\n  features_col = \"features\",\n  output_col = NULL,\n  label_col = \"label\",\n  selector_type = \"numTopFeatures\",\n  fdr = 0.05,\n  fpr = 0.05,\n  fwe = 0.05,\n  num_top_features = 50,\n  percentile = 0.1,\n  uid = random_string(\"chisq_selector_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_chisq_selector.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\noutput_col\nThe name of the output column.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula .\n\n\nselector_type\n(Spark 2.1.0+) The selector type of the ChisqSelector. Supported options: “numTopFeatures” (default), “percentile”, “fpr”, “fdr”, “fwe”.\n\n\nfdr\n(Spark 2.2.0+) The upper bound of the expected false discovery rate. Only applicable when selector_type = “fdr”. Default value is 0.05.\n\n\nfpr\n(Spark 2.1.0+) The highest p-value for features to be kept. Only applicable when selector_type= “fpr”. Default value is 0.05.\n\n\nfwe\n(Spark 2.2.0+) The upper bound of the expected family-wise error rate. Only applicable when selector_type = “fwe”. Default value is 0.05.\n\n\nnum_top_features\nNumber of features that selector will select, ordered by ascending p-value. If the number of features is less than num_top_features , then this will select all features. Only applicable when selector_type = “numTopFeatures”. The default value of num_top_features is 50.\n\n\npercentile\n(Spark 2.1.0+) Percentile of features that selector will select, ordered by statistics value descending. Only applicable when selector_type = “percentile”. Default value is 0.1.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_chisq_selector.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "In the case where x is a tbl_spark , the estimator fits against x to obtain a transformer, which is then immediately used to transform x , returning a tbl_spark ."
  },
  {
    "href": "reference/ft_chisq_selector.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_chisq_selector.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/stream_watermark.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Ensures a stream has a watermark defined, which is required for some operations over streams."
  },
  {
    "href": "reference/stream_watermark.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_watermark(x, column = \"timestamp\", threshold = \"10 minutes\")"
  },
  {
    "href": "reference/stream_watermark.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn object coercable to a Spark Streaming DataFrame.\n\n\ncolumn\nThe name of the column that contains the event time of the row, if the column is missing, a column with the current time will be added.\n\n\nthreshold\nThe minimum delay to wait to data to arrive late, defaults to ten minutes."
  },
  {
    "href": "reference/stream_name.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Retrieves the name of the Spark stream if available."
  },
  {
    "href": "reference/stream_name.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_name(stream)"
  },
  {
    "href": "reference/stream_name.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nstream\nThe spark stream object."
  },
  {
    "href": "reference/ml_tree_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "These methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "href": "reference/ml_tree_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "list(list(\"tidy\"), list(\"ml_model_decision_tree_classification\"))(x, ...)\nlist(list(\"tidy\"), list(\"ml_model_decision_tree_regression\"))(x, ...)\nlist(list(\"augment\"), list(\"ml_model_decision_tree_classification\"))(x, newdata = NULL, ...)\nlist(list(\"augment\"), list(\"ml_model_decision_tree_regression\"))(x, newdata = NULL, ...)\nlist(list(\"glance\"), list(\"ml_model_decision_tree_classification\"))(x, ...)\nlist(list(\"glance\"), list(\"ml_model_decision_tree_regression\"))(x, ...)\nlist(list(\"tidy\"), list(\"ml_model_random_forest_classification\"))(x, ...)\nlist(list(\"tidy\"), list(\"ml_model_random_forest_regression\"))(x, ...)\nlist(list(\"augment\"), list(\"ml_model_random_forest_classification\"))(x, newdata = NULL, ...)\nlist(list(\"augment\"), list(\"ml_model_random_forest_regression\"))(x, newdata = NULL, ...)\nlist(list(\"glance\"), list(\"ml_model_random_forest_classification\"))(x, ...)\nlist(list(\"glance\"), list(\"ml_model_random_forest_regression\"))(x, ...)\nlist(list(\"tidy\"), list(\"ml_model_gbt_classification\"))(x, ...)\nlist(list(\"tidy\"), list(\"ml_model_gbt_regression\"))(x, ...)\nlist(list(\"augment\"), list(\"ml_model_gbt_classification\"))(x, newdata = NULL, ...)\nlist(list(\"augment\"), list(\"ml_model_gbt_regression\"))(x, newdata = NULL, ...)\nlist(list(\"glance\"), list(\"ml_model_gbt_classification\"))(x, ...)\nlist(list(\"glance\"), list(\"ml_model_gbt_regression\"))(x, ...)"
  },
  {
    "href": "reference/ml_tree_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n...\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "href": "reference/stream_write_orc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Writes a Spark dataframe stream into an ORC stream."
  },
  {
    "href": "reference/stream_write_orc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_write_orc(\n  x,\n  path,\n  mode = c(\"append\", \"complete\", \"update\"),\n  trigger = stream_trigger_interval(),\n  checkpoint = file.path(path, \"checkpoints\", random_string(\"\")),\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/stream_write_orc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe destination path. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nmode\nSpecifies how data is written to a streaming sink. Valid values are \"append\" , \"complete\" or \"update\" .\n\n\ntrigger\nThe trigger for the stream query, defaults to micro-batches runnnig every 5 seconds. See stream_trigger_interval and stream_trigger_continuous .\n\n\ncheckpoint\nThe location where the system will write all the checkpoint information to guarantee end-to-end fault-tolerance.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nPartitions the output by the given list of columns.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/stream_write_orc.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark stream serialization: stream_read_csv , stream_read_delta , stream_read_json , stream_read_kafka , stream_read_orc , stream_read_parquet , stream_read_socket , stream_read_text , stream_write_console , stream_write_csv , stream_write_delta , stream_write_json , stream_write_kafka , stream_write_memory , stream_write_parquet , stream_write_text"
  },
  {
    "href": "reference/stream_write_orc.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\n\nsdf_len(sc, 10) %>% spark_write_orc(\"orc-in\")\n\nstream <- stream_read_orc(sc, \"orc-in\") %>% stream_write_orc(\"orc-out\")\n\nstream_stop(stream)"
  },
  {
    "href": "reference/grapes-greater-than-grapes.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Infix operator that allows a lambda expression to be composed in R and be translated to Spark SQL equivalent using ’ dbplyr::translate_sql functionalities"
  },
  {
    "href": "reference/grapes-greater-than-grapes.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "params %->% ..."
  },
  {
    "href": "reference/grapes-greater-than-grapes.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nparams\nParameter(s) of the lambda expression, can be either a single parameter or a comma separated listed of parameters in the form of .(param1, param2, ... ) (see examples)\n\n\n...\nBody of the lambda expression, must be within parentheses"
  },
  {
    "href": "reference/grapes-greater-than-grapes.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Notice when composing a lambda expression in R, the body of the lambda expression must always be surrounded with parentheses, otherwise a parsing error will occur."
  },
  {
    "href": "reference/grapes-greater-than-grapes.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "a %->% (mean(a) + 1) # translates to <SQL> `a` -> (AVG(`a`) OVER () + 1.0)\n\n.(a, b) %->% (a < 1 && b > 1) # translates to <SQL> `a`,`b` -> (`a` < 1.0 AND `b` > 1.0)"
  },
  {
    "href": "reference/ft_vector_indexer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Indexing categorical feature columns in a dataset of Vector."
  },
  {
    "href": "reference/ft_vector_indexer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_vector_indexer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  handle_invalid = \"error\",\n  max_categories = 20,\n  uid = random_string(\"vector_indexer_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_vector_indexer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error”\n\n\nmax_categories\nThreshold for the number of values a categorical feature can take. If a feature is found to have > max_categories values, then it is declared continuous. Must be greater than or equal to 2. Defaults to 20.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_vector_indexer.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "In the case where x is a tbl_spark , the estimator fits against x to obtain a transformer, which is then immediately used to transform x , returning a tbl_spark ."
  },
  {
    "href": "reference/ft_vector_indexer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_vector_indexer.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/spark_extension.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Creates an R package ready to be used as an Spark extension."
  },
  {
    "href": "reference/spark_extension.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_extension(path)"
  },
  {
    "href": "reference/spark_extension.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\npath\nLocation where the extension will be created."
  },
  {
    "href": "reference/ml_corr.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Compute correlation matrix"
  },
  {
    "href": "reference/ml_corr.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_corr(x, columns = NULL, method = c(\"pearson\", \"spearman\"))"
  },
  {
    "href": "reference/ml_corr.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA tbl_spark .\n\n\ncolumns\nThe names of the columns to calculate correlations of. If only one column is specified, it must be a vector column (for example, assembled using ft_vector_assember() ).\n\n\nmethod\nThe method to use, either \"pearson\" or \"spearman\" ."
  },
  {
    "href": "reference/ml_corr.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "A correlation matrix organized as a data frame."
  },
  {
    "href": "reference/ml_corr.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\nfeatures <- c(\"Petal_Width\", \"Petal_Length\", \"Sepal_Length\", \"Sepal_Width\")\n\nml_corr(iris_tbl, columns = features, method = \"pearson\")"
  },
  {
    "href": "reference/hof_map_filter.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Filters entries in a map using the function specified (this is essentially a dplyr wrapper to the map_filter(expr, func) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "href": "reference/hof_map_filter.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "hof_map_filter(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "href": "reference/hof_map_filter.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\nfunc\nThe filter function to apply (it should take (key, value) as arguments and return a boolean value, with FALSE indicating the key-value pair should be discarded and TRUE otherwise)\n\n\nexpr\nThe map being filtered, could be any SQL expression evaluating to a map (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the filtered result (default: expr)\n\n\n...\nAdditional params to dplyr::mutate"
  },
  {
    "href": "reference/hof_map_filter.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"3.0.0\")\nsdf <- sdf_len(sc, 1) %>% dplyr::mutate(m = map(1, 0, 2, 2, 3, -1))\nfiltered_sdf <- sdf %>% hof_map_filter(~ .x > .y)"
  },
  {
    "href": "reference/spark_log.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "View the most recent entries in the Spark log. This can be useful when inspecting output / errors produced by Spark during the invocation of various commands."
  },
  {
    "href": "reference/spark_log.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_log(sc, n = 100, filter = NULL, ...)"
  },
  {
    "href": "reference/spark_log.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nn\nThe max number of log entries to retrieve. Use NULL to retrieve all entries within the log.\n\n\nfilter\nCharacter string to filter log entries.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/sdf_rweibull.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Weibull distribution."
  },
  {
    "href": "reference/sdf_rweibull.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_rweibull(\n  sc,\n  n,\n  shape,\n  scale = 1,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "href": "reference/sdf_rweibull.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nshape\nThe shape of the Weibull distribution.\n\n\nscale\nThe scale of the Weibull distribution (default: 1).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "href": "reference/sdf_rweibull.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark statistical routines: sdf_rbeta , sdf_rbinom , sdf_rcauchy , sdf_rchisq , sdf_rexp , sdf_rgamma , sdf_rgeom , sdf_rhyper , sdf_rlnorm , sdf_rnorm , sdf_rpois , sdf_rt , sdf_runif"
  },
  {
    "href": "reference/spark_coalesce_shuffle_partitions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Retrieves or sets whether coalescing contiguous shuffle partitions is enabled"
  },
  {
    "href": "reference/spark_coalesce_shuffle_partitions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_coalesce_shuffle_partitions(sc, enable = NULL)"
  },
  {
    "href": "reference/spark_coalesce_shuffle_partitions.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nenable\nWhether to enable coalescing of contiguous shuffle partitions. Defaults to NULL to retrieve configuration entries."
  },
  {
    "href": "reference/spark_coalesce_shuffle_partitions.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark runtime configuration: spark_adaptive_query_execution , spark_advisory_shuffle_partition_size , spark_auto_broadcast_join_threshold , spark_coalesce_initial_num_partitions , spark_coalesce_min_num_partitions , spark_session_config"
  },
  {
    "href": "reference/sdf_checkpoint.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Checkpoint a Spark DataFrame"
  },
  {
    "href": "reference/sdf_checkpoint.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_checkpoint(x, eager = TRUE)"
  },
  {
    "href": "reference/sdf_checkpoint.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nan object coercible to a Spark DataFrame\n\n\neager\nwhether to truncate the lineage of the DataFrame"
  },
  {
    "href": "reference/ml_multilayer_perceptron_classifier.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Classification model based on the Multilayer Perceptron. Each layer has sigmoid activation function, output layer has softmax."
  },
  {
    "href": "reference/ml_multilayer_perceptron_classifier.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_multilayer_perceptron_classifier(\n  x,\n  formula = NULL,\n  layers = NULL,\n  max_iter = 100,\n  step_size = 0.03,\n  tol = 1e-06,\n  block_size = 128,\n  solver = \"l-bfgs\",\n  seed = NULL,\n  initial_weights = NULL,\n  thresholds = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"multilayer_perceptron_classifier_\"),\n  ...\n)\nml_multilayer_perceptron(\n  x,\n  formula = NULL,\n  layers,\n  max_iter = 100,\n  step_size = 0.03,\n  tol = 1e-06,\n  block_size = 128,\n  solver = \"l-bfgs\",\n  seed = NULL,\n  initial_weights = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  thresholds = NULL,\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"multilayer_perceptron_classifier_\"),\n  response = NULL,\n  features = NULL,\n  ...\n)"
  },
  {
    "href": "reference/ml_multilayer_perceptron_classifier.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nUsed when x is a tbl_spark . R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nlayers\nA numeric vector describing the layers – each element in the vector gives the size of a layer. For example, c(4, 5, 2) would imply three layers, with an input (feature) layer of size 4, an intermediate layer of size 5, and an output (class) layer of size 2.\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nstep_size\nStep size to be used for each iteration of optimization (> 0).\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nblock_size\nBlock size for stacking input data in matrices to speed up the computation. Data is stacked within partitions. If block size is more than remaining data in a partition then it is adjusted to the size of this data. Recommended size is between 10 and 1000. Default: 128\n\n\nsolver\nThe solver algorithm for optimization. Supported options: “gd” (minibatch gradient descent) or “l-bfgs”. Default: “l-bfgs”\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\ninitial_weights\nThe initial weights of the model.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula .\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; see Details.\n\n\nresponse\n(Deprecated) The name of the response column (as a length-one character vector.)\n\n\nfeatures\n(Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "href": "reference/ml_multilayer_perceptron_classifier.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "When x is a tbl_spark and formula (alternatively, response and features ) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\" ) can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model , ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows.\nml_multilayer_perceptron() is an alias for ml_multilayer_perceptron_classifier() for backwards compatibility."
  },
  {
    "href": "reference/ml_multilayer_perceptron_classifier.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a predictor is constructed then immediately fit with the input tbl_spark , returning a prediction model.\ntbl_spark , with formula : specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model ."
  },
  {
    "href": "reference/ml_multilayer_perceptron_classifier.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression , ml_decision_tree_classifier , ml_gbt_classifier , ml_generalized_linear_regression , ml_isotonic_regression , ml_linear_regression , ml_linear_svc , ml_logistic_regression , ml_naive_bayes , ml_one_vs_rest , ml_random_forest_classifier"
  },
  {
    "href": "reference/ml_multilayer_perceptron_classifier.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\n\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\npartitions <- iris_tbl %>%\nsdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\niris_training <- partitions$training\niris_test <- partitions$test\n\nmlp_model <- iris_training %>%\nml_multilayer_perceptron_classifier(Species ~ ., layers = c(4, 3, 3))\n\npred <- ml_predict(mlp_model, iris_test)\n\nml_multiclass_classification_evaluator(pred)"
  },
  {
    "href": "reference/j_invoke_method.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Call a Java method and retrieve the return value through a JVM object reference."
  },
  {
    "href": "reference/j_invoke_method.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "j_invoke_method(sc, static, object, method, ...)"
  },
  {
    "href": "reference/j_invoke_method.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nspark_connection\n\n\nstatic\nIs this a static method call (including a constructor). If so then the object parameter should be the name of a class (otherwise it should be a spark_jobj instance).\n\n\nobject\nObject instance or name of class (for static )\n\n\nmethod\nName of method\n\n\n...\nCall parameters"
  },
  {
    "href": "reference/ml_power_iteration.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Power iteration clustering (PIC) is a scalable and efficient algorithm for clustering vertices of a graph given pairwise similarities as edge properties, described in the paper “Power Iteration Clustering” by Frank Lin and William W. Cohen. It computes a pseudo-eigenvector of the normalized affinity matrix of the graph via power iteration and uses it to cluster vertices. spark.mllib includes an implementation of PIC using GraphX as its backend. It takes an RDD of (srcId, dstId, similarity) tuples and outputs a model with the clustering assignments. The similarities must be nonnegative. PIC assumes that the similarity measure is symmetric. A pair (srcId, dstId) regardless of the ordering should appear at most once in the input data. If a pair is missing from input, their similarity is treated as zero."
  },
  {
    "href": "reference/ml_power_iteration.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_power_iteration(\n  x,\n  k = 4,\n  max_iter = 20,\n  init_mode = \"random\",\n  src_col = \"src\",\n  dst_col = \"dst\",\n  weight_col = \"weight\",\n  ...\n)"
  },
  {
    "href": "reference/ml_power_iteration.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA ‘spark_connection’ or a ‘tbl_spark’.\n\n\nk\nThe number of clusters to create.\n\n\nmax_iter\nThe maximum number of iterations to run.\n\n\ninit_mode\nThis can be either “random”, which is the default, to use a random vector as vertex properties, or “degree” to use normalized sum similarities.\n\n\nsrc_col\nColumn in the input Spark dataframe containing 0-based indexes of all source vertices in the affinity matrix described in the PIC paper.\n\n\ndst_col\nColumn in the input Spark dataframe containing 0-based indexes of all destination vertices in the affinity matrix described in the PIC paper.\n\n\nweight_col\nColumn in the input Spark dataframe containing non-negative edge weights in the affinity matrix described in the PIC paper.\n\n\n...\nOptional arguments. Currently unused."
  },
  {
    "href": "reference/ml_power_iteration.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "A 2-column R dataframe with columns named “id” and “cluster” describing the resulting cluster assignments"
  },
  {
    "href": "reference/ml_power_iteration.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nr1 <- 1\nn1 <- 80L\nr2 <- 4\nn2 <- 80L\n\ngen_circle <- function(radius, num_pts) {\n# generate evenly distributed points on a circle centered at the origin\nseq(0, num_pts - 1) %>%\nlapply(\nfunction(pt) {\ntheta <- 2 * pi * pt / num_pts\n\nradius * c(cos(theta), sin(theta))\n}\n)\n}\n\nguassian_similarity <- function(pt1, pt2) {\ndist2 <- sum((pt2 - pt1)^2)\n\nexp(-dist2 / 2)\n}\n\ngen_pic_data <- function() {\n# generate points on 2 concentric circle centered at the origin and then\n# compute pairwise Gaussian similarity values of all unordered pair of\n# points\nn <- n1 + n2\npts <- append(gen_circle(r1, n1), gen_circle(r2, n2))\nnum_unordered_pairs <- n * (n - 1) / 2\n\nsrc <- rep(0L, num_unordered_pairs)\ndst <- rep(0L, num_unordered_pairs)\nsim <- rep(0, num_unordered_pairs)\n\nidx <- 1\nfor (i in seq(2, n)) {\nfor (j in seq(i - 1)) {\nsrc[[idx]] <- i - 1L\ndst[[idx]] <- j - 1L\nsim[[idx]] <- guassian_similarity(pts[[i]], pts[[j]])\nidx <- idx + 1\n}\n}\n\ntibble::tibble(src = src, dst = dst, sim = sim)\n}\n\npic_data <- copy_to(sc, gen_pic_data())\n\nclusters <- ml_power_iteration(\npic_data,\nsrc_col = \"src\", dst_col = \"dst\", weight_col = \"sim\", k = 2, max_iter = 40\n)\nprint(clusters)"
  },
  {
    "href": "reference/spark_jobj.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "This S3 generic is used for accessing the underlying Java Virtual Machine (JVM) Spark objects associated with list() objects. These objects act as references to Spark objects living in the JVM. Methods on these objects can be called with the invoke family of functions."
  },
  {
    "href": "reference/spark_jobj.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_jobj(x, ...)"
  },
  {
    "href": "reference/spark_jobj.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn list() object containing, or wrapping, a spark_jobj .\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_jobj.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "invoke , for calling methods on Java object references."
  },
  {
    "href": "reference/spark-api.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Access the commonly-used Spark objects associated with a Spark instance. These objects provide access to different facets of the Spark API."
  },
  {
    "href": "reference/spark-api.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_context(sc)\njava_context(sc)\nhive_context(sc)\nspark_session(sc)"
  },
  {
    "href": "reference/spark-api.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection ."
  },
  {
    "href": "reference/spark-api.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "The Scala API documentation is useful for discovering what methods are available for each of these objects. Use invoke to call methods on these objects."
  },
  {
    "href": "reference/reexports.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "These objects are imported from other packages. Follow the links below to see their documentation.\nlist(“”, ” “, list(list(”generics”), list(list(list(“augment”)), “,”, list(list(“glance”)), “,”, list(list(“tidy”)))), “”)"
  },
  {
    "href": "reference/hof_map_zip_with.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Merges two maps into a single map by applying the function specified to pairs of values with the same key (this is essentially a dplyr wrapper to the map_zip_with(map1, map2, func) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "href": "reference/hof_map_zip_with.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "hof_map_zip_with(x, func, dest_col = NULL, map1 = NULL, map2 = NULL, ...)"
  },
  {
    "href": "reference/hof_map_zip_with.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\nfunc\nThe function to apply (it should take (key, value1, value2) as arguments, where (key, value1) is a key-value pair present in map1, (key, value2) is a key-value pair present in map2, and return a transformed value associated with key in the resulting map\n\n\ndest_col\nColumn to store the query result (default: the last column of the Spark data frame)\n\n\nmap1\nThe first map being merged, could be any SQL expression evaluating to a map (default: the first column of the Spark data frame)\n\n\nmap2\nThe second map being merged, could be any SQL expression evaluating to a map (default: the second column of the Spark data frame)\n\n\n...\nAdditional params to dplyr::mutate"
  },
  {
    "href": "reference/hof_map_zip_with.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"3.0.0\")\n\n# create a Spark dataframe with 2 columns of type MAP<STRING, INT>\ntwo_maps_tbl <- sdf_copy_to(\nsc,\ntibble::tibble(\nm1 = c(\"{\\\"1\\\":2,\\\"3\\\":4,\\\"5\\\":6}\", \"{\\\"2\\\":1,\\\"4\\\":3,\\\"6\\\":5}\"),\nm2 = c(\"{\\\"1\\\":1,\\\"3\\\":3,\\\"5\\\":5}\", \"{\\\"2\\\":2,\\\"4\\\":4,\\\"6\\\":6}\")\n),\noverwrite = TRUE\n) %>%\ndplyr::mutate(m1 = from_json(m1, \"MAP<STRING, INT>\"),\nm2 = from_json(m2, \"MAP<STRING, INT>\"))\n\n# create a 3rd column containing MAP<STRING, INT> values derived from the\n# first 2 columns\n\ntransformed_two_maps_tbl <- two_maps_tbl %>%\nhof_map_zip_with(\nfunc = .(k, v1, v2) %->% (CONCAT(k, \"_\", v1, \"_\", v2)),\ndest_col = m3\n)"
  },
  {
    "href": "reference/nest.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See nest for more details."
  },
  {
    "href": "reference/hof_forall.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Checks whether the predicate specified holds for all elements in an array (this is essentially a dplyr wrapper to the forall(expr, pred) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "href": "reference/hof_forall.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "hof_forall(x, pred, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "href": "reference/hof_forall.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\npred\nThe predicate to test (it should take an array element as argument and return a boolean value)\n\n\nexpr\nThe array being tested, could be any SQL expression evaluating to an array (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the boolean result (default: expr)\n\n\n...\nAdditional params to dplyr::mutate"
  },
  {
    "href": "reference/hof_forall.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\", version = \"3.0.0\")\ndf <- tibble::tibble(\nx = list(c(1, 2, 3, 4, 5), c(6, 7, 8, 9, 10)),\ny = list(c(1, 4, 2, 8, 5), c(7, 1, 4, 2, 8)),\n)\nsdf <- sdf_copy_to(sc, df, overwrite = TRUE)\n\nall_positive_tbl <- sdf %>%\nhof_forall(pred = ~ .x > 0, expr = y, dest_col = all_positive) %>%\ndplyr::select(all_positive)"
  },
  {
    "href": "reference/spark_read_json.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Read a table serialized in the c(“JavaScript”, “Object Notation”) format into a Spark DataFrame."
  },
  {
    "href": "reference/spark_read_json.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_read_json(\n  sc,\n  name = NULL,\n  path = name,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  columns = NULL,\n  ...\n)"
  },
  {
    "href": "reference/spark_read_json.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\noptions\nA list of strings with additional options.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType , \"boolean\" for BooleanType , \"byte\" for ByteType , \"integer\" for IntegerType , \"integer64\" for LongType , \"double\" for DoubleType , \"character\" for StringType , \"timestamp\" for TimestampType and \"date\" for DateType .\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_read_json.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "You can read data from HDFS ( hdfs:// ), S3 ( s3a:// ), as well as the local file system ( file:// ).\nIf you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf spark.hadoop.fs.s3a.access.key , spark.hadoop.fs.s3a.secret.key or any of the methods outlined in the aws-sdk documentation Working with AWS credentials In order to work with the newer s3a:// protocol also set the values for spark.hadoop.fs.s3a.impl and spark.hadoop.fs.s3a.endpoint . In addition, to support v4 of the S3 api be sure to pass the -Dcom.amazonaws.services.s3.enableV4 driver options for the config key spark.driver.extraJavaOptions For instructions on how to configure s3n:// check the hadoop documentation: s3n authentication properties"
  },
  {
    "href": "reference/spark_read_json.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/ml_naive_bayes.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Naive Bayes Classifiers. It supports Multinomial NB (see here ) which can handle finitely supported discrete data. For example, by converting documents into TF-IDF vectors, it can be used for document classification. By making every vector a binary (0/1) data, it can also be used as Bernoulli NB (see here ). The input feature values must be nonnegative."
  },
  {
    "href": "reference/ml_naive_bayes.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_naive_bayes(\n  x,\n  formula = NULL,\n  model_type = \"multinomial\",\n  smoothing = 1,\n  thresholds = NULL,\n  weight_col = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"naive_bayes_\"),\n  ...\n)"
  },
  {
    "href": "reference/ml_naive_bayes.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nUsed when x is a tbl_spark . R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nmodel_type\nThe model type. Supported options: \"multinomial\" and \"bernoulli\" . (default = multinomial )\n\n\nsmoothing\nThe (Laplace) smoothing parameter. Defaults to 1.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\nweight_col\n(Spark 2.1.0+) Weight column name. If this is not set or empty, we treat all instance weights as 1.0.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula .\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; see Details."
  },
  {
    "href": "reference/ml_naive_bayes.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "When x is a tbl_spark and formula (alternatively, response and features ) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\" ) can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model , ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "href": "reference/ml_naive_bayes.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a predictor is constructed then immediately fit with the input tbl_spark , returning a prediction model.\ntbl_spark , with formula : specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model ."
  },
  {
    "href": "reference/ml_naive_bayes.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression , ml_decision_tree_classifier , ml_gbt_classifier , ml_generalized_linear_regression , ml_isotonic_regression , ml_linear_regression , ml_linear_svc , ml_logistic_regression , ml_multilayer_perceptron_classifier , ml_one_vs_rest , ml_random_forest_classifier"
  },
  {
    "href": "reference/ml_naive_bayes.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\npartitions <- iris_tbl %>%\nsdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\niris_training <- partitions$training\niris_test <- partitions$test\n\nnb_model <- iris_training %>%\nml_naive_bayes(Species ~ .)\n\npred <- ml_predict(nb_model, iris_test)\n\nml_multiclass_classification_evaluator(pred)"
  },
  {
    "href": "reference/sdf_debug_string.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Prints plan of execution to generate x . This plan will, among other things, show the number of partitions in parenthesis at the far left and indicate stages using indentation."
  },
  {
    "href": "reference/sdf_debug_string.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_debug_string(x, print = TRUE)"
  },
  {
    "href": "reference/sdf_debug_string.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn list() object wrapping, or containing, a Spark DataFrame.\n\n\nprint\nPrint debug information?"
  },
  {
    "href": "reference/ft_max_abs_scaler.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Rescale each feature individually to range [-1, 1] by dividing through the largest maximum absolute value in each feature. It does not shift/center the data, and thus does not destroy any sparsity."
  },
  {
    "href": "reference/ft_max_abs_scaler.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_max_abs_scaler(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  uid = random_string(\"max_abs_scaler_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_max_abs_scaler.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_max_abs_scaler.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "In the case where x is a tbl_spark , the estimator fits against x to obtain a transformer, which is then immediately used to transform x , returning a tbl_spark ."
  },
  {
    "href": "reference/ft_max_abs_scaler.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_max_abs_scaler.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/ft_max_abs_scaler.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\nfeatures <- c(\"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\")\n\niris_tbl %>%\nft_vector_assembler(\ninput_col = features,\noutput_col = \"features_temp\"\n) %>%\nft_max_abs_scaler(\ninput_col = \"features_temp\",\noutput_col = \"features\"\n)"
  },
  {
    "href": "reference/transform_sdf.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "transform a subset of column(s) in a Spark Dataframe"
  },
  {
    "href": "reference/transform_sdf.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "transform_sdf(x, cols, fn)"
  },
  {
    "href": "reference/transform_sdf.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn object coercible to a Spark DataFrame\n\n\ncols\nSubset of columns to apply transformation to\n\n\nfn\nTransformation function taking column name as the 1st parameter, the corresponding org.apache.spark.sql.Column object as the 2nd parameter, and returning a transformed org.apache.spark.sql.Column object"
  },
  {
    "href": "reference/find_scalac.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Find the scalac compiler for a particular version of scala , by scanning some common directories containing scala installations."
  },
  {
    "href": "reference/find_scalac.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "find_scalac(version, locations = NULL)"
  },
  {
    "href": "reference/find_scalac.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nversion\nThe scala version to search for. Versions of the form major.minor will be matched against the scalac installation with version major.minor.patch ; if multiple compilers are discovered the most recent one will be used.\n\n\nlocations\nAdditional locations to scan. By default, the directories /opt/scala and /usr/local/scala will be scanned."
  },
  {
    "href": "reference/hof_zip_with.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Applies an element-wise function to combine elements from 2 array columns (this is essentially a dplyr wrapper for the zip_with(array<T>, array<U>, function<T, U, R>): array<R> built-in function in Spark SQL)"
  },
  {
    "href": "reference/hof_zip_with.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "hof_zip_with(x, func, dest_col = NULL, left = NULL, right = NULL, ...)"
  },
  {
    "href": "reference/hof_zip_with.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nThe Spark data frame to process\n\n\nfunc\nElement-wise combining function to be applied\n\n\ndest_col\nColumn to store the query result (default: the last column of the Spark data frame)\n\n\nleft\nAny expression evaluating to an array (default: the first column of the Spark data frame)\n\n\nright\nAny expression evaluating to an array (default: the second column of the Spark data frame)\n\n\n...\nAdditional params to dplyr::mutate"
  },
  {
    "href": "reference/hof_zip_with.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\")\n# compute element-wise products of 2 arrays from each row of `left` and `right`\n# and store the resuling array in `res`\ncopy_to(\nsc,\ntibble::tibble(\nleft = list(1:5, 21:25),\nright = list(6:10, 16:20),\nres = c(0, 0)\n)\n) %>%\nhof_zip_with(~ .x * .y)"
  },
  {
    "href": "reference/ml_random_forest.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Perform classification and regression using random forests."
  },
  {
    "href": "reference/ml_random_forest.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_random_forest_classifier(\n  x,\n  formula = NULL,\n  num_trees = 20,\n  subsampling_rate = 1,\n  max_depth = 5,\n  min_instances_per_node = 1,\n  feature_subset_strategy = \"auto\",\n  impurity = \"gini\",\n  min_info_gain = 0,\n  max_bins = 32,\n  seed = NULL,\n  thresholds = NULL,\n  checkpoint_interval = 10,\n  cache_node_ids = FALSE,\n  max_memory_in_mb = 256,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"random_forest_classifier_\"),\n  ...\n)\nml_random_forest(\n  x,\n  formula = NULL,\n  type = c(\"auto\", \"regression\", \"classification\"),\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  feature_subset_strategy = \"auto\",\n  impurity = \"auto\",\n  checkpoint_interval = 10,\n  max_bins = 32,\n  max_depth = 5,\n  num_trees = 20,\n  min_info_gain = 0,\n  min_instances_per_node = 1,\n  subsampling_rate = 1,\n  seed = NULL,\n  thresholds = NULL,\n  cache_node_ids = FALSE,\n  max_memory_in_mb = 256,\n  uid = random_string(\"random_forest_\"),\n  response = NULL,\n  features = NULL,\n  ...\n)\nml_random_forest_regressor(\n  x,\n  formula = NULL,\n  num_trees = 20,\n  subsampling_rate = 1,\n  max_depth = 5,\n  min_instances_per_node = 1,\n  feature_subset_strategy = \"auto\",\n  impurity = \"variance\",\n  min_info_gain = 0,\n  max_bins = 32,\n  seed = NULL,\n  checkpoint_interval = 10,\n  cache_node_ids = FALSE,\n  max_memory_in_mb = 256,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"random_forest_regressor_\"),\n  ...\n)"
  },
  {
    "href": "reference/ml_random_forest.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nUsed when x is a tbl_spark . R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nnum_trees\nNumber of trees to train (>= 1). If 1, then no bootstrapping is used. If > 1, then bootstrapping is done.\n\n\nsubsampling_rate\nFraction of the training data used for learning each decision tree, in range (0, 1]. (default = 1.0)\n\n\nmax_depth\nMaximum depth of the tree (>= 0); that is, the maximum number of nodes separating any leaves from the root of the tree.\n\n\nmin_instances_per_node\nMinimum number of instances each child must have after split.\n\n\nfeature_subset_strategy\nThe number of features to consider for splits at each tree node. See details for options.\n\n\nimpurity\nCriterion used for information gain calculation. Supported: “entropy” and “gini” (default) for classification and “variance” (default) for regression. For ml_decision_tree , setting \"auto\" will default to the appropriate criterion based on model type.\n\n\nmin_info_gain\nMinimum information gain for a split to be considered at a tree node. Should be >= 0, defaults to 0.\n\n\nmax_bins\nThe maximum number of bins used for discretizing continuous features and for choosing how to split on features at each node. More bins give higher granularity.\n\n\nseed\nSeed for random numbers.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\ncheckpoint_interval\nSet checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.\n\n\ncache_node_ids\nIf FALSE , the algorithm will pass trees to executors to match instances with nodes. If TRUE , the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Defaults to FALSE .\n\n\nmax_memory_in_mb\nMaximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. Defaults to 256.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula .\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; see Details.\n\n\ntype\nThe type of model to fit. \"regression\" treats the response as a continuous variable, while \"classification\" treats the response as a categorical variable. When \"auto\" is used, the model type is inferred based on the response variable type – if it is a numeric type, then regression is used; classification otherwise.\n\n\nresponse\n(Deprecated) The name of the response column (as a length-one character vector.)\n\n\nfeatures\n(Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "href": "reference/ml_random_forest.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "When x is a tbl_spark and formula (alternatively, response and features ) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\" ) can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model , ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows.\nThe supported options for feature_subset_strategy are\n\n\"auto\" : Choose automatically for task: If num_trees == 1 , set to \"all\" . If num_trees > 1 (forest), set to \"sqrt\" for classification and to \"onethird\" for regression.\n\"all\" : use all features\n\"onethird\" : use 1/3 of the features\n\"sqrt\" : use use sqrt(number of features)\n\"log2\" : use log2(number of features)\n\"n\" : when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features. (default = \"auto\" )\n\nml_random_forest is a wrapper around ml_random_forest_regressor.tbl_spark and ml_random_forest_classifier.tbl_spark and calls the appropriate method based on model type."
  },
  {
    "href": "reference/ml_random_forest.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a predictor is constructed then immediately fit with the input tbl_spark , returning a prediction model.\ntbl_spark , with formula : specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model ."
  },
  {
    "href": "reference/ml_random_forest.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression , ml_decision_tree_classifier , ml_gbt_classifier , ml_generalized_linear_regression , ml_isotonic_regression , ml_linear_regression , ml_linear_svc , ml_logistic_regression , ml_multilayer_perceptron_classifier , ml_naive_bayes , ml_one_vs_rest"
  },
  {
    "href": "reference/ml_random_forest.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\npartitions <- iris_tbl %>%\nsdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\niris_training <- partitions$training\niris_test <- partitions$test\n\nrf_model <- iris_training %>%\nml_random_forest(Species ~ ., type = \"classification\")\n\npred <- ml_predict(rf_model, iris_test)\n\nml_multiclass_classification_evaluator(pred)"
  },
  {
    "href": "reference/list_sparklyr_jars.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "list all sparklyr-*.jar files that have been built"
  },
  {
    "href": "reference/list_sparklyr_jars.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "list_sparklyr_jars()"
  },
  {
    "href": "reference/join.tbl_spark.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "These functions are wrappers around their dplyr equivalents that set Spark SQL-compliant values for the suffix argument by replacing dots (.) with underscores (_). See [join] for a description of the general purpose of the functions."
  },
  {
    "href": "reference/join.tbl_spark.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "list(list(\"inner_join\"), list(\"tbl_spark\"))(\n  x,\n  y,\n  by = NULL,\n  copy = FALSE,\n  suffix = c(\"_x\", \"_y\"),\n  auto_index = FALSE,\n  ...,\n  sql_on = NULL\n)\nlist(list(\"left_join\"), list(\"tbl_spark\"))(\n  x,\n  y,\n  by = NULL,\n  copy = FALSE,\n  suffix = c(\"_x\", \"_y\"),\n  auto_index = FALSE,\n  ...,\n  sql_on = NULL\n)\nlist(list(\"right_join\"), list(\"tbl_spark\"))(\n  x,\n  y,\n  by = NULL,\n  copy = FALSE,\n  suffix = c(\"_x\", \"_y\"),\n  auto_index = FALSE,\n  ...,\n  sql_on = NULL\n)\nlist(list(\"full_join\"), list(\"tbl_spark\"))(\n  x,\n  y,\n  by = NULL,\n  copy = FALSE,\n  suffix = c(\"_x\", \"_y\"),\n  auto_index = FALSE,\n  ...,\n  sql_on = NULL\n)"
  },
  {
    "href": "reference/join.tbl_spark.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA pair of lazy data frames backed by database queries.\n\n\ny\nA pair of lazy data frames backed by database queries.\n\n\nby\nA character vector of variables to join by. If NULL , the default, *_join() will perform a natural join, using all variables in common across x and y . A message lists the variables so that you can check they’re correct; suppress the message by supplying by explicitly. To join by different variables on x and y , use a named vector. For example, by = c(\"a\" = \"b\") will match x$a to y$b . To join by multiple variables, use a vector with length > 1. For example, by = c(\"a\", \"b\") will match x$a to y$a and x$b to y$b . Use a named vector to match different variables in x and y . For example, by = c(\"a\" = \"b\", \"c\" = \"d\") will match x$a to y$b and x$c to y$d . To perform a cross-join, generating all combinations of x and y , use by = character() .\n\n\ncopy\nIf x and y are not from the same data source, and copy is TRUE , then y will be copied into a temporary table in same database as x . *_join() will automatically run ANALYZE on the created table in the hope that this will make you queries as efficient as possible by giving more data to the query planner. This allows you to join tables across srcs, but it’s potentially expensive operation so you must opt into it.\n\n\nsuffix\nIf there are non-joined duplicate variables in x and y , these suffixes will be added to the output to disambiguate them. Should be a character vector of length 2.\n\n\nauto_index\nif copy is TRUE , automatically create indices for the variables in by . This may speed up the join if there are matching indexes in x .\n\n\n...\nOther parameters passed onto methods.\n\n\nsql_on\nA custom join predicate as an SQL expression. Usually joins use column equality, but you can perform more complex queries by supply sql_on which should be a SQL expression that uses LHS and RHS aliases to refer to the left-hand side or right-hand side of the join respectively."
  },
  {
    "href": "reference/ml_logistic_regression_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "These methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "href": "reference/ml_logistic_regression_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "list(list(\"tidy\"), list(\"ml_model_logistic_regression\"))(x, ...)\nlist(list(\"augment\"), list(\"ml_model_logistic_regression\"))(x, newdata = NULL, ...)\nlist(list(\"glance\"), list(\"ml_model_logistic_regression\"))(x, ...)"
  },
  {
    "href": "reference/ml_logistic_regression_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n...\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "href": "reference/stream_read_parquet.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Reads a parquet stream as a Spark dataframe stream."
  },
  {
    "href": "reference/stream_read_parquet.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_read_parquet(\n  sc,\n  path,\n  name = NULL,\n  columns = NULL,\n  options = list(),\n  ...\n)"
  },
  {
    "href": "reference/stream_read_parquet.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType , \"boolean\" for BooleanType , \"byte\" for ByteType , \"integer\" for IntegerType , \"integer64\" for LongType , \"double\" for DoubleType , \"character\" for StringType , \"timestamp\" for TimestampType and \"date\" for DateType .\n\n\noptions\nA list of strings with additional options.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/stream_read_parquet.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark stream serialization: stream_read_csv , stream_read_delta , stream_read_json , stream_read_kafka , stream_read_orc , stream_read_socket , stream_read_text , stream_write_console , stream_write_csv , stream_write_delta , stream_write_json , stream_write_kafka , stream_write_memory , stream_write_orc , stream_write_parquet , stream_write_text"
  },
  {
    "href": "reference/stream_read_parquet.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\n\nsdf_len(sc, 10) %>% spark_write_parquet(\"parquet-in\")\n\nstream <- stream_read_parquet(sc, \"parquet-in\") %>% stream_write_parquet(\"parquet-out\")\n\nstream_stop(stream)"
  },
  {
    "href": "reference/ml_feature_importances.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Spark ML - Feature Importance for Tree Models"
  },
  {
    "href": "reference/ml_feature_importances.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_feature_importances(model, ...)\nml_tree_feature_importance(model, ...)"
  },
  {
    "href": "reference/ml_feature_importances.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nmodel\nA decision tree-based model.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ml_feature_importances.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "For ml_model , a sorted data frame with feature labels and their relative importance. For ml_prediction_model , a vector of relative importances."
  },
  {
    "href": "reference/fill.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See fill for more details."
  },
  {
    "href": "reference/inner_join.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See inner_join for more details."
  },
  {
    "href": "reference/spark_dependency_fallback.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Helper function to assist falling back to previous Spark versions."
  },
  {
    "href": "reference/spark_dependency_fallback.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_dependency_fallback(spark_version, supported_versions)"
  },
  {
    "href": "reference/spark_dependency_fallback.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nspark_version\nThe Spark version being requested in spark_dependencies .\n\n\nsupported_versions\nThe Spark versions that are supported by this extension."
  },
  {
    "href": "reference/spark_dependency_fallback.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "A Spark version to use."
  },
  {
    "href": "reference/spark_read_csv.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Read a tabular data file into a Spark DataFrame."
  },
  {
    "href": "reference/spark_read_csv.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_read_csv(\n  sc,\n  name = NULL,\n  path = name,\n  header = TRUE,\n  columns = NULL,\n  infer_schema = is.null(columns),\n  delimiter = \",\",\n  quote = \"\\\"\",\n  escape = \"\\\\\",\n  charset = \"UTF-8\",\n  null_value = NULL,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  ...\n)"
  },
  {
    "href": "reference/spark_read_csv.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nheader\nBoolean; should the first row of data be used as a header? Defaults to TRUE .\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType , \"boolean\" for BooleanType , \"byte\" for ByteType , \"integer\" for IntegerType , \"integer64\" for LongType , \"double\" for DoubleType , \"character\" for StringType , \"timestamp\" for TimestampType and \"date\" for DateType .\n\n\ninfer_schema\nBoolean; should column types be automatically inferred? Requires one extra pass over the data. Defaults to is.null(columns) .\n\n\ndelimiter\nThe character used to delimit each column. Defaults to ‘,’ .\n\n\nquote\nThe character used as a quote. Defaults to ‘“’ .\n\n\nescape\nThe character used to escape other characters. Defaults to ’' .\n\n\ncharset\nThe character set. Defaults to “UTF-8” .\n\n\nnull_value\nThe character to use for null, or missing, values. Defaults to NULL .\n\n\noptions\nA list of strings with additional options.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_read_csv.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "You can read data from HDFS ( hdfs:// ), S3 ( s3a:// ), as well as the local file system ( file:// ).\nIf you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf spark.hadoop.fs.s3a.access.key , spark.hadoop.fs.s3a.secret.key or any of the methods outlined in the aws-sdk documentation Working with AWS credentials In order to work with the newer s3a:// protocol also set the values for spark.hadoop.fs.s3a.impl and spark.hadoop.fs.s3a.endpoint . In addition, to support v4 of the S3 api be sure to pass the -Dcom.amazonaws.services.s3.enableV4 driver options for the config key spark.driver.extraJavaOptions For instructions on how to configure s3n:// check the hadoop documentation: s3n authentication properties\nWhen header is FALSE , the column names are generated with a V prefix; e.g. V1, V2, ... ."
  },
  {
    "href": "reference/spark_read_csv.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/stream_id.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Retrieves the identifier of the Spark stream."
  },
  {
    "href": "reference/stream_id.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_id(stream)"
  },
  {
    "href": "reference/stream_id.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nstream\nThe spark stream object."
  },
  {
    "href": "reference/sdf_drop_duplicates.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Remove duplicates from a Spark DataFrame"
  },
  {
    "href": "reference/sdf_drop_duplicates.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_drop_duplicates(x, cols = NULL)"
  },
  {
    "href": "reference/sdf_drop_duplicates.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn object coercible to a Spark DataFrame\n\n\ncols\nSubset of Columns to consider, given as a character vector"
  },
  {
    "href": "reference/spark_write_jdbc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Writes a Spark DataFrame into a JDBC table."
  },
  {
    "href": "reference/spark_write_jdbc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_write_jdbc(\n  x,\n  name,\n  mode = NULL,\n  options = list(),\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/spark_write_jdbc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\nname\nThe name to assign to the newly generated table.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure. For more details see also http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_write_jdbc.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/spark_write_jdbc.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(\nmaster = \"local\",\nconfig = list(\n`sparklyr.shell.driver-class-path` = \"/usr/share/java/mysql-connector-java-8.0.25.jar\"\n)\n)\nspark_write_jdbc(\nsdf_len(sc, 10),\nname = \"my_sql_table\",\noptions = list(\nurl = \"jdbc:mysql://localhost:3306/my_sql_schema\",\ndriver = \"com.mysql.jdbc.Driver\",\nuser = \"me\",\npassword = \"******\",\ndbtable = \"my_sql_table\"\n)\n)"
  },
  {
    "href": "reference/ml_naive_bayes_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "These methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "href": "reference/ml_naive_bayes_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "list(list(\"tidy\"), list(\"ml_model_naive_bayes\"))(x, ...)\nlist(list(\"augment\"), list(\"ml_model_naive_bayes\"))(x, newdata = NULL, ...)\nlist(list(\"glance\"), list(\"ml_model_naive_bayes\"))(x, ...)"
  },
  {
    "href": "reference/ml_naive_bayes_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n...\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "href": "reference/spark_context_config.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Retrieves the runtime configuration interface for the Spark Context."
  },
  {
    "href": "reference/spark_context_config.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_context_config(sc)"
  },
  {
    "href": "reference/spark_context_config.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection ."
  },
  {
    "href": "reference/spark_web.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Open the Spark web interface"
  },
  {
    "href": "reference/spark_web.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_web(sc, ...)"
  },
  {
    "href": "reference/spark_web.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_install_sync.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See: https://github.com/rstudio/spark-install"
  },
  {
    "href": "reference/spark_install_sync.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_install_sync(project_path)"
  },
  {
    "href": "reference/spark_install_sync.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nproject_path\nThe path to the sparkinstall project"
  },
  {
    "href": "reference/sdf_residuals.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "This generic method returns a Spark DataFrame with model residuals added as a column to the model training data."
  },
  {
    "href": "reference/sdf_residuals.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "list(list(\"sdf_residuals\"), list(\"ml_model_generalized_linear_regression\"))(\n  object,\n  type = c(\"deviance\", \"pearson\", \"working\", \"response\"),\n  ...\n)\nlist(list(\"sdf_residuals\"), list(\"ml_model_linear_regression\"))(object, ...)\nsdf_residuals(object, ...)"
  },
  {
    "href": "reference/sdf_residuals.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nobject\nSpark ML model object.\n\n\ntype\ntype of residuals which should be returned.\n\n\n...\nadditional arguments"
  },
  {
    "href": "reference/sdf_rbeta.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Betal distribution."
  },
  {
    "href": "reference/sdf_rbeta.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_rbeta(\n  sc,\n  n,\n  shape1,\n  shape2,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "href": "reference/sdf_rbeta.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nshape1\nNon-negative parameter (alpha) of the Beta distribution.\n\n\nshape2\nNon-negative parameter (beta) of the Beta distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "href": "reference/sdf_rbeta.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark statistical routines: sdf_rbinom , sdf_rcauchy , sdf_rchisq , sdf_rexp , sdf_rgamma , sdf_rgeom , sdf_rhyper , sdf_rlnorm , sdf_rnorm , sdf_rpois , sdf_rt , sdf_runif , sdf_rweibull"
  },
  {
    "href": "reference/sdf_rbinom.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a binomial distribution."
  },
  {
    "href": "reference/sdf_rbinom.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_rbinom(\n  sc,\n  n,\n  size,\n  prob,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "href": "reference/sdf_rbinom.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nsize\nNumber of trials (zero or more).\n\n\nprob\nProbability of success on each trial.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "href": "reference/sdf_rbinom.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark statistical routines: sdf_rbeta , sdf_rcauchy , sdf_rchisq , sdf_rexp , sdf_rgamma , sdf_rgeom , sdf_rhyper , sdf_rlnorm , sdf_rnorm , sdf_rpois , sdf_rt , sdf_runif , sdf_rweibull"
  },
  {
    "href": "reference/ml_linear_svc_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "These methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "href": "reference/ml_linear_svc_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "list(list(\"tidy\"), list(\"ml_model_linear_svc\"))(x, ...)\nlist(list(\"augment\"), list(\"ml_model_linear_svc\"))(x, newdata = NULL, ...)\nlist(list(\"glance\"), list(\"ml_model_linear_svc\"))(x, ...)"
  },
  {
    "href": "reference/ml_linear_svc_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n...\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "href": "reference/ml_als.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Perform recommendation using Alternating Least Squares (ALS) matrix factorization."
  },
  {
    "href": "reference/ml_als.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_als(\n  x,\n  formula = NULL,\n  rating_col = \"rating\",\n  user_col = \"user\",\n  item_col = \"item\",\n  rank = 10,\n  reg_param = 0.1,\n  implicit_prefs = FALSE,\n  alpha = 1,\n  nonnegative = FALSE,\n  max_iter = 10,\n  num_user_blocks = 10,\n  num_item_blocks = 10,\n  checkpoint_interval = 10,\n  cold_start_strategy = \"nan\",\n  intermediate_storage_level = \"MEMORY_AND_DISK\",\n  final_storage_level = \"MEMORY_AND_DISK\",\n  uid = random_string(\"als_\"),\n  ...\n)\nml_recommend(model, type = c(\"items\", \"users\"), n = 1)"
  },
  {
    "href": "reference/ml_als.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nUsed when x is a tbl_spark . R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details. The ALS model requires a specific formula format, please use rating_col ~ user_col + item_col .\n\n\nrating_col\nColumn name for ratings. Default: “rating”\n\n\nuser_col\nColumn name for user ids. Ids must be integers. Other numeric types are supported for this column, but will be cast to integers as long as they fall within the integer value range. Default: “user”\n\n\nitem_col\nColumn name for item ids. Ids must be integers. Other numeric types are supported for this column, but will be cast to integers as long as they fall within the integer value range. Default: “item”\n\n\nrank\nRank of the matrix factorization (positive). Default: 10\n\n\nreg_param\nRegularization parameter.\n\n\nimplicit_prefs\nWhether to use implicit preference. Default: FALSE.\n\n\nalpha\nAlpha parameter in the implicit preference formulation (nonnegative).\n\n\nnonnegative\nWhether to apply nonnegativity constraints. Default: FALSE.\n\n\nmax_iter\nMaximum number of iterations.\n\n\nnum_user_blocks\nNumber of user blocks (positive). Default: 10\n\n\nnum_item_blocks\nNumber of item blocks (positive). Default: 10\n\n\ncheckpoint_interval\nSet checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.\n\n\ncold_start_strategy\n(Spark 2.2.0+) Strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: - “nan”: predicted value for unknown ids will be NaN. - “drop”: rows in the input DataFrame containing unknown ids will be dropped from the output DataFrame containing predictions. Default: “nan”.\n\n\nintermediate_storage_level\n(Spark 2.0.0+) StorageLevel for intermediate datasets. Pass in a string representation of StorageLevel . Cannot be “NONE”. Default: “MEMORY_AND_DISK”.\n\n\nfinal_storage_level\n(Spark 2.0.0+) StorageLevel for ALS model factors. Pass in a string representation of StorageLevel . Default: “MEMORY_AND_DISK”.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; currently unused.\n\n\nmodel\nAn ALS model object\n\n\ntype\nWhat to recommend, one of items or users\n\n\nn\nMaximum number of recommendations to return"
  },
  {
    "href": "reference/ml_als.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "ml_recommend() returns the top n users/items recommended for each item/user, for all items/users. The output has been transformed (exploded and separated) from the default Spark outputs to be more user friendly."
  },
  {
    "href": "reference/ml_als.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "ALS attempts to estimate the ratings matrix R as the product of two lower-rank matrices, X and Y, i.e. X * Yt = R. Typically these approximations are called ‘factor’ matrices. The general approach is iterative. During each iteration, one of the factor matrices is held constant, while the other is solved for using least squares. The newly-solved factor matrix is then held constant while solving for the other factor matrix.\nThis is a blocked implementation of the ALS factorization algorithm that groups the two sets of factors (referred to as “users” and “products”) into blocks and reduces communication by only sending one copy of each user vector to each product block on each iteration, and only for the product blocks that need that user’s feature vector. This is achieved by pre-computing some information about the ratings matrix to determine the “out-links” of each user (which blocks of products it will contribute to) and “in-link” information for each product (which of the feature vectors it receives from each user block it will depend on). This allows us to send only an array of feature vectors between each user block and product block, and have the product block find the users’ ratings and update the products based on these messages.\nFor implicit preference data, the algorithm used is based on “Collaborative Filtering for Implicit Feedback Datasets”, available at c(“\\Sexpr[results=rd,stage=build]{tools:::Rd_expr_doi(\"#1\")}”, “10.1109/ICDM.2008.22”) list(“tools:::Rd_expr_doi(\"10.1109/ICDM.2008.22\")”) , adapted for the blocked approach used here.\nEssentially instead of finding the low-rank approximations to the rating matrix R, this finds the approximations for a preference matrix P where the elements of P are 1 if r is greater than 0 and 0 if r is less than or equal to 0. The ratings then act as ‘confidence’ values related to strength of indicated user preferences rather than explicit ratings given to items.\nThe object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_als recommender object, which is an Estimator.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the recommender appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a recommender estimator is constructed then immediately fit with the input tbl_spark , returning a recommendation model, i.e. ml_als_model ."
  },
  {
    "href": "reference/ml_als.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\")\n\nmovies <- data.frame(\nuser   = c(1, 2, 0, 1, 2, 0),\nitem   = c(1, 1, 1, 2, 2, 0),\nrating = c(3, 1, 2, 4, 5, 4)\n)\nmovies_tbl <- sdf_copy_to(sc, movies)\n\nmodel <- ml_als(movies_tbl, rating ~ user + item)\n\nml_predict(model, movies_tbl)\n\nml_recommend(model, type = \"item\", 1)"
  },
  {
    "href": "reference/livy_service.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Starts the livy service.\nStops the running instances of the livy service."
  },
  {
    "href": "reference/livy_service.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "livy_service_start(\n  version = NULL,\n  spark_version = NULL,\n  stdout = \"\",\n  stderr = \"\",\n  ...\n)\nlivy_service_stop()"
  },
  {
    "href": "reference/livy_service.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nversion\nThe version of livy to use.\n\n\nspark_version\nThe version of spark to connect to.\n\n\nstdout, stderr\nwhere output to ‘stdout’ or ‘stderr’ should be sent. Same options as system2 .\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_robust_scaler.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "RobustScaler removes the median and scales the data according to the quantile range. The quantile range is by default IQR (Interquartile Range, quantile range between the 1st quartile = 25th quantile and the 3rd quartile = 75th quantile) but can be configured. Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and quantile range are then stored to be used on later data using the transform method. Note that missing values are ignored in the computation of medians and ranges."
  },
  {
    "href": "reference/ft_robust_scaler.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_robust_scaler(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  lower = 0.25,\n  upper = 0.75,\n  with_centering = TRUE,\n  with_scaling = TRUE,\n  relative_error = 0.001,\n  uid = random_string(\"ft_robust_scaler_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_robust_scaler.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nlower\nLower quantile to calculate quantile range.\n\n\nupper\nUpper quantile to calculate quantile range.\n\n\nwith_centering\nWhether to center data with median.\n\n\nwith_scaling\nWhether to scale the data to quantile range.\n\n\nrelative_error\nThe target relative error for quantile computation.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_robust_scaler.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "In the case where x is a tbl_spark , the estimator fits against x to obtain a transformer, which is then immediately used to transform x , returning a tbl_spark ."
  },
  {
    "href": "reference/ft_robust_scaler.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_robust_scaler.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/ft_count_vectorizer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Extracts a vocabulary from document collections."
  },
  {
    "href": "reference/ft_count_vectorizer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_count_vectorizer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  binary = FALSE,\n  min_df = 1,\n  min_tf = 1,\n  vocab_size = 2^18,\n  uid = random_string(\"count_vectorizer_\"),\n  ...\n)\nml_vocabulary(model)"
  },
  {
    "href": "reference/ft_count_vectorizer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nbinary\nBinary toggle to control the output vector values. If TRUE , all nonzero counts (after min_tf filter applied) are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. Default: FALSE\n\n\nmin_df\nSpecifies the minimum number of different documents a term must appear in to be included in the vocabulary. If this is an integer greater than or equal to 1, this specifies the number of documents the term must appear in; if this is a double in [0,1), then this specifies the fraction of documents. Default: 1.\n\n\nmin_tf\nFilter to ignore rare words in a document. For each document, terms with frequency/count less than the given threshold are ignored. If this is an integer greater than or equal to 1, then this specifies a count (of times the term must appear in the document); if this is a double in [0,1), then this specifies a fraction (out of the document’s token count). Default: 1.\n\n\nvocab_size\nBuild a vocabulary that only considers the top vocab_size terms ordered by term frequency across the corpus. Default: 2^18 .\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused.\n\n\nmodel\nA ml_count_vectorizer_model ."
  },
  {
    "href": "reference/ft_count_vectorizer.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "In the case where x is a tbl_spark , the estimator fits against x to obtain a transformer, which is then immediately used to transform x , returning a tbl_spark ."
  },
  {
    "href": "reference/ft_count_vectorizer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark\n\nml_vocabulary() returns a vector of vocabulary built."
  },
  {
    "href": "reference/ft_count_vectorizer.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/sdf_schema.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Read the schema of a Spark DataFrame."
  },
  {
    "href": "reference/sdf_schema.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_schema(x, expand_nested_cols = FALSE, expand_struct_cols = FALSE)"
  },
  {
    "href": "reference/sdf_schema.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nexpand_nested_cols\nWhether to expand columns containing nested array of structs (which are usually created by tidyr::nest on a Spark data frame)\n\n\nexpand_struct_cols\nWhether to expand columns containing structs"
  },
  {
    "href": "reference/sdf_schema.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "The type column returned gives the string representation of the underlying Spark type for that column; for example, a vector of numeric values would be returned with the type \"DoubleType\" . Please see the Spark Scala API Documentation for information on what types are available and exposed by Spark."
  },
  {
    "href": "reference/sdf_schema.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "An list() list , with each list element describing the name and type of a column."
  },
  {
    "href": "reference/stream_trigger_continuous.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Creates a Spark structured streaming trigger to execute continuously. This mode is the most performant but not all operations are supported."
  },
  {
    "href": "reference/stream_trigger_continuous.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_trigger_continuous(checkpoint = 5000)"
  },
  {
    "href": "reference/stream_trigger_continuous.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\ncheckpoint\nThe checkpoint interval specified in milliseconds."
  },
  {
    "href": "reference/stream_trigger_continuous.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "stream_trigger_interval"
  },
  {
    "href": "reference/ml_aft_survival_regression.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Fit a parametric survival regression model named accelerated failure time (AFT) model (see Accelerated failure time model (Wikipedia) ) based on the Weibull distribution of the survival time."
  },
  {
    "href": "reference/ml_aft_survival_regression.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_aft_survival_regression(\n  x,\n  formula = NULL,\n  censor_col = \"censor\",\n  quantile_probabilities = c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99),\n  fit_intercept = TRUE,\n  max_iter = 100L,\n  tol = 1e-06,\n  aggregation_depth = 2,\n  quantiles_col = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"aft_survival_regression_\"),\n  ...\n)\nml_survival_regression(\n  x,\n  formula = NULL,\n  censor_col = \"censor\",\n  quantile_probabilities = c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99),\n  fit_intercept = TRUE,\n  max_iter = 100L,\n  tol = 1e-06,\n  aggregation_depth = 2,\n  quantiles_col = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"aft_survival_regression_\"),\n  response = NULL,\n  features = NULL,\n  ...\n)"
  },
  {
    "href": "reference/ml_aft_survival_regression.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nUsed when x is a tbl_spark . R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\ncensor_col\nCensor column name. The value of this column could be 0 or 1. If the value is 1, it means the event has occurred i.e. uncensored; otherwise censored.\n\n\nquantile_probabilities\nQuantile probabilities array. Values of the quantile probabilities array should be in the range (0, 1) and the array should be non-empty.\n\n\nfit_intercept\nBoolean; should the model be fit with an intercept term?\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\naggregation_depth\n(Spark 2.1.0+) Suggested depth for treeAggregate (>= 2).\n\n\nquantiles_col\nQuantiles column name. This column will output quantiles of corresponding quantileProbabilities if it is set.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula .\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; see Details.\n\n\nresponse\n(Deprecated) The name of the response column (as a length-one character vector.)\n\n\nfeatures\n(Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "href": "reference/ml_aft_survival_regression.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "When x is a tbl_spark and formula (alternatively, response and features ) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\" ) can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model , ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows.\nml_survival_regression() is an alias for ml_aft_survival_regression() for backwards compatibility."
  },
  {
    "href": "reference/ml_aft_survival_regression.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a predictor is constructed then immediately fit with the input tbl_spark , returning a prediction model.\ntbl_spark , with formula : specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model ."
  },
  {
    "href": "reference/ml_aft_survival_regression.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_decision_tree_classifier , ml_gbt_classifier , ml_generalized_linear_regression , ml_isotonic_regression , ml_linear_regression , ml_linear_svc , ml_logistic_regression , ml_multilayer_perceptron_classifier , ml_naive_bayes , ml_one_vs_rest , ml_random_forest_classifier"
  },
  {
    "href": "reference/ml_aft_survival_regression.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(survival)\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\novarian_tbl <- sdf_copy_to(sc, ovarian, name = \"ovarian_tbl\", overwrite = TRUE)\n\npartitions <- ovarian_tbl %>%\nsdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\novarian_training <- partitions$training\novarian_test <- partitions$test\n\nsur_reg <- ovarian_training %>%\nml_aft_survival_regression(futime ~ ecog_ps + rx + age + resid_ds, censor_col = \"fustat\")\n\npred <- ml_predict(sur_reg, ovarian_test)\npred"
  },
  {
    "href": "reference/spark_save_table.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Saves a Spark DataFrame and as a Spark table."
  },
  {
    "href": "reference/spark_save_table.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_save_table(x, path, mode = NULL, options = list())"
  },
  {
    "href": "reference/spark_save_table.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure. For more details see also http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\noptions\nA list of strings with additional options."
  },
  {
    "href": "reference/spark_save_table.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/sdf_rnorm.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generator method for creating a single-column Spark dataframes comprised of i.i.d. samples from the standard normal distribution."
  },
  {
    "href": "reference/sdf_rnorm.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_rnorm(\n  sc,\n  n,\n  mean = 0,\n  sd = 1,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "href": "reference/sdf_rnorm.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nmean\nThe mean value of the normal distribution.\n\n\nsd\nThe standard deviation of the normal distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "href": "reference/sdf_rnorm.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark statistical routines: sdf_rbeta , sdf_rbinom , sdf_rcauchy , sdf_rchisq , sdf_rexp , sdf_rgamma , sdf_rgeom , sdf_rhyper , sdf_rlnorm , sdf_rpois , sdf_rt , sdf_runif , sdf_rweibull"
  },
  {
    "href": "reference/sdf_rgeom.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a geometric distribution."
  },
  {
    "href": "reference/sdf_rgeom.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_rgeom(sc, n, prob, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "href": "reference/sdf_rgeom.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nprob\nProbability of success in each trial.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "href": "reference/sdf_rgeom.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark statistical routines: sdf_rbeta , sdf_rbinom , sdf_rcauchy , sdf_rchisq , sdf_rexp , sdf_rgamma , sdf_rhyper , sdf_rlnorm , sdf_rnorm , sdf_rpois , sdf_rt , sdf_runif , sdf_rweibull"
  },
  {
    "href": "reference/ft_one_hot_encoder_estimator.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "A one-hot encoder that maps a column of category indices to a column of binary vectors, with at most a single one-value per row that indicates the input category index. For example with 5 categories, an input value of 2.0 would map to an output vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included by default (configurable via dropLast), because it makes the vector entries sum up to one, and hence linearly dependent. So an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0]."
  },
  {
    "href": "reference/ft_one_hot_encoder_estimator.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_one_hot_encoder_estimator(\n  x,\n  input_cols = NULL,\n  output_cols = NULL,\n  handle_invalid = \"error\",\n  drop_last = TRUE,\n  uid = random_string(\"one_hot_encoder_estimator_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_one_hot_encoder_estimator.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_cols\nNames of input columns.\n\n\noutput_cols\nNames of output columns.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error”\n\n\ndrop_last\nWhether to drop the last category. Defaults to TRUE .\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_one_hot_encoder_estimator.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "In the case where x is a tbl_spark , the estimator fits against x to obtain a transformer, which is then immediately used to transform x , returning a tbl_spark ."
  },
  {
    "href": "reference/ft_one_hot_encoder_estimator.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_one_hot_encoder_estimator.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/spark_write_csv.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Write a Spark DataFrame to a tabular (typically, comma-separated) file."
  },
  {
    "href": "reference/spark_write_csv.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_write_csv(\n  x,\n  path,\n  header = TRUE,\n  delimiter = \",\",\n  quote = \"\\\"\",\n  escape = \"\\\\\",\n  charset = \"UTF-8\",\n  null_value = NULL,\n  options = list(),\n  mode = NULL,\n  partition_by = NULL,\n  ...\n)"
  },
  {
    "href": "reference/spark_write_csv.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nheader\nShould the first row of data be used as a header? Defaults to TRUE .\n\n\ndelimiter\nThe character used to delimit each column, defaults to , .\n\n\nquote\nThe character used as a quote. Defaults to ‘“’ .\n\n\nescape\nThe character used to escape other characters, defaults to \\ .\n\n\ncharset\nThe character set, defaults to \"UTF-8\" .\n\n\nnull_value\nThe character to use for default values, defaults to NULL .\n\n\noptions\nA list of strings with additional options.\n\n\nmode\nA character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ignore. Notice that ‘overwrite’ will also change the column structure. For more details see also http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark.\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_write_csv.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/sdf_project.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Project features onto principal components"
  },
  {
    "href": "reference/sdf_project.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_project(\n  object,\n  newdata,\n  features = dimnames(object$pc)[[1]],\n  feature_prefix = NULL,\n  ...\n)"
  },
  {
    "href": "reference/sdf_project.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nobject\nA Spark PCA model object\n\n\nnewdata\nAn object coercible to a Spark DataFrame\n\n\nfeatures\nA vector of names of columns to be projected\n\n\nfeature_prefix\nThe prefix used in naming the output features\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/collect.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See collect for more details."
  },
  {
    "href": "reference/hof_filter.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Apply an element-wise filtering function to an array column (this is essentially a dplyr wrapper for the filter(array<T>, function<T, Boolean>): array<T> built-in Spark SQL functions)"
  },
  {
    "href": "reference/hof_filter.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "hof_filter(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "href": "reference/hof_filter.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nThe Spark data frame to filter\n\n\nfunc\nThe filtering function\n\n\nexpr\nThe array being filtered, could be any SQL expression evaluating to an array (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the filtered result (default: expr)\n\n\n...\nAdditional params to dplyr::mutate"
  },
  {
    "href": "reference/hof_filter.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\")\n# only keep odd elements in each array in `array_column`\ncopy_to(sc, tibble::tibble(array_column = list(1:5, 21:25))) %>%\nhof_filter(~ .x %% 2 == 1)"
  },
  {
    "href": "reference/ft_bucketizer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Similar to list() ’s cut function, this transforms a numeric column into a discretized column, with breaks specified through the splits parameter."
  },
  {
    "href": "reference/ft_bucketizer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_bucketizer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  splits = NULL,\n  input_cols = NULL,\n  output_cols = NULL,\n  splits_array = NULL,\n  handle_invalid = \"error\",\n  uid = random_string(\"bucketizer_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_bucketizer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nsplits\nA numeric vector of cutpoints, indicating the bucket boundaries.\n\n\ninput_cols\nNames of input columns.\n\n\noutput_cols\nNames of output columns.\n\n\nsplits_array\nParameter for specifying multiple splits parameters. Each element in this array can be used to map continuous features into buckets.\n\n\nhandle_invalid\n(Spark 2.1.0+) Param for how to handle invalid entries. Options are ‘skip’ (filter out rows with invalid values), ‘error’ (throw an error), or ‘keep’ (keep invalid values in a special additional bucket). Default: “error”\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_bucketizer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_bucketizer.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/ft_bucketizer.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(dplyr)\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\niris_tbl %>%\nft_bucketizer(\ninput_col = \"Sepal_Length\",\noutput_col = \"Sepal_Length_bucket\",\nsplits = c(0, 4.5, 5, 8)\n) %>%\nselect(Sepal_Length, Sepal_Length_bucket, Species)"
  },
  {
    "href": "reference/checkpoint_directory.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Set/Get Spark checkpoint directory"
  },
  {
    "href": "reference/checkpoint_directory.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_set_checkpoint_dir(sc, dir)\nspark_get_checkpoint_dir(sc)"
  },
  {
    "href": "reference/checkpoint_directory.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\ndir\ncheckpoint directory, must be HDFS path of running on cluster"
  },
  {
    "href": "reference/ml-persistence.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Save/load Spark ML objects"
  },
  {
    "href": "reference/ml-persistence.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_save(x, path, overwrite = FALSE, ...)\nlist(list(\"ml_save\"), list(\"ml_model\"))(\n  x,\n  path,\n  overwrite = FALSE,\n  type = c(\"pipeline_model\", \"pipeline\"),\n  ...\n)\nml_load(sc, path)"
  },
  {
    "href": "reference/ml-persistence.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA ML object, which could be a ml_pipeline_stage or a ml_model\n\n\npath\nThe path where the object is to be serialized/deserialized.\n\n\noverwrite\nWhether to overwrite the existing path, defaults to FALSE .\n\n\n...\nOptional arguments; currently unused.\n\n\ntype\nWhether to save the pipeline model or the pipeline.\n\n\nsc\nA Spark connection."
  },
  {
    "href": "reference/ml-persistence.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "ml_save() serializes a Spark object into a format that can be read back into sparklyr or by the Scala or PySpark APIs. When called on ml_model objects, i.e. those that were created via the tbl_spark - formula signature, the associated pipeline model is serialized. In other words, the saved model contains both the data processing ( RFormulaModel ) stage and the machine learning stage.\nml_load() reads a saved Spark object into sparklyr . It calls the correct Scala load method based on parsing the saved metadata. Note that a PipelineModel object saved from a sparklyr ml_model via ml_save() will be read back in as an ml_pipeline_model , rather than the ml_model object."
  },
  {
    "href": "reference/distinct.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See distinct for more details."
  },
  {
    "href": "reference/ft_tokenizer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "A tokenizer that converts the input string to lowercase and then splits it by white spaces."
  },
  {
    "href": "reference/ft_tokenizer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_tokenizer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  uid = random_string(\"tokenizer_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_tokenizer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_tokenizer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_tokenizer.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/sdf_sort.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Sort a Spark DataFrame by one or more columns, with each column sorted in ascending order."
  },
  {
    "href": "reference/sdf_sort.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_sort(x, columns)"
  },
  {
    "href": "reference/sdf_sort.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\ncolumns\nThe column(s) to sort by."
  },
  {
    "href": "reference/sdf_sort.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark data frames: sdf_copy_to , sdf_distinct , sdf_random_split , sdf_register , sdf_sample , sdf_weighted_sample"
  },
  {
    "href": "reference/spark_configuration.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Retrieves or sets runtime configuration entries for the Spark Session"
  },
  {
    "href": "reference/spark_configuration.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_session_config(sc, config = TRUE, value = NULL)"
  },
  {
    "href": "reference/spark_configuration.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nconfig\nThe configuration entry name(s) (e.g., \"spark.sql.shuffle.partitions\" ). Defaults to NULL to retrieve all configuration entries.\n\n\nvalue\nThe configuration value to be set. Defaults to NULL to retrieve configuration entries."
  },
  {
    "href": "reference/spark_configuration.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark runtime configuration: spark_adaptive_query_execution , spark_advisory_shuffle_partition_size , spark_auto_broadcast_join_threshold , spark_coalesce_initial_num_partitions , spark_coalesce_min_num_partitions , spark_coalesce_shuffle_partitions"
  },
  {
    "href": "reference/ft_lsh.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Locality Sensitive Hashing functions for Euclidean distance (Bucketed Random Projection) and Jaccard distance (MinHash)."
  },
  {
    "href": "reference/ft_lsh.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_bucketed_random_projection_lsh(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  bucket_length = NULL,\n  num_hash_tables = 1,\n  seed = NULL,\n  uid = random_string(\"bucketed_random_projection_lsh_\"),\n  ...\n)\nft_minhash_lsh(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  num_hash_tables = 1L,\n  seed = NULL,\n  uid = random_string(\"minhash_lsh_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_lsh.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nbucket_length\nThe length of each hash bucket, a larger bucket lowers the false negative rate. The number of buckets will be (max L2 norm of input vectors) / bucketLength.\n\n\nnum_hash_tables\nNumber of hash tables used in LSH OR-amplification. LSH OR-amplification can be used to reduce the false negative rate. Higher values for this param lead to a reduced false negative rate, at the expense of added computational complexity.\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_lsh.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "In the case where x is a tbl_spark , the estimator fits against x to obtain a transformer, which is then immediately used to transform x , returning a tbl_spark ."
  },
  {
    "href": "reference/ft_lsh.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_lsh.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nft_lsh_utils\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/ft_binarizer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Apply thresholding to a column, such that values less than or equal to the threshold are assigned the value 0.0, and values greater than the threshold are assigned the value 1.0. Column output is numeric for compatibility with other modeling functions."
  },
  {
    "href": "reference/ft_binarizer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_binarizer(\n  x,\n  input_col,\n  output_col,\n  threshold = 0,\n  uid = random_string(\"binarizer_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_binarizer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nthreshold\nThreshold used to binarize continuous features.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_binarizer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_binarizer.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/ft_binarizer.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(dplyr)\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\niris_tbl %>%\nft_binarizer(\ninput_col = \"Sepal_Length\",\noutput_col = \"Sepal_Length_bin\",\nthreshold = 5\n) %>%\nselect(Sepal_Length, Sepal_Length_bin, Species)"
  },
  {
    "href": "reference/spark_read.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Run a custom R function on Spark workers to ingest data from one or more files into a Spark DataFrame, assuming all files follow the same schema."
  },
  {
    "href": "reference/spark_read.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_read(sc, paths, reader, columns, packages = TRUE, ...)"
  },
  {
    "href": "reference/spark_read.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\npaths\nA character vector of one or more file URIs (e.g., c(“hdfs://localhost:9000/file.txt”, “hdfs://localhost:9000/file2.txt”))\n\n\nreader\nA self-contained R function that takes a single file URI as argument and returns the data read from that file as a data frame.\n\n\ncolumns\na named list of column names and column types of the resulting data frame (e.g., list(column_1 = “integer”, column_2 = “character”)), or a list of column names only if column types should be inferred from the data (e.g., list(“column_1”, “column_2”), or NULL if column types should be inferred and resulting data frame can have arbitrary column names\n\n\npackages\nA list of R packages to distribute to Spark workers\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_read.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/spark_read.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(\nmaster = \"yarn\",\nspark_home = \"~/spark/spark-2.4.5-bin-hadoop2.7\"\n)\n\n# This is a contrived example to show reader tasks will be distributed across\n# all Spark worker nodes\nspark_read(\nsc,\nrep(\"/dev/null\", 10),\nreader = function(path) system(\"hostname\", intern = TRUE),\ncolumns = c(hostname = \"string\")\n) %>% sdf_collect()"
  },
  {
    "href": "reference/sdf_crosstab.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Builds a contingency table at each combination of factor levels."
  },
  {
    "href": "reference/sdf_crosstab.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_crosstab(x, col1, col2)"
  },
  {
    "href": "reference/sdf_crosstab.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame\n\n\ncol1\nThe name of the first column. Distinct items will make the first item of each row.\n\n\ncol2\nThe name of the second column. Distinct items will make the column names of the DataFrame."
  },
  {
    "href": "reference/sdf_crosstab.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "A DataFrame containing the contingency table."
  },
  {
    "href": "reference/ml_gradient_boosted_trees.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Perform binary classification and regression using gradient boosted trees. Multiclass classification is not supported yet."
  },
  {
    "href": "reference/ml_gradient_boosted_trees.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_gbt_classifier(\n  x,\n  formula = NULL,\n  max_iter = 20,\n  max_depth = 5,\n  step_size = 0.1,\n  subsampling_rate = 1,\n  feature_subset_strategy = \"auto\",\n  min_instances_per_node = 1L,\n  max_bins = 32,\n  min_info_gain = 0,\n  loss_type = \"logistic\",\n  seed = NULL,\n  thresholds = NULL,\n  checkpoint_interval = 10,\n  cache_node_ids = FALSE,\n  max_memory_in_mb = 256,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"gbt_classifier_\"),\n  ...\n)\nml_gradient_boosted_trees(\n  x,\n  formula = NULL,\n  type = c(\"auto\", \"regression\", \"classification\"),\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  checkpoint_interval = 10,\n  loss_type = c(\"auto\", \"logistic\", \"squared\", \"absolute\"),\n  max_bins = 32,\n  max_depth = 5,\n  max_iter = 20L,\n  min_info_gain = 0,\n  min_instances_per_node = 1,\n  step_size = 0.1,\n  subsampling_rate = 1,\n  feature_subset_strategy = \"auto\",\n  seed = NULL,\n  thresholds = NULL,\n  cache_node_ids = FALSE,\n  max_memory_in_mb = 256,\n  uid = random_string(\"gradient_boosted_trees_\"),\n  response = NULL,\n  features = NULL,\n  ...\n)\nml_gbt_regressor(\n  x,\n  formula = NULL,\n  max_iter = 20,\n  max_depth = 5,\n  step_size = 0.1,\n  subsampling_rate = 1,\n  feature_subset_strategy = \"auto\",\n  min_instances_per_node = 1,\n  max_bins = 32,\n  min_info_gain = 0,\n  loss_type = \"squared\",\n  seed = NULL,\n  checkpoint_interval = 10,\n  cache_node_ids = FALSE,\n  max_memory_in_mb = 256,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"gbt_regressor_\"),\n  ...\n)"
  },
  {
    "href": "reference/ml_gradient_boosted_trees.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nUsed when x is a tbl_spark . R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nmax_iter\nMaxmimum number of iterations.\n\n\nmax_depth\nMaximum depth of the tree (>= 0); that is, the maximum number of nodes separating any leaves from the root of the tree.\n\n\nstep_size\nStep size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default = 0.1)\n\n\nsubsampling_rate\nFraction of the training data used for learning each decision tree, in range (0, 1]. (default = 1.0)\n\n\nfeature_subset_strategy\nThe number of features to consider for splits at each tree node. See details for options.\n\n\nmin_instances_per_node\nMinimum number of instances each child must have after split.\n\n\nmax_bins\nThe maximum number of bins used for discretizing continuous features and for choosing how to split on features at each node. More bins give higher granularity.\n\n\nmin_info_gain\nMinimum information gain for a split to be considered at a tree node. Should be >= 0, defaults to 0.\n\n\nloss_type\nLoss function which GBT tries to minimize. Supported: \"squared\" (L2) and \"absolute\" (L1) (default = squared) for regression and \"logistic\" (default) for classification. For ml_gradient_boosted_trees , setting \"auto\" will default to the appropriate loss type based on model type.\n\n\nseed\nSeed for random numbers.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\ncheckpoint_interval\nSet checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.\n\n\ncache_node_ids\nIf FALSE , the algorithm will pass trees to executors to match instances with nodes. If TRUE , the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Defaults to FALSE .\n\n\nmax_memory_in_mb\nMaximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. Defaults to 256.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula .\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; see Details.\n\n\ntype\nThe type of model to fit. \"regression\" treats the response as a continuous variable, while \"classification\" treats the response as a categorical variable. When \"auto\" is used, the model type is inferred based on the response variable type – if it is a numeric type, then regression is used; classification otherwise.\n\n\nresponse\n(Deprecated) The name of the response column (as a length-one character vector.)\n\n\nfeatures\n(Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "href": "reference/ml_gradient_boosted_trees.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "When x is a tbl_spark and formula (alternatively, response and features ) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\" ) can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model , ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows.\nThe supported options for feature_subset_strategy are\n\n\"auto\" : Choose automatically for task: If num_trees == 1 , set to \"all\" . If num_trees > 1 (forest), set to \"sqrt\" for classification and to \"onethird\" for regression.\n\"all\" : use all features\n\"onethird\" : use 1/3 of the features\n\"sqrt\" : use use sqrt(number of features)\n\"log2\" : use log2(number of features)\n\"n\" : when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features. (default = \"auto\" )\n\nml_gradient_boosted_trees is a wrapper around ml_gbt_regressor.tbl_spark and ml_gbt_classifier.tbl_spark and calls the appropriate method based on model type."
  },
  {
    "href": "reference/ml_gradient_boosted_trees.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a predictor is constructed then immediately fit with the input tbl_spark , returning a prediction model.\ntbl_spark , with formula : specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model ."
  },
  {
    "href": "reference/ml_gradient_boosted_trees.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression , ml_decision_tree_classifier , ml_generalized_linear_regression , ml_isotonic_regression , ml_linear_regression , ml_linear_svc , ml_logistic_regression , ml_multilayer_perceptron_classifier , ml_naive_bayes , ml_one_vs_rest , ml_random_forest_classifier"
  },
  {
    "href": "reference/ml_gradient_boosted_trees.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\npartitions <- iris_tbl %>%\nsdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\niris_training <- partitions$training\niris_test <- partitions$test\n\ngbt_model <- iris_training %>%\nml_gradient_boosted_trees(Sepal_Length ~ Petal_Length + Petal_Width)\n\npred <- ml_predict(gbt_model, iris_test)\n\nml_regression_evaluator(pred, label_col = \"Sepal_Length\")"
  },
  {
    "href": "reference/stream_view.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Opens a Shiny gadget to visualize the given stream."
  },
  {
    "href": "reference/stream_view.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_view(stream, ...)"
  },
  {
    "href": "reference/stream_view.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nstream\nThe stream to visualize.\n\n\n...\nAdditional optional arguments."
  },
  {
    "href": "reference/stream_view.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\")\n\ndir.create(\"iris-in\")\nwrite.csv(iris, \"iris-in/iris.csv\", row.names = FALSE)\n\nstream_read_csv(sc, \"iris-in/\") %>%\nstream_write_csv(\"iris-out/\") %>%\nstream_view() %>%\nstream_stop()"
  },
  {
    "href": "reference/separate.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See separate for more details."
  },
  {
    "href": "reference/jobj_class.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Extract the classes that a Java object inherits from. This is the jobj equivalent of class() ."
  },
  {
    "href": "reference/jobj_class.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "jobj_class(jobj, simple_name = TRUE)"
  },
  {
    "href": "reference/jobj_class.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\njobj\nA spark_jobj\n\n\nsimple_name\nWhether to return simple names, defaults to TRUE"
  },
  {
    "href": "reference/tbl_cache.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Force a Spark table with name name to be loaded into memory. Operations on cached tables should normally (although not always) be more performant than the same operation performed on an uncached table."
  },
  {
    "href": "reference/tbl_cache.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "tbl_cache(sc, name, force = TRUE)"
  },
  {
    "href": "reference/tbl_cache.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe table name.\n\n\nforce\nForce the data to be loaded into memory? This is accomplished by calling the count API on the associated Spark DataFrame."
  },
  {
    "href": "reference/connection_spark_shinyapp.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "A Shiny app that can be used to construct a spark_connect statement"
  },
  {
    "href": "reference/connection_spark_shinyapp.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "connection_spark_shinyapp()"
  },
  {
    "href": "reference/spark_load_table.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Reads from a Spark Table into a Spark DataFrame."
  },
  {
    "href": "reference/spark_load_table.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_load_table(\n  sc,\n  name,\n  path,\n  options = list(),\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE\n)"
  },
  {
    "href": "reference/spark_load_table.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\noptions\nA list of strings with additional options. See http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration .\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?"
  },
  {
    "href": "reference/spark_load_table.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/ft_vector_assembler.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Combine multiple vectors into a single row-vector; that is, where each row element of the newly generated column is a vector formed by concatenating each row element from the specified input columns."
  },
  {
    "href": "reference/ft_vector_assembler.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_vector_assembler(\n  x,\n  input_cols = NULL,\n  output_col = NULL,\n  uid = random_string(\"vector_assembler_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_vector_assembler.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_cols\nThe names of the input columns\n\n\noutput_col\nThe name of the output column.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_vector_assembler.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_vector_assembler.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/ml_prefixspan.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "PrefixSpan algorithm for mining frequent itemsets."
  },
  {
    "href": "reference/ml_prefixspan.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_prefixspan(\n  x,\n  seq_col = \"sequence\",\n  min_support = 0.1,\n  max_pattern_length = 10,\n  max_local_proj_db_size = 3.2e+07,\n  uid = random_string(\"prefixspan_\"),\n  ...\n)\nml_freq_seq_patterns(model)"
  },
  {
    "href": "reference/ml_prefixspan.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nseq_col\nThe name of the sequence column in dataset (default “sequence”). Rows with nulls in this column are ignored.\n\n\nmin_support\nThe minimum support required to be considered a frequent sequential pattern.\n\n\nmax_pattern_length\nThe maximum length of a frequent sequential pattern. Any frequent pattern exceeding this length will not be included in the results.\n\n\nmax_local_proj_db_size\nThe maximum number of items allowed in a prefix-projected database before local iterative processing of the projected database begins. This parameter should be tuned with respect to the size of your executors.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; currently unused.\n\n\nmodel\nA Prefix Span model."
  },
  {
    "href": "reference/ml_prefixspan.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.4.0\")\n\nitems_df <- tibble::tibble(\nseq = list(\nlist(list(1, 2), list(3)),\nlist(list(1), list(3, 2), list(1, 2)),\nlist(list(1, 2), list(5)),\nlist(list(6))\n)\n)\nitems_sdf <- copy_to(sc, items_df, overwrite = TRUE)\n\nprefix_span_model <- ml_prefixspan(\nsc,\nseq_col = \"seq\",\nmin_support = 0.5,\nmax_pattern_length = 5,\nmax_local_proj_db_size = 32000000\n)\n\nfrequent_items <- prefix_span_model$frequent_sequential_patterns(items_sdf) %>% collect()"
  },
  {
    "href": "reference/register_extension.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Registering an extension package will result in the package being automatically scanned for spark dependencies when a connection to Spark is created."
  },
  {
    "href": "reference/register_extension.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "register_extension(package)\nregistered_extensions()"
  },
  {
    "href": "reference/register_extension.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\npackage\nThe package(s) to register."
  },
  {
    "href": "reference/register_extension.html#note",
    "title": "sparklyr",
    "section": "Note",
    "text": "Packages should typically register their extensions in their .onLoad hook – this ensures that their extensions are registered when their namespaces are loaded."
  },
  {
    "href": "reference/ft_imputer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Imputation estimator for completing missing values, either using the mean or the median of the columns in which the missing values are located. The input columns should be of numeric type. This function requires Spark 2.2.0+."
  },
  {
    "href": "reference/ft_imputer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_imputer(\n  x,\n  input_cols = NULL,\n  output_cols = NULL,\n  missing_value = NULL,\n  strategy = \"mean\",\n  uid = random_string(\"imputer_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_imputer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_cols\nThe names of the input columns\n\n\noutput_cols\nThe names of the output columns.\n\n\nmissing_value\nThe placeholder for the missing values. All occurrences of missing_value will be imputed. Note that null values are always treated as missing.\n\n\nstrategy\nThe imputation strategy. Currently only “mean” and “median” are supported. If “mean”, then replace missing values using the mean value of the feature. If “median”, then replace missing values using the approximate median value of the feature. Default: mean\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_imputer.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "In the case where x is a tbl_spark , the estimator fits against x to obtain a transformer, which is then immediately used to transform x , returning a tbl_spark ."
  },
  {
    "href": "reference/ft_imputer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_imputer.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/spark_version.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Retrieve the version of Spark associated with a Spark connection."
  },
  {
    "href": "reference/spark_version.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_version(sc)"
  },
  {
    "href": "reference/spark_version.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection ."
  },
  {
    "href": "reference/spark_version.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Suffixes for e.g. preview versions, or snapshotted versions, are trimmed – if you require the full Spark version, you can retrieve it with invoke(spark_context(sc), \"version\") ."
  },
  {
    "href": "reference/spark_version.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The Spark version as a numeric_version ."
  },
  {
    "href": "reference/sdf_rchisq.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a chi-squared distribution."
  },
  {
    "href": "reference/sdf_rchisq.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_rchisq(sc, n, df, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "href": "reference/sdf_rchisq.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\ndf\nDegrees of freedom (non-negative, but can be non-integer).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "href": "reference/sdf_rchisq.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark statistical routines: sdf_rbeta , sdf_rbinom , sdf_rcauchy , sdf_rexp , sdf_rgamma , sdf_rgeom , sdf_rhyper , sdf_rlnorm , sdf_rnorm , sdf_rpois , sdf_rt , sdf_runif , sdf_rweibull"
  },
  {
    "href": "reference/sdf_random_split.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Partition a Spark DataFrame into multiple groups. This routine is useful for splitting a DataFrame into, for example, training and test datasets."
  },
  {
    "href": "reference/sdf_random_split.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_random_split(\n  x,\n  ...,\n  weights = NULL,\n  seed = sample(.Machine$integer.max, 1)\n)\nsdf_partition(x, ..., weights = NULL, seed = sample(.Machine$integer.max, 1))"
  },
  {
    "href": "reference/sdf_random_split.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\n...\nNamed parameters, mapping table names to weights. The weights will be normalized such that they sum to 1.\n\n\nweights\nAn alternate mechanism for supplying weights – when specified, this takes precedence over the ... arguments.\n\n\nseed\nRandom seed to use for randomly partitioning the dataset. Set this if you want your partitioning to be reproducible on repeated runs."
  },
  {
    "href": "reference/sdf_random_split.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "The sampling weights define the probability that a particular observation will be assigned to a particular partition, not the resulting size of the partition. This implies that partitioning a DataFrame with, for example,\nsdf_random_split(x, training = 0.5, test = 0.5)\nis not guaranteed to produce training and test partitions of equal size."
  },
  {
    "href": "reference/sdf_random_split.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "An list() list of tbl_spark s."
  },
  {
    "href": "reference/sdf_random_split.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark data frames: sdf_copy_to , sdf_distinct , sdf_register , sdf_sample , sdf_sort , sdf_weighted_sample"
  },
  {
    "href": "reference/sdf_random_split.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "# randomly partition data into a 'training' and 'test'\n# dataset, with 60% of the observations assigned to the\n# 'training' dataset, and 40% assigned to the 'test' dataset\ndata(diamonds, package = \"ggplot2\")\ndiamonds_tbl <- copy_to(sc, diamonds, \"diamonds\")\npartitions <- diamonds_tbl %>%\nsdf_random_split(training = 0.6, test = 0.4)\nprint(partitions)\n\n# alternate way of specifying weights\nweights <- c(training = 0.6, test = 0.4)\ndiamonds_tbl %>% sdf_random_split(weights = weights)"
  },
  {
    "href": "reference/ml_linear_svc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Perform classification using linear support vector machines (SVM). This binary classifier optimizes the Hinge Loss using the OWLQN optimizer. Only supports L2 regularization currently."
  },
  {
    "href": "reference/ml_linear_svc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_linear_svc(\n  x,\n  formula = NULL,\n  fit_intercept = TRUE,\n  reg_param = 0,\n  max_iter = 100,\n  standardization = TRUE,\n  weight_col = NULL,\n  tol = 1e-06,\n  threshold = 0,\n  aggregation_depth = 2,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"linear_svc_\"),\n  ...\n)"
  },
  {
    "href": "reference/ml_linear_svc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nUsed when x is a tbl_spark . R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nfit_intercept\nBoolean; should the model be fit with an intercept term?\n\n\nreg_param\nRegularization parameter (aka lambda)\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nstandardization\nWhether to standardize the training features before fitting the model.\n\n\nweight_col\nThe name of the column to use as weights for the model fit.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nthreshold\nin binary classification prediction, in range [0, 1].\n\n\naggregation_depth\n(Spark 2.1.0+) Suggested depth for treeAggregate (>= 2).\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula .\n\n\nprediction_col\nPrediction column name.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; see Details."
  },
  {
    "href": "reference/ml_linear_svc.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "When x is a tbl_spark and formula (alternatively, response and features ) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\" ) can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model , ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "href": "reference/ml_linear_svc.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a predictor is constructed then immediately fit with the input tbl_spark , returning a prediction model.\ntbl_spark , with formula : specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model ."
  },
  {
    "href": "reference/ml_linear_svc.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression , ml_decision_tree_classifier , ml_gbt_classifier , ml_generalized_linear_regression , ml_isotonic_regression , ml_linear_regression , ml_logistic_regression , ml_multilayer_perceptron_classifier , ml_naive_bayes , ml_one_vs_rest , ml_random_forest_classifier"
  },
  {
    "href": "reference/ml_linear_svc.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(dplyr)\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\npartitions <- iris_tbl %>%\nfilter(Species != \"setosa\") %>%\nsdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\niris_training <- partitions$training\niris_test <- partitions$test\n\nsvc_model <- iris_training %>%\nml_linear_svc(Species ~ .)\n\npred <- ml_predict(svc_model, iris_test)\n\nml_binary_classification_evaluator(pred)"
  },
  {
    "href": "reference/stream_generate_test.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generates a local test stream, useful when testing streams locally."
  },
  {
    "href": "reference/stream_generate_test.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_generate_test(\n  df = rep(1:1000),\n  path = \"source\",\n  distribution = floor(10 + 1e+05 * stats::dbinom(1:20, 20, 0.5)),\n  iterations = 50,\n  interval = 1\n)"
  },
  {
    "href": "reference/stream_generate_test.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\ndf\nThe data frame used as a source of rows to the stream, will be cast to data frame if needed. Defaults to a sequence of one thousand entries.\n\n\npath\nPath to save stream of files to, defaults to \"source\" .\n\n\ndistribution\nThe distribution of rows to use over each iteration, defaults to a binomial distribution. The stream will cycle through the distribution if needed.\n\n\niterations\nNumber of iterations to execute before stopping, defaults to fifty.\n\n\ninterval\nThe inverval in seconds use to write the stream, defaults to one second."
  },
  {
    "href": "reference/stream_generate_test.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "This function requires the callr package to be installed."
  },
  {
    "href": "reference/ml_decision_tree.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Perform classification and regression using decision trees."
  },
  {
    "href": "reference/ml_decision_tree.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_decision_tree_classifier(\n  x,\n  formula = NULL,\n  max_depth = 5,\n  max_bins = 32,\n  min_instances_per_node = 1,\n  min_info_gain = 0,\n  impurity = \"gini\",\n  seed = NULL,\n  thresholds = NULL,\n  cache_node_ids = FALSE,\n  checkpoint_interval = 10,\n  max_memory_in_mb = 256,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"decision_tree_classifier_\"),\n  ...\n)\nml_decision_tree(\n  x,\n  formula = NULL,\n  type = c(\"auto\", \"regression\", \"classification\"),\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  variance_col = NULL,\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  checkpoint_interval = 10L,\n  impurity = \"auto\",\n  max_bins = 32L,\n  max_depth = 5L,\n  min_info_gain = 0,\n  min_instances_per_node = 1L,\n  seed = NULL,\n  thresholds = NULL,\n  cache_node_ids = FALSE,\n  max_memory_in_mb = 256L,\n  uid = random_string(\"decision_tree_\"),\n  response = NULL,\n  features = NULL,\n  ...\n)\nml_decision_tree_regressor(\n  x,\n  formula = NULL,\n  max_depth = 5,\n  max_bins = 32,\n  min_instances_per_node = 1,\n  min_info_gain = 0,\n  impurity = \"variance\",\n  seed = NULL,\n  cache_node_ids = FALSE,\n  checkpoint_interval = 10,\n  max_memory_in_mb = 256,\n  variance_col = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"decision_tree_regressor_\"),\n  ...\n)"
  },
  {
    "href": "reference/ml_decision_tree.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nUsed when x is a tbl_spark . R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nmax_depth\nMaximum depth of the tree (>= 0); that is, the maximum number of nodes separating any leaves from the root of the tree.\n\n\nmax_bins\nThe maximum number of bins used for discretizing continuous features and for choosing how to split on features at each node. More bins give higher granularity.\n\n\nmin_instances_per_node\nMinimum number of instances each child must have after split.\n\n\nmin_info_gain\nMinimum information gain for a split to be considered at a tree node. Should be >= 0, defaults to 0.\n\n\nimpurity\nCriterion used for information gain calculation. Supported: “entropy” and “gini” (default) for classification and “variance” (default) for regression. For ml_decision_tree , setting \"auto\" will default to the appropriate criterion based on model type.\n\n\nseed\nSeed for random numbers.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\ncache_node_ids\nIf FALSE , the algorithm will pass trees to executors to match instances with nodes. If TRUE , the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Defaults to FALSE .\n\n\ncheckpoint_interval\nSet checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.\n\n\nmax_memory_in_mb\nMaximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. Defaults to 256.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula .\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; see Details.\n\n\ntype\nThe type of model to fit. \"regression\" treats the response as a continuous variable, while \"classification\" treats the response as a categorical variable. When \"auto\" is used, the model type is inferred based on the response variable type – if it is a numeric type, then regression is used; classification otherwise.\n\n\nvariance_col\n(Optional) Column name for the biased sample variance of prediction.\n\n\nresponse\n(Deprecated) The name of the response column (as a length-one character vector.)\n\n\nfeatures\n(Deprecated) The name of features (terms) to use for the model fit."
  },
  {
    "href": "reference/ml_decision_tree.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "When x is a tbl_spark and formula (alternatively, response and features ) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\" ) can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model , ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows.\nml_decision_tree is a wrapper around ml_decision_tree_regressor.tbl_spark and ml_decision_tree_classifier.tbl_spark and calls the appropriate method based on model type."
  },
  {
    "href": "reference/ml_decision_tree.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a predictor is constructed then immediately fit with the input tbl_spark , returning a prediction model.\ntbl_spark , with formula : specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model ."
  },
  {
    "href": "reference/ml_decision_tree.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression , ml_gbt_classifier , ml_generalized_linear_regression , ml_isotonic_regression , ml_linear_regression , ml_linear_svc , ml_logistic_regression , ml_multilayer_perceptron_classifier , ml_naive_bayes , ml_one_vs_rest , ml_random_forest_classifier"
  },
  {
    "href": "reference/ml_decision_tree.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\npartitions <- iris_tbl %>%\nsdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\niris_training <- partitions$training\niris_test <- partitions$test\n\ndt_model <- iris_training %>%\nml_decision_tree(Species ~ .)\n\npred <- ml_predict(dt_model, iris_test)\n\nml_multiclass_classification_evaluator(pred)"
  },
  {
    "href": "reference/copy_to.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See copy_to for more details."
  },
  {
    "href": "reference/sdf_with_sequential_id.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Add a sequential ID column to a Spark DataFrame. The Spark zipWithIndex function is used to produce these. This differs from sdf_with_unique_id in that the IDs generated are independent of partitioning."
  },
  {
    "href": "reference/sdf_with_sequential_id.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_with_sequential_id(x, id = \"id\", from = 1L)"
  },
  {
    "href": "reference/sdf_with_sequential_id.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nid\nThe name of the column to host the generated IDs.\n\n\nfrom\nThe starting value of the id column"
  },
  {
    "href": "reference/stream_read_csv.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Reads a CSV stream as a Spark dataframe stream."
  },
  {
    "href": "reference/stream_read_csv.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_read_csv(\n  sc,\n  path,\n  name = NULL,\n  header = TRUE,\n  columns = NULL,\n  delimiter = \",\",\n  quote = \"\\\"\",\n  escape = \"\\\\\",\n  charset = \"UTF-8\",\n  null_value = NULL,\n  options = list(),\n  ...\n)"
  },
  {
    "href": "reference/stream_read_csv.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nname\nThe name to assign to the newly generated stream.\n\n\nheader\nBoolean; should the first row of data be used as a header? Defaults to TRUE .\n\n\ncolumns\nA vector of column names or a named vector of column types. If specified, the elements can be \"binary\" for BinaryType , \"boolean\" for BooleanType , \"byte\" for ByteType , \"integer\" for IntegerType , \"integer64\" for LongType , \"double\" for DoubleType , \"character\" for StringType , \"timestamp\" for TimestampType and \"date\" for DateType .\n\n\ndelimiter\nThe character used to delimit each column. Defaults to ‘,’ .\n\n\nquote\nThe character used as a quote. Defaults to ‘“’ .\n\n\nescape\nThe character used to escape other characters. Defaults to ’' .\n\n\ncharset\nThe character set. Defaults to “UTF-8” .\n\n\nnull_value\nThe character to use for null, or missing, values. Defaults to NULL .\n\n\noptions\nA list of strings with additional options.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/stream_read_csv.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark stream serialization: stream_read_delta , stream_read_json , stream_read_kafka , stream_read_orc , stream_read_parquet , stream_read_socket , stream_read_text , stream_write_console , stream_write_csv , stream_write_delta , stream_write_json , stream_write_kafka , stream_write_memory , stream_write_orc , stream_write_parquet , stream_write_text"
  },
  {
    "href": "reference/stream_read_csv.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\n\ndir.create(\"csv-in\")\nwrite.csv(iris, \"csv-in/data.csv\", row.names = FALSE)\n\ncsv_path <- file.path(\"file://\", getwd(), \"csv-in\")\n\nstream <- stream_read_csv(sc, csv_path) %>% stream_write_csv(\"csv-out\")\n\nstream_stop(stream)"
  },
  {
    "href": "reference/sdf_num_partitions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Gets number of partitions of a Spark DataFrame"
  },
  {
    "href": "reference/sdf_num_partitions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_num_partitions(x)"
  },
  {
    "href": "reference/sdf_num_partitions.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark ."
  },
  {
    "href": "reference/spark-connections.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "These routines allow you to manage your connections to Spark.\nCall spark_disconnect() on each open Spark connection"
  },
  {
    "href": "reference/spark-connections.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_connect(\n  master,\n  spark_home = Sys.getenv(\"SPARK_HOME\"),\n  method = c(\"shell\", \"livy\", \"databricks\", \"test\", \"qubole\"),\n  app_name = \"sparklyr\",\n  version = NULL,\n  config = spark_config(),\n  extensions = sparklyr::registered_extensions(),\n  packages = NULL,\n  scala_version = NULL,\n  ...\n)\nspark_connection_is_open(sc)\nspark_disconnect(sc, ...)\nspark_disconnect_all(...)\nspark_submit(\n  master,\n  file,\n  spark_home = Sys.getenv(\"SPARK_HOME\"),\n  app_name = \"sparklyr\",\n  version = NULL,\n  config = spark_config(),\n  extensions = sparklyr::registered_extensions(),\n  scala_version = NULL,\n  ...\n)"
  },
  {
    "href": "reference/spark-connections.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nmaster\nSpark cluster url to connect to. Use \"local\" to connect to a local instance of Spark installed via spark_install .\n\n\nspark_home\nThe path to a Spark installation. Defaults to the path provided by the SPARK_HOME environment variable. If SPARK_HOME is defined, it will always be used unless the version parameter is specified to force the use of a locally installed version.\n\n\nmethod\nThe method used to connect to Spark. Default connection method is \"shell\" to connect using spark-submit, use \"livy\" to perform remote connections using HTTP, or \"databricks\" when using a Databricks clusters.\n\n\napp_name\nThe application name to be used while running in the Spark cluster.\n\n\nversion\nThe version of Spark to use. Required for \"local\" Spark connections, optional otherwise.\n\n\nconfig\nCustom configuration for the generated Spark connection. See spark_config for details.\n\n\nextensions\nExtension R packages to enable for this connection. By default, all packages enabled through the use of sparklyr::register_extension will be passed here.\n\n\npackages\nA list of Spark packages to load. For example, \"delta\" or \"kafka\" to enable Delta Lake or Kafka. Also supports full versions like \"io.delta:delta-core_2.11:0.4.0\" . This is similar to adding packages into the sparklyr.shell.packages configuration option. Notice that the version parameter is used to choose the correct package, otherwise assumes the latest version is being used.\n\n\nscala_version\nLoad the sparklyr jar file that is built with the version of Scala specified (this currently only makes sense for Spark 2.4, where sparklyr will by default assume Spark 2.4 on current host is built with Scala 2.11, and therefore scala_version = '2.12' is needed if sparklyr is connecting to Spark 2.4 built with Scala 2.12)\n\n\n...\nAdditional params to be passed to each spark_disconnect() call (e.g., terminate = TRUE)\n\n\nsc\nA spark_connection .\n\n\nfile\nPath to R source file to submit for batch execution."
  },
  {
    "href": "reference/spark-connections.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "By default, when using method = \"livy\" , jars are downloaded from GitHub. But an alternative path (local to Livy server or on HDFS or HTTP(s)) to sparklyr JAR can also be specified through the sparklyr.livy.jar setting."
  },
  {
    "href": "reference/spark-connections.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "conf <- spark_config()\nconf$`sparklyr.shell.conf` <- c(\n\"spark.executor.extraJavaOptions=-Duser.timezone='UTC'\",\n\"spark.driver.extraJavaOptions=-Duser.timezone='UTC'\",\n\"spark.sql.session.timeZone='UTC'\"\n)\n\nsc <- spark_connect(\nmaster = \"spark://HOST:PORT\", config = conf\n)\nconnection_is_open(sc)\n\nspark_disconnect(sc)"
  },
  {
    "href": "reference/sdf_weighted_sample.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Draw a random sample of rows (with or without replacement) from a Spark DataFrame If the sampling is done without replacement, then it will be conceptually equivalent to an iterative process such that in each step the probability of adding a row to the sample set is equal to its weight divided by summation of weights of all rows that are not in the sample set yet in that step."
  },
  {
    "href": "reference/sdf_weighted_sample.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_weighted_sample(x, weight_col, k, replacement = TRUE, seed = NULL)"
  },
  {
    "href": "reference/sdf_weighted_sample.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\nweight_col\nName of the weight column\n\n\nk\nSample set size\n\n\nreplacement\nWhether to sample with replacement\n\n\nseed\nAn (optional) integer seed"
  },
  {
    "href": "reference/sdf_weighted_sample.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark data frames: sdf_copy_to , sdf_distinct , sdf_random_split , sdf_register , sdf_sample , sdf_sort"
  },
  {
    "href": "reference/spark_default_version.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "determine the version that will be used by default if version is NULL"
  },
  {
    "href": "reference/spark_default_version.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_default_version()"
  },
  {
    "href": "reference/spark_config.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Read Spark Configuration"
  },
  {
    "href": "reference/spark_config.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_config(file = \"config.yml\", use_default = TRUE)"
  },
  {
    "href": "reference/spark_config.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nfile\nName of the configuration file\n\n\nuse_default\nTRUE to use the built-in defaults provided in this package"
  },
  {
    "href": "reference/spark_config.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Read Spark configuration using the list(“config”) package."
  },
  {
    "href": "reference/spark_config.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Named list with configuration data"
  },
  {
    "href": "reference/ml_uid.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Extracts the UID of an ML object."
  },
  {
    "href": "reference/ml_uid.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_uid(x)"
  },
  {
    "href": "reference/ml_uid.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark ML object"
  },
  {
    "href": "reference/ml_standardize_formula.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generates a formula string from user inputs, to be used in ml_model constructor."
  },
  {
    "href": "reference/ml_standardize_formula.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_standardize_formula(formula = NULL, response = NULL, features = NULL)"
  },
  {
    "href": "reference/ml_standardize_formula.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nformula\nThe formula argument.\n\n\nresponse\nThe response argument.\n\n\nfeatures\nThe features argument."
  },
  {
    "href": "reference/unite.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "See unite for more details."
  },
  {
    "href": "reference/spark_read_text.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Read a text file into a Spark DataFrame."
  },
  {
    "href": "reference/spark_read_text.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_read_text(\n  sc,\n  name = NULL,\n  path = name,\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE,\n  options = list(),\n  whole = FALSE,\n  ...\n)"
  },
  {
    "href": "reference/spark_read_text.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?\n\n\noptions\nA list of strings with additional options.\n\n\nwhole\nRead the entire text file as a single entry? Defaults to FALSE .\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/spark_read_text.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "You can read data from HDFS ( hdfs:// ), S3 ( s3a:// ), as well as the local file system ( file:// ).\nIf you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf spark.hadoop.fs.s3a.access.key , spark.hadoop.fs.s3a.secret.key or any of the methods outlined in the aws-sdk documentation Working with AWS credentials In order to work with the newer s3a:// protocol also set the values for spark.hadoop.fs.s3a.impl and spark.hadoop.fs.s3a.endpoint . In addition, to support v4 of the S3 api be sure to pass the -Dcom.amazonaws.services.s3.enableV4 driver options for the config key spark.driver.extraJavaOptions For instructions on how to configure s3n:// check the hadoop documentation: s3n authentication properties"
  },
  {
    "href": "reference/spark_read_text.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/sdf_unnest_wider.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Flatten a struct column within a Spark dataframe into one or more columns, similar what to tidyr::unnest_wider does to an R dataframe"
  },
  {
    "href": "reference/sdf_unnest_wider.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_unnest_wider(\n  data,\n  col,\n  names_sep = NULL,\n  names_repair = \"check_unique\",\n  ptype = list(),\n  transform = list()\n)"
  },
  {
    "href": "reference/sdf_unnest_wider.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\ndata\nThe Spark dataframe to be unnested\n\n\ncol\nThe struct column to extract components from\n\n\nnames_sep\nIf NULL, the default, the names will be left as is. If a string, the inner and outer names will be pasted together using names_sep as the delimiter.\n\n\nnames_repair\nStrategy for fixing duplicate column names (the semantic will be exactly identical to that of .name_repair option in tibble )\n\n\nptype\nOptionally, supply an R data frame prototype for the output. Each column of the unnested result will be casted based on the Spark equivalent of the type of the column with the same name within ptype, e.g., if ptype has a column x of type character, then column x of the unnested result will be casted from its original SQL type to StringType.\n\n\ntransform\nOptionally, a named list of transformation functions applied to each component (e.g., list(x = as.character) to cast column x to String)."
  },
  {
    "href": "reference/sdf_unnest_wider.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.4.0\")\n\nsdf <- copy_to(\nsc,\ntibble::tibble(\nx = 1:3,\ny = list(list(a = 1, b = 2), list(a = 3, b = 4), list(a = 5, b = 6))\n)\n)\n\n# flatten struct column 'y' into two separate columns 'y_a' and 'y_b'\nunnested <- sdf %>% sdf_unnest_wider(y, names_sep = \"_\")"
  },
  {
    "href": "reference/src_databases.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Show database list"
  },
  {
    "href": "reference/src_databases.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "src_databases(sc, ...)"
  },
  {
    "href": "reference/src_databases.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/worker_spark_apply_unbundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Extracts a bundle of dependencies required by spark_apply()"
  },
  {
    "href": "reference/worker_spark_apply_unbundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "worker_spark_apply_unbundle(bundle_path, base_path, bundle_name)"
  },
  {
    "href": "reference/worker_spark_apply_unbundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nbundle_path\nPath to the bundle created using spark_apply_bundle()\n\n\nbase_path\nBase path to use while extracting bundles"
  },
  {
    "href": "reference/jarray.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Given a list of Java object references, instantiate an Array[T] containing the same list of references, where T is a non-primitive type that is more specific than java.lang.Object ."
  },
  {
    "href": "reference/jarray.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "jarray(sc, x, element_type)"
  },
  {
    "href": "reference/jarray.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nx\nA list of Java object references.\n\n\nelement_type\nA valid Java class name representing the generic type parameter of the Java array to be instantiated. Each element of x must refer to a Java object that is assignable to element_type ."
  },
  {
    "href": "reference/jarray.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"spark://HOST:PORT\")\n\nstring_arr <- jarray(sc, letters, element_type = \"java.lang.String\")\n# string_arr is now a reference to an array of type String[]"
  },
  {
    "href": "reference/spark_write_avro.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Serialize a Spark DataFrame into Apache Avro format. Notice this functionality requires the Spark connection sc to be instantiated with either an explicitly specified Spark version (i.e., spark_connect(..., version = <version>, packages = c(\"avro\", <other package(s)>), ...) ) or a specific version of Spark avro package to use (e.g., spark_connect(..., packages = c(\"org.apache.spark:spark-avro_2.12:3.0.0\", <other package(s)>), ...) )."
  },
  {
    "href": "reference/spark_write_avro.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_write_avro(\n  x,\n  path,\n  avro_schema = NULL,\n  record_name = \"topLevelRecord\",\n  record_namespace = \"\",\n  compression = \"snappy\",\n  partition_by = NULL\n)"
  },
  {
    "href": "reference/spark_write_avro.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark DataFrame or dplyr operation\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://” , “s3a://” and “file://” protocols.\n\n\navro_schema\nOptional Avro schema in JSON format\n\n\nrecord_name\nOptional top level record name in write result (default: “topLevelRecord”)\n\n\nrecord_namespace\nRecord namespace in write result (default: ““)\n\n\ncompression\nCompression codec to use (default: “snappy”)\n\n\npartition_by\nA character vector. Partitions the output by the given columns on the file system."
  },
  {
    "href": "reference/spark_write_avro.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_binary , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/spark_write.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Run a custom R function on Spark worker to write a Spark DataFrame into file(s). If Spark’s speculative execution feature is enabled (i.e., spark.speculation is true), then each write task may be executed more than once and the user-defined writer function will need to ensure no concurrent writes happen to the same file path (e.g., by appending UUID to each file name)."
  },
  {
    "href": "reference/spark_write.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_write(x, writer, paths, packages = NULL)"
  },
  {
    "href": "reference/spark_write.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA Spark Dataframe to be saved into file(s)\n\n\nwriter\nA writer function with the signature function(partition, path) where partition is a R dataframe containing all rows from one partition of the original Spark Dataframe x and path is a string specifying the file to write partition to\n\n\npaths\nA single destination path or a list of destination paths, each one specifying a location for a partition from x to be written to. If number of partition(s) in x is not equal to length(paths) then x will be re-partitioned to contain length(paths) partition(s)\n\n\npackages\nBoolean to distribute .libPaths() packages to each node, a list of packages to distribute, or a package bundle created with"
  },
  {
    "href": "reference/spark_write.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\n\nsc <- spark_connect(master = \"local[3]\")\n\n# copy some test data into a Spark Dataframe\nsdf <- sdf_copy_to(sc, iris, overwrite = TRUE)\n\n# create a writer function\nwriter <- function(df, path) {\nwrite.csv(df, path)\n}\n\nspark_write(\nsdf,\nwriter,\n# re-partition sdf into 3 partitions and write them to 3 separate files\npaths = list(\"file:///tmp/file1\", \"file:///tmp/file2\", \"file:///tmp/file3\"),\n)\n\nspark_write(\nsdf,\nwriter,\n# save all rows into a single file\npaths = list(\"file:///tmp/all_rows\")\n)"
  },
  {
    "href": "reference/ft_standard_scaler.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Standardizes features by removing the mean and scaling to unit variance using column summary statistics on the samples in the training set. The “unit std” is computed using the corrected sample standard deviation, which is computed as the square root of the unbiased sample variance."
  },
  {
    "href": "reference/ft_standard_scaler.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_standard_scaler(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  with_mean = FALSE,\n  with_std = TRUE,\n  uid = random_string(\"standard_scaler_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_standard_scaler.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nwith_mean\nWhether to center the data with mean before scaling. It will build a dense output, so take care when applying to sparse input. Default: FALSE\n\n\nwith_std\nWhether to scale the data to unit standard deviation. Default: TRUE\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_standard_scaler.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "In the case where x is a tbl_spark , the estimator fits against x to obtain a transformer, which is then immediately used to transform x , returning a tbl_spark ."
  },
  {
    "href": "reference/ft_standard_scaler.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_standard_scaler.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/ft_standard_scaler.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\nfeatures <- c(\"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\")\n\niris_tbl %>%\nft_vector_assembler(\ninput_col = features,\noutput_col = \"features_temp\"\n) %>%\nft_standard_scaler(\ninput_col = \"features_temp\",\noutput_col = \"features\",\nwith_mean = TRUE\n)"
  },
  {
    "href": "reference/ml_logistic_regression.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Perform classification using logistic regression."
  },
  {
    "href": "reference/ml_logistic_regression.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_logistic_regression(\n  x,\n  formula = NULL,\n  fit_intercept = TRUE,\n  elastic_net_param = 0,\n  reg_param = 0,\n  max_iter = 100,\n  threshold = 0.5,\n  thresholds = NULL,\n  tol = 1e-06,\n  weight_col = NULL,\n  aggregation_depth = 2,\n  lower_bounds_on_coefficients = NULL,\n  lower_bounds_on_intercepts = NULL,\n  upper_bounds_on_coefficients = NULL,\n  upper_bounds_on_intercepts = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  family = \"auto\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"logistic_regression_\"),\n  ...\n)"
  },
  {
    "href": "reference/ml_logistic_regression.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nUsed when x is a tbl_spark . R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nfit_intercept\nBoolean; should the model be fit with an intercept term?\n\n\nelastic_net_param\nElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n\n\nreg_param\nRegularization parameter (aka lambda)\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nthreshold\nin binary classification prediction, in range [0, 1].\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\ntol\nParam for the convergence tolerance for iterative algorithms.\n\n\nweight_col\nThe name of the column to use as weights for the model fit.\n\n\naggregation_depth\n(Spark 2.1.0+) Suggested depth for treeAggregate (>= 2).\n\n\nlower_bounds_on_coefficients\n(Spark 2.2.0+) Lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression.\n\n\nlower_bounds_on_intercepts\n(Spark 2.2.0+) Lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression.\n\n\nupper_bounds_on_coefficients\n(Spark 2.2.0+) Upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression.\n\n\nupper_bounds_on_intercepts\n(Spark 2.2.0+) Upper bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula .\n\n\nfamily\n(Spark 2.1.0+) Param for the name of family which is a description of the label distribution to be used in the model. Supported options: “auto”, “binomial”, and “multinomial.”\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; see Details."
  },
  {
    "href": "reference/ml_logistic_regression.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "When x is a tbl_spark and formula (alternatively, response and features ) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\" ) can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model , ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "href": "reference/ml_logistic_regression.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a predictor is constructed then immediately fit with the input tbl_spark , returning a prediction model.\ntbl_spark , with formula : specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model ."
  },
  {
    "href": "reference/ml_logistic_regression.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms.\nOther ml algorithms: ml_aft_survival_regression , ml_decision_tree_classifier , ml_gbt_classifier , ml_generalized_linear_regression , ml_isotonic_regression , ml_linear_regression , ml_linear_svc , ml_multilayer_perceptron_classifier , ml_naive_bayes , ml_one_vs_rest , ml_random_forest_classifier"
  },
  {
    "href": "reference/ml_logistic_regression.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, name = \"mtcars_tbl\", overwrite = TRUE)\n\npartitions <- mtcars_tbl %>%\nsdf_random_split(training = 0.7, test = 0.3, seed = 1111)\n\nmtcars_training <- partitions$training\nmtcars_test <- partitions$test\n\nlr_model <- mtcars_training %>%\nml_logistic_regression(am ~ gear + carb)\n\npred <- ml_predict(lr_model, mtcars_test)\n\nml_binary_classification_evaluator(pred)"
  },
  {
    "href": "reference/ft_vector_slicer.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Takes a feature vector and outputs a new feature vector with a subarray of the original features."
  },
  {
    "href": "reference/ft_vector_slicer.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_vector_slicer(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  indices = NULL,\n  uid = random_string(\"vector_slicer_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_vector_slicer.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nindices\nAn vector of indices to select features from a vector column. Note that the indices are 0-based.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_vector_slicer.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_vector_slicer.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_word2vec"
  },
  {
    "href": "reference/registerDoSpark.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Registers a parallel backend using the foreach package."
  },
  {
    "href": "reference/registerDoSpark.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "registerDoSpark(spark_conn, parallelism = NULL, ...)"
  },
  {
    "href": "reference/registerDoSpark.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nspark_conn\nSpark connection to use\n\n\nparallelism\nLevel of parallelism to use for task execution (if unspecified, then it will take the value of SparkContext.defaultParallelism() which by default is the number of cores available to the sparklyr application)\n\n\n...\nadditional options for sparklyr parallel backend (currently only the only valid option is nocompile = T, F )"
  },
  {
    "href": "reference/registerDoSpark.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "None"
  },
  {
    "href": "reference/registerDoSpark.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\nregisterDoSpark(sc, nocompile = FALSE)"
  },
  {
    "href": "reference/jobj_set_param.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Sets a parameter value for a pipeline stage object."
  },
  {
    "href": "reference/jobj_set_param.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "jobj_set_param(jobj, setter, value, min_version = NULL, default = NULL)"
  },
  {
    "href": "reference/jobj_set_param.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\njobj\nA pipeline stage jobj.\n\n\nsetter\nThe name of the setter method as a string.\n\n\nvalue\nThe value to be set.\n\n\nmin_version\nThe minimum required Spark version for this parameter to be valid.\n\n\ndefault\nThe default value of the parameter, to be used together with min_version. An error is thrown if the user’s Spark version is older than min_version and value differs from default."
  },
  {
    "href": "reference/sdf_broadcast.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Used to force broadcast hash joins."
  },
  {
    "href": "reference/sdf_broadcast.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_broadcast(x)"
  },
  {
    "href": "reference/sdf_broadcast.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark ."
  },
  {
    "href": "reference/sdf_rpois.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Poisson distribution."
  },
  {
    "href": "reference/sdf_rpois.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_rpois(sc, n, lambda, num_partitions = NULL, seed = NULL, output_col = \"x\")"
  },
  {
    "href": "reference/sdf_rpois.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nlambda\nMean, or lambda, of the Poisson distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "href": "reference/sdf_rpois.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark statistical routines: sdf_rbeta , sdf_rbinom , sdf_rcauchy , sdf_rchisq , sdf_rexp , sdf_rgamma , sdf_rgeom , sdf_rhyper , sdf_rlnorm , sdf_rnorm , sdf_rt , sdf_runif , sdf_rweibull"
  },
  {
    "href": "reference/spark_config_kubernetes.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Convenience function to initialize a Kubernetes configuration instead of spark_config() , exposes common properties to set in Kubernetes clusters."
  },
  {
    "href": "reference/spark_config_kubernetes.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_config_kubernetes(\n  master,\n  version = \"2.3.2\",\n  image = \"spark:sparklyr\",\n  driver = random_string(\"sparklyr-\"),\n  account = \"spark\",\n  jars = \"local:///opt/sparklyr\",\n  forward = TRUE,\n  executors = NULL,\n  conf = NULL,\n  timeout = 120,\n  ports = c(8880, 8881, 4040),\n  fix_config = identical(.Platform$OS.type, \"windows\"),\n  ...\n)"
  },
  {
    "href": "reference/spark_config_kubernetes.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nmaster\nKubernetes url to connect to, found by running kubectl cluster-info .\n\n\nversion\nThe version of Spark being used.\n\n\nimage\nContainer image to use to launch Spark and sparklyr. Also known as spark.kubernetes.container.image .\n\n\ndriver\nName of the driver pod. If not set, the driver pod name is set to “sparklyr” suffixed by id to avoid name conflicts. Also known as spark.kubernetes.driver.pod.name .\n\n\naccount\nService account that is used when running the driver pod. The driver pod uses this service account when requesting executor pods from the API server. Also known as spark.kubernetes.authenticate.driver.serviceAccountName .\n\n\njars\nPath to the sparklyr jars; either, a local path inside the container image with the sparklyr jars copied when the image was created or, a path accesible by the container where the sparklyr jars were copied. You can find a path to the sparklyr jars by running system.file(\"java/\", package = \"sparklyr\") .\n\n\nforward\nShould ports used in sparklyr be forwarded automatically through Kubernetes? Default to TRUE which runs kubectl port-forward and pkill kubectl on disconnection.\n\n\nexecutors\nNumber of executors to request while connecting.\n\n\nconf\nA named list of additional entries to add to sparklyr.shell.conf .\n\n\ntimeout\nTotal seconds to wait before giving up on connection.\n\n\nports\nPorts to forward using kubectl.\n\n\nfix_config\nShould the spark-defaults.conf get fixed? TRUE for Windows.\n\n\n...\nAdditional parameters, currently not in use."
  },
  {
    "href": "reference/hof_transform.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Apply an element-wise transformation function to an array column (this is essentially a dplyr wrapper for the transform(array<T>, function<T, U>): array<U> and the transform(array<T>, function<T, Int, U>): array<U> built-in Spark SQL functions)"
  },
  {
    "href": "reference/hof_transform.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "hof_transform(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "href": "reference/hof_transform.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nThe Spark data frame to transform\n\n\nfunc\nThe transformation to apply\n\n\nexpr\nThe array being transformed, could be any SQL expression evaluating to an array (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the transformed result (default: expr)\n\n\n...\nAdditional params to dplyr::mutate"
  },
  {
    "href": "reference/hof_transform.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\")\n# applies the (x -> x * x) transformation to elements of all arrays\ncopy_to(sc, tibble::tibble(arr = list(1:5, 21:25))) %>%\nhof_transform(~ .x * .x)"
  },
  {
    "href": "reference/ensure.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "These routines are useful when preparing to pass objects to a Spark routine, as it is often necessary to ensure certain parameters are scalar integers, or scalar doubles, and so on."
  },
  {
    "href": "reference/ensure.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nobject\nAn list() object.\n\n\nallow.na\nAre NA values permitted for this object?\n\n\nallow.null\nAre NULL values permitted for this object?\n\n\ndefault\nIf object is NULL , what value should be used in its place? If default is specified, allow.null is ignored (and assumed to be TRUE )."
  },
  {
    "href": "reference/spark_home_dir.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Find the SPARK_HOME directory for a given version of Spark that was previously installed using spark_install ."
  },
  {
    "href": "reference/spark_home_dir.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_home_dir(version = NULL, hadoop_version = NULL)"
  },
  {
    "href": "reference/spark_home_dir.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nversion\nVersion of Spark\n\n\nhadoop_version\nVersion of Hadoop"
  },
  {
    "href": "reference/spark_home_dir.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Path to SPARK_HOME (or NULL if the specified version was not found)."
  },
  {
    "href": "reference/ml_isotonic_regression_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "These methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "href": "reference/ml_isotonic_regression_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "list(list(\"tidy\"), list(\"ml_model_isotonic_regression\"))(x, ...)\nlist(list(\"augment\"), list(\"ml_model_isotonic_regression\"))(x, newdata = NULL, ...)\nlist(list(\"glance\"), list(\"ml_model_isotonic_regression\"))(x, ...)"
  },
  {
    "href": "reference/ml_isotonic_regression_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n...\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  },
  {
    "href": "reference/ft_ngram.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "A feature transformer that converts the input array of strings into an array of n-grams. Null values in the input array are ignored. It returns an array of n-grams where each n-gram is represented by a space-separated string of words."
  },
  {
    "href": "reference/ft_ngram.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_ngram(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  n = 2,\n  uid = random_string(\"ngram_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_ngram.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nn\nMinimum n-gram length, greater than or equal to 1. Default: 2, bigram features\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_ngram.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "When the input is empty, an empty array is returned. When the input array length is less than n (number of elements per n-gram), no n-grams are returned."
  },
  {
    "href": "reference/ft_ngram.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_ngram.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/stream_find.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Finds and returns a stream based on the stream’s identifier."
  },
  {
    "href": "reference/stream_find.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "stream_find(sc, id)"
  },
  {
    "href": "reference/stream_find.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nid\nThe stream identifier to find."
  },
  {
    "href": "reference/stream_find.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\nsdf_len(sc, 10) %>%\nspark_write_parquet(path = \"parquet-in\")\n\nstream <- stream_read_parquet(sc, \"parquet-in\") %>%\nstream_write_parquet(\"parquet-out\")\n\nstream_id <- stream_id(stream)\nstream_find(sc, stream_id)"
  },
  {
    "href": "reference/spark_connection_find.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Finds an active spark connection in the environment given the connection parameters."
  },
  {
    "href": "reference/spark_connection_find.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_connection_find(master = NULL, app_name = NULL, method = NULL)"
  },
  {
    "href": "reference/spark_connection_find.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nmaster\nThe Spark master parameter.\n\n\napp_name\nThe Spark application name.\n\n\nmethod\nThe method used to connect to Spark."
  },
  {
    "href": "reference/ft_index_to_string.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "A Transformer that maps a column of indices back to a new column of corresponding string values. The index-string mapping is either from the ML attributes of the input column, or from user-supplied labels (which take precedence over ML attributes). This function is the inverse of ft_string_indexer ."
  },
  {
    "href": "reference/ft_index_to_string.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_index_to_string(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  labels = NULL,\n  uid = random_string(\"index_to_string_\"),\n  ...\n)"
  },
  {
    "href": "reference/ft_index_to_string.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nlabels\nOptional param for array of labels specifying index-string mapping.\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused."
  },
  {
    "href": "reference/ft_index_to_string.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_index_to_string.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nft_string_indexer\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_pca , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/generic_call_interface.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generic Call Interface"
  },
  {
    "href": "reference/generic_call_interface.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nspark_connection\n\n\nstatic\nIs this a static method call (including a constructor). If so then the object parameter should be the name of a class (otherwise it should be a spark_jobj instance).\n\n\nobject\nObject instance or name of class (for static )\n\n\nmethod\nName of method\n\n\n...\nCall parameters"
  },
  {
    "href": "reference/na.replace.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "This S3 generic provides an interface for replacing NA values within an object."
  },
  {
    "href": "reference/na.replace.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "na.replace(object, ...)"
  },
  {
    "href": "reference/na.replace.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nobject\nAn list() object.\n\n\n...\nArguments passed along to implementing methods."
  },
  {
    "href": "reference/ml-transform-methods.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Methods for transformation, fit, and prediction. These are mirrors of the corresponding sdf-transform-methods ."
  },
  {
    "href": "reference/ml-transform-methods.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "is_ml_transformer(x)\nis_ml_estimator(x)\nml_fit(x, dataset, ...)\nml_transform(x, dataset, ...)\nml_fit_and_transform(x, dataset, ...)\nml_predict(x, dataset, ...)\nlist(list(\"ml_predict\"), list(\"ml_model_classification\"))(x, dataset, probability_prefix = \"probability_\", ...)"
  },
  {
    "href": "reference/ml-transform-methods.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA ml_estimator , ml_transformer (or a list thereof), or ml_model object.\n\n\ndataset\nA tbl_spark .\n\n\n...\nOptional arguments; currently unused.\n\n\nprobability_prefix\nString used to prepend the class probability output columns."
  },
  {
    "href": "reference/ml-transform-methods.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "These methods are"
  },
  {
    "href": "reference/ml-transform-methods.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "When x is an estimator, ml_fit() returns a transformer whereas ml_fit_and_transform() returns a transformed dataset. When x is a transformer, ml_transform() and ml_predict() return a transformed dataset. When ml_predict() is called on a ml_model object, additional columns (e.g. probabilities in case of classification models) are appended to the transformed output for the user’s convenience."
  },
  {
    "href": "reference/ml-constructors.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Functions for developers writing extensions for Spark ML."
  },
  {
    "href": "reference/ml-constructors.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "new_ml_transformer(jobj, ..., class = character())\nnew_ml_prediction_model(jobj, ..., class = character())\nnew_ml_classification_model(jobj, ..., class = character())\nnew_ml_probabilistic_classification_model(jobj, ..., class = character())\nnew_ml_clustering_model(jobj, ..., class = character())\nnew_ml_estimator(jobj, ..., class = character())\nnew_ml_predictor(jobj, ..., class = character())\nnew_ml_classifier(jobj, ..., class = character())\nnew_ml_probabilistic_classifier(jobj, ..., class = character())"
  },
  {
    "href": "reference/ml-constructors.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\njobj\nPointer to the pipeline stage object.\n\n\n...\n(Optional) additional attributes of the object.\n\n\nclass\nName of class."
  },
  {
    "href": "reference/tbl_uncache.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Force a Spark table with name name to be unloaded from memory."
  },
  {
    "href": "reference/tbl_uncache.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "tbl_uncache(sc, name)"
  },
  {
    "href": "reference/tbl_uncache.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe table name."
  },
  {
    "href": "reference/spark_config_settings.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Retrieves available sparklyr settings that can be used in configuration files or spark_config() ."
  },
  {
    "href": "reference/spark_config_settings.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_config_settings()"
  },
  {
    "href": "reference/sdf_fast_bind_cols.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "This is a version of sdf_bind_cols that works by zipping RDDs. From the API docs: “Assumes that the two RDDs have the same number of partitions and the same number of elements in each partition (e.g. one was made through a map on the other).”"
  },
  {
    "href": "reference/sdf_fast_bind_cols.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_fast_bind_cols(...)"
  },
  {
    "href": "reference/sdf_fast_bind_cols.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\n...\nSpark DataFrames to cbind"
  },
  {
    "href": "reference/sdf_rgamma.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Gamma distribution."
  },
  {
    "href": "reference/sdf_rgamma.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_rgamma(\n  sc,\n  n,\n  shape,\n  rate = 1,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "href": "reference/sdf_rgamma.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nshape\nShape parameter (greater than 0) for the Gamma distribution.\n\n\nrate\nRate parameter (greater than 0) for the Gamma distribution (scale is 1/rate).\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "href": "reference/sdf_rgamma.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark statistical routines: sdf_rbeta , sdf_rbinom , sdf_rcauchy , sdf_rchisq , sdf_rexp , sdf_rgeom , sdf_rhyper , sdf_rlnorm , sdf_rnorm , sdf_rpois , sdf_rt , sdf_runif , sdf_rweibull"
  },
  {
    "href": "reference/sdf_rcauchy.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Cauchy distribution."
  },
  {
    "href": "reference/sdf_rcauchy.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_rcauchy(\n  sc,\n  n,\n  location = 0,\n  scale = 1,\n  num_partitions = NULL,\n  seed = NULL,\n  output_col = \"x\"\n)"
  },
  {
    "href": "reference/sdf_rcauchy.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSample Size (default: 1000).\n\n\nlocation\nLocation parameter of the distribution.\n\n\nscale\nScale parameter of the distribution.\n\n\nnum_partitions\nNumber of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster).\n\n\nseed\nRandom seed (default: a random long integer).\n\n\noutput_col\nName of the output column containing sample values (default: “x”)."
  },
  {
    "href": "reference/sdf_rcauchy.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark statistical routines: sdf_rbeta , sdf_rbinom , sdf_rchisq , sdf_rexp , sdf_rgamma , sdf_rgeom , sdf_rhyper , sdf_rlnorm , sdf_rnorm , sdf_rpois , sdf_rt , sdf_runif , sdf_rweibull"
  },
  {
    "href": "reference/ft_pca.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "PCA trains a model to project vectors to a lower dimensional space of the top k principal components."
  },
  {
    "href": "reference/ft_pca.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ft_pca(\n  x,\n  input_col = NULL,\n  output_col = NULL,\n  k = NULL,\n  uid = random_string(\"pca_\"),\n  ...\n)\nml_pca(x, features = tbl_vars(x), k = length(features), pc_prefix = \"PC\", ...)"
  },
  {
    "href": "reference/ft_pca.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\ninput_col\nThe name of the input column.\n\n\noutput_col\nThe name of the output column.\n\n\nk\nThe number of principal components\n\n\nuid\nA character string used to uniquely identify the feature transformer.\n\n\n...\nOptional arguments; currently unused.\n\n\nfeatures\nThe columns to use in the principal components analysis. Defaults to all columns in x .\n\n\npc_prefix\nLength-one character vector used to prepend names of components."
  },
  {
    "href": "reference/ft_pca.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "In the case where x is a tbl_spark , the estimator fits against x to obtain a transformer, which is then immediately used to transform x , returning a tbl_spark .\nml_pca() is a wrapper around ft_pca() that returns a ml_model ."
  },
  {
    "href": "reference/ft_pca.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns a ml_transformer , a ml_estimator , or one of their subclasses. The object contains a pointer to a Spark Transformer or Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the transformer or estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a transformer is constructed then immediately applied to the input tbl_spark , returning a tbl_spark"
  },
  {
    "href": "reference/ft_pca.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-features.html for more information on the set of transformations available for DataFrame columns in Spark.\nOther feature transformers: ft_binarizer , ft_bucketizer , ft_chisq_selector , ft_count_vectorizer , ft_dct , ft_elementwise_product , ft_feature_hasher , ft_hashing_tf , ft_idf , ft_imputer , ft_index_to_string , ft_interaction , ft_lsh , ft_max_abs_scaler , ft_min_max_scaler , ft_ngram , ft_normalizer , ft_one_hot_encoder_estimator , ft_one_hot_encoder , ft_polynomial_expansion , ft_quantile_discretizer , ft_r_formula , ft_regex_tokenizer , ft_robust_scaler , ft_sql_transformer , ft_standard_scaler , ft_stop_words_remover , ft_string_indexer , ft_tokenizer , ft_vector_assembler , ft_vector_indexer , ft_vector_slicer , ft_word2vec"
  },
  {
    "href": "reference/ft_pca.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(dplyr)\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\niris_tbl %>%\nselect(-Species) %>%\nml_pca(k = 2)"
  },
  {
    "href": "reference/hof_transform_keys.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Applies the transformation function specified to all keys of a map (this is essentially a dplyr wrapper to the transform_keys(expr, func) higher- order function, which is supported since Spark 3.0)"
  },
  {
    "href": "reference/hof_transform_keys.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "hof_transform_keys(x, func, expr = NULL, dest_col = NULL, ...)"
  },
  {
    "href": "reference/hof_transform_keys.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nThe Spark data frame to be processed\n\n\nfunc\nThe transformation function to apply (it should take (key, value) as arguments and return a transformed key)\n\n\nexpr\nThe map being transformed, could be any SQL expression evaluating to a map (default: the last column of the Spark data frame)\n\n\ndest_col\nColumn to store the transformed result (default: expr)\n\n\n...\nAdditional params to dplyr::mutate"
  },
  {
    "href": "reference/hof_transform_keys.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"3.0.0\")\nsdf <- sdf_len(sc, 1) %>% dplyr::mutate(m = map(\"a\", 0L, \"b\", 2L, \"c\", -1L))\ntransformed_sdf <- sdf %>% hof_transform_keys(~ CONCAT(.x, \" == \", .y))"
  },
  {
    "href": "reference/ml_bisecting_kmeans.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "A bisecting k-means algorithm based on the paper “A comparison of document clustering techniques” by Steinbach, Karypis, and Kumar, with modification to fit Spark. The algorithm starts from a single cluster that contains all points. Iteratively it finds divisible clusters on the bottom level and bisects each of them using k-means, until there are k leaf clusters in total or no leaf clusters are divisible. The bisecting steps of clusters on the same level are grouped together to increase parallelism. If bisecting all divisible clusters on the bottom level would result more than k leaf clusters, larger clusters get higher priority."
  },
  {
    "href": "reference/ml_bisecting_kmeans.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_bisecting_kmeans(\n  x,\n  formula = NULL,\n  k = 4,\n  max_iter = 20,\n  seed = NULL,\n  min_divisible_cluster_size = 1,\n  features_col = \"features\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"bisecting_bisecting_kmeans_\"),\n  ...\n)"
  },
  {
    "href": "reference/ml_bisecting_kmeans.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nformula\nUsed when x is a tbl_spark . R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\nk\nThe number of clusters to create\n\n\nmax_iter\nThe maximum number of iterations to use.\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\nmin_divisible_cluster_size\nThe minimum number of points (if greater than or equal to 1.0) or the minimum proportion of points (if less than 1.0) of a divisible cluster (default: 1.0).\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula .\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments, see Details."
  },
  {
    "href": "reference/ml_bisecting_kmeans.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Estimator object and can be used to compose Pipeline objects.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the clustering estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , an estimator is constructed then immediately fit with the input tbl_spark , returning a clustering model.\ntbl_spark , with formula or features specified: When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the estimator. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model . This signature does not apply to ml_lda() ."
  },
  {
    "href": "reference/ml_bisecting_kmeans.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "See http://spark.apache.org/docs/latest/ml-clustering.html for more information on the set of clustering algorithms.\nOther ml clustering algorithms: ml_gaussian_mixture , ml_kmeans , ml_lda"
  },
  {
    "href": "reference/ml_bisecting_kmeans.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "library(dplyr)\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\niris_tbl %>%\nselect(-Species) %>%\nml_bisecting_kmeans(k = 4, Species ~ .)"
  },
  {
    "href": "reference/sdf_along.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Creates a DataFrame along the given object."
  },
  {
    "href": "reference/sdf_along.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_along(sc, along, repartition = NULL, type = c(\"integer\", \"integer64\"))"
  },
  {
    "href": "reference/sdf_along.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nalong\nTakes the length from the length of this argument.\n\n\nrepartition\nThe number of partitions to use when distributing the data across the Spark cluster.\n\n\ntype\nThe data type to use for the index, either \"integer\" or \"integer64\" ."
  },
  {
    "href": "reference/connection_is_open.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Check whether the connection is open"
  },
  {
    "href": "reference/connection_is_open.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "connection_is_open(sc)"
  },
  {
    "href": "reference/connection_is_open.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nspark_connection"
  },
  {
    "href": "reference/invoke_method.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Generic Call Interface"
  },
  {
    "href": "reference/invoke_method.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "invoke_method(sc, static, object, method, ...)"
  },
  {
    "href": "reference/invoke_method.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nspark_connection\n\n\nstatic\nIs this a static method call (including a constructor). If so then the object parameter should be the name of a class (otherwise it should be a spark_jobj instance).\n\n\nobject\nObject instance or name of class (for static )\n\n\nmethod\nName of method\n\n\n...\nCall parameters"
  },
  {
    "href": "reference/spark_read_binary.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Read binary files within a directory and convert each file into a record within the resulting Spark dataframe. The output will be a Spark dataframe with the following columns and possibly partition columns:\n\npath: StringType\nmodificationTime: TimestampType\nlength: LongType\ncontent: BinaryType"
  },
  {
    "href": "reference/spark_read_binary.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "spark_read_binary(\n  sc,\n  name = NULL,\n  dir = name,\n  path_glob_filter = \"*\",\n  recursive_file_lookup = FALSE,\n  repartition = 0,\n  memory = TRUE,\n  overwrite = TRUE\n)"
  },
  {
    "href": "reference/spark_read_binary.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nA spark_connection .\n\n\nname\nThe name to assign to the newly generated table.\n\n\ndir\nDirectory to read binary files from.\n\n\npath_glob_filter\nGlob pattern of binary files to be loaded (e.g., “*.jpg”).\n\n\nrecursive_file_lookup\nIf FALSE (default), then partition discovery will be enabled (i.e., if a partition naming scheme is present, then partitions specified by subdirectory names such as “date=2019-07-01” will be created and files outside subdirectories following a partition naming scheme will be ignored). If TRUE, then all nested directories will be searched even if their names do not follow a partition naming scheme.\n\n\nrepartition\nThe number of partitions used to distribute the generated table. Use 0 (the default) to avoid partitioning.\n\n\nmemory\nBoolean; should the data be loaded eagerly into memory? (That is, should the table be cached?)\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?"
  },
  {
    "href": "reference/spark_read_binary.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark serialization routines: collect_from_rds , spark_load_table , spark_read_avro , spark_read_csv , spark_read_delta , spark_read_image , spark_read_jdbc , spark_read_json , spark_read_libsvm , spark_read_orc , spark_read_parquet , spark_read_source , spark_read_table , spark_read_text , spark_read , spark_save_table , spark_write_avro , spark_write_csv , spark_write_delta , spark_write_jdbc , spark_write_json , spark_write_orc , spark_write_parquet , spark_write_source , spark_write_table , spark_write_text"
  },
  {
    "href": "reference/ml-tuning.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Perform hyper-parameter tuning using either K-fold cross validation or train-validation split."
  },
  {
    "href": "reference/ml-tuning.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "ml_sub_models(model)\nml_validation_metrics(model)\nml_cross_validator(\n  x,\n  estimator = NULL,\n  estimator_param_maps = NULL,\n  evaluator = NULL,\n  num_folds = 3,\n  collect_sub_models = FALSE,\n  parallelism = 1,\n  seed = NULL,\n  uid = random_string(\"cross_validator_\"),\n  ...\n)\nml_train_validation_split(\n  x,\n  estimator = NULL,\n  estimator_param_maps = NULL,\n  evaluator = NULL,\n  train_ratio = 0.75,\n  collect_sub_models = FALSE,\n  parallelism = 1,\n  seed = NULL,\n  uid = random_string(\"train_validation_split_\"),\n  ...\n)"
  },
  {
    "href": "reference/ml-tuning.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nmodel\nA cross validation or train-validation-split model.\n\n\nx\nA spark_connection , ml_pipeline , or a tbl_spark .\n\n\nestimator\nA ml_estimator object.\n\n\nestimator_param_maps\nA named list of stages and hyper-parameter sets to tune. See details.\n\n\nevaluator\nA ml_evaluator object, see ml_evaluator .\n\n\nnum_folds\nNumber of folds for cross validation. Must be >= 2. Default: 3\n\n\ncollect_sub_models\nWhether to collect a list of sub-models trained during tuning. If set to FALSE , then only the single best sub-model will be available after fitting. If set to true, then all sub-models will be available. Warning: For large models, collecting all sub-models can cause OOMs on the Spark driver.\n\n\nparallelism\nThe number of threads to use when running parallel algorithms. Default is 1 for serial execution.\n\n\nseed\nA random seed. Set this value if you need your results to be reproducible across repeated calls.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n...\nOptional arguments; currently unused.\n\n\ntrain_ratio\nRatio between train and validation data. Must be between 0 and 1. Default: 0.75"
  },
  {
    "href": "reference/ml-tuning.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "ml_cross_validator() performs k-fold cross validation while ml_train_validation_split() performs tuning on one pair of train and validation datasets."
  },
  {
    "href": "reference/ml-tuning.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "The object returned depends on the class of x .\n\nspark_connection : When x is a spark_connection , the function returns an instance of a ml_cross_validator or ml_traing_validation_split object.\nml_pipeline : When x is a ml_pipeline , the function returns a ml_pipeline with the tuning estimator appended to the pipeline.\ntbl_spark : When x is a tbl_spark , a tuning estimator is constructed then immediately fit with the input tbl_spark , returning a ml_cross_validation_model or a ml_train_validation_split_model object.\n\nFor cross validation, ml_sub_models() returns a nested list of models, where the first layer represents fold indices and the second layer represents param maps. For train-validation split, ml_sub_models() returns a list of models, corresponding to the order of the estimator param maps.\nml_validation_metrics() returns a data frame of performance metrics and hyperparameter combinations."
  },
  {
    "href": "reference/ml-tuning.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "sc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE)\n\n# Create a pipeline\npipeline <- ml_pipeline(sc) %>%\nft_r_formula(Species ~ .) %>%\nml_random_forest_classifier()\n\n# Specify hyperparameter grid\ngrid <- list(\nrandom_forest = list(\nnum_trees = c(5, 10),\nmax_depth = c(5, 10),\nimpurity = c(\"entropy\", \"gini\")\n)\n)\n\n# Create the cross validator object\ncv <- ml_cross_validator(\nsc,\nestimator = pipeline, estimator_param_maps = grid,\nevaluator = ml_multiclass_classification_evaluator(sc),\nnum_folds = 3,\nparallelism = 4\n)\n\n# Train the models\ncv_model <- ml_fit(cv, iris_tbl)\n\n# Print the metrics\nml_validation_metrics(cv_model)"
  },
  {
    "href": "reference/sdf_len.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Creates a DataFrame for the given length."
  },
  {
    "href": "reference/sdf_len.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_len(sc, length, repartition = NULL, type = c(\"integer\", \"integer64\"))"
  },
  {
    "href": "reference/sdf_len.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nsc\nThe associated Spark connection.\n\n\nlength\nThe desired length of the sequence.\n\n\nrepartition\nThe number of partitions to use when distributing the data across the Spark cluster.\n\n\ntype\nThe data type to use for the index, either \"integer\" or \"integer64\" ."
  },
  {
    "href": "reference/sdf_sample.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Draw a random sample of rows (with or without replacement) from a Spark DataFrame."
  },
  {
    "href": "reference/sdf_sample.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "sdf_sample(x, fraction = 1, replacement = TRUE, seed = NULL)"
  },
  {
    "href": "reference/sdf_sample.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\nAn object coercable to a Spark DataFrame.\n\n\nfraction\nThe fraction to sample.\n\n\nreplacement\nBoolean; sample with replacement?\n\n\nseed\nAn (optional) integer seed."
  },
  {
    "href": "reference/sdf_sample.html#seealso",
    "title": "sparklyr",
    "section": "Seealso",
    "text": "Other Spark data frames: sdf_copy_to , sdf_distinct , sdf_random_split , sdf_register , sdf_sort , sdf_weighted_sample"
  },
  {
    "href": "reference/ml_survival_regression_tidiers.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "These methods summarize the results of Spark ML models into tidy forms."
  },
  {
    "href": "reference/ml_survival_regression_tidiers.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "list(list(\"tidy\"), list(\"ml_model_aft_survival_regression\"))(x, ...)\nlist(list(\"augment\"), list(\"ml_model_aft_survival_regression\"))(x, newdata = NULL, ...)\nlist(list(\"glance\"), list(\"ml_model_aft_survival_regression\"))(x, ...)"
  },
  {
    "href": "reference/ml_survival_regression_tidiers.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Argument\nDescription\n\n\n\n\nx\na Spark ML model.\n\n\n...\nextra arguments (not used.)\n\n\nnewdata\na tbl_spark of new data to use for prediction."
  }
]