<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Function reference â€¢ sparklyr</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous" />

<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script>

<!-- bootstrap-toc -->
<link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="Function reference" />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body data-spy="scroll" data-target="#toc">
    <div class="container template-reference-index">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">sparklyr</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">1.4.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/sparklyr/sparklyr/">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="contents col-md-9">
    <div class="page-header">
      
    </div>

    <table class="ref-index">

    <colgroup>
      
      <col class="alias" />
      <col class="title" />
    </colgroup>

    <tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-spark-operations" class="hasAnchor"><a href="#section-spark-operations" class="anchor"></a>Spark Operations</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="spark_config.html">spark_config()</a></code> </p>
        </td>
        <td><p>Read Spark Configuration</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark-connections.html">spark_connect()</a></code> <code><a href="spark-connections.html">spark_connection_is_open()</a></code> <code><a href="spark-connections.html">spark_disconnect()</a></code> <code><a href="spark-connections.html">spark_disconnect_all()</a></code> <code><a href="spark-connections.html">spark_submit()</a></code> </p>
        </td>
        <td><p>Manage Spark Connections</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_install.html">spark_install_find()</a></code> <code><a href="spark_install.html">spark_install()</a></code> <code><a href="spark_install.html">spark_uninstall()</a></code> <code><a href="spark_install.html">spark_install_dir()</a></code> <code><a href="spark_install.html">spark_install_tar()</a></code> <code><a href="spark_install.html">spark_installed_versions()</a></code> <code><a href="spark_install.html">spark_available_versions()</a></code> </p>
        </td>
        <td><p>Find a given Spark installation by version.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_log.html">spark_log()</a></code> </p>
        </td>
        <td><p>View Entries in the Spark Log</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_web.html">spark_web()</a></code> </p>
        </td>
        <td><p>Open the Spark web interface</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="connection_is_open.html">connection_is_open()</a></code> </p>
        </td>
        <td><p>Check whether the connection is open</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="connection_spark_shinyapp.html">connection_spark_shinyapp()</a></code> </p>
        </td>
        <td><p>A Shiny app that can be used to construct a <code>spark_connect</code> statement</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_configuration.html">spark_session_config()</a></code> </p>
        </td>
        <td><p>Runtime configuration interface for the Spark Session</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="checkpoint_directory.html">spark_set_checkpoint_dir()</a></code> <code><a href="checkpoint_directory.html">spark_get_checkpoint_dir()</a></code> </p>
        </td>
        <td><p>Set/Get Spark checkpoint directory</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_table_name.html">spark_table_name()</a></code> </p>
        </td>
        <td><p>Generate a Table Name from Expression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_version_from_home.html">spark_version_from_home()</a></code> </p>
        </td>
        <td><p>Get the Spark Version Associated with a Spark Installation</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_versions.html">spark_versions()</a></code> </p>
        </td>
        <td><p>Retrieves a dataframe available Spark versions that van be installed.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_config_kubernetes.html">spark_config_kubernetes()</a></code> </p>
        </td>
        <td><p>Kubernetes Configuration</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_config_settings.html">spark_config_settings()</a></code> </p>
        </td>
        <td><p>Retrieve Available Settings</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_connection_find.html">spark_connection_find()</a></code> </p>
        </td>
        <td><p>Find Spark Connection</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_dependency_fallback.html">spark_dependency_fallback()</a></code> </p>
        </td>
        <td><p>Fallback to Spark Dependency</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_extension.html">spark_extension()</a></code> </p>
        </td>
        <td><p>Create Spark Extension</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_load_table.html">spark_load_table()</a></code> </p>
        </td>
        <td><p>Reads from a Spark Table into a Spark DataFrame.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_libsvm.html">spark_read_libsvm()</a></code> </p>
        </td>
        <td><p>Read libsvm file into a Spark DataFrame.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="list_sparklyr_jars.html">list_sparklyr_jars()</a></code> </p>
        </td>
        <td><p>list all sparklyr-*.jar files that have been built</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_config_packages.html">spark_config_packages()</a></code> </p>
        </td>
        <td><p>Creates Spark Configuration</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_connection.html">spark_connection()</a></code> </p>
        </td>
        <td><p>Retrieve the Spark Connection Associated with an R Object</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-spark-data" class="hasAnchor"><a href="#section-spark-data" class="anchor"></a>Spark Data</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="spark_read.html">spark_read()</a></code> </p>
        </td>
        <td><p>Read file(s) into a Spark DataFrame using a custom reader</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_avro.html">spark_read_avro()</a></code> </p>
        </td>
        <td><p>Read Apache Avro data into a Spark DataFrame.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_csv.html">spark_read_csv()</a></code> </p>
        </td>
        <td><p>Read a CSV file into a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_delta.html">spark_read_delta()</a></code> </p>
        </td>
        <td><p>Read from Delta Lake into a Spark DataFrame.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_jdbc.html">spark_read_jdbc()</a></code> </p>
        </td>
        <td><p>Read from JDBC connection into a Spark DataFrame.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_json.html">spark_read_json()</a></code> </p>
        </td>
        <td><p>Read a JSON file into a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_parquet.html">spark_read_parquet()</a></code> </p>
        </td>
        <td><p>Read a Parquet file into a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_source.html">spark_read_source()</a></code> </p>
        </td>
        <td><p>Read from a generic source into a Spark DataFrame.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_table.html">spark_read_table()</a></code> </p>
        </td>
        <td><p>Reads from a Spark Table into a Spark DataFrame.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_orc.html">spark_read_orc()</a></code> </p>
        </td>
        <td><p>Read a ORC file into a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_read_text.html">spark_read_text()</a></code> </p>
        </td>
        <td><p>Read a Text file into a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_save_table.html">spark_save_table()</a></code> </p>
        </td>
        <td><p>Saves a Spark DataFrame as a Spark table</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write.html">spark_write()</a></code> </p>
        </td>
        <td><p>Write Spark DataFrame to file using a custom writer</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_avro.html">spark_write_avro()</a></code> </p>
        </td>
        <td><p>Serialize a Spark DataFrame into Apache Avro format</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_orc.html">spark_write_orc()</a></code> </p>
        </td>
        <td><p>Write a Spark DataFrame to a ORC file</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_text.html">spark_write_text()</a></code> </p>
        </td>
        <td><p>Write a Spark DataFrame to a Text file</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_csv.html">spark_write_csv()</a></code> </p>
        </td>
        <td><p>Write a Spark DataFrame to a CSV</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_delta.html">spark_write_delta()</a></code> </p>
        </td>
        <td><p>Writes a Spark DataFrame into Delta Lake</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_jdbc.html">spark_write_jdbc()</a></code> </p>
        </td>
        <td><p>Writes a Spark DataFrame into a JDBC table</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_json.html">spark_write_json()</a></code> </p>
        </td>
        <td><p>Write a Spark DataFrame to a JSON file</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_parquet.html">spark_write_parquet()</a></code> </p>
        </td>
        <td><p>Write a Spark DataFrame to a Parquet file</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_source.html">spark_write_source()</a></code> </p>
        </td>
        <td><p>Writes a Spark DataFrame into a generic source</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_write_table.html">spark_write_table()</a></code> </p>
        </td>
        <td><p>Writes a Spark DataFrame into a Spark table</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-spark-tables" class="hasAnchor"><a href="#section-spark-tables" class="anchor"></a>Spark Tables</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="src_databases.html">src_databases()</a></code> </p>
        </td>
        <td><p>Show database list</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="tbl_cache.html">tbl_cache()</a></code> </p>
        </td>
        <td><p>Cache a Spark Table</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="tbl_change_db.html">tbl_change_db()</a></code> </p>
        </td>
        <td><p>Use specific database</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="tbl_uncache.html">tbl_uncache()</a></code> </p>
        </td>
        <td><p>Uncache a Spark Table</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-spark-dataframes" class="hasAnchor"><a href="#section-spark-dataframes" class="anchor"></a>Spark DataFrames</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="copy_to.spark_connection.html">copy_to(<i>&lt;spark_connection&gt;</i>)</a></code> </p>
        </td>
        <td><p>Copy an R Data Frame to Spark</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_along.html">sdf_along()</a></code> </p>
        </td>
        <td><p>Create DataFrame for along Object</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_bind.html">sdf_bind_rows()</a></code> <code><a href="sdf_bind.html">sdf_bind_cols()</a></code> </p>
        </td>
        <td><p>Bind multiple Spark DataFrames by row and column</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_broadcast.html">sdf_broadcast()</a></code> </p>
        </td>
        <td><p>Broadcast hint</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_checkpoint.html">sdf_checkpoint()</a></code> </p>
        </td>
        <td><p>Checkpoint a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_coalesce.html">sdf_coalesce()</a></code> </p>
        </td>
        <td><p>Coalesces a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_copy_to.html">sdf_copy_to()</a></code> <code><a href="sdf_copy_to.html">sdf_import()</a></code> </p>
        </td>
        <td><p>Copy an Object into Spark</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_drop_duplicates.html">sdf_drop_duplicates()</a></code> </p>
        </td>
        <td><p>Remove duplicates from a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_from_avro.html">sdf_from_avro()</a></code> </p>
        </td>
        <td><p>Convert column(s) from avro format</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_len.html">sdf_len()</a></code> </p>
        </td>
        <td><p>Create DataFrame for Length</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_num_partitions.html">sdf_num_partitions()</a></code> </p>
        </td>
        <td><p>Gets number of partitions of a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_random_split.html">sdf_random_split()</a></code> <code><a href="sdf_random_split.html">sdf_partition()</a></code> </p>
        </td>
        <td><p>Partition a Spark Dataframe</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_pivot.html">sdf_pivot()</a></code> </p>
        </td>
        <td><p>Pivot a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf-transform-methods.html">sdf_predict()</a></code> <code><a href="sdf-transform-methods.html">sdf_transform()</a></code> <code><a href="sdf-transform-methods.html">sdf_fit()</a></code> <code><a href="sdf-transform-methods.html">sdf_fit_and_transform()</a></code> </p>
        </td>
        <td><p>Spark ML -- Transform, fit, and predict methods (sdf_ interface)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_read_column.html">sdf_read_column()</a></code> </p>
        </td>
        <td><p>Read a Column from a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_register.html">sdf_register()</a></code> </p>
        </td>
        <td><p>Register a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_repartition.html">sdf_repartition()</a></code> </p>
        </td>
        <td><p>Repartition a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_residuals.html">sdf_residuals()</a></code> </p>
        </td>
        <td><p>Model Residuals</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_sample.html">sdf_sample()</a></code> </p>
        </td>
        <td><p>Randomly Sample Rows from a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_separate_column.html">sdf_separate_column()</a></code> </p>
        </td>
        <td><p>Separate a Vector Column into Scalar Columns</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_seq.html">sdf_seq()</a></code> </p>
        </td>
        <td><p>Create DataFrame for Range</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_sort.html">sdf_sort()</a></code> </p>
        </td>
        <td><p>Sort a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_to_avro.html">sdf_to_avro()</a></code> </p>
        </td>
        <td><p>Convert column(s) to avro format</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_with_unique_id.html">sdf_with_unique_id()</a></code> </p>
        </td>
        <td><p>Add a Unique ID Column to a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_collect.html">sdf_collect()</a></code> </p>
        </td>
        <td><p>Collect a Spark DataFrame into R.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_crosstab.html">sdf_crosstab()</a></code> </p>
        </td>
        <td><p>Cross Tabulation</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_debug_string.html">sdf_debug_string()</a></code> </p>
        </td>
        <td><p>Debug Info for Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_describe.html">sdf_describe()</a></code> </p>
        </td>
        <td><p>Compute summary statistics for columns of a data frame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_dim.html">sdf_dim()</a></code> <code><a href="sdf_dim.html">sdf_nrow()</a></code> <code><a href="sdf_dim.html">sdf_ncol()</a></code> </p>
        </td>
        <td><p>Support for Dimension Operations</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_is_streaming.html">sdf_is_streaming()</a></code> </p>
        </td>
        <td><p>Spark DataFrame is Streaming</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_last_index.html">sdf_last_index()</a></code> </p>
        </td>
        <td><p>Returns the last index of a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf-saveload.html">sdf_save_table()</a></code> <code><a href="sdf-saveload.html">sdf_load_table()</a></code> <code><a href="sdf-saveload.html">sdf_save_parquet()</a></code> <code><a href="sdf-saveload.html">sdf_load_parquet()</a></code> </p>
        </td>
        <td><p>Save / Load a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_persist.html">sdf_persist()</a></code> </p>
        </td>
        <td><p>Persist a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_project.html">sdf_project()</a></code> </p>
        </td>
        <td><p>Project features onto principal components</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_quantile.html">sdf_quantile()</a></code> </p>
        </td>
        <td><p>Compute (Approximate) Quantiles with a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_schema.html">sdf_schema()</a></code> </p>
        </td>
        <td><p>Read the Schema of a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_sql.html">sdf_sql()</a></code> </p>
        </td>
        <td><p>Spark DataFrame from SQL</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_with_sequential_id.html">sdf_with_sequential_id()</a></code> </p>
        </td>
        <td><p>Add a Sequential ID Column to a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="join.tbl_spark.html">inner_join(<i>&lt;tbl_spark&gt;</i>)</a></code> <code><a href="join.tbl_spark.html">left_join(<i>&lt;tbl_spark&gt;</i>)</a></code> <code><a href="join.tbl_spark.html">right_join(<i>&lt;tbl_spark&gt;</i>)</a></code> <code><a href="join.tbl_spark.html">full_join(<i>&lt;tbl_spark&gt;</i>)</a></code> </p>
        </td>
        <td><p>Join Spark tbls.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="separate.html">separate</a></code> </p>
        </td>
        <td><p>Separate</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="unite.html">unite</a></code> </p>
        </td>
        <td><p>Unite</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="nest.html">nest</a></code> </p>
        </td>
        <td><p>Nest</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="unnest.html">unnest</a></code> </p>
        </td>
        <td><p>Unnest</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="pivot_wider.html">pivot_wider</a></code> </p>
        </td>
        <td><p>Pivot wider</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="pivot_longer.html">pivot_longer</a></code> </p>
        </td>
        <td><p>Pivot longer</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="fill.html">fill</a></code> </p>
        </td>
        <td><p>Fill</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="left_join.html">left_join</a></code> </p>
        </td>
        <td><p>Left join</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="right_join.html">right_join</a></code> </p>
        </td>
        <td><p>Right join</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="inner_join.html">inner_join</a></code> </p>
        </td>
        <td><p>Inner join</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="full_join.html">full_join</a></code> </p>
        </td>
        <td><p>Full join</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="hof_aggregate.html">hof_aggregate()</a></code> </p>
        </td>
        <td><p>Apply Aggregate Function to Array Column</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="hof_array_sort.html">hof_array_sort()</a></code> </p>
        </td>
        <td><p>Sorts array using a custom comparator</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="hof_exists.html">hof_exists()</a></code> </p>
        </td>
        <td><p>Determine Whether Some Element Exists in an Array Column</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="hof_filter.html">hof_filter()</a></code> </p>
        </td>
        <td><p>Filter Array Column</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="hof_forall.html">hof_forall()</a></code> </p>
        </td>
        <td><p>Checks whether all elements in an array satisfy a predicate</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="hof_map_filter.html">hof_map_filter()</a></code> </p>
        </td>
        <td><p>Filters a map</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="hof_map_zip_with.html">hof_map_zip_with()</a></code> </p>
        </td>
        <td><p>Merges two maps into one</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="hof_transform.html">hof_transform()</a></code> </p>
        </td>
        <td><p>Transform Array Column</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="hof_transform_keys.html">hof_transform_keys()</a></code> </p>
        </td>
        <td><p>Transforms keys of a map</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="hof_transform_values.html">hof_transform_values()</a></code> </p>
        </td>
        <td><p>Transforms values of a map</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="hof_zip_with.html">hof_zip_with()</a></code> </p>
        </td>
        <td><p>Combines 2 Array Columns</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sdf_weighted_sample.html">sdf_weighted_sample()</a></code> </p>
        </td>
        <td><p>Perform Weighted Random Sampling on a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="transform_sdf.html">transform_sdf()</a></code> </p>
        </td>
        <td><p>transform a subset of column(s) in a Spark Dataframe</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-spark-machine-learning" class="hasAnchor"><a href="#section-spark-machine-learning" class="anchor"></a>Spark Machine Learning</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="ml_decision_tree.html">ml_decision_tree_classifier()</a></code> <code><a href="ml_decision_tree.html">ml_decision_tree()</a></code> <code><a href="ml_decision_tree.html">ml_decision_tree_regressor()</a></code> </p>
        </td>
        <td><p>Spark ML -- Decision Trees</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_generalized_linear_regression.html">ml_generalized_linear_regression()</a></code> </p>
        </td>
        <td><p>Spark ML -- Generalized Linear Regression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_gradient_boosted_trees.html">ml_gbt_classifier()</a></code> <code><a href="ml_gradient_boosted_trees.html">ml_gradient_boosted_trees()</a></code> <code><a href="ml_gradient_boosted_trees.html">ml_gbt_regressor()</a></code> </p>
        </td>
        <td><p>Spark ML -- Gradient Boosted Trees</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_kmeans.html">ml_kmeans()</a></code> <code><a href="ml_kmeans.html">ml_compute_cost()</a></code> </p>
        </td>
        <td><p>Spark ML -- K-Means Clustering</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_lda.html">ml_lda()</a></code> <code><a href="ml_lda.html">ml_describe_topics()</a></code> <code><a href="ml_lda.html">ml_log_likelihood()</a></code> <code><a href="ml_lda.html">ml_log_perplexity()</a></code> <code><a href="ml_lda.html">ml_topics_matrix()</a></code> </p>
        </td>
        <td><p>Spark ML -- Latent Dirichlet Allocation</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_linear_regression.html">ml_linear_regression()</a></code> </p>
        </td>
        <td><p>Spark ML -- Linear Regression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_logistic_regression.html">ml_logistic_regression()</a></code> </p>
        </td>
        <td><p>Spark ML -- Logistic Regression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_model_data.html">ml_model_data()</a></code> </p>
        </td>
        <td><p>Extracts data associated with a Spark ML model</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_multilayer_perceptron_classifier.html">ml_multilayer_perceptron_classifier()</a></code> <code><a href="ml_multilayer_perceptron_classifier.html">ml_multilayer_perceptron()</a></code> </p>
        </td>
        <td><p>Spark ML -- Multilayer Perceptron</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_naive_bayes.html">ml_naive_bayes()</a></code> </p>
        </td>
        <td><p>Spark ML -- Naive-Bayes</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_one_vs_rest.html">ml_one_vs_rest()</a></code> </p>
        </td>
        <td><p>Spark ML -- OneVsRest</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_pca.html">ft_pca()</a></code> <code><a href="ft_pca.html">ml_pca()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- PCA (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_random_forest.html">ml_random_forest_classifier()</a></code> <code><a href="ml_random_forest.html">ml_random_forest()</a></code> <code><a href="ml_random_forest.html">ml_random_forest_regressor()</a></code> </p>
        </td>
        <td><p>Spark ML -- Random Forest</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_aft_survival_regression.html">ml_aft_survival_regression()</a></code> <code><a href="ml_aft_survival_regression.html">ml_survival_regression()</a></code> </p>
        </td>
        <td><p>Spark ML -- Survival Regression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_add_stage.html">ml_add_stage()</a></code> </p>
        </td>
        <td><p>Add a Stage to a Pipeline</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_als.html">ml_als()</a></code> <code><a href="ml_als.html">ml_recommend()</a></code> </p>
        </td>
        <td><p>Spark ML -- ALS</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_lsh_utils.html">ml_approx_nearest_neighbors()</a></code> <code><a href="ft_lsh_utils.html">ml_approx_similarity_join()</a></code> </p>
        </td>
        <td><p>Utility functions for LSH models</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_fpgrowth.html">ml_fpgrowth()</a></code> <code><a href="ml_fpgrowth.html">ml_association_rules()</a></code> <code><a href="ml_fpgrowth.html">ml_freq_itemsets()</a></code> </p>
        </td>
        <td><p>Frequent Pattern Mining -- FPGrowth</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_evaluator.html">ml_binary_classification_evaluator()</a></code> <code><a href="ml_evaluator.html">ml_binary_classification_eval()</a></code> <code><a href="ml_evaluator.html">ml_multiclass_classification_evaluator()</a></code> <code><a href="ml_evaluator.html">ml_classification_eval()</a></code> <code><a href="ml_evaluator.html">ml_regression_evaluator()</a></code> </p>
        </td>
        <td><p>Spark ML - Evaluators</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_bisecting_kmeans.html">ml_bisecting_kmeans()</a></code> </p>
        </td>
        <td><p>Spark ML -- Bisecting K-Means Clustering</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_call_constructor.html">ml_call_constructor()</a></code> </p>
        </td>
        <td><p>Wrap a Spark ML JVM object</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_chisquare_test.html">ml_chisquare_test()</a></code> </p>
        </td>
        <td><p>Chi-square hypothesis testing for categorical data.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_clustering_evaluator.html">ml_clustering_evaluator()</a></code> </p>
        </td>
        <td><p>Spark ML - Clustering Evaluator</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml-model-constructors.html">new_ml_model_prediction()</a></code> <code><a href="ml-model-constructors.html">new_ml_model()</a></code> <code><a href="ml-model-constructors.html">new_ml_model_classification()</a></code> <code><a href="ml-model-constructors.html">new_ml_model_regression()</a></code> <code><a href="ml-model-constructors.html">new_ml_model_clustering()</a></code> <code><a href="ml-model-constructors.html">ml_supervised_pipeline()</a></code> <code><a href="ml-model-constructors.html">ml_clustering_pipeline()</a></code> <code><a href="ml-model-constructors.html">ml_construct_model_supervised()</a></code> <code><a href="ml-model-constructors.html">ml_construct_model_clustering()</a></code> </p>
        </td>
        <td><p>Constructors for `ml_model` Objects</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_corr.html">ml_corr()</a></code> </p>
        </td>
        <td><p>Compute correlation matrix</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml-tuning.html">ml_sub_models()</a></code> <code><a href="ml-tuning.html">ml_validation_metrics()</a></code> <code><a href="ml-tuning.html">ml_cross_validator()</a></code> <code><a href="ml-tuning.html">ml_train_validation_split()</a></code> </p>
        </td>
        <td><p>Spark ML -- Tuning</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_default_stop_words.html">ml_default_stop_words()</a></code> </p>
        </td>
        <td><p>Default stop words</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_evaluate.html">ml_evaluate()</a></code> </p>
        </td>
        <td><p>Evaluate the Model on a Validation Set</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_feature_importances.html">ml_feature_importances()</a></code> <code><a href="ml_feature_importances.html">ml_tree_feature_importance()</a></code> </p>
        </td>
        <td><p>Spark ML - Feature Importance for Tree Models</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_word2vec.html">ft_word2vec()</a></code> <code><a href="ft_word2vec.html">ml_find_synonyms()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Word2Vec (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml-transform-methods.html">is_ml_transformer()</a></code> <code><a href="ml-transform-methods.html">is_ml_estimator()</a></code> <code><a href="ml-transform-methods.html">ml_fit()</a></code> <code><a href="ml-transform-methods.html">ml_transform()</a></code> <code><a href="ml-transform-methods.html">ml_fit_and_transform()</a></code> <code><a href="ml-transform-methods.html">ml_predict()</a></code> </p>
        </td>
        <td><p>Spark ML -- Transform, fit, and predict methods (ml_ interface)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_gaussian_mixture.html">ml_gaussian_mixture()</a></code> </p>
        </td>
        <td><p>Spark ML -- Gaussian Mixture clustering.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml-params.html">ml_is_set()</a></code> <code><a href="ml-params.html">ml_param_map()</a></code> <code><a href="ml-params.html">ml_param()</a></code> <code><a href="ml-params.html">ml_params()</a></code> </p>
        </td>
        <td><p>Spark ML -- ML Params</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_isotonic_regression.html">ml_isotonic_regression()</a></code> </p>
        </td>
        <td><p>Spark ML -- Isotonic Regression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_string_indexer.html">ft_string_indexer()</a></code> <code><a href="ft_string_indexer.html">ml_labels()</a></code> <code><a href="ft_string_indexer.html">ft_string_indexer_model()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- StringIndexer (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_linear_svc.html">ml_linear_svc()</a></code> </p>
        </td>
        <td><p>Spark ML -- LinearSVC</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml-persistence.html">ml_save()</a></code> <code><a href="ml-persistence.html">ml_load()</a></code> </p>
        </td>
        <td><p>Spark ML -- Model Persistence</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_pipeline.html">ml_pipeline()</a></code> </p>
        </td>
        <td><p>Spark ML -- Pipelines</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_stage.html">ml_stage()</a></code> <code><a href="ml_stage.html">ml_stages()</a></code> </p>
        </td>
        <td><p>Spark ML -- Pipeline stage extraction</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_standardize_formula.html">ml_standardize_formula()</a></code> </p>
        </td>
        <td><p>Standardize Formula Input for `ml_model`</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_summary.html">ml_summary()</a></code> </p>
        </td>
        <td><p>Spark ML -- Extraction of summary metrics</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_uid.html">ml_uid()</a></code> </p>
        </td>
        <td><p>Spark ML -- UID</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_count_vectorizer.html">ft_count_vectorizer()</a></code> <code><a href="ft_count_vectorizer.html">ml_vocabulary()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- CountVectorizer (Estimator)</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-spark-feature-transformers" class="hasAnchor"><a href="#section-spark-feature-transformers" class="anchor"></a>Spark Feature Transformers</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="ft_binarizer.html">ft_binarizer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Binarizer (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_bucketizer.html">ft_bucketizer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Bucketizer (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_count_vectorizer.html">ft_count_vectorizer()</a></code> <code><a href="ft_count_vectorizer.html">ml_vocabulary()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- CountVectorizer (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_dct.html">ft_dct()</a></code> <code><a href="ft_dct.html">ft_discrete_cosine_transform()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Discrete Cosine Transform (DCT) (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_elementwise_product.html">ft_elementwise_product()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- ElementwiseProduct (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_index_to_string.html">ft_index_to_string()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- IndexToString (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_one_hot_encoder.html">ft_one_hot_encoder()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- OneHotEncoder (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_quantile_discretizer.html">ft_quantile_discretizer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- QuantileDiscretizer (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="sql-transformer.html">ft_sql_transformer()</a></code> <code><a href="sql-transformer.html">ft_dplyr_transformer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- SQLTransformer</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_string_indexer.html">ft_string_indexer()</a></code> <code><a href="ft_string_indexer.html">ml_labels()</a></code> <code><a href="ft_string_indexer.html">ft_string_indexer_model()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- StringIndexer (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_vector_assembler.html">ft_vector_assembler()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- VectorAssembler (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_tokenizer.html">ft_tokenizer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Tokenizer (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_regex_tokenizer.html">ft_regex_tokenizer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- RegexTokenizer (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_lsh.html">ft_bucketed_random_projection_lsh()</a></code> <code><a href="ft_lsh.html">ft_minhash_lsh()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- LSH (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_chisq_selector.html">ft_chisq_selector()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- ChiSqSelector (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_feature_hasher.html">ft_feature_hasher()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- FeatureHasher (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_hashing_tf.html">ft_hashing_tf()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- HashingTF (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_idf.html">ft_idf()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- IDF (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_imputer.html">ft_imputer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Imputer (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_interaction.html">ft_interaction()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Interaction (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_max_abs_scaler.html">ft_max_abs_scaler()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- MaxAbsScaler (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_min_max_scaler.html">ft_min_max_scaler()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- MinMaxScaler (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_ngram.html">ft_ngram()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- NGram (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_normalizer.html">ft_normalizer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Normalizer (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_one_hot_encoder_estimator.html">ft_one_hot_encoder_estimator()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- OneHotEncoderEstimator (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_pca.html">ft_pca()</a></code> <code><a href="ft_pca.html">ml_pca()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- PCA (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_polynomial_expansion.html">ft_polynomial_expansion()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- PolynomialExpansion (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_r_formula.html">ft_r_formula()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- RFormula (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_standard_scaler.html">ft_standard_scaler()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- StandardScaler (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_stop_words_remover.html">ft_stop_words_remover()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- StopWordsRemover (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_vector_indexer.html">ft_vector_indexer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- VectorIndexer (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_vector_slicer.html">ft_vector_slicer()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- VectorSlicer (Transformer)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_word2vec.html">ft_word2vec()</a></code> <code><a href="ft_word2vec.html">ml_find_synonyms()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- Word2Vec (Estimator)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ft_robust_scaler.html">ft_robust_scaler()</a></code> </p>
        </td>
        <td><p>Feature Transformation -- RobustScaler (Estimator)</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-spark-machine-learning-utilities" class="hasAnchor"><a href="#section-spark-machine-learning-utilities" class="anchor"></a>Spark Machine Learning Utilities</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="ml_evaluator.html">ml_binary_classification_evaluator()</a></code> <code><a href="ml_evaluator.html">ml_binary_classification_eval()</a></code> <code><a href="ml_evaluator.html">ml_multiclass_classification_evaluator()</a></code> <code><a href="ml_evaluator.html">ml_classification_eval()</a></code> <code><a href="ml_evaluator.html">ml_regression_evaluator()</a></code> </p>
        </td>
        <td><p>Spark ML - Evaluators</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_feature_importances.html">ml_feature_importances()</a></code> <code><a href="ml_feature_importances.html">ml_tree_feature_importance()</a></code> </p>
        </td>
        <td><p>Spark ML - Feature Importance for Tree Models</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_als_tidiers.html">tidy(<i>&lt;ml_model_als&gt;</i>)</a></code> <code><a href="ml_als_tidiers.html">augment(<i>&lt;ml_model_als&gt;</i>)</a></code> <code><a href="ml_als_tidiers.html">glance(<i>&lt;ml_model_als&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML ALS</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_glm_tidiers.html">tidy(<i>&lt;ml_model_generalized_linear_regression&gt;</i>)</a></code> <code><a href="ml_glm_tidiers.html">tidy(<i>&lt;ml_model_linear_regression&gt;</i>)</a></code> <code><a href="ml_glm_tidiers.html">augment(<i>&lt;ml_model_generalized_linear_regression&gt;</i>)</a></code> <code><a href="ml_glm_tidiers.html">augment(<i>&lt;ml_model_linear_regression&gt;</i>)</a></code> <code><a href="ml_glm_tidiers.html">glance(<i>&lt;ml_model_generalized_linear_regression&gt;</i>)</a></code> <code><a href="ml_glm_tidiers.html">glance(<i>&lt;ml_model_linear_regression&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML linear models</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_isotonic_regression_tidiers.html">tidy(<i>&lt;ml_model_isotonic_regression&gt;</i>)</a></code> <code><a href="ml_isotonic_regression_tidiers.html">augment(<i>&lt;ml_model_isotonic_regression&gt;</i>)</a></code> <code><a href="ml_isotonic_regression_tidiers.html">glance(<i>&lt;ml_model_isotonic_regression&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML Isotonic Regression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_lda_tidiers.html">tidy(<i>&lt;ml_model_lda&gt;</i>)</a></code> <code><a href="ml_lda_tidiers.html">augment(<i>&lt;ml_model_lda&gt;</i>)</a></code> <code><a href="ml_lda_tidiers.html">glance(<i>&lt;ml_model_lda&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML LDA models</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_linear_svc_tidiers.html">tidy(<i>&lt;ml_model_linear_svc&gt;</i>)</a></code> <code><a href="ml_linear_svc_tidiers.html">augment(<i>&lt;ml_model_linear_svc&gt;</i>)</a></code> <code><a href="ml_linear_svc_tidiers.html">glance(<i>&lt;ml_model_linear_svc&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML linear svc</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_logistic_regression_tidiers.html">tidy(<i>&lt;ml_model_logistic_regression&gt;</i>)</a></code> <code><a href="ml_logistic_regression_tidiers.html">augment(<i>&lt;ml_model_logistic_regression&gt;</i>)</a></code> <code><a href="ml_logistic_regression_tidiers.html">glance(<i>&lt;ml_model_logistic_regression&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML Logistic Regression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_multilayer_perceptron_tidiers.html">tidy(<i>&lt;ml_model_multilayer_perceptron_classification&gt;</i>)</a></code> <code><a href="ml_multilayer_perceptron_tidiers.html">augment(<i>&lt;ml_model_multilayer_perceptron_classification&gt;</i>)</a></code> <code><a href="ml_multilayer_perceptron_tidiers.html">glance(<i>&lt;ml_model_multilayer_perceptron_classification&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML MLP</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_naive_bayes_tidiers.html">tidy(<i>&lt;ml_model_naive_bayes&gt;</i>)</a></code> <code><a href="ml_naive_bayes_tidiers.html">augment(<i>&lt;ml_model_naive_bayes&gt;</i>)</a></code> <code><a href="ml_naive_bayes_tidiers.html">glance(<i>&lt;ml_model_naive_bayes&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML Naive Bayes</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_pca_tidiers.html">tidy(<i>&lt;ml_model_pca&gt;</i>)</a></code> <code><a href="ml_pca_tidiers.html">augment(<i>&lt;ml_model_pca&gt;</i>)</a></code> <code><a href="ml_pca_tidiers.html">glance(<i>&lt;ml_model_pca&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML Principal Component Analysis</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_survival_regression_tidiers.html">tidy(<i>&lt;ml_model_aft_survival_regression&gt;</i>)</a></code> <code><a href="ml_survival_regression_tidiers.html">augment(<i>&lt;ml_model_aft_survival_regression&gt;</i>)</a></code> <code><a href="ml_survival_regression_tidiers.html">glance(<i>&lt;ml_model_aft_survival_regression&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML Survival Regression</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_tree_tidiers.html">tidy(<i>&lt;ml_model_decision_tree_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers.html">tidy(<i>&lt;ml_model_decision_tree_regression&gt;</i>)</a></code> <code><a href="ml_tree_tidiers.html">augment(<i>&lt;ml_model_decision_tree_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers.html">augment(<i>&lt;ml_model_decision_tree_regression&gt;</i>)</a></code> <code><a href="ml_tree_tidiers.html">glance(<i>&lt;ml_model_decision_tree_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers.html">glance(<i>&lt;ml_model_decision_tree_regression&gt;</i>)</a></code> <code><a href="ml_tree_tidiers.html">tidy(<i>&lt;ml_model_random_forest_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers.html">tidy(<i>&lt;ml_model_random_forest_regression&gt;</i>)</a></code> <code><a href="ml_tree_tidiers.html">augment(<i>&lt;ml_model_random_forest_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers.html">augment(<i>&lt;ml_model_random_forest_regression&gt;</i>)</a></code> <code><a href="ml_tree_tidiers.html">glance(<i>&lt;ml_model_random_forest_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers.html">glance(<i>&lt;ml_model_random_forest_regression&gt;</i>)</a></code> <code><a href="ml_tree_tidiers.html">tidy(<i>&lt;ml_model_gbt_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers.html">tidy(<i>&lt;ml_model_gbt_regression&gt;</i>)</a></code> <code><a href="ml_tree_tidiers.html">augment(<i>&lt;ml_model_gbt_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers.html">augment(<i>&lt;ml_model_gbt_regression&gt;</i>)</a></code> <code><a href="ml_tree_tidiers.html">glance(<i>&lt;ml_model_gbt_classification&gt;</i>)</a></code> <code><a href="ml_tree_tidiers.html">glance(<i>&lt;ml_model_gbt_regression&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML tree models</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="ml_unsupervised_tidiers.html">tidy(<i>&lt;ml_model_kmeans&gt;</i>)</a></code> <code><a href="ml_unsupervised_tidiers.html">augment(<i>&lt;ml_model_kmeans&gt;</i>)</a></code> <code><a href="ml_unsupervised_tidiers.html">glance(<i>&lt;ml_model_kmeans&gt;</i>)</a></code> <code><a href="ml_unsupervised_tidiers.html">tidy(<i>&lt;ml_model_bisecting_kmeans&gt;</i>)</a></code> <code><a href="ml_unsupervised_tidiers.html">augment(<i>&lt;ml_model_bisecting_kmeans&gt;</i>)</a></code> <code><a href="ml_unsupervised_tidiers.html">glance(<i>&lt;ml_model_bisecting_kmeans&gt;</i>)</a></code> <code><a href="ml_unsupervised_tidiers.html">tidy(<i>&lt;ml_model_gaussian_mixture&gt;</i>)</a></code> <code><a href="ml_unsupervised_tidiers.html">augment(<i>&lt;ml_model_gaussian_mixture&gt;</i>)</a></code> <code><a href="ml_unsupervised_tidiers.html">glance(<i>&lt;ml_model_gaussian_mixture&gt;</i>)</a></code> </p>
        </td>
        <td><p>Tidying methods for Spark ML unsupervised models</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-extensions" class="hasAnchor"><a href="#section-extensions" class="anchor"></a>Extensions</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="compile_package_jars.html">compile_package_jars()</a></code> </p>
        </td>
        <td><p>Compile Scala sources into a Java Archive (jar)</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="connection_config.html">connection_config()</a></code> </p>
        </td>
        <td><p>Read configuration values for a connection</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="download_scalac.html">download_scalac()</a></code> </p>
        </td>
        <td><p>Downloads default Scala Compilers</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="find_scalac.html">find_scalac()</a></code> </p>
        </td>
        <td><p>Discover the Scala Compiler</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark-api.html">spark_context()</a></code> <code><a href="spark-api.html">java_context()</a></code> <code><a href="spark-api.html">hive_context()</a></code> <code><a href="spark-api.html">spark_session()</a></code> </p>
        </td>
        <td><p>Access the Spark API</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="hive_context_config.html">hive_context_config()</a></code> </p>
        </td>
        <td><p>Runtime configuration interface for Hive</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="invoke.html">invoke()</a></code> <code><a href="invoke.html">invoke_static()</a></code> <code><a href="invoke.html">invoke_new()</a></code> </p>
        </td>
        <td><p>Invoke a Method on a JVM Object</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="register_extension.html">register_extension()</a></code> <code><a href="register_extension.html">registered_extensions()</a></code> </p>
        </td>
        <td><p>Register a Package that Implements a Spark Extension</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_compilation_spec.html">spark_compilation_spec()</a></code> </p>
        </td>
        <td><p>Define a Spark Compilation Specification</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_default_compilation_spec.html">spark_default_compilation_spec()</a></code> </p>
        </td>
        <td><p>Default Compilation Specification for Spark Extensions</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_context_config.html">spark_context_config()</a></code> </p>
        </td>
        <td><p>Runtime configuration interface for the Spark Context.</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_dataframe.html">spark_dataframe()</a></code> </p>
        </td>
        <td><p>Retrieve a Spark DataFrame</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_dependency.html">spark_dependency()</a></code> </p>
        </td>
        <td><p>Define a Spark dependency</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_home_set.html">spark_home_set()</a></code> </p>
        </td>
        <td><p>Set the SPARK_HOME environment variable</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_jobj.html">spark_jobj()</a></code> </p>
        </td>
        <td><p>Retrieve a Spark JVM Object Reference</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_version.html">spark_version()</a></code> </p>
        </td>
        <td><p>Get the Spark Version Associated with a Spark Connection</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-distributed-computing" class="hasAnchor"><a href="#section-distributed-computing" class="anchor"></a>Distributed Computing</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="spark_apply.html">spark_apply()</a></code> </p>
        </td>
        <td><p>Apply an R Function in Spark</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_apply_bundle.html">spark_apply_bundle()</a></code> </p>
        </td>
        <td><p>Create Bundle for Spark Apply</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="spark_apply_log.html">spark_apply_log()</a></code> </p>
        </td>
        <td><p>Log Writer for Spark Apply</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="registerDoSpark.html">registerDoSpark()</a></code> </p>
        </td>
        <td><p>Register a Prallel Backend</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-livy" class="hasAnchor"><a href="#section-livy" class="anchor"></a>Livy</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="livy_install.html">livy_install()</a></code> <code><a href="livy_install.html">livy_available_versions()</a></code> <code><a href="livy_install.html">livy_install_dir()</a></code> <code><a href="livy_install.html">livy_installed_versions()</a></code> <code><a href="livy_install.html">livy_home_dir()</a></code> </p>
        </td>
        <td><p>Install Livy</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="livy_config.html">livy_config()</a></code> </p>
        </td>
        <td><p>Create a Spark Configuration for Livy</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="livy_service.html">livy_service_start()</a></code> <code><a href="livy_service.html">livy_service_stop()</a></code> </p>
        </td>
        <td><p>Start Livy</p></td>
      </tr>
    </tbody><tbody>
      <tr>
        <th colspan="2">
          <h2 id="section-streaming" class="hasAnchor"><a href="#section-streaming" class="anchor"></a>Streaming</h2>
          <p class="section-desc"></p>
        </th>
      </tr>
      
      
    </tbody><tbody>
      
      
      <tr>
        
        <td>
          <p><code><a href="stream_find.html">stream_find()</a></code> </p>
        </td>
        <td><p>Find Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_generate_test.html">stream_generate_test()</a></code> </p>
        </td>
        <td><p>Generate Test Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_id.html">stream_id()</a></code> </p>
        </td>
        <td><p>Spark Stream's Identifier</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_name.html">stream_name()</a></code> </p>
        </td>
        <td><p>Spark Stream's Name</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_read_csv.html">stream_read_csv()</a></code> </p>
        </td>
        <td><p>Read CSV Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_read_json.html">stream_read_json()</a></code> </p>
        </td>
        <td><p>Read JSON Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_read_delta.html">stream_read_delta()</a></code> </p>
        </td>
        <td><p>Read Delta Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_read_kafka.html">stream_read_kafka()</a></code> </p>
        </td>
        <td><p>Read Kafka Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_read_orc.html">stream_read_orc()</a></code> </p>
        </td>
        <td><p>Read ORC Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_read_parquet.html">stream_read_parquet()</a></code> </p>
        </td>
        <td><p>Read Parquet Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_read_socket.html">stream_read_socket()</a></code> </p>
        </td>
        <td><p>Read Socket Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_read_text.html">stream_read_text()</a></code> </p>
        </td>
        <td><p>Read Text Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_render.html">stream_render()</a></code> </p>
        </td>
        <td><p>Render Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_stats.html">stream_stats()</a></code> </p>
        </td>
        <td><p>Stream Statistics</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_stop.html">stream_stop()</a></code> </p>
        </td>
        <td><p>Stops a Spark Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_trigger_continuous.html">stream_trigger_continuous()</a></code> </p>
        </td>
        <td><p>Spark Stream Continuous Trigger</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_trigger_interval.html">stream_trigger_interval()</a></code> </p>
        </td>
        <td><p>Spark Stream Interval Trigger</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_view.html">stream_view()</a></code> </p>
        </td>
        <td><p>View Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_watermark.html">stream_watermark()</a></code> </p>
        </td>
        <td><p>Watermark Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_console.html">stream_write_console()</a></code> </p>
        </td>
        <td><p>Write Console Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_csv.html">stream_write_csv()</a></code> </p>
        </td>
        <td><p>Write CSV Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_delta.html">stream_write_delta()</a></code> </p>
        </td>
        <td><p>Write Delta Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_json.html">stream_write_json()</a></code> </p>
        </td>
        <td><p>Write JSON Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_kafka.html">stream_write_kafka()</a></code> </p>
        </td>
        <td><p>Write Kafka Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_memory.html">stream_write_memory()</a></code> </p>
        </td>
        <td><p>Write Memory Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_orc.html">stream_write_orc()</a></code> </p>
        </td>
        <td><p>Write a ORC Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_parquet.html">stream_write_parquet()</a></code> </p>
        </td>
        <td><p>Write Parquet Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="stream_write_text.html">stream_write_text()</a></code> </p>
        </td>
        <td><p>Write Text Stream</p></td>
      </tr><tr>
        
        <td>
          <p><code><a href="reactiveSpark.html">reactiveSpark()</a></code> </p>
        </td>
        <td><p>Reactive spark reader</p></td>
      </tr>
    </tbody>
    </table>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top">
      <h2 data-toc-skip>Contents</h2>
    </nav>
  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Javier Luraschi, Kevin Kuo, Kevin Ushey, JJ Allaire, Hossein Falaki, Lu Wang, Andy Zhang, Yitao Li,  The Apache Software Foundation.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


