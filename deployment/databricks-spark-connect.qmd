---
title: "Spark Connect, and Databrick Connect v2"
format:
  html:
    theme: default
    toc: true
execute:
    eval: false
    freeze: false
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| include: false

install_github("sparklyr/sparklyr")
install_github("mlverse/pysparklyr")

library(remotes)
library(dplyr)
library(dbplyr)
library(sparklyr)
library(pysparklyr)

# install_pyspark()
```

*Last updated: `r lubridate::date()`*

## Intro

Intoduced in Spark 3.4, [Spark
Connect](https://spark.apache.org/docs/latest/spark-connect-overview.html) is a new
architecture that enables remote connectivity to Spark Clusters.  Spark Connect
will make it much easier for R users to interact with Spark clusters right from
their laptops.
[Databricks Connect](https://docs.databricks.com/dev-tools/databricks-connect.html), 
available in Databricks Runtime version 13 and above, is based on this new
architecture. 

Because it is a new architecture, the way `sparklyr` currently integrates with 
Spark does not work with Connect.  This is why we are working on a solution that
extends `sparklyr`'s capabilities in order to enable R users to work within the 
Connect framework.

## The Solution

`sparklyr` integrates with Spark using the Spark Shell. In order for the Spark 
Shell to work in your laptop, or server, you would need to setup Java.

As mentioned in the previous section, `sparklyr` cannot use its current method
to integrate with Connect. It requires the use of a remote procedure call
framework called gRPC.  The Spark team are developing solutions to integrate with
Connect via Python, and Scala. These solutions interact directly with gRPC.  

In `sparklyr`, we will use `reticulate` to interact with the Python solution. At
this time, `reticulate` offers a very robust integration with Python, that is very
easy to setup and use.  While it is possible to use the `reticulate` package to start
and interact with Connect directly, the additional functionality that `sparklyr`
offers would not be present. Such functionality includes: `dplyr` back-end, `DBI` back-end,
RStudio's Connections pane integration, and others.

In order to quickly iterate enhancements and bug fixes, we have decided to isolate
the Python integration into its own package. The new package, called `pysparklyr`,
is an extension of `sparklyr`. 


## Package Installation

As mentioned above, we are still working integrating `sparklyr` with Connect.
For now, to access the new capabilities, you will need the development versions of the
`sparklyr`, and `pysparklyr`, packages. These versions are available in their
respective GitHub repositories. To install use: 

``` r
library(remotes)

install_github("sparklyr/sparklyr")
install_github("mlverse/pysparklyr")
```

## Initial Setup

To communicate with Connect, `sparklyr` needs specific Python components.
We provide a helper function that installs those components. 
You should only need to run this command the first time you install `pysparklyr`:

``` r
pysparklyr::install_pyspark()
```

### Additional setup details

The function will attempt to install Python if one is not found. It will also
create a new Python Virtual Environment. The new virtual environment will contain
the necessary Python libraries that `sparklyr` will use to interact with Connect. 
In addition, `install_pyspark()` installs specific versions of the libraries in order
to insure compatability. It does this by taking advantage of the snapshot
capabilities of 
[Posit's Package Manager](https://packagemanager.posit.co/client/#/repos/5/packages/appdirs).

By default, `sparklyr` will use whatever virtual environment is currently loaded
in your session. If none is loaded, then `sparklyr` will load the one installed
by `install_pyspark()`.  The default name of the Virtual Environment that `sparklyr`
uses is `r-sparklyr`.  If you wish to use your own Python environment, then 
just make sure to load it before calling `spark_connect()`. If there is a
Python environment already loaded when you connect to your Spark cluster, then
`sparklyr` will use that environment instead.



## Databricks Connect (ML 13+)

### Connecting

For convenience, and safety, you can save your authentication token
(PAT), and your company's Databrick's URL in a environment variable. The
two variables are:

-   `DATABRICKS_TOKEN`
-   `DATABRICKS_HOST`

This will prevent your token to be shown in plain text in your code. You
can set the environment variables at the beginning of your R session
using `Sys.setenv()`. Preferably, the two variables can be set for all R
sessions by saving them to the `.Renviron` file. The `usethis` package
has a handy function that opens the file so you can edit it:
`usethis::edit_r_environ()`. Just add the two entries to your
`.Renviron` file.

After that, use `spark_connect()` to open the connection to Databricks.
You will need to only pass the `cluster_id` of your cluster, and tell
`sparklyr` that you are connecting to a cluster version ML 13+ by using
`method = "databricks_connect"`

```{r}
library(sparklyr)

sc <- spark_connect(
  cluster_id = "0608-170338-jwkec0wi",
  method = "databricks_connect"
)
```

### RStudio's Connection pane

Thanks to the new way we are integrating with Spark, it is now possible to 
display the same structure that you would see in the  Databricks website under
**Data Explorer**. In Databricks, the current data structure levels are:

- Catalog
  - Database
    - Table

In RStudio, you can navigate the data structure by expanding from the top level,
all the way down to the table you wish to explore. Once expanded, the table's 
fields, and their types are displayed. 

![](/images/deployment/connect/rstudio-connection.png)

In the Connection Pane, you can click on the **table** icon, situated to the
right of the table name, to preview the first 1,000 rows:

![](/images/deployment/connect/preview.png)


### Using 

Through `pysparklyr`, `sparklyr` is able to display the navigation to
the full data catalog accessible to your user in Databricks. That will
be displayed in the RStudio's Connections Pane, as shown below.

You can use `dbplyr`'s `in_catalog()` function to access the different
tables inside your data catalog. After pointing `tbl()` to a specific
table, you can use `dplyr`.

```{r}
library(dplyr)
library(dbplyr)

trips <- tbl(sc, in_catalog("samples", "nyctaxi", "trips"))

trips
```

```{r}
trips %>%
  group_by(pickup_zip) %>%
  summarise(
    count = n(),
    avg_distance = mean(trip_distance, na.rm = TRUE)
  )
```

```{r}
spark_disconnect(sc)
```

## Spark Connect

### Connecting

### Using

## What is supported
