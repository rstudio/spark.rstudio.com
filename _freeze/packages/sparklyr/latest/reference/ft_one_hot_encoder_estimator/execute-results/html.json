{
  "hash": "bcd7de570ead5659f6df8a52cd88be46",
  "result": {
    "markdown": "---\ntitle: \"Feature Transformation -- OneHotEncoderEstimator (Estimator)\"\nexecute:\n  freeze: true\n---\n\n\n\n\n*R/ml_feature_one_hot_encoder_estimator.R*\n\n## ft_one_hot_encoder_estimator\n\n## Description\nA one-hot encoder that maps a column of category indices   to a column of binary vectors, with at most a single one-value   per row that indicates the input category index. For example   with 5 categories, an input value of 2.0 would map to an output   vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included   by default (configurable via dropLast), because it makes the   vector entries sum up to one, and hence linearly dependent. So   an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0]. \n\n\n## Usage\n```r\nft_one_hot_encoder_estimator( \n  x, \n  input_cols = NULL, \n  output_cols = NULL, \n  handle_invalid = \"error\", \n  drop_last = TRUE, \n  uid = random_string(\"one_hot_encoder_estimator_\"), \n  ... \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | A `spark_connection`, `ml_pipeline`, or a `tbl_spark`. |\n| input_cols | Names of input columns. |\n| output_cols | Names of output columns. |\n| handle_invalid | (Spark 2.1.0+) Param for how to handle invalid entries. Options are 'skip' (filter out rows with invalid values), 'error' (throw an error), or 'keep' (keep invalid values in a special additional bucket). Default: \"error\" |\n| drop_last | Whether to drop the last category. Defaults to `TRUE`. |\n| uid | A character string used to uniquely identify the feature transformer. |\n| ... | Optional arguments; currently unused. |\n\n## Details\n\nIn the case where `x` is a `tbl_spark`, the estimator fits against `x`\n\n  to obtain a transformer, which is then immediately used to transform `x`, returning a `tbl_spark`. \n\n\n## Value\n\nThe object returned depends on the class of `x`. \n\n  \n\n- `spark_connection`: When `x` is a `spark_connection`, the function returns a `ml_transformer`,   a `ml_estimator`, or one of their subclasses. The object contains a pointer to   a Spark `Transformer` or `Estimator` object and can be used to compose   `Pipeline` objects. \n\n  \n\n- `ml_pipeline`: When `x` is a `ml_pipeline`, the function returns a `ml_pipeline` with   the transformer or estimator appended to the pipeline. \n\n  \n\n- `tbl_spark`: When `x` is a `tbl_spark`, a transformer is constructed then   immediately applied to the input `tbl_spark`, returning a `tbl_spark`\n\n\n\n## See Also\n\nSee [https://spark.apache.org/docs/latest/ml-features.html](https://spark.apache.org/docs/latest/ml-features.html) for   more information on the set of transformations available for DataFrame   columns in Spark. \n\nOther feature transformers:  `ft_binarizer()`, `ft_bucketizer()`, `ft_chisq_selector()`, `ft_count_vectorizer()`, `ft_dct()`, `ft_elementwise_product()`, `ft_feature_hasher()`, `ft_hashing_tf()`, `ft_idf()`, `ft_imputer()`, `ft_index_to_string()`, `ft_interaction()`, `ft_lsh`, `ft_max_abs_scaler()`, `ft_min_max_scaler()`, `ft_ngram()`, `ft_normalizer()`, `ft_one_hot_encoder()`, `ft_pca()`, `ft_polynomial_expansion()`, `ft_quantile_discretizer()`, `ft_r_formula()`, `ft_regex_tokenizer()`, `ft_robust_scaler()`, `ft_sql_transformer()`, `ft_standard_scaler()`, `ft_stop_words_remover()`, `ft_string_indexer()`, `ft_tokenizer()`, `ft_vector_assembler()`, `ft_vector_indexer()`, `ft_vector_slicer()`, `ft_word2vec()`\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}