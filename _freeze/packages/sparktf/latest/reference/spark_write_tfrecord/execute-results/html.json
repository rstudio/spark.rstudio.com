{
  "hash": "5bda8e8c0ec7cb0f9f8b907611065d1c",
  "result": {
    "markdown": "---\ntitle: \"Write a Spark DataFrame to a TFRecord file\"\nexecute:\n  freeze: true\n---\n\n\n\n\n*R/writer.R*\n\n## spark_write_tfrecord\n\n## Description\nSerialize a Spark DataFrame to the TensorFlow TFRecord format for   training or inference. \n\n\n## Usage\n```r\nspark_write_tfrecord(x, path, record_type = c(\"Example\", \n  \"SequenceExample\"), write_locality = c(\"distributed\", \"local\"), \n  mode = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | A Spark DataFrame |\n| path | The path to the file. Needs to be accessible from the cluster. Supports the \"hdfs://\", \"s3a://\", and \"file://\" protocols. |\n| record_type | Output format of TensorFlow records. One of `\"Example\"` and  `\"SequenceExample\"`. |\n| write_locality | Determines whether the TensorFlow records are written locally on the workers or on a distributed file system. One of `\"distributed\"` and `\"local\"`. See Details for more information. |\n| mode | A `character` element. Specifies the behavior when data or   table already exists. Supported values include: 'error', 'append', 'overwrite' and   'ignore'. Notice that 'overwrite' will also change the column structure. <br>  For more details see also [http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes](http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes)<br>  for your version of Spark. |\n\n## Details\nFor `write_locality = local`, each of the workers stores on the   local disk a subset of the data. The subset that is stored on each worker   is determined by the partitioning of the DataFrame. Each of the partitions   is coalesced into a single TFRecord file and written on the node where the   partition lives. This is useful in the context of distributed training, in which   each of the workers gets a subset of the data to work on. When this mode is   activated, the path provided to the writer is interpreted as a base path   that is created on each of the worker nodes, and that will be populated with data   from the DataFrame. \n\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}