[
  {
    "objectID": "dplyr.html#overview",
    "href": "dplyr.html#overview",
    "title": "sparklyr",
    "section": "Overview",
    "text": "Overview\ndplyr is an R package for working with structured data both in and outside of R. dplyr makes data manipulation for R users easy, consistent, and performant. With dplyr as an interface to manipulating Spark DataFrames, you can:\n\nSelect, filter, and aggregate data\nUse window functions (e.g. for sampling)\nPerform joins on DataFrames\nCollect data from Spark into R\n\nStatements in dplyr can be chained together using pipes defined by the magrittr R package. dplyr also supports non-standard evalution of its arguments. For more information on dplyr, see the introduction, a guide for connecting to databases, and a variety of vignettes."
  },
  {
    "objectID": "dplyr.html#reading-data",
    "href": "dplyr.html#reading-data",
    "title": "sparklyr",
    "section": "Reading Data",
    "text": "Reading Data\nYou can read data into Spark DataFrames using the following functions:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nspark_read_csv\nReads a CSV file and provides a data source compatible with dplyr\n\n\nspark_read_json\nReads a JSON file and provides a data source compatible with dplyr\n\n\nspark_read_parquet\nReads a parquet file and provides a data source compatible with dplyr\n\n\n\nRegardless of the format of your data, Spark supports reading data from a variety of different data sources. These include data stored on HDFS (hdfs:// protocol), Amazon S3 (s3n:// protocol), or local files available to the Spark worker nodes (file:// protocol)\nEach of these functions returns a reference to a Spark DataFrame which can be used as a dplyr table (tbl).\n\nFlights Data\nThis guide will demonstrate some of the basic data manipulation verbs of dplyr by using data from the nycflights13 R package. This package contains data for all 336,776 flights departing New York City in 2013. It also includes useful metadata on airlines, airports, weather, and planes. The data comes from the US Bureau of Transportation Statistics, and is documented in ?nycflights13\nConnect to the cluster and copy the flights data using the copy_to function. Caveat: The flight data in nycflights13 is convenient for dplyr demonstrations because it is small, but in practice large data should rarely be copied directly from R objects.\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(nycflights13)\nlibrary(ggplot2)\n\nsc <- spark_connect(master=\"local\")\nflights <- copy_to(sc, flights, \"flights\")\nairlines <- copy_to(sc, airlines, \"airlines\")\ndplyr::src_tbls(sc)\n## [1] \"airlines\" \"flights\""
  },
  {
    "objectID": "dplyr.html#dplyr-verbs",
    "href": "dplyr.html#dplyr-verbs",
    "title": "sparklyr",
    "section": "dplyr Verbs",
    "text": "dplyr Verbs\nVerbs are dplyr commands for manipulating data. When connected to a Spark DataFrame, dplyr translates the commands into Spark SQL statements. Remote data sources use exactly the same five verbs as local data sources. Here are the five verbs with their corresponding SQL commands:\n\nselect ~ SELECT\nfilter ~ WHERE\narrange ~ ORDER\nsummarise ~ aggregators: sum, min, sd, etc.\nmutate ~ operators: +, *, log, etc.\n\n\nselect(flights, year:day, arr_delay, dep_delay)\n## # Source: lazy query [?? x 5]\n## # Database: spark_connection\n##     year month   day arr_delay dep_delay\n##    <int> <int> <int>     <dbl>     <dbl>\n##  1  2013     1     1     11.0       2.00\n##  2  2013     1     1     20.0       4.00\n##  3  2013     1     1     33.0       2.00\n##  4  2013     1     1    -18.0      -1.00\n##  5  2013     1     1    -25.0      -6.00\n##  6  2013     1     1     12.0      -4.00\n##  7  2013     1     1     19.0      -5.00\n##  8  2013     1     1    -14.0      -3.00\n##  9  2013     1     1    - 8.00     -3.00\n## 10  2013     1     1      8.00     -2.00\n## # ... with more rows\nfilter(flights, dep_delay > 1000)\n## # Source: lazy query [?? x 19]\n## # Database: spark_connection\n##    year month   day dep_t~ sche~ dep_~ arr_~ sche~ arr_~ carr~ flig~ tail~\n##   <int> <int> <int>  <int> <int> <dbl> <int> <int> <dbl> <chr> <int> <chr>\n## 1  2013     1     9    641   900  1301  1242  1530  1272 HA       51 N384~\n## 2  2013     1    10   1121  1635  1126  1239  1810  1109 MQ     3695 N517~\n## 3  2013     6    15   1432  1935  1137  1607  2120  1127 MQ     3535 N504~\n## 4  2013     7    22    845  1600  1005  1044  1815   989 MQ     3075 N665~\n## 5  2013     9    20   1139  1845  1014  1457  2210  1007 AA      177 N338~\n## # ... with 7 more variables: origin <chr>, dest <chr>, air_time <dbl>,\n## #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dbl>\narrange(flights, desc(dep_delay))\n## # Source: table<flights> [?? x 19]\n## # Database: spark_connection\n## # Ordered by: desc(dep_delay)\n##     year month   day dep_~ sche~ dep_~ arr_~ sche~ arr_~ carr~ flig~ tail~\n##    <int> <int> <int> <int> <int> <dbl> <int> <int> <dbl> <chr> <int> <chr>\n##  1  2013     1     9   641   900  1301  1242  1530  1272 HA       51 N384~\n##  2  2013     6    15  1432  1935  1137  1607  2120  1127 MQ     3535 N504~\n##  3  2013     1    10  1121  1635  1126  1239  1810  1109 MQ     3695 N517~\n##  4  2013     9    20  1139  1845  1014  1457  2210  1007 AA      177 N338~\n##  5  2013     7    22   845  1600  1005  1044  1815   989 MQ     3075 N665~\n##  6  2013     4    10  1100  1900   960  1342  2211   931 DL     2391 N959~\n##  7  2013     3    17  2321   810   911   135  1020   915 DL     2119 N927~\n##  8  2013     6    27   959  1900   899  1236  2226   850 DL     2007 N376~\n##  9  2013     7    22  2257   759   898   121  1026   895 DL     2047 N671~\n## 10  2013    12     5   756  1700   896  1058  2020   878 AA      172 N5DM~\n## # ... with more rows, and 7 more variables: origin <chr>, dest <chr>,\n## #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour\n## #   <dbl>\nsummarise(flights, mean_dep_delay = mean(dep_delay))\n## Warning: Missing values are always removed in SQL.\n## Use `AVG(x, na.rm = TRUE)` to silence this warning\n\n## # Source: lazy query [?? x 1]\n## # Database: spark_connection\n##   mean_dep_delay\n##            <dbl>\n## 1           12.6\nmutate(flights, speed = distance / air_time * 60)\n## # Source: lazy query [?? x 20]\n## # Database: spark_connection\n##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~\n##    <int> <int> <int>  <int>   <int>  <dbl> <int>  <int>  <dbl> <chr> <int>\n##  1  2013     1     1    517     515   2.00   830    819  11.0  UA     1545\n##  2  2013     1     1    533     529   4.00   850    830  20.0  UA     1714\n##  3  2013     1     1    542     540   2.00   923    850  33.0  AA     1141\n##  4  2013     1     1    544     545  -1.00  1004   1022 -18.0  B6      725\n##  5  2013     1     1    554     600  -6.00   812    837 -25.0  DL      461\n##  6  2013     1     1    554     558  -4.00   740    728  12.0  UA     1696\n##  7  2013     1     1    555     600  -5.00   913    854  19.0  B6      507\n##  8  2013     1     1    557     600  -3.00   709    723 -14.0  EV     5708\n##  9  2013     1     1    557     600  -3.00   838    846 - 8.00 B6       79\n## 10  2013     1     1    558     600  -2.00   753    745   8.00 AA      301\n## # ... with more rows, and 9 more variables: tailnum <chr>, origin <chr>,\n## #   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n## #   time_hour <dbl>, speed <dbl>"
  },
  {
    "objectID": "dplyr.html#laziness",
    "href": "dplyr.html#laziness",
    "title": "sparklyr",
    "section": "Laziness",
    "text": "Laziness\nWhen working with databases, dplyr tries to be as lazy as possible:\n\nIt never pulls data into R unless you explicitly ask for it.\nIt delays doing any work until the last possible moment: it collects together everything you want to do and then sends it to the database in one step.\n\nFor example, take the following code:\nc1 <- filter(flights, day == 17, month == 5, carrier %in% c('UA', 'WN', 'AA', 'DL'))\nc2 <- select(c1, year, month, day, carrier, dep_delay, air_time, distance)\nc3 <- arrange(c2, year, month, day, carrier)\nc4 <- mutate(c3, air_time_hours = air_time / 60)\nThis sequence of operations never actually touches the database. It’s not until you ask for the data (e.g. by printing c4) that dplyr requests the results from the database.\nc4\n## # Source: lazy query [?? x 8]\n## # Database: spark_connection\n## # Ordered by: year, month, day, carrier\n##     year month   day carrier dep_delay air_time distance air_time_hours\n##    <int> <int> <int> <chr>       <dbl>    <dbl>    <dbl>          <dbl>\n##  1  2013     5    17 AA          -2.00      294     2248           4.90\n##  2  2013     5    17 AA          -1.00      146     1096           2.43\n##  3  2013     5    17 AA          -2.00      185     1372           3.08\n##  4  2013     5    17 AA          -9.00      186     1389           3.10\n##  5  2013     5    17 AA           2.00      147     1096           2.45\n##  6  2013     5    17 AA          -4.00      114      733           1.90\n##  7  2013     5    17 AA          -7.00      117      733           1.95\n##  8  2013     5    17 AA          -7.00      142     1089           2.37\n##  9  2013     5    17 AA          -6.00      148     1089           2.47\n## 10  2013     5    17 AA          -7.00      137      944           2.28\n## # ... with more rows"
  },
  {
    "objectID": "dplyr.html#piping",
    "href": "dplyr.html#piping",
    "title": "sparklyr",
    "section": "Piping",
    "text": "Piping\nYou can use magrittr pipes to write cleaner syntax. Using the same example from above, you can write a much cleaner version like this:\nc4 <- flights %>%\n  filter(month == 5, day == 17, carrier %in% c('UA', 'WN', 'AA', 'DL')) %>%\n  select(carrier, dep_delay, air_time, distance) %>%\n  arrange(carrier) %>%\n  mutate(air_time_hours = air_time / 60)"
  },
  {
    "objectID": "dplyr.html#grouping",
    "href": "dplyr.html#grouping",
    "title": "sparklyr",
    "section": "Grouping",
    "text": "Grouping\nThe group_by function corresponds to the GROUP BY statement in SQL.\nc4 %>%\n  group_by(carrier) %>%\n  summarize(count = n(), mean_dep_delay = mean(dep_delay))\n## Warning: Missing values are always removed in SQL.\n## Use `AVG(x, na.rm = TRUE)` to silence this warning\n\n## # Source: lazy query [?? x 3]\n## # Database: spark_connection\n##   carrier count mean_dep_delay\n##   <chr>   <dbl>          <dbl>\n## 1 AA       94.0           1.47\n## 2 DL      136             6.24\n## 3 UA      172             9.63\n## 4 WN       34.0           7.97"
  },
  {
    "objectID": "dplyr.html#collecting-to-r",
    "href": "dplyr.html#collecting-to-r",
    "title": "sparklyr",
    "section": "Collecting to R",
    "text": "Collecting to R\nYou can copy data from Spark into R’s memory by using collect().\ncarrierhours <- collect(c4)\ncollect() executes the Spark query and returns the results to R for further analysis and visualization.\n# Test the significance of pairwise differences and plot the results\nwith(carrierhours, pairwise.t.test(air_time, carrier))\n## \n##  Pairwise comparisons using t tests with pooled SD \n## \n## data:  air_time and carrier \n## \n##    AA      DL      UA     \n## DL 0.25057 -       -      \n## UA 0.07957 0.00044 -      \n## WN 0.07957 0.23488 0.00041\n## \n## P value adjustment method: holm\nggplot(carrierhours, aes(carrier, air_time_hours)) + geom_boxplot()"
  },
  {
    "objectID": "dplyr.html#sql-translation",
    "href": "dplyr.html#sql-translation",
    "title": "sparklyr",
    "section": "SQL Translation",
    "text": "SQL Translation\nIt’s relatively straightforward to translate R code to SQL (or indeed to any programming language) when doing simple mathematical operations of the form you normally use when filtering, mutating and summarizing. dplyr knows how to convert the following R functions to Spark SQL:\n# Basic math operators\n+, -, *, /, %%, ^\n  \n# Math functions\nabs, acos, asin, asinh, atan, atan2, ceiling, cos, cosh, exp, floor, log, log10, round, sign, sin, sinh, sqrt, tan, tanh\n\n# Logical comparisons\n<, <=, !=, >=, >, ==, %in%\n\n# Boolean operations\n&, &&, |, ||, !\n\n# Character functions\npaste, tolower, toupper, nchar\n\n# Casting\nas.double, as.integer, as.logical, as.character, as.date\n\n# Basic aggregations\nmean, sum, min, max, sd, var, cor, cov, n"
  },
  {
    "objectID": "dplyr.html#window-functions",
    "href": "dplyr.html#window-functions",
    "title": "sparklyr",
    "section": "Window Functions",
    "text": "Window Functions\ndplyr supports Spark SQL window functions. Window functions are used in conjunction with mutate and filter to solve a wide range of problems. You can compare the dplyr syntax to the query it has generated by using dbplyr::sql_render().\n# Find the most and least delayed flight each day\nbestworst <- flights %>%\n  group_by(year, month, day) %>%\n  select(dep_delay) %>% \n  filter(dep_delay == min(dep_delay) || dep_delay == max(dep_delay))\ndbplyr::sql_render(bestworst)\n## Warning: Missing values are always removed in SQL.\n## Use `min(x, na.rm = TRUE)` to silence this warning\n## Warning: Missing values are always removed in SQL.\n## Use `max(x, na.rm = TRUE)` to silence this warning\n## <SQL> SELECT `year`, `month`, `day`, `dep_delay`\n## FROM (SELECT `year`, `month`, `day`, `dep_delay`, min(`dep_delay`) OVER (PARTITION BY `year`, `month`, `day`) AS `zzz3`, max(`dep_delay`) OVER (PARTITION BY `year`, `month`, `day`) AS `zzz4`\n## FROM (SELECT `year`, `month`, `day`, `dep_delay`\n## FROM `flights`) `coaxmtqqbj`) `efznnpuovy`\n## WHERE (`dep_delay` = `zzz3` OR `dep_delay` = `zzz4`)\nbestworst\n## Warning: Missing values are always removed in SQL.\n## Use `min(x, na.rm = TRUE)` to silence this warning\n\n## Warning: Missing values are always removed in SQL.\n## Use `max(x, na.rm = TRUE)` to silence this warning\n## # Source: lazy query [?? x 4]\n## # Database: spark_connection\n## # Groups: year, month, day\n##     year month   day dep_delay\n##    <int> <int> <int>     <dbl>\n##  1  2013     1     1     853  \n##  2  2013     1     1   -  15.0\n##  3  2013     1     1   -  15.0\n##  4  2013     1     9    1301  \n##  5  2013     1     9   -  17.0\n##  6  2013     1    24   -  15.0\n##  7  2013     1    24     329  \n##  8  2013     1    29   -  27.0\n##  9  2013     1    29     235  \n## 10  2013     2     1   -  15.0\n## # ... with more rows\n# Rank each flight within a daily\nranked <- flights %>%\n  group_by(year, month, day) %>%\n  select(dep_delay) %>% \n  mutate(rank = rank(desc(dep_delay)))\ndbplyr::sql_render(ranked)\n## <SQL> SELECT `year`, `month`, `day`, `dep_delay`, rank() OVER (PARTITION BY `year`, `month`, `day` ORDER BY `dep_delay` DESC) AS `rank`\n## FROM (SELECT `year`, `month`, `day`, `dep_delay`\n## FROM `flights`) `mauqwkxuam`\nranked\n## # Source: lazy query [?? x 5]\n## # Database: spark_connection\n## # Groups: year, month, day\n##     year month   day dep_delay  rank\n##    <int> <int> <int>     <dbl> <int>\n##  1  2013     1     1       853     1\n##  2  2013     1     1       379     2\n##  3  2013     1     1       290     3\n##  4  2013     1     1       285     4\n##  5  2013     1     1       260     5\n##  6  2013     1     1       255     6\n##  7  2013     1     1       216     7\n##  8  2013     1     1       192     8\n##  9  2013     1     1       157     9\n## 10  2013     1     1       155    10\n## # ... with more rows"
  },
  {
    "objectID": "dplyr.html#peforming-joins",
    "href": "dplyr.html#peforming-joins",
    "title": "sparklyr",
    "section": "Peforming Joins",
    "text": "Peforming Joins\nIt’s rare that a data analysis involves only a single table of data. In practice, you’ll normally have many tables that contribute to an analysis, and you need flexible tools to combine them. In dplyr, there are three families of verbs that work with two tables at a time:\n\nMutating joins, which add new variables to one table from matching rows in another.\nFiltering joins, which filter observations from one table based on whether or not they match an observation in the other table.\nSet operations, which combine the observations in the data sets as if they were set elements.\n\nAll two-table verbs work similarly. The first two arguments are x and y, and provide the tables to combine. The output is always a new table with the same type as x.\nThe following statements are equivalent:\nflights %>% left_join(airlines)\n## Joining, by = \"carrier\"\n\n## # Source: lazy query [?? x 20]\n## # Database: spark_connection\n##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~\n##    <int> <int> <int>  <int>   <int>  <dbl> <int>  <int>  <dbl> <chr> <int>\n##  1  2013     1     1    517     515   2.00   830    819  11.0  UA     1545\n##  2  2013     1     1    533     529   4.00   850    830  20.0  UA     1714\n##  3  2013     1     1    542     540   2.00   923    850  33.0  AA     1141\n##  4  2013     1     1    544     545  -1.00  1004   1022 -18.0  B6      725\n##  5  2013     1     1    554     600  -6.00   812    837 -25.0  DL      461\n##  6  2013     1     1    554     558  -4.00   740    728  12.0  UA     1696\n##  7  2013     1     1    555     600  -5.00   913    854  19.0  B6      507\n##  8  2013     1     1    557     600  -3.00   709    723 -14.0  EV     5708\n##  9  2013     1     1    557     600  -3.00   838    846 - 8.00 B6       79\n## 10  2013     1     1    558     600  -2.00   753    745   8.00 AA      301\n## # ... with more rows, and 9 more variables: tailnum <chr>, origin <chr>,\n## #   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n## #   time_hour <dbl>, name <chr>\nflights %>% left_join(airlines, by = \"carrier\")\n## # Source: lazy query [?? x 20]\n## # Database: spark_connection\n##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~\n##    <int> <int> <int>  <int>   <int>  <dbl> <int>  <int>  <dbl> <chr> <int>\n##  1  2013     1     1    517     515   2.00   830    819  11.0  UA     1545\n##  2  2013     1     1    533     529   4.00   850    830  20.0  UA     1714\n##  3  2013     1     1    542     540   2.00   923    850  33.0  AA     1141\n##  4  2013     1     1    544     545  -1.00  1004   1022 -18.0  B6      725\n##  5  2013     1     1    554     600  -6.00   812    837 -25.0  DL      461\n##  6  2013     1     1    554     558  -4.00   740    728  12.0  UA     1696\n##  7  2013     1     1    555     600  -5.00   913    854  19.0  B6      507\n##  8  2013     1     1    557     600  -3.00   709    723 -14.0  EV     5708\n##  9  2013     1     1    557     600  -3.00   838    846 - 8.00 B6       79\n## 10  2013     1     1    558     600  -2.00   753    745   8.00 AA      301\n## # ... with more rows, and 9 more variables: tailnum <chr>, origin <chr>,\n## #   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n## #   time_hour <dbl>, name <chr>\nflights %>% left_join(airlines, by = c(\"carrier\", \"carrier\"))\n## # Source: lazy query [?? x 20]\n## # Database: spark_connection\n##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~\n##    <int> <int> <int>  <int>   <int>  <dbl> <int>  <int>  <dbl> <chr> <int>\n##  1  2013     1     1    517     515   2.00   830    819  11.0  UA     1545\n##  2  2013     1     1    533     529   4.00   850    830  20.0  UA     1714\n##  3  2013     1     1    542     540   2.00   923    850  33.0  AA     1141\n##  4  2013     1     1    544     545  -1.00  1004   1022 -18.0  B6      725\n##  5  2013     1     1    554     600  -6.00   812    837 -25.0  DL      461\n##  6  2013     1     1    554     558  -4.00   740    728  12.0  UA     1696\n##  7  2013     1     1    555     600  -5.00   913    854  19.0  B6      507\n##  8  2013     1     1    557     600  -3.00   709    723 -14.0  EV     5708\n##  9  2013     1     1    557     600  -3.00   838    846 - 8.00 B6       79\n## 10  2013     1     1    558     600  -2.00   753    745   8.00 AA      301\n## # ... with more rows, and 9 more variables: tailnum <chr>, origin <chr>,\n## #   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n## #   time_hour <dbl>, name <chr>"
  },
  {
    "objectID": "dplyr.html#sampling",
    "href": "dplyr.html#sampling",
    "title": "sparklyr",
    "section": "Sampling",
    "text": "Sampling\nYou can use sample_n() and sample_frac() to take a random sample of rows: use sample_n() for a fixed number and sample_frac() for a fixed fraction.\nsample_n(flights, 10)\n## # Source: lazy query [?? x 19]\n## # Database: spark_connection\n##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~\n##    <int> <int> <int>  <int>   <int>  <dbl> <int>  <int>  <dbl> <chr> <int>\n##  1  2013     1     1    517     515   2.00   830    819  11.0  UA     1545\n##  2  2013     1     1    533     529   4.00   850    830  20.0  UA     1714\n##  3  2013     1     1    542     540   2.00   923    850  33.0  AA     1141\n##  4  2013     1     1    544     545  -1.00  1004   1022 -18.0  B6      725\n##  5  2013     1     1    554     600  -6.00   812    837 -25.0  DL      461\n##  6  2013     1     1    554     558  -4.00   740    728  12.0  UA     1696\n##  7  2013     1     1    555     600  -5.00   913    854  19.0  B6      507\n##  8  2013     1     1    557     600  -3.00   709    723 -14.0  EV     5708\n##  9  2013     1     1    557     600  -3.00   838    846 - 8.00 B6       79\n## 10  2013     1     1    558     600  -2.00   753    745   8.00 AA      301\n## # ... with more rows, and 8 more variables: tailnum <chr>, origin <chr>,\n## #   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n## #   time_hour <dbl>\nsample_frac(flights, 0.01)\n## # Source: lazy query [?? x 19]\n## # Database: spark_connection\n##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~\n##    <int> <int> <int>  <int>   <int>  <dbl> <int>  <int>  <dbl> <chr> <int>\n##  1  2013     1     1    655     655   0     1021   1030 - 9.00 DL     1415\n##  2  2013     1     1    656     700 - 4.00   854    850   4.00 AA      305\n##  3  2013     1     1   1044    1045 - 1.00  1231   1212  19.0  EV     4322\n##  4  2013     1     1   1056    1059 - 3.00  1203   1209 - 6.00 EV     4479\n##  5  2013     1     1   1317    1325 - 8.00  1454   1505 -11.0  MQ     4475\n##  6  2013     1     1   1708    1700   8.00  2037   2005  32.0  WN     1066\n##  7  2013     1     1   1825    1829 - 4.00  2056   2053   3.00 9E     3286\n##  8  2013     1     1   1843    1845 - 2.00  1955   2024 -29.0  DL      904\n##  9  2013     1     1   2108    2057  11.0     25     39 -14.0  UA     1517\n## 10  2013     1     2    557     605 - 8.00   832    823   9.00 DL      544\n## # ... with more rows, and 8 more variables: tailnum <chr>, origin <chr>,\n## #   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n## #   time_hour <dbl>"
  },
  {
    "objectID": "dplyr.html#writing-data",
    "href": "dplyr.html#writing-data",
    "title": "sparklyr",
    "section": "Writing Data",
    "text": "Writing Data\nIt is often useful to save the results of your analysis or the tables that you have generated on your Spark cluster into persistent storage. The best option in many scenarios is to write the table out to a Parquet file using the spark_write_parquet function. For example:\nspark_write_parquet(tbl, \"hdfs://hdfs.company.org:9000/hdfs-path/data\")\nThis will write the Spark DataFrame referenced by the tbl R variable to the given HDFS path. You can use the spark_read_parquet function to read the same table back into a subsequent Spark session:\ntbl <- spark_read_parquet(sc, \"data\", \"hdfs://hdfs.company.org:9000/hdfs-path/data\")\nYou can also write data as CSV or JSON using the spark_write_csv and spark_write_json functions."
  },
  {
    "objectID": "dplyr.html#hive-functions",
    "href": "dplyr.html#hive-functions",
    "title": "sparklyr",
    "section": "Hive Functions",
    "text": "Hive Functions\nMany of Hive’s built-in functions (UDF) and built-in aggregate functions (UDAF) can be called inside dplyr’s mutate and summarize. The Languange Reference UDF page provides the list of available functions.\nThe following example uses the datediff and current_date Hive UDFs to figure the difference between the flight_date and the current system date:\nflights %>% \n  mutate(flight_date = paste(year,month,day,sep=\"-\"),\n         days_since = datediff(current_date(), flight_date)) %>%\n  group_by(flight_date,days_since) %>%\n  tally() %>%\n  arrange(-days_since)\n## # Source: lazy query [?? x 3]\n## # Database: spark_connection\n## # Groups: flight_date\n## # Ordered by: -days_since\n##    flight_date days_since     n\n##    <chr>            <int> <dbl>\n##  1 2013-1-1          1844   842\n##  2 2013-1-2          1843   943\n##  3 2013-1-3          1842   914\n##  4 2013-1-4          1841   915\n##  5 2013-1-5          1840   720\n##  6 2013-1-6          1839   832\n##  7 2013-1-7          1838   933\n##  8 2013-1-8          1837   899\n##  9 2013-1-9          1836   902\n## 10 2013-1-10         1835   932\n## # ... with more rows"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "sparklyr",
    "section": "Installation",
    "text": "Installation\nYou can install the sparklyr package from CRAN as follows:\ninstall.packages(\"sparklyr\")\nYou should also install a local version of Spark for development purposes:\nlibrary(sparklyr)\nspark_install(version = \"2.1.0\")\nTo upgrade to the latest version of sparklyr, run the following command and restart your r session:\ndevtools::install_github(\"rstudio/sparklyr\")\nIf you use the RStudio IDE, you should also download the latest preview release of the IDE which includes several enhancements for interacting with Spark (see the RStudio IDE section below for more details)."
  },
  {
    "objectID": "index.html#connecting-to-spark",
    "href": "index.html#connecting-to-spark",
    "title": "sparklyr",
    "section": "Connecting to Spark",
    "text": "Connecting to Spark\nYou can connect to both local instances of Spark as well as remote Spark clusters. Here we’ll connect to a local instance of Spark via the spark_connect function:\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nThe returned Spark connection (sc) provides a remote dplyr data source to the Spark cluster.\nFor more information on connecting to remote Spark clusters see the Deployment section of the sparklyr website."
  },
  {
    "objectID": "index.html#using-dplyr",
    "href": "index.html#using-dplyr",
    "title": "sparklyr",
    "section": "Using dplyr",
    "text": "Using dplyr\nWe can now use all of the available dplyr verbs against the tables within the cluster.\nWe’ll start by copying some datasets from R into the Spark cluster (note that you may need to install the nycflights13 and Lahman packages in order to execute this code):\ninstall.packages(c(\"nycflights13\", \"Lahman\"))\nlibrary(dplyr)\niris_tbl <- copy_to(sc, iris)\nflights_tbl <- copy_to(sc, nycflights13::flights, \"flights\")\nbatting_tbl <- copy_to(sc, Lahman::Batting, \"batting\")\ndplyr::src_tbls(sc)\n## [1] \"batting\" \"flights\" \"iris\"\nTo start with here’s a simple filtering example:\n# filter by departure delay and print the first few records\nflights_tbl %>% filter(dep_delay == 2)\n## # Source:   lazy query [?? x 19]\n## # Database: spark_connection\n##     year month   day dep_time sched_dep_time dep_delay arr_time\n##    <int> <int> <int>    <int>          <int>     <dbl>    <int>\n##  1  2013     1     1      517            515         2      830\n##  2  2013     1     1      542            540         2      923\n##  3  2013     1     1      702            700         2     1058\n##  4  2013     1     1      715            713         2      911\n##  5  2013     1     1      752            750         2     1025\n##  6  2013     1     1      917            915         2     1206\n##  7  2013     1     1      932            930         2     1219\n##  8  2013     1     1     1028           1026         2     1350\n##  9  2013     1     1     1042           1040         2     1325\n## 10  2013     1     1     1231           1229         2     1523\n## # ... with more rows, and 12 more variables: sched_arr_time <int>,\n## #   arr_delay <dbl>, carrier <chr>, flight <int>, tailnum <chr>,\n## #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n## #   minute <dbl>, time_hour <dbl>\nIntroduction to dplyr provides additional dplyr examples you can try. For example, consider the last example from the tutorial which plots data on flight delays:\ndelay <- flights_tbl %>%\n  group_by(tailnum) %>%\n  summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %>%\n  filter(count > 20, dist < 2000, !is.na(delay)) %>%\n  collect\n\n# plot delays\nlibrary(ggplot2)\nggplot(delay, aes(dist, delay)) +\n  geom_point(aes(size = count), alpha = 1/2) +\n  geom_smooth() +\n  scale_size_area(max_size = 2)\n## `geom_smooth()` using method = 'gam'\n\n\nWindow Functions\ndplyr window functions are also supported, for example:\nbatting_tbl %>%\n  select(playerID, yearID, teamID, G, AB:H) %>%\n  arrange(playerID, yearID, teamID) %>%\n  group_by(playerID) %>%\n  filter(min_rank(desc(H)) <= 2 & H > 0)\n## # Source:     lazy query [?? x 7]\n## # Database:   spark_connection\n## # Groups:     playerID\n## # Ordered by: playerID, yearID, teamID\n##     playerID yearID teamID     G    AB     R     H\n##        <chr>  <int>  <chr> <int> <int> <int> <int>\n##  1 aaronha01   1959    ML1   154   629   116   223\n##  2 aaronha01   1963    ML1   161   631   121   201\n##  3 abbotji01   1999    MIL    20    21     0     2\n##  4 abnersh01   1992    CHA    97   208    21    58\n##  5 abnersh01   1990    SDN    91   184    17    45\n##  6 acklefr01   1963    CHA     2     5     0     1\n##  7 acklefr01   1964    CHA     3     1     0     1\n##  8 adamecr01   2016    COL   121   225    25    49\n##  9 adamecr01   2015    COL    26    53     4    13\n## 10 adamsac01   1943    NY1    70    32     3     4\n## # ... with more rows\nFor additional documentation on using dplyr with Spark see the dplyr section of the sparklyr website."
  },
  {
    "objectID": "index.html#using-sql",
    "href": "index.html#using-sql",
    "title": "sparklyr",
    "section": "Using SQL",
    "text": "Using SQL\nIt’s also possible to execute SQL queries directly against tables within a Spark cluster. The spark_connection object implements a DBI interface for Spark, so you can use dbGetQuery to execute SQL and return the result as an R data frame:\nlibrary(DBI)\niris_preview <- dbGetQuery(sc, \"SELECT * FROM iris LIMIT 10\")\niris_preview\n##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species\n## 1           5.1         3.5          1.4         0.2  setosa\n## 2           4.9         3.0          1.4         0.2  setosa\n## 3           4.7         3.2          1.3         0.2  setosa\n## 4           4.6         3.1          1.5         0.2  setosa\n## 5           5.0         3.6          1.4         0.2  setosa\n## 6           5.4         3.9          1.7         0.4  setosa\n## 7           4.6         3.4          1.4         0.3  setosa\n## 8           5.0         3.4          1.5         0.2  setosa\n## 9           4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa"
  },
  {
    "objectID": "index.html#machine-learning",
    "href": "index.html#machine-learning",
    "title": "sparklyr",
    "section": "Machine Learning",
    "text": "Machine Learning\nYou can orchestrate machine learning algorithms in a Spark cluster via the machine learning functions within sparklyr. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning workflows.\nHere’s an example where we use ml_linear_regression to fit a linear regression model. We’ll use the built-in mtcars dataset, and see if we can predict a car’s fuel consumption (mpg) based on its weight (wt), and the number of cylinders the engine contains (cyl). We’ll assume in each case that the relationship between mpg and each of our features is linear.\n# copy mtcars into spark\nmtcars_tbl <- copy_to(sc, mtcars)\n\n# transform our data set, and then partition into 'training', 'test'\npartitions <- mtcars_tbl %>%\n  filter(hp >= 100) %>%\n  mutate(cyl8 = cyl == 8) %>%\n  sdf_partition(training = 0.5, test = 0.5, seed = 1099)\n\n# fit a linear model to the training dataset\nfit <- partitions$training %>%\n  ml_linear_regression(response = \"mpg\", features = c(\"wt\", \"cyl\"))\nfit\n## Call: ml_linear_regression.tbl_spark(., response = \"mpg\", features = c(\"wt\", \"cyl\"))\n##\n## Formula: mpg ~ wt + cyl\n##\n## Coefficients:\n## (Intercept)          wt         cyl\n##   33.499452   -2.818463   -0.923187\nFor linear regression models produced by Spark, we can use summary() to learn a bit more about the quality of our fit, and the statistical significance of each of our predictors.\nsummary(fit)\n## Call: ml_linear_regression.tbl_spark(., response = \"mpg\", features = c(\"wt\", \"cyl\"))\n##\n## Deviance Residuals:\n##    Min     1Q Median     3Q    Max\n## -1.752 -1.134 -0.499  1.296  2.282\n##\n## Coefficients:\n## (Intercept)          wt         cyl\n##   33.499452   -2.818463   -0.923187\n##\n## R-Squared: 0.8274\n## Root Mean Squared Error: 1.422\nSpark machine learning supports a wide array of algorithms and feature transformations and as illustrated above it’s easy to chain these functions together with dplyr pipelines. To learn more see the machine learning section."
  },
  {
    "objectID": "index.html#reading-and-writing-data",
    "href": "index.html#reading-and-writing-data",
    "title": "sparklyr",
    "section": "Reading and Writing Data",
    "text": "Reading and Writing Data\nYou can read and write data in CSV, JSON, and Parquet formats. Data can be stored in HDFS, S3, or on the local filesystem of cluster nodes.\ntemp_csv <- tempfile(fileext = \".csv\")\ntemp_parquet <- tempfile(fileext = \".parquet\")\ntemp_json <- tempfile(fileext = \".json\")\n\nspark_write_csv(iris_tbl, temp_csv)\niris_csv_tbl <- spark_read_csv(sc, \"iris_csv\", temp_csv)\n\nspark_write_parquet(iris_tbl, temp_parquet)\niris_parquet_tbl <- spark_read_parquet(sc, \"iris_parquet\", temp_parquet)\n\nspark_write_json(iris_tbl, temp_json)\niris_json_tbl <- spark_read_json(sc, \"iris_json\", temp_json)\n\ndplyr::src_tbls(sc)\n## [1] \"batting\"      \"flights\"      \"iris\"         \"iris_csv\"\n## [5] \"iris_json\"    \"iris_parquet\" \"mtcars\""
  },
  {
    "objectID": "index.html#distributed-r",
    "href": "index.html#distributed-r",
    "title": "sparklyr",
    "section": "Distributed R",
    "text": "Distributed R\nYou can execute arbitrary r code across your cluster using spark_apply. For example, we can apply rgamma over iris as follows:\nspark_apply(iris_tbl, function(data) {\n  data[1:4] + rgamma(1,2)\n})\n## # Source:   table<sparklyr_tmp_115c74acb6510> [?? x 4]\n## # Database: spark_connection\n##    Sepal_Length Sepal_Width Petal_Length Petal_Width\n##           <dbl>       <dbl>        <dbl>       <dbl>\n##  1     5.336757    3.736757     1.636757   0.4367573\n##  2     5.136757    3.236757     1.636757   0.4367573\n##  3     4.936757    3.436757     1.536757   0.4367573\n##  4     4.836757    3.336757     1.736757   0.4367573\n##  5     5.236757    3.836757     1.636757   0.4367573\n##  6     5.636757    4.136757     1.936757   0.6367573\n##  7     4.836757    3.636757     1.636757   0.5367573\n##  8     5.236757    3.636757     1.736757   0.4367573\n##  9     4.636757    3.136757     1.636757   0.4367573\n## 10     5.136757    3.336757     1.736757   0.3367573\n## # ... with more rows\nYou can also group by columns to perform an operation over each group of rows and make use of any package within the closure:\nspark_apply(\n  iris_tbl,\n  function(e) broom::tidy(lm(Petal_Width ~ Petal_Length, e)),\n  names = c(\"term\", \"estimate\", \"std.error\", \"statistic\", \"p.value\"),\n  group_by = \"Species\"\n)\n## # Source:   table<sparklyr_tmp_115c73965f30> [?? x 6]\n## # Database: spark_connection\n##      Species         term    estimate  std.error  statistic      p.value\n##        <chr>        <chr>       <dbl>      <dbl>      <dbl>        <dbl>\n## 1 versicolor  (Intercept) -0.08428835 0.16070140 -0.5245029 6.023428e-01\n## 2 versicolor Petal_Length  0.33105360 0.03750041  8.8279995 1.271916e-11\n## 3  virginica  (Intercept)  1.13603130 0.37936622  2.9945505 4.336312e-03\n## 4  virginica Petal_Length  0.16029696 0.06800119  2.3572668 2.253577e-02\n## 5     setosa  (Intercept) -0.04822033 0.12164115 -0.3964146 6.935561e-01\n## 6     setosa Petal_Length  0.20124509 0.08263253  2.4354220 1.863892e-02"
  },
  {
    "objectID": "index.html#extensions",
    "href": "index.html#extensions",
    "title": "sparklyr",
    "section": "Extensions",
    "text": "Extensions\nThe facilities used internally by sparklyr for its dplyr and machine learning interfaces are available to extension packages. Since Spark is a general purpose cluster computing system there are many potential applications for extensions (e.g. interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).\nHere’s a simple example that wraps a Spark text file line counting function with an R function:\n# write a CSV\ntempfile <- tempfile(fileext = \".csv\")\nwrite.csv(nycflights13::flights, tempfile, row.names = FALSE, na = \"\")\n\n# define an R interface to Spark line counting\ncount_lines <- function(sc, path) {\n  spark_context(sc) %>%\n    invoke(\"textFile\", path, 1L) %>%\n      invoke(\"count\")\n}\n\n# call spark to count the lines of the CSV\ncount_lines(sc, tempfile)\n## [1] 336777\nTo learn more about creating extensions see the Extensions section of the sparklyr website."
  },
  {
    "objectID": "index.html#table-utilities",
    "href": "index.html#table-utilities",
    "title": "sparklyr",
    "section": "Table Utilities",
    "text": "Table Utilities\nYou can cache a table into memory with:\ntbl_cache(sc, \"batting\")\nand unload from memory using:\ntbl_uncache(sc, \"batting\")"
  },
  {
    "objectID": "index.html#connection-utilities",
    "href": "index.html#connection-utilities",
    "title": "sparklyr",
    "section": "Connection Utilities",
    "text": "Connection Utilities\nYou can view the Spark web console using the spark_web function:\nspark_web(sc)\nYou can show the log using the spark_log function:\nspark_log(sc, n = 10)\n## 17/11/09 15:55:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 69 (/var/folders/fz/v6wfsg2x1fb1rw4f6r0x4jwm0000gn/T//RtmpyR8oP9/file115c74b94924.csv MapPartitionsRDD[258] at textFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n## 17/11/09 15:55:18 INFO TaskSchedulerImpl: Adding task set 69.0 with 1 tasks\n## 17/11/09 15:55:18 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 140, localhost, executor driver, partition 0, PROCESS_LOCAL, 4904 bytes)\n## 17/11/09 15:55:18 INFO Executor: Running task 0.0 in stage 69.0 (TID 140)\n## 17/11/09 15:55:18 INFO HadoopRDD: Input split: file:/var/folders/fz/v6wfsg2x1fb1rw4f6r0x4jwm0000gn/T/RtmpyR8oP9/file115c74b94924.csv:0+33313106\n## 17/11/09 15:55:18 INFO Executor: Finished task 0.0 in stage 69.0 (TID 140). 832 bytes result sent to driver\n## 17/11/09 15:55:18 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 140) in 126 ms on localhost (executor driver) (1/1)\n## 17/11/09 15:55:18 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool\n## 17/11/09 15:55:18 INFO DAGScheduler: ResultStage 69 (count at NativeMethodAccessorImpl.java:0) finished in 0.126 s\n## 17/11/09 15:55:18 INFO DAGScheduler: Job 47 finished: count at NativeMethodAccessorImpl.java:0, took 0.131380 s\nFinally, we disconnect from Spark:\nspark_disconnect(sc)"
  },
  {
    "objectID": "index.html#rstudio-ide",
    "href": "index.html#rstudio-ide",
    "title": "sparklyr",
    "section": "RStudio IDE",
    "text": "RStudio IDE\nThe latest RStudio Preview Release of the RStudio IDE includes integrated support for Spark and the sparklyr package, including tools for:\n\nCreating and managing Spark connections\nBrowsing the tables and columns of Spark DataFrames\nPreviewing the first 1,000 rows of Spark DataFrames\n\nOnce you’ve installed the sparklyr package, you should find a new Spark pane within the IDE. This pane includes a New Connection dialog which can be used to make connections to local or remote Spark instances:\n\nOnce you’ve connected to Spark you’ll be able to browse the tables contained within the Spark cluster and preview Spark DataFrames using the standard RStudio data viewer:\n\nYou can also connect to Spark through Livy through a new connection dialog:\n\nThe RStudio IDE features for sparklyr are available now as part of the RStudio Preview Release."
  },
  {
    "objectID": "index.html#using-h2o",
    "href": "index.html#using-h2o",
    "title": "sparklyr",
    "section": "Using H2O",
    "text": "Using H2O\nrsparkling is a CRAN package from H2O that extends sparklyr to provide an interface into Sparkling Water. For instance, the following example installs, configures and runs h2o.glm:\noptions(rsparkling.sparklingwater.version = \"2.1.14\")\n\nlibrary(rsparkling)\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(h2o)\n\nsc <- spark_connect(master = \"local\", version = \"2.1.0\")\nmtcars_tbl <- copy_to(sc, mtcars, \"mtcars\")\n\nmtcars_h2o <- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)\n\nmtcars_glm <- h2o.glm(x = c(\"wt\", \"cyl\"),\n                      y = \"mpg\",\n                      training_frame = mtcars_h2o,\n                      lambda_search = TRUE)\nmtcars_glm\n## Model Details:\n## ==============\n##\n## H2ORegressionModel: glm\n## Model ID:  GLM_model_R_1510271749678_1\n## GLM Model: summary\n##     family     link                              regularization\n## 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.1013 )\n##                                                                lambda_search\n## 1 nlambda = 100, lambda.max = 10.132, lambda.min = 0.1013, lambda.1se = -1.0\n##   number_of_predictors_total number_of_active_predictors\n## 1                          2                           2\n##   number_of_iterations                                training_frame\n## 1                  100 frame_rdd_29_b907d4915799eac74fb1ea60ad594bbf\n##\n## Coefficients: glm coefficients\n##       names coefficients standardized_coefficients\n## 1 Intercept    38.941654                 20.090625\n## 2       cyl    -1.468783                 -2.623132\n## 3        wt    -3.034558                 -2.969186\n##\n## H2ORegressionMetrics: glm\n## ** Reported on training data. **\n##\n## MSE:  6.017684\n## RMSE:  2.453097\n## MAE:  1.940985\n## RMSLE:  0.1114801\n## Mean Residual Deviance :  6.017684\n## R^2 :  0.8289895\n## Null Deviance :1126.047\n## Null D.o.F. :31\n## Residual Deviance :192.5659\n## Residual D.o.F. :29\n## AIC :156.2425\nspark_disconnect(sc)"
  },
  {
    "objectID": "index.html#connecting-through-livy",
    "href": "index.html#connecting-through-livy",
    "title": "sparklyr",
    "section": "Connecting through Livy",
    "text": "Connecting through Livy\nLivy enables remote connections to Apache Spark clusters. Before connecting to Livy, you will need the connection information to an existing service running Livy. Otherwise, to test livy in your local environment, you can install it and run it locally as follows:\nlivy_install()\nlivy_service_start()\nTo connect, use the Livy service address as master and method = \"livy\" in spark_connect. Once connection completes, use sparklyr as usual, for instance:\nsc <- spark_connect(master = \"http://localhost:8998\", method = \"livy\")\ncopy_to(sc, iris)\n## # Source:   table<iris> [?? x 5]\n## # Database: spark_connection\n##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl>   <chr>\n##  1          5.1         3.5          1.4         0.2  setosa\n##  2          4.9         3.0          1.4         0.2  setosa\n##  3          4.7         3.2          1.3         0.2  setosa\n##  4          4.6         3.1          1.5         0.2  setosa\n##  5          5.0         3.6          1.4         0.2  setosa\n##  6          5.4         3.9          1.7         0.4  setosa\n##  7          4.6         3.4          1.4         0.3  setosa\n##  8          5.0         3.4          1.5         0.2  setosa\n##  9          4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## # ... with more rows\nspark_disconnect(sc)\nOnce you are done using livy locally, you should stop this service with:\nlivy_service_stop()\nTo connect to remote livy clusters that support basic authentication connect as:\nconfig <- livy_config(username=\"<username>\", password=\"<password\">)\nsc <- spark_connect(master = \"<address>\", method = \"livy\", config = config)\nspark_disconnect(sc)"
  },
  {
    "objectID": "mlib.html#overview",
    "href": "mlib.html#overview",
    "title": "sparklyr",
    "section": "Overview",
    "text": "Overview\nsparklyr provides bindings to Spark’s distributed machine learning library. In particular, sparklyr allows you to access the machine learning routines provided by the spark.ml package. Together with sparklyr’s dplyr interface, you can easily create and tune machine learning workflows on Spark, orchestrated entirely within R.\nsparklyr provides three families of functions that you can use with Spark machine learning:\n\nMachine learning algorithms for analyzing data (ml_*)\nFeature transformers for manipulating individual features (ft_*)\nFunctions for manipulating Spark DataFrames (sdf_*)\n\nAn analytic workflow with sparklyr might be composed of the following stages. For an example see Example Workflow.\n\nPerform SQL queries through the sparklyr dplyr interface,\nUse the sdf_* and ft_* family of functions to generate new columns, or partition your data set,\nChoose an appropriate machine learning algorithm from the ml_* family of functions to model your data,\nInspect the quality of your model fit, and use it to make predictions with new data.\nCollect the results for visualization and further analysis in R"
  },
  {
    "objectID": "mlib.html#algorithms",
    "href": "mlib.html#algorithms",
    "title": "sparklyr",
    "section": "Algorithms",
    "text": "Algorithms\nSpark’s machine learning library can be accessed from sparklyr through the ml_* set of functions:\n\n\n\nFunction\nDescription\n\n\n\n\nml_kmeans\nK-Means Clustering\n\n\nml_linear_regression\nLinear Regression\n\n\nml_logistic_regression\nLogistic Regression\n\n\nml_survival_regression\nSurvival Regression\n\n\nml_generalized_linear_regression\nGeneralized Linear Regression\n\n\nml_decision_tree\nDecision Trees\n\n\nml_random_forest\nRandom Forests\n\n\nml_gradient_boosted_trees\nGradient-Boosted Trees\n\n\nml_pca\nPrincipal Components Analysis\n\n\nml_naive_bayes\nNaive-Bayes\n\n\nml_multilayer_perceptron\nMultilayer Perceptron\n\n\nml_lda\nLatent Dirichlet Allocation\n\n\nml_one_vs_rest\nOne vs Rest\n\n\n\n\nFormulas\nThe ml_* functions take the arguments response and features. But features can also be a formula with main effects (it currently does not accept interaction terms). The intercept term can be omitted by using -1.\n# Equivalent statements\nml_linear_regression(z ~ -1 + x + y)\nml_linear_regression(intercept = FALSE, response = \"z\", features = c(\"x\", \"y\"))\n\n\nOptions\nThe Spark model output can be modified with the ml_options argument in the ml_* functions. The ml_options is an experts only interface for tweaking the model output. For example, model.transform can be used to mutate the Spark model object before the fit is performed."
  },
  {
    "objectID": "mlib.html#transformers",
    "href": "mlib.html#transformers",
    "title": "sparklyr",
    "section": "Transformers",
    "text": "Transformers\nA model is often fit not on a dataset as-is, but instead on some transformation of that dataset. Spark provides feature transformers, facilitating many common transformations of data within a Spark DataFrame, and sparklyr exposes these within the ft_* family of functions. These routines generally take one or more input columns, and generate a new output column formed as a transformation of those columns.\n\n\n\n\n\n\n\n\nFunction\n\n\nDescription\n\n\n\n\n\n\nft_binarizer\n\n\nThreshold numerical features to binary (0/1) feature\n\n\n\n\nft_bucketizer\n\n\nBucketizer transforms a column of continuous features to a column of feature buckets\n\n\n\n\nft_discrete_cosine_transform\n\n\nTransforms a length NN real-valued sequence in the time domain into another length NN real-valued sequence in the frequency domain\n\n\n\n\nft_elementwise_product\n\n\nMultiplies each input vector by a provided weight vector, using element-wise multiplication.\n\n\n\n\nft_index_to_string\n\n\nMaps a column of label indices back to a column containing the original labels as strings\n\n\n\n\nft_quantile_discretizer\n\n\nTakes a column with continuous features and outputs a column with binned categorical features\n\n\n\n\nsql_transformer\n\n\nImplements the transformations which are defined by a SQL statement\n\n\n\n\nft_string_indexer\n\n\nEncodes a string column of labels to a column of label indices\n\n\n\n\nft_vector_assembler\n\n\nCombines a given list of columns into a single vector column"
  },
  {
    "objectID": "mlib.html#examples",
    "href": "mlib.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\nWe will use the iris data set to examine a handful of learning algorithms and transformers. The iris data set measures attributes for 150 flowers in 3 different species of iris.\nlibrary(sparklyr)\n## Warning: package 'sparklyr' was built under R version 3.4.3\nlibrary(ggplot2)\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nsc <- spark_connect(master = \"local\")\n## * Using Spark: 2.1.0\niris_tbl <- copy_to(sc, iris, \"iris\", overwrite = TRUE)\niris_tbl\n## # Source:   table<iris> [?? x 5]\n## # Database: spark_connection\n##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl>   <chr>\n##  1          5.1         3.5          1.4         0.2  setosa\n##  2          4.9         3.0          1.4         0.2  setosa\n##  3          4.7         3.2          1.3         0.2  setosa\n##  4          4.6         3.1          1.5         0.2  setosa\n##  5          5.0         3.6          1.4         0.2  setosa\n##  6          5.4         3.9          1.7         0.4  setosa\n##  7          4.6         3.4          1.4         0.3  setosa\n##  8          5.0         3.4          1.5         0.2  setosa\n##  9          4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## # ... with more rows\n\nK-Means Clustering\nUse Spark’s K-means clustering to partition a dataset into groups. K-means clustering partitions points into k groups, such that the sum of squares from points to the assigned cluster centers is minimized.\nkmeans_model <- iris_tbl %>%\n  ml_kmeans(k = 3, features = c(\"Petal_Length\", \"Petal_Width\"))\n## * No rows dropped by 'na.omit' call\n# print our model fit\nkmeans_model\n## K-means clustering with 3 clusters\n## \n## Cluster centers:\n##   Petal_Width Petal_Length\n## 1    1.359259     4.292593\n## 2    0.246000     1.462000\n## 3    2.047826     5.626087\n## \n## Within Set Sum of Squared Errors =  31.41289\n# predict the associated class\npredicted <- ml_predict(kmeans_model, iris_tbl) %>%\n  collect\ntable(predicted$Species, predicted$prediction)\n##             \n##               0  1  2\n##   setosa      0 50  0\n##   versicolor 48  0  2\n##   virginica   6  0 44\n# plot cluster membership\nml_predict(kmeans_model) %>%\n  collect() %>%\n  ggplot(aes(Petal_Length, Petal_Width)) +\n  geom_point(aes(Petal_Width, Petal_Length, col = factor(prediction + 1)),\n             size = 2, alpha = 0.5) + \n  geom_point(data = kmeans_model$centers, aes(Petal_Width, Petal_Length),\n             col = scales::muted(c(\"red\", \"green\", \"blue\")),\n             pch = 'x', size = 12) +\n  scale_color_discrete(name = \"Predicted Cluster\",\n                       labels = paste(\"Cluster\", 1:3)) +\n  labs(\n    x = \"Petal Length\",\n    y = \"Petal Width\",\n    title = \"K-Means Clustering\",\n    subtitle = \"Use Spark.ML to predict cluster membership with the iris dataset.\"\n  )\n\n\n\nLinear Regression\nUse Spark’s linear regression to model the linear relationship between a response variable and one or more explanatory variables.\nlm_model <- iris_tbl %>%\n  select(Petal_Width, Petal_Length) %>%\n  ml_linear_regression(Petal_Length ~ Petal_Width)\n## * No rows dropped by 'na.omit' call\niris_tbl %>%\n  select(Petal_Width, Petal_Length) %>%\n  collect %>%\n  ggplot(aes(Petal_Length, Petal_Width)) +\n    geom_point(aes(Petal_Width, Petal_Length), size = 2, alpha = 0.5) +\n    geom_abline(aes(slope = coef(lm_model)[[\"Petal_Width\"]],\n                    intercept = coef(lm_model)[[\"(Intercept)\"]]),\n                color = \"red\") +\n  labs(\n    x = \"Petal Width\",\n    y = \"Petal Length\",\n    title = \"Linear Regression: Petal Length ~ Petal Width\",\n    subtitle = \"Use Spark.ML linear regression to predict petal length as a function of petal width.\"\n  )\n\n\n\nLogistic Regression\nUse Spark’s logistic regression to perform logistic regression, modeling a binary outcome as a function of one or more explanatory variables.\n# Prepare beaver dataset\nbeaver <- beaver2\nbeaver$activ <- factor(beaver$activ, labels = c(\"Non-Active\", \"Active\"))\ncopy_to(sc, beaver, \"beaver\")\n## # Source:   table<beaver> [?? x 4]\n## # Database: spark_connection\n##      day  time  temp      activ\n##    <dbl> <dbl> <dbl>      <chr>\n##  1   307   930 36.58 Non-Active\n##  2   307   940 36.73 Non-Active\n##  3   307   950 36.93 Non-Active\n##  4   307  1000 37.15 Non-Active\n##  5   307  1010 37.23 Non-Active\n##  6   307  1020 37.24 Non-Active\n##  7   307  1030 37.24 Non-Active\n##  8   307  1040 36.90 Non-Active\n##  9   307  1050 36.95 Non-Active\n## 10   307  1100 36.89 Non-Active\n## # ... with more rows\nbeaver_tbl <- tbl(sc, \"beaver\")\n\nglm_model <- beaver_tbl %>%\n  mutate(binary_response = as.numeric(activ == \"Active\")) %>%\n  ml_logistic_regression(binary_response ~ temp)\n## * No rows dropped by 'na.omit' call\nglm_model\n## Call: binary_response ~ temp\n## \n## Coefficients:\n## (Intercept)        temp \n##  -550.52331    14.69184\n\n\nPCA\nUse Spark’s Principal Components Analysis (PCA) to perform dimensionality reduction. PCA is a statistical method to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate in turn has the largest variance possible.\npca_model <- tbl(sc, \"iris\") %>%\n  select(-Species) %>%\n  ml_pca()\n## * No rows dropped by 'na.omit' call\nprint(pca_model)\n## Explained variance:\n## \n##         PC1         PC2         PC3         PC4 \n## 0.924618723 0.053066483 0.017102610 0.005212184 \n## \n## Rotation:\n##                      PC1         PC2         PC3        PC4\n## Sepal_Length -0.36138659 -0.65658877  0.58202985  0.3154872\n## Sepal_Width   0.08452251 -0.73016143 -0.59791083 -0.3197231\n## Petal_Length -0.85667061  0.17337266 -0.07623608 -0.4798390\n## Petal_Width  -0.35828920  0.07548102 -0.54583143  0.7536574\n\n\nRandom Forest\nUse Spark’s Random Forest to perform regression or multiclass classification.\nrf_model <- iris_tbl %>%\n  ml_random_forest(Species ~ Petal_Length + Petal_Width, type = \"classification\")\n## * No rows dropped by 'na.omit' call\nrf_predict <- sdf_predict(rf_model, iris_tbl) %>%\n  ft_string_indexer(\"Species\", \"Species_idx\") %>%\n  collect\n\ntable(rf_predict$Species_idx, rf_predict$prediction)\n##    \n##      0  1  2\n##   0 49  1  0\n##   1  0 50  0\n##   2  0  0 50\n\n\nSDF Partitioning\nSplit a Spark DataFrame into training, test datasets.\npartitions <- tbl(sc, \"iris\") %>%\n  sdf_partition(training = 0.75, test = 0.25, seed = 1099)\n\nfit <- partitions$training %>%\n  ml_linear_regression(Petal_Length ~ Petal_Width)\n## * No rows dropped by 'na.omit' call\nestimate_mse <- function(df){\n  sdf_predict(fit, df) %>%\n  mutate(resid = Petal_Length - prediction) %>%\n  summarize(mse = mean(resid ^ 2)) %>%\n  collect\n}\n\nsapply(partitions, estimate_mse)\n## $training.mse\n## [1] 0.2374596\n## \n## $test.mse\n## [1] 0.1898848\n\n\nFT String Indexing\nUse ft_string_indexer and ft_index_to_string to convert a character column into a numeric column and back again.\nft_string2idx <- iris_tbl %>%\n  ft_string_indexer(\"Species\", \"Species_idx\") %>%\n  ft_index_to_string(\"Species_idx\", \"Species_remap\") %>%\n  collect\n\ntable(ft_string2idx$Species, ft_string2idx$Species_remap)\n##             \n##              setosa versicolor virginica\n##   setosa         50          0         0\n##   versicolor      0         50         0\n##   virginica       0          0        50\n\n\nSDF Mutate\nsdf_mutate is provided as a helper function, to allow you to use feature transformers. For example, the previous code snippet could have been written as:\nft_string2idx <- iris_tbl %>%\n  sdf_mutate(Species_idx = ft_string_indexer(Species)) %>%\n  sdf_mutate(Species_remap = ft_index_to_string(Species_idx)) %>%\n  collect\n  \nft_string2idx %>%\n  select(Species, Species_idx, Species_remap) %>%\n  distinct\n## # A tibble: 3 x 3\n##      Species Species_idx Species_remap\n##        <chr>       <dbl>         <chr>\n## 1     setosa           2        setosa\n## 2 versicolor           0    versicolor\n## 3  virginica           1     virginica\n\n\nExample Workflow\nLet’s walk through a simple example to demonstrate the use of Spark’s machine learning algorithms within R. We’ll use ml_linear_regression to fit a linear regression model. Using the built-in mtcars dataset, we’ll try to predict a car’s fuel consumption (mpg) based on its weight (wt), and the number of cylinders the engine contains (cyl).\nFirst, we will copy the mtcars dataset into Spark.\nmtcars_tbl <- copy_to(sc, mtcars, \"mtcars\")\nTransform the data with Spark SQL, feature transformers, and DataFrame functions.\n\nUse Spark SQL to remove all cars with horsepower less than 100\nUse Spark feature transformers to bucket cars into two groups based on cylinders\nUse Spark DataFrame functions to partition the data into test and training\n\nThen fit a linear model using spark ML. Model MPG as a function of weight and cylinders.\n# transform our data set, and then partition into 'training', 'test'\npartitions <- mtcars_tbl %>%\n  filter(hp >= 100) %>%\n  sdf_mutate(cyl8 = ft_bucketizer(cyl, c(0,8,12))) %>%\n  sdf_partition(training = 0.5, test = 0.5, seed = 888)\n\n# fit a linear mdoel to the training dataset\nfit <- partitions$training %>%\n  ml_linear_regression(mpg ~ wt + cyl)\n## * No rows dropped by 'na.omit' call\n# summarize the model\nsummary(fit)\n## Call: ml_linear_regression(., mpg ~ wt + cyl)\n## \n## Deviance Residuals::\n##     Min      1Q  Median      3Q     Max \n## -2.0947 -1.2747 -0.1129  1.0876  2.2185 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 33.79558    2.67240 12.6462 4.92e-07 ***\n## wt          -1.59625    0.73729 -2.1650  0.05859 .  \n## cyl         -1.58036    0.49670 -3.1817  0.01115 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-Squared: 0.8267\n## Root Mean Squared Error: 1.437\nThe summary() suggests that our model is a fairly good fit, and that both a cars weight, as well as the number of cylinders in its engine, will be powerful predictors of its average fuel consumption. (The model suggests that, on average, heavier cars consume more fuel.)\nLet’s use our Spark model fit to predict the average fuel consumption on our test data set, and compare the predicted response with the true measured fuel consumption. We’ll build a simple ggplot2 plot that will allow us to inspect the quality of our predictions.\n# Score the data\npred <- sdf_predict(fit, partitions$test) %>%\n  collect\n\n# Plot the predicted versus actual mpg\nggplot(pred, aes(x = mpg, y = prediction)) +\n  geom_abline(lty = \"dashed\", col = \"red\") +\n  geom_point() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  coord_fixed(ratio = 1) +\n  labs(\n    x = \"Actual Fuel Consumption\",\n    y = \"Predicted Fuel Consumption\",\n    title = \"Predicted vs. Actual Fuel Consumption\"\n  )\n\nAlthough simple, our model appears to do a fairly good job of predicting a car’s average fuel consumption.\nAs you can see, we can easily and effectively combine feature transformers, machine learning algorithms, and Spark DataFrame functions into a complete analysis with Spark and R."
  },
  {
    "objectID": "troubleshooting.html#help-with-code-debugging",
    "href": "troubleshooting.html#help-with-code-debugging",
    "title": "sparklyr",
    "section": "Help with code debugging",
    "text": "Help with code debugging\nFor general programming questions with sparklyr, please ask on Stack Overflow."
  },
  {
    "objectID": "troubleshooting.html#code-does-not-work-after-upgrading-to-the-latest-sparklyr-version",
    "href": "troubleshooting.html#code-does-not-work-after-upgrading-to-the-latest-sparklyr-version",
    "title": "sparklyr",
    "section": "Code does not work after upgrading to the latest sparklyr version",
    "text": "Code does not work after upgrading to the latest sparklyr version\nPlease refer to the NEWS section of the sparklyr package to find out if any of the updates listed may have changed the way your code needs to work.\nIf it seems that current version of the package has a bug, or the new functionality does not perform as stated, please refer to the sparklyr ISSUES page. If no existing issue matches to what your problem is, please open a new issue."
  },
  {
    "objectID": "troubleshooting.html#not-able-to-connect-or-the-jobs-take-a-long-time-when-working-with-a-data-lake",
    "href": "troubleshooting.html#not-able-to-connect-or-the-jobs-take-a-long-time-when-working-with-a-data-lake",
    "title": "sparklyr",
    "section": "Not able to connect, or the jobs take a long time when working with a Data Lake",
    "text": "Not able to connect, or the jobs take a long time when working with a Data Lake\nThe Configuration connections contains an overview and recommendations for requesting resources form the cluster.\nThe articles in the Guides section provide best-practice information about specific operations that may match to the intent of your code.\nTo verify your infrastructure, please review the Deployment Examples section."
  },
  {
    "objectID": "deployment.html#deployment",
    "href": "deployment.html#deployment",
    "title": "Deployment and Configuration",
    "section": "Deployment",
    "text": "Deployment\nThere are two well supported deployment modes for sparklyr:\n\nLocal — Working on a local desktop typically with smaller/sampled datasets\nCluster — Working directly within or alongside a Spark cluster (standalone, YARN, Mesos, etc.)\n\n\nLocal Deployment\nLocal mode is an excellent way to learn and experiment with Spark. Local mode also provides a convenient development environment for analyses, reports, and applications that you plan to eventually deploy to a multi-node Spark cluster.\nTo work in local mode you should first install a version of Spark for local use. You can do this using the spark_install function, for example:\n\nsparklyr::spark_install(version = \"2.1.0\")\n\nTo connect to the local Spark instance you pass “local” as the value of the Spark master node to spark_connect:\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\n\nFor the local development scenario, see the Configuration section below for details on how to have the same code work seamlessly in both development and production environments.\n\n\nCluster Deployment\nA common deployment strategy is to submit your application from a gateway machine that is physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster). In this setup, client mode is appropriate. In client mode, the driver is launched directly within the spark-submit process which acts as a client to the cluster. The input and output of the application is attached to the console. Thus, this mode is especially suitable for applications that involve the REPL (e.g. Spark shell). For more information see Submitting Applications.\nTo use spaklyr with a Spark cluster you should locate your R session on a machine that is either directly on one of the cluster nodes or is close to the cluster (for networking performance). In the case where R is not running directly on the cluster you should also ensure that the machine has a Spark version and configuration identical to that of the cluster nodes.\nThe most straightforward way to run R within or near to the cluster is either a remote SSH session or via RStudio Server.\nIn cluster mode you use the version of Spark already deployed on the cluster node. This version is located via the SPARK_HOME environment variable, so you should be sure that this variable is correctly defined on your server before attempting a connection. This would typically be done within the Renviron.site configuration file. For example:\nSPARK_HOME=/opt/spark/spark-2.0.0-bin-hadoop2.6\nTo connect, pass the address of the master node to spark_connect, for example:\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"spark://local:7077\")\n\nFor a Hadoop YARN cluster, you can connect using the YARN master, for example:\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"yarn-client\")\n\nIf you are running on EC2 using the Spark EC2 deployment scripts then you can read the master from /root/spark-ec2/cluster-url, for example:\n\nlibrary(sparklyr)\ncluster_url <- system('cat /root/spark-ec2/cluster-url', intern=TRUE)\nsc <- spark_connect(master = cluster_url)\n\n\n\nLivy Connections\nLivy, “An Open Source REST Service for Apache Spark (Apache License)” , is available starting in sparklyr 0.5 as an experimental feature. Among many scenarios, this enables connections from the RStudio desktop to Apache Spark when Livy is available and correctly configured in the remote cluster.\nTo work with Livy locally, sparklyr supports livy_install() which installs Livy in your local environment, this is similar to spark_install(). Since Livy is a service to enable remote connections into Apache Spark, the service needs to be started with livy_service_start(). Once the service is running, spark_connect() needs to reference the running service and use method = \"Livy\", then sparklyr can be used as usual. A short example follows:\n\nlivy_install()\nlivy_service_start()\n\nsc <- spark_connect(master = \"http://localhost:8998\", method = \"livy\")\ncopy_to(sc, iris)\n\nspark_disconnect(sc)\nlivy_service_stop()\n\n\n\nConnection Tools\nYou can view the Spark web UI via the spark_web function, and view the Spark log via the spark_log function:\n\nspark_web(sc)\nspark_log(sc)\n\nYou can disconnect from Spark using the spark_disconnect function:\n\nspark_disconnect(sc)\n\n\n\nCollect\nThe collect function transfers data from Spark into R. The data are collected from a cluster environment and transfered into local R memory. In the process, all data is first transfered from executor nodes to the driver node. Therefore, the driver node must have enough memory to collect all the data.\nCollecting data on the driver node is relatively slow. The process also inflates the data as it moves from the executor nodes to the driver node. Caution should be used when collecting large data.\nThe following parameters could be adjusted to avoid OutOfMemory and Timeout errors:\n\nspark.executor.heartbeatInterval\nspark.network.timeout\nspark.driver.extraJavaOptions\nspark.driver.memory\nspark.yarn.driver.memoryOverhead\nspark.driver.maxResultSize"
  },
  {
    "objectID": "deployment.html#configuration",
    "href": "deployment.html#configuration",
    "title": "Deployment and Configuration",
    "section": "Configuration",
    "text": "Configuration\nThis section describes the various options available for configuring both the behavior of the sparklyr package as well as the underlying Spark cluster. Creating multiple configuration profiles (e.g. development, test, production) is also covered.\n\nConfig Files\nThe configuration for a Spark connection is specified via the config parameter of the spark_connect function. By default the configuration is established by calling the spark_config function. This code represents the default behavior:\n\nspark_connect(master = \"local\", config = spark_config())\n\nBy default the spark_config function reads configuration data from a file named config.yml located in the current working directory (or in parent directories if not located in the working directory). This file is not required and only need be provided for overriding default behavior. You can also specify an alternate config file name and/or location.\nThe config.yml file is in turn processed using the config package, which enables support for multiple named configuration profiles.\n\n\nPackage Options\nThere are a number of options available to configure the behavior of the sparklyr package:\nFor example, this configuration file sets the number of local cores to 4 and the amount of memory allocated for the Spark driver to 4G:\ndefault:\n  sparklyr.cores.local: 4\n  sparklyr.shell.driver-memory: 4G\nNote that the use of default will be explained below in Multiple Profiles.\n\nSpark\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nsparklyr.shell.*\nCommand line parameters to pass to spark-submit. For example, sparklyr.shell.executor-memory: 20G configures --executor-memory 20G (see the Spark documentation for details on supported options).\n\n\n\n\n\nRuntime\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nsparklyr.cores.local\nNumber of cores to use when running in local mode (defaults to parallel::detectCores).\n\n\nsparklyr.sparkui.url\nConfigures the url to the Spark UI web interface when calling spark_web.\n\n\nsparklyr.defaultPackages\nList of default Spark packages to install in the cluster (defaults to “com.databricks:spark-csv_2.11:1.3.0” and “com.amazonaws:aws-java-sdk-pom:1.10.34”).\n\n\nsparklyr.sanitize.column.names\nAllows Spark to automatically rename column names to conform to Spark naming restrictions.\n\n\n\n\n\nDiagnostics\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nsparklyr.backend.threads\nNumber of threads to use in the sparklyr backend to process incoming connections form the sparklyr client.\n\n\nsparklyr.app.jar\nThe application jar to be submitted in Spark submit.\n\n\nsparklyr.ports.file\nPath to the ports file used to share connection information to the sparklyr backend.\n\n\nsparklyr.ports.wait.seconds\nNumber of seconds to wait while for the Spark connection to initialize.\n\n\nsparklyr.verbose\nProvide additional feedback while performing operations. Currently used to communicate which column names are being sanitized in sparklyr.sanitize.column.names.\n\n\n\n\n\n\nSpark Options\nYou can also use config.yml to specify arbitrary Spark configuration properties:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nspark.*\nConfiguration settings for the Spark context (applied by creating a SparkConf containing the specified properties). For example, spark.executor.memory: 1g configures the memory available in each executor (see Spark Configuration for additional options.)\n\n\nspark.sql.*\nConfiguration settings for the Spark SQL context (applied using SET). For instance, spark.sql.shuffle.partitions configures number of partitions to use while shuffling (see SQL Programming Guide for additional options).\n\n\n\nFor example, this configuration file sets a custom scratch directory for Spark and specifies 100 as the number of partitions to use when shuffling data for joins or aggregations:\ndefault:\n  spark.local.dir: /tmp/spark-scratch\n  spark.sql.shuffle.partitions: 100\n\n\nUser Options\nYou can also include arbitrary custom user options within the config.yml file. These can be named anything you like so long as they do not use either spark or sparklyr as a prefix. For example, this configuration file defines dataset and sample-size options:\ndefault:\n  dataset: \"observations.parquet\"\n  sample-size: 10000\n\n\nMultiple Profiles\nThe config package enables the definition of multiple named configuration profiles for different environments (e.g. default, test, production). All environments automatically inherit from the default environment and can optionally also inherit from each other.\nFor example, you might want to use a distinct datasets for development and testing or might want to use custom Spark configuration properties that are only applied when running on a production cluster. Here’s how that would be expressed in config.yml:\ndefault:\n  dataset: \"observations-dev.parquet\"\n  sample-size: 10000\n\nproduction:\n  spark.memory.fraction: 0.9\n  spark.rdd.compress: true\n  dataset: \"observations.parquet\"\n  sample-size: null\nYou can also use this feature to specify distinct Spark master nodes for different environments, for example:\ndefault:\n  spark.master: \"local\"\n\nproduction:\n  spark.master: \"spark://local:7077\"\nWith this configuration, you can omit the master argument entirely from the call to spark_connect:\n\nsc <- spark_connect()\n\nNote that the currently active configuration is determined via the value of R_CONFIG_ACTIVE environment variable. See the config package documentation for additional details.\n\n\nTuning\nIn general, you will need to tune a Spark cluster for it to perform well. Spark applications tend to consume a lot of resources. There are many knobs to control the performance of Yarn and executor (i.e. worker) nodes in a cluster. Some of the parameters to pay attention to are as follows:\n\nspark.executor.heartbeatInterval\nspark.network.timeout\nspark.executor.extraJavaOptions\nspark.executor.memory\nspark.yarn.executor.memoryOverhead\nspark.executor.cores\nspark.executor.instances (if is not enabled)\n\n\nExample Config\nHere is an example spark configuration for an EMR cluster on AWS with 1 master and 2 worker nodes. Eache node has 8 vCPU and 61 GiB of memory.\n\n\n\nParameter\nValue\n\n\n\n\nspark.driver.extraJavaOptions\nappend -XX:MaxPermSize=30G\n\n\nspark.driver.maxResultSize\n0\n\n\nspark.driver.memory\n30G\n\n\nspark.yarn.driver.memoryOverhead\n4096\n\n\nspark.yarn.executor.memoryOverhead\n4096\n\n\nspark.executor.memory\n4G\n\n\nspark.executor.cores\n2\n\n\nspark.dynamicAllocation.maxExecutors\n15\n\n\n\nConfiguration parameters can be set in the config R object or can be set in the config.yml. Alternatively, they can be set in the spark-defaults.conf.\n\nConfiguration in R script\n\nconfig <- spark_config()\nconfig$spark.executor.cores <- 2\nconfig$spark.executor.memory <- \"4G\"\nsc <- spark_connect(master = \"yarn-client\", config = config, version = '2.0.0')\n\n\n\nConfiguration in YAML script\ndefault:\n  spark.executor.cores: 2\n  spark.executor.memory: 4G"
  },
  {
    "objectID": "deployment.html#rstudio-server",
    "href": "deployment.html#rstudio-server",
    "title": "Deployment and Configuration",
    "section": "RStudio Server",
    "text": "RStudio Server\nRStudio Server provides a web-based IDE interface to a remote R session, making it ideal for use as a front-end to a Spark cluster. This section covers some additional configuration options that are useful for RStudio Server.\n\nConnection Options\nThe RStudio IDE Spark pane provides a New Connection dialog to assist in connecting with both local instances of Spark and Spark clusters:\n\nYou can configure which connection choices are presented using the rstudio.spark.connections option. By default, users are presented with possibility of both local and cluster connections, however, you can modify this behavior to present only one of these, or even a specific Spark master URL. Some commonly used combinations of connection choices include:\n\n\n\n\n\n\n\nValue\nDescription\n\n\n\n\nc(\"local\", \"cluster\")\nDefault. Present connections to both local and cluster Spark instances.\n\n\n\"local\"\nPresent only connections to local Spark instances.\n\n\n\"spark://local:7077\"\nPresent only a connection to a specific Spark cluster.\n\n\nc(\"spark://local:7077\", \"cluster\")\nPresent a connection to a specific Spark cluster and other clusters.\n\n\n\nThis option should generally be set within Rprofile.site. For example:\n\noptions(rstudio.spark.connections = \"spark://local:7077\")\n\n\n\nSpark Installations\nIf you are running within local mode (as opposed to cluster mode) you may want to provide pre-installed Spark version(s) to be shared by all users of the server. You can do this by installing Spark versions within a shared directory (e.g. /opt/spark) then designating it as the Spark installation directory.\nFor example, after installing one or more versions of Spark to /opt/spark you would add the following to Rprofile.site:\n\noptions(spark.install.dir = \"/opt/spark\")\n\nIf this directory is read-only for ordinary users then RStudio will not offer installation of additional versions, which will help guide users to a version that is known to be compatible with versions of Spark deployed on clusters in the same organization."
  },
  {
    "objectID": "extensions.html#introduction",
    "href": "extensions.html#introduction",
    "title": "Creating Extensions for sparklyr",
    "section": "Introduction",
    "text": "Introduction\nThe sparklyr package provides a dplyr interface to Spark DataFrames as well as an R interface to Spark’s distributed machine learning pipelines. However, since Spark is a general-purpose cluster computing system there are many other R interfaces that could be built (e.g. interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).\nThe facilities used internally by sparklyr for its dplyr and machine learning interfaces are available to extension packages. This guide describes how you can use these tools to create your own custom R interfaces to Spark.\n\nExamples\nHere’s an example of an extension function that calls the text file line counting function available via the SparkContext:\n\nlibrary(sparklyr)\ncount_lines <- function(sc, file) {\n  spark_context(sc) %>% \n    invoke(\"textFile\", file, 1L) %>% \n    invoke(\"count\")\n}\n\nThe count_lines function takes a spark_connection (sc) argument which enables it to obtain a reference to the SparkContext object, and in turn call the textFile().count() method.\nYou can use this function with an existing sparklyr connection as follows:\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\ncount_lines(sc, \"hdfs://path/data.csv\")\n\nHere are links to some additional examples of extension packages:\n\n\n\n\n\n\n\nPackage\nDescription\n\n\n\n\nspark.sas7bdat\nRead in SAS data in parallel into Apache Spark.\n\n\nrsparkling\nExtension for using H2O machine learning algorithms against Spark Data Frames.\n\n\nsparkhello\nSimple example of including a custom JAR file within an extension package.\n\n\nrddlist\nImplements some methods of an R list as a Spark RDD (resilient distributed dataset).\n\n\nsparkwarc\nLoad WARC files into Apache Spark with sparklyr.\n\n\nsparkavro\nLoad Avro data into Spark with sparklyr. It is a wrapper of spark-avro\n\n\ncrassy\nConnect to Cassandra with sparklyr using the Spark-Cassandra-Connector.\n\n\nsparklygraphs\nR interface for GraphFrames which aims to provide the functionality of GraphX.\n\n\nsparklyr.nested\nExtension for working with nested data.\n\n\nsparklyudf\nSimple example registering an Scala UDF within an extension package.\n\n\nmleap\nR Interface to MLeap.\n\n\nsparkbq\nSparklyr extension package to connect to Google BigQuery.\n\n\nsparkgeo\nSparklyr extension package providing geospatial analytics capabilities.\n\n\nsparklytd\nSpaklyr plugin for td-spark to connect TD from R.\n\n\nsparkts\nExtensions for the spark-timeseries framework.\n\n\nsparkxgb\nR interface for XGBoost on Spark.\n\n\nsparktf\nR interface to Spark TensorFlow Connector.\n\n\ngeospark\nR interface to GeoSpark to perform spatial analysis in Spark.\n\n\nmmlspark\nMicrosoft Machine Learning for Apache Spark."
  },
  {
    "objectID": "extensions.html#core-types",
    "href": "extensions.html#core-types",
    "title": "Creating Extensions for sparklyr",
    "section": "Core Types",
    "text": "Core Types\nThree classes are defined for representing the fundamental types of the R to Java bridge:\n\n\n\nFunction\nDescription\n\n\n\n\nspark_connection\nConnection between R and the Spark shell process\n\n\nspark_jobj\nInstance of a remote Spark object\n\n\nspark_dataframe\nInstance of a remote Spark DataFrame object\n\n\n\nS3 methods are defined for each of these classes so they can be easily converted to or from objects that contain or wrap them. Note that for any given spark_jobj it’s possible to discover the underlying spark_connection."
  },
  {
    "objectID": "extensions.html#calling-spark-from-r",
    "href": "extensions.html#calling-spark-from-r",
    "title": "Creating Extensions for sparklyr",
    "section": "Calling Spark from R",
    "text": "Calling Spark from R\nThere are several functions available for calling the methods of Java objects and static methods of Java classes:\n\n\n\nFunction\nDescription\n\n\n\n\ninvoke\nCall a method on an object\n\n\ninvoke_new\nCreate a new object by invoking a constructor\n\n\ninvoke_static\nCall a static method on an object\n\n\n\nFor example, to create a new instance of the java.math.BigInteger class and then call the longValue() method on it you would use code like this:\n\nbillionBigInteger <- invoke_new(sc, \"java.math.BigInteger\", \"1000000000\")\nbillion <- invoke(billionBigInteger, \"longValue\")\n\nNote the sc argument: that’s the spark_connection object which is provided by the front-end package (e.g. sparklyr).\nThe previous example can be re-written to be more compact and clear using magrittr pipes:\n\nbillion <- sc %>% \n  invoke_new(\"java.math.BigInteger\", \"1000000000\") %>%\n    invoke(\"longValue\")\n\nThis syntax is similar to the method-chaining syntax often used with Scala code so is generally preferred.\nCalling a static method of a class is also straightforward. For example, to call the Math::hypot() static function you would use this code:\n\nhypot <- sc %>% \n  invoke_static(\"java.lang.Math\", \"hypot\", 10, 20)"
  },
  {
    "objectID": "extensions.html#wrapper-functions",
    "href": "extensions.html#wrapper-functions",
    "title": "Creating Extensions for sparklyr",
    "section": "Wrapper Functions",
    "text": "Wrapper Functions\nCreating an extension typically consists of writing R wrapper functions for a set of Spark services. In this section we’ll describe the typical form of these functions as well as how to handle special types like Spark DataFrames.\nHere’s the wrapper function for textFile().count() which we defined earlier:\n\ncount_lines <- function(sc, file) {\n  spark_context(sc) %>% \n    invoke(\"textFile\", file, 1L) %>% \n      invoke(\"count\")\n}\n\nThe count_lines function takes a spark_connection (sc) argument which enables it to obtain a reference to the SparkContext object, and in turn call the textFile().count() method.\nThe following functions are useful for implementing wrapper functions of various kinds:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nspark_connection\nGet the Spark connection associated with an object (S3)\n\n\nspark_jobj\nGet the Spark jobj associated with an object (S3)\n\n\nspark_dataframe\nGet the Spark DataFrame associated with an object (S3)\n\n\nspark_context\nGet the SparkContext for a spark_connection\n\n\nhive_context\nGet the HiveContext for a spark_connection\n\n\nspark_version\nGet the version of Spark (as a numeric_version) for a spark_connection\n\n\n\nThe use of these functions is illustrated in this simple example:\n\nanalyze <- function(x, features) {\n  \n  # normalize whatever we were passed (e.g. a dplyr tbl) into a DataFrame\n  df <- spark_dataframe(x)\n  \n  # get the underlying connection so we can create new objects\n  sc <- spark_connection(df)\n  \n  # create an object to do the analysis and call its `analyze` and `summary`\n  # methods (note that the df and features are passed to the analyze function)\n  summary <- sc %>%  \n    invoke_new(\"com.example.tools.Analyzer\") %>% \n      invoke(\"analyze\", df, features) %>% \n      invoke(\"summary\")\n\n  # return the results\n  summary\n}\n\nThe first argument is an object that can be accessed using the Spark DataFrame API (this might be an actual reference to a DataFrame or could rather be a dplyr tbl which has a DataFrame reference inside it).\nAfter using the spark_dataframe function to normalize the reference, we extract the underlying Spark connection associated with the data frame using the spark_connection function. Finally, we create a new Analyzer object, call it’s analyze method with the DataFrame and list of features, and then call the summary method on the results of the analysis.\nAccepting a spark_jobj or spark_dataframe as the first argument of a function makes it very easy to incorporate into magrittr pipelines so this pattern is highly recommended when possible."
  },
  {
    "objectID": "extensions.html#dependencies",
    "href": "extensions.html#dependencies",
    "title": "Creating Extensions for sparklyr",
    "section": "Dependencies",
    "text": "Dependencies\nWhen creating R packages which implement interfaces to Spark you may need to include additional dependencies. Your dependencies might be a set of Spark Packages or might be a custom JAR file. In either case, you’ll need a way to specify that these dependencies should be included during the initialization of a Spark session. A Spark dependency is defined using the spark_dependency function:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nspark_dependency\nDefine a Spark dependency consisting of JAR files and Spark packages\n\n\n\nYour extension package can specify it’s dependencies by implementing a function named spark_dependencies within the package (this function should not be publicly exported). For example, let’s say you were creating an extension package named sparkds that needs to include a custom JAR as well as the Redshift and Apache Avro packages:\n\nspark_dependencies <- function(spark_version, scala_version, ...) {\n  spark_dependency(\n    jars = c(\n      system.file(\n        sprintf(\"java/sparkds-%s-%s.jar\", spark_version, scala_version), \n        package = \"sparkds\"\n      )\n    ),\n    packages = c(\n      sprintf(\"com.databricks:spark-redshift_%s:0.6.0\", scala_version),\n      sprintf(\"com.databricks:spark-avro_%s:2.0.1\", scala_version)\n    )\n  )\n}\n\n.onLoad <- function(libname, pkgname) {\n  sparklyr::register_extension(pkgname)\n}\n\nThe spark_version argument is provided so that a package can support multiple Spark versions for it’s JARs. Note that the argument will include just the major and minor versions (e.g. 1.6 or 2.0) and will not include the patch level (as JARs built for a given major/minor version are expected to work for all patch levels).\nThe scala_version argument is provided so that a single package can support multiple Scala compiler versions for it’s JARs and packages (currently Scala 1.6 downloadable binaries are compiled with Scala 2.10 and Scala 2.0 downloadable binaries are compiled with Scala 2.11).\nThe ... argument is unused but nevertheless should be included to ensure compatibility if new arguments are added to spark_dependencies in the future.\nThe .onLoad function registers your extension package so that it’s spark_dependencies function will be automatically called when new connections to Spark are made via spark_connect:\n\nlibrary(sparklyr)\nlibrary(sparkds)\nsc <- spark_connect(master = \"local\")\n\n\nCompiling JARs\nThe sparklyr package includes a utility function (compile_package_jars) that will automatically compile a JAR file from your Scala source code for the required permutations of Spark and Scala compiler versions. To use the function just invoke it from the root directory of your R package as follows:\n\nsparklyr::compile_package_jars()\n\nNote that a prerequisite to calling compile_package_jars is the installation of the Scala 2.10 and 2.11 compilers to one of the following paths:\n\n/opt/scala\n/opt/local/scala\n/usr/local/scala\n~/scala (Windows-only)\n\nSee the sparkhello repository for a complete example of including a custom JAR within an extension package.\n\nCRAN\nWhen including a JAR file within an R package distributed on CRAN, you should follow the guidelines provided in Writing R Extensions:\n\nJava code is a special case: except for very small programs, .java files should be byte-compiled (to a .class file) and distributed as part of a .jar file: the conventional location for the .jar file(s) is inst/java. It is desirable (and required under an Open Source license) to make the Java source files available: this is best done in a top-level java directory in the package – the source files should not be installed."
  },
  {
    "objectID": "extensions.html#data-types",
    "href": "extensions.html#data-types",
    "title": "Creating Extensions for sparklyr",
    "section": "Data Types",
    "text": "Data Types\nThe ensure_* family of functions can be used to enforce specific data types that are passed to a Spark routine. For example, Spark routines that require an integer will not accept an R numeric element. Use these functions ensure certain parameters are scalar integers, or scalar doubles, and so on.\n\nensure_scalar_integer\nensure_scalar_double\nensure_scalar_boolean\nensure_scalar_character\n\nIn order to match the correct data types while calling Scala code from R, or retrieving results from Scala back to R, consider the following types mapping:\n\n\n\nFrom R\nScala\nTo R\n\n\n\n\nNULL\nvoid\nNULL\n\n\ninteger\nInt\ninteger\n\n\ncharacter\nString\ncharacter\n\n\nlogical\nBoolean\nlogical\n\n\ndouble\nDouble\ndouble\n\n\nnumeric\nDouble\ndouble\n\n\n\nFloat\ndouble\n\n\n\nDecimal\ndouble\n\n\n\nLong\ndouble\n\n\nraw\nArray[Byte]\nraw\n\n\nDate\nDate\nDate\n\n\nPOSIXlt\nTime\n\n\n\nPOSIXct\nTime\nPOSIXct\n\n\nlist\nArray[T]\nlist\n\n\nenvironment\nMap[String, T]\n\n\n\njobj\nObject\njobj"
  },
  {
    "objectID": "extensions.html#compiling",
    "href": "extensions.html#compiling",
    "title": "Creating Extensions for sparklyr",
    "section": "Compiling",
    "text": "Compiling\nMost Spark extensions won’t need to define their own compilation specification, and can instead rely on the default behavior of compile_package_jars. For users who would like to take more control over where the scalac compilers should be looked up, use the spark_compilation_spec fucnction. The Spark compilation specification is used when compiling Spark extension Java Archives, and defines which versions of Spark, as well as which versions of Scala, should be used for compilation."
  },
  {
    "objectID": "examples/cloudera-aws.html#hive-data",
    "href": "examples/cloudera-aws.html#hive-data",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Hive data",
    "text": "Hive data\nFor this demo, we have created and populated 3 tables in Hive. The table names are: flights, airlines and airports. Using Hue, we can see the loaded tables. For the links to the data files and their Hive import scripts please see Appendix A."
  },
  {
    "objectID": "examples/cloudera-aws.html#cache-the-tables-into-memory",
    "href": "examples/cloudera-aws.html#cache-the-tables-into-memory",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Cache the tables into memory",
    "text": "Cache the tables into memory\nUse tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame.\n\n# Cache flights Hive table into Spark\ntbl_cache(sc, 'flights')\nflights_tbl <- tbl(sc, 'flights')\n\n# Cache airlines Hive table into Spark\ntbl_cache(sc, 'airlines')\nairlines_tbl <- tbl(sc, 'airlines')\n\n# Cache airports Hive table into Spark\ntbl_cache(sc, 'airports')\nairports_tbl <- tbl(sc, 'airports')"
  },
  {
    "objectID": "examples/cloudera-aws.html#create-a-model-data-set",
    "href": "examples/cloudera-aws.html#create-a-model-data-set",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Create a model data set",
    "text": "Create a model data set\nFilter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight.\n\n# Filter records and create target variable 'gain'\nmodel_data <- flights_tbl %>%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>%\n  filter(depdelay > 15 & depdelay < 240) %>%\n  filter(arrdelay > -60 & arrdelay < 360) %>%\n  filter(year >= 2003 & year <= 2007) %>%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>%\n  mutate(gain = depdelay - arrdelay) %>%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain)\n\n# Summarize data by carrier\nmodel_data %>%\n  group_by(uniquecarrier) %>%\n  summarize(description = min(description), gain=mean(gain), \n            distance=mean(distance), depdelay=mean(depdelay)) %>%\n  select(description, gain, distance, depdelay) %>%\n  arrange(gain)\n\nSource:   query [?? x 4]\nDatabase: spark connection master=yarn-client app=sparklyr local=FALSE\n\n                    description       gain  distance depdelay\n                          <chr>      <dbl>     <dbl>    <dbl>\n1        ATA Airlines d/b/a ATA -5.5679651 1240.7219 61.84391\n2       Northwest Airlines Inc. -3.1134556  779.1926 48.84979\n3                     Envoy Air -2.2056576  437.0883 54.54923\n4             PSA Airlines Inc. -1.9267647  500.6955 55.60335\n5  ExpressJet Airlines Inc. (1) -1.5886314  537.3077 61.58386\n6               JetBlue Airways -1.3742524 1087.2337 59.80750\n7         SkyWest Airlines Inc. -1.1265678  419.6489 54.04198\n8          Delta Air Lines Inc. -0.9829374  956.9576 50.19338\n9        American Airlines Inc. -0.9631200 1066.8396 56.78222\n10  AirTran Airways Corporation -0.9411572  665.6574 53.38363\n# ... with more rows"
  },
  {
    "objectID": "examples/cloudera-aws.html#train-a-linear-model",
    "href": "examples/cloudera-aws.html#train-a-linear-model",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Train a linear model",
    "text": "Train a linear model\nPredict time gained or lost in flight as a function of distance, departure delay, and airline carrier.\n\n# Partition the data into training and validation sets\nmodel_partition <- model_data %>% \n  sdf_partition(train = 0.8, valid = 0.2, seed = 5555)\n\n# Fit a linear model\nml1 <- model_partition$train %>%\n  ml_linear_regression(gain ~ distance + depdelay + uniquecarrier)\n\n# Summarize the linear model\nsummary(ml1)\n\nCall: ml_linear_regression(., gain ~ distance + depdelay + uniquecarrier)\n\nDeviance Residuals: (approximate):\n     Min       1Q   Median       3Q      Max \n-302.343   -5.669    2.714    9.832  104.130 \n\nCoefficients:\n                    Estimate  Std. Error  t value  Pr(>|t|)    \n(Intercept)      -1.26566581  0.10385870 -12.1864 < 2.2e-16 ***\ndistance          0.00308711  0.00002404 128.4155 < 2.2e-16 ***\ndepdelay         -0.01397013  0.00028816 -48.4812 < 2.2e-16 ***\nuniquecarrier_AA -2.18483090  0.10985406 -19.8885 < 2.2e-16 ***\nuniquecarrier_AQ  3.14330242  0.29114487  10.7964 < 2.2e-16 ***\nuniquecarrier_AS  0.09210380  0.12825003   0.7182 0.4726598    \nuniquecarrier_B6 -2.66988794  0.12682192 -21.0523 < 2.2e-16 ***\nuniquecarrier_CO -1.11611186  0.11795564  -9.4621 < 2.2e-16 ***\nuniquecarrier_DL -1.95206198  0.11431110 -17.0767 < 2.2e-16 ***\nuniquecarrier_EV  1.70420830  0.11337215  15.0320 < 2.2e-16 ***\nuniquecarrier_F9 -1.03178176  0.15384863  -6.7065 1.994e-11 ***\nuniquecarrier_FL -0.99574060  0.12034738  -8.2739 2.220e-16 ***\nuniquecarrier_HA -1.16970713  0.34894788  -3.3521 0.0008020 ***\nuniquecarrier_MQ -1.55569040  0.10975613 -14.1741 < 2.2e-16 ***\nuniquecarrier_NW -3.58502418  0.11534938 -31.0797 < 2.2e-16 ***\nuniquecarrier_OH -1.40654797  0.12034858 -11.6873 < 2.2e-16 ***\nuniquecarrier_OO -0.39069404  0.11132164  -3.5096 0.0004488 ***\nuniquecarrier_TZ -7.26285217  0.34428509 -21.0955 < 2.2e-16 ***\nuniquecarrier_UA -0.56995737  0.11186757  -5.0949 3.489e-07 ***\nuniquecarrier_US -0.52000028  0.11218498  -4.6352 3.566e-06 ***\nuniquecarrier_WN  4.22838982  0.10629405  39.7801 < 2.2e-16 ***\nuniquecarrier_XE -1.13836940  0.11332176 -10.0455 < 2.2e-16 ***\nuniquecarrier_YV  3.17149538  0.11709253  27.0854 < 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nR-Squared: 0.02301\nRoot Mean Squared Error: 17.83"
  },
  {
    "objectID": "examples/cloudera-aws.html#assess-model-performance",
    "href": "examples/cloudera-aws.html#assess-model-performance",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Assess model performance",
    "text": "Assess model performance\nCompare the model performance using the validation data.\n\n# Calculate average gains by predicted decile\nmodel_deciles <- lapply(model_partition, function(x) {\n  sdf_predict(ml1, x) %>%\n    mutate(decile = ntile(desc(prediction), 10)) %>%\n    group_by(decile) %>%\n    summarize(gain = mean(gain)) %>%\n    select(decile, gain) %>%\n    collect()\n})\n\n# Create a summary dataset for plotting\ndeciles <- rbind(\n  data.frame(data = 'train', model_deciles$train),\n  data.frame(data = 'valid', model_deciles$valid),\n  make.row.names = FALSE\n)\n\n# Plot average gains by predicted decile\ndeciles %>%\n  ggplot(aes(factor(decile), gain, fill = data)) +\n  geom_bar(stat = 'identity', position = 'dodge') +\n  labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes')"
  },
  {
    "objectID": "examples/cloudera-aws.html#visualize-predictions",
    "href": "examples/cloudera-aws.html#visualize-predictions",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Visualize predictions",
    "text": "Visualize predictions\nCompare actual gains to predicted gains for an out of time sample.\n\n# Select data from an out of time sample\ndata_2008 <- flights_tbl %>%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>%\n  filter(depdelay > 15 & depdelay < 240) %>%\n  filter(arrdelay > -60 & arrdelay < 360) %>%\n  filter(year == 2008) %>%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>%\n  mutate(gain = depdelay - arrdelay) %>%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest)\n\n# Summarize data by carrier\ncarrier <- sdf_predict(ml1, data_2008) %>%\n  group_by(description) %>%\n  summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>%\n  filter(freq > 10000) %>%\n  collect\n\n# Plot actual gains and predicted gains by airline carrier\nggplot(carrier, aes(gain, prediction)) + \n  geom_point(alpha = 0.75, color = 'red', shape = 3) +\n  geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') +\n  geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) +\n  labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted')\n\n\n\n\nSome carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights."
  },
  {
    "objectID": "examples/cloudera-aws.html#build-dashboard",
    "href": "examples/cloudera-aws.html#build-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Build dashboard",
    "text": "Build dashboard\nAggregate the scored data by origin, destination, and airline. Save the aggregated data.\n\n# Summarize by origin, destination, and carrier\nsummary_2008 <- sdf_predict(ml1, data_2008) %>%\n  rename(carrier = uniquecarrier, airline = description) %>%\n  group_by(origin, dest, carrier, airline) %>%\n  summarize(\n    flights = n(),\n    distance = mean(distance),\n    avg_dep_delay = mean(depdelay),\n    avg_arr_delay = mean(arrdelay),\n    avg_gain = mean(gain),\n    pred_gain = mean(prediction)\n    )\n\n# Collect and save objects\npred_data <- collect(summary_2008)\nairports <- collect(select(airports_tbl, name, faa, lat, lon))\nml1_summary <- capture.output(summary(ml1))\nsave(pred_data, airports, ml1_summary, file = 'flights_pred_2008.RData')"
  },
  {
    "objectID": "examples/cloudera-aws.html#publish-dashboard",
    "href": "examples/cloudera-aws.html#publish-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Publish dashboard",
    "text": "Publish dashboard\nUse the saved data to build an R Markdown flexdashboard. Publish the flexdashboard\n\n#Appendix\n\nAppendix A - Data files\nRun the following script to download data from the web onto your master node. Download the yearly flight data and the airlines lookup table.\n\n# Make download directory\nmkdir /tmp/flights\n\n# Download flight data by year\nfor i in {2006..2008}\n  do\n    echo \"$(date) $i Download\"\n    fnam=$i.csv.bz2\n    wget -O /tmp/flights/$fnam http://stat-computing.org/dataexpo/2009/$fnam\n    echo \"$(date) $i Unzip\"\n    bunzip2 /tmp/flights/$fnam\n  done\n\n# Download airline carrier data\nwget -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS\n\n# Download airports data\nwget -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\n\n\n\nHive tables\nWe used the Hue interface, logged in as ‘admin’ to load the data into HDFS and then into Hive.\nCREATE EXTERNAL TABLE IF NOT EXISTS flights\n(\nyear int,\nmonth int,\ndayofmonth int,\ndayofweek int,\ndeptime int,\ncrsdeptime int,\narrtime int, \ncrsarrtime int,\nuniquecarrier string,\nflightnum int,\ntailnum string, \nactualelapsedtime int,\ncrselapsedtime int,\nairtime string,\narrdelay int,\ndepdelay int, \norigin string,\ndest string,\ndistance int,\ntaxiin string,\ntaxiout string,\ncancelled int,\ncancellationcode string,\ndiverted int,\ncarrierdelay string,\nweatherdelay string,\nnasdelay string,\nsecuritydelay string,\nlateaircraftdelay string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLINES TERMINATED BY '\\n'\nTBLPROPERTIES(\"skip.header.line.count\"=\"1\");\nLOAD DATA INPATH '/user/admin/flights/2006.csv/' INTO TABLE flights;\nLOAD DATA INPATH '/user/admin/flights/2007.csv/' INTO TABLE flights;\nLOAD DATA INPATH '/user/admin/flights/2008.csv/' INTO TABLE flights;\n# Create metadata for airlines\nCREATE EXTERNAL TABLE IF NOT EXISTS airlines\n(\nCode string,\nDescription string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE\ntblproperties(\"skip.header.line.count\"=\"1\");\nLOAD DATA INPATH '/user/admin/L_UNIQUE_CARRIERS.csv' INTO TABLE airlines;\nCREATE EXTERNAL TABLE IF NOT EXISTS airports\n(\nid string,\nname string,\ncity string,\ncountry string,\nfaa string,\nicao string,\nlat double,\nlon double,\nalt int,\ntz_offset double,\ndst string,\ntz_name string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE;\nLOAD DATA INPATH '/user/admin/airports.dat' INTO TABLE airports;"
  },
  {
    "objectID": "examples/databricks-cluster-local.html#overview",
    "href": "examples/databricks-cluster-local.html#overview",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Overview",
    "text": "Overview\nIf the recommended path of connecting to Spark remotely with Databricks Connect does not apply to your use case, then you can install RStudio Workbench directly within a Databricks cluster as described in the sections below.\nWith this configuration, RStudio Workbench is installed on the Spark driver node and allows users to work locally with Spark using sparklyr.\n\nThis configuration can result in increased complexity, limited connectivity to other storage and compute resources, resource contention between RStudio Workbench and Databricks, and maintenance concerns due to the ephemeral nature of Databricks clusters.\nFor additional details, refer to the FAQ for RStudio in the Databricks Documentation."
  },
  {
    "objectID": "examples/databricks-cluster-local.html#advantages-and-limitations",
    "href": "examples/databricks-cluster-local.html#advantages-and-limitations",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Advantages and limitations",
    "text": "Advantages and limitations\nAdvantages:\n\nAbility for users to connect sparklyr to Spark without configuring remote connectivity\nProvides a high-bandwidth connection between R and the Spark JVM processes because they are running on the same machine\nCan load data from the cluster directly into an R session since RStudio Workbench is installed within the Databricks cluster\n\nLimitations:\n\nIf the Databricks cluster is restarted or terminated, then the instance of RStudio Workbench will be terminated and its configuration will be lost\nIf users do not persist their code through version control or the Databricks File System, then you risk losing user’s work if the cluster is restarted or terminated\nRStudio Workbench (and other RStudio products) installed within a Databricks cluster will be limited to the compute resources and lifecycle of that particular Spark cluster\nNon-Spark jobs will use CPU and RAM resources within the Databricks cluster\nNeed to install one instance of RStudio Workbench per Spark cluster that you want to run jobs on"
  },
  {
    "objectID": "examples/databricks-cluster-local.html#requirements",
    "href": "examples/databricks-cluster-local.html#requirements",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Requirements",
    "text": "Requirements\n\nA running Databricks cluster with a runtime version 4.1 or above\nThe cluster must not have “table access control” or “automatic termination” enabled\nYou must have “Can Attach To” permission for the Databricks cluster"
  },
  {
    "objectID": "examples/databricks-cluster-local.html#preparation",
    "href": "examples/databricks-cluster-local.html#preparation",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Preparation",
    "text": "Preparation\nThe following steps walk through the process to install RStudio Workbench on the Spark driver node within your Databricks cluster.\nThe recommended method for installing RStudio Workbench to the Spark driver node is via SSH. However, an alternative method is available if you are not able to access the Spark driver node via SSH."
  },
  {
    "objectID": "examples/databricks-cluster-local.html#configure-ssh-access-to-the-spark-driver-node",
    "href": "examples/databricks-cluster-local.html#configure-ssh-access-to-the-spark-driver-node",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Configure SSH access to the Spark driver node",
    "text": "Configure SSH access to the Spark driver node\nConfigure SSH access to the Spark driver node in Databricks by following the steps in the SSH access to clusters section of the Databricks Cluster configurations documentation.\nNote: If you are unable to configure SSH access or connect to the Spark driver node via SSH, then you can follow the steps in the Get started with RStudio Workbench section of the RStudio on Databricks documentation to install RStudio Workbench from a Databricks notebook, then skip to the access RStudio Workbench section of this documentation."
  },
  {
    "objectID": "examples/databricks-cluster-local.html#connect-to-the-spark-driver-node-via-ssh",
    "href": "examples/databricks-cluster-local.html#connect-to-the-spark-driver-node-via-ssh",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Connect to the Spark driver node via SSH",
    "text": "Connect to the Spark driver node via SSH\nConnect to the Spark driver node via SSH on port 2200 by using the following command on your local machine:\nssh ubuntu@<spark-driver-node-address> -p 2200 -i <path-to-private-SSH-key>\nReplace <spark-driver-node-address> with the DNS name or IP address of the Spark driver node, and <path-to-private-SSH-key> with the path to your private SSH key on your local machine."
  },
  {
    "objectID": "examples/databricks-cluster-local.html#install-rstudio-workbench-on-the-spark-driver-node",
    "href": "examples/databricks-cluster-local.html#install-rstudio-workbench-on-the-spark-driver-node",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Install RStudio Workbench on the Spark driver node",
    "text": "Install RStudio Workbench on the Spark driver node\nAfter you SSH into the Spark driver node, then you can follow the typical steps to install RStudio Workbench in the RStudio documentation. In the installation steps, you can select Ubuntu as the target Linux distribution."
  },
  {
    "objectID": "examples/databricks-cluster-local.html#configure-rstudio-workbench",
    "href": "examples/databricks-cluster-local.html#configure-rstudio-workbench",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Configure RStudio Workbench",
    "text": "Configure RStudio Workbench\nThe following configuration steps are required to be able to use RStudio Workbench with Databricks.\nAdd the following configuration lines to /etc/rstudio/rserver.conf to use proxied authentication with Databricks and enable the administrator dashboard:\nauth-proxy=1\nauth-proxy-user-header-rewrite=^(.*)$ $1\nauth-proxy-sign-in-url=<domain>/login.html\nadmin-enabled=1\nAdd the following configuration line to /etc/rstudio/rsession-profile to set the PATH to be used with RStudio Workbench:\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:$PATH\nAdd the following configuration lines to /etc/rstudio/rsession.conf to configure sessions in RStudio Workbench to work with Databricks:\nsession-rprofile-on-resume-default=1\nallow-terminal-websockets=0\nRestart RStudio Workbench:\nsudo rstudio-server restart"
  },
  {
    "objectID": "examples/databricks-cluster-local.html#access-rstudio-workbench",
    "href": "examples/databricks-cluster-local.html#access-rstudio-workbench",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Access RStudio Workbench",
    "text": "Access RStudio Workbench\nFrom the Databricks console, click on the Databricks cluster that you want to work with:\n\nFrom within the Databricks cluster, click on the Apps tab:\n\nClick on the Set up RStudio button:\n\nTo access RStudio Workbench, click on the link to Open RStudio:\n\nIf you configured proxied authentication in RStudio Workbench as described in the previous section, then you do not need to use the username or password that is displayed. Instead, RStudio Workbench will automatically login and start a new RStudio session as your logged-in Databricks user:\n\nOther users can access RStudio Workbench from the Databricks console by following the same steps described above. You do not need to create those users in RStudio Workbench or their home directory beforehand."
  },
  {
    "objectID": "examples/databricks-cluster-local.html#configure-sparklyr",
    "href": "examples/databricks-cluster-local.html#configure-sparklyr",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Configure sparklyr",
    "text": "Configure sparklyr\nUse the following R code to establish a connection from sparklyr to the Databricks cluster:\nSparkR::sparkR.session()\nlibrary(sparklyr)\nsc <- spark_connect(method = \"databricks\")"
  },
  {
    "objectID": "examples/databricks-cluster-local.html#additional-information",
    "href": "examples/databricks-cluster-local.html#additional-information",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Additional information",
    "text": "Additional information\nFor more information on using RStudio Workbench inside of Databricks, refer to the sections on RStudio on Databricks (AWS) or RStudio on Databricks (Azure) in the Databricks documentation."
  },
  {
    "objectID": "examples/databricks-cluster-odbc.html#overview",
    "href": "examples/databricks-cluster-odbc.html#overview",
    "title": "Using an ODBC connection with Databricks",
    "section": "Overview",
    "text": "Overview\nThis configuration details how to connect to Databricks using an ODBC connection. With this setup, R can connect to Databricks using the odbc and DBI R packages. This type of configuration is the recommended approach for connecting to Databricks from RStudio Connect and can also be used from RStudio Workbench."
  },
  {
    "objectID": "examples/databricks-cluster-odbc.html#advantages-and-limitations",
    "href": "examples/databricks-cluster-odbc.html#advantages-and-limitations",
    "title": "Using an ODBC connection with Databricks",
    "section": "Advantages and limitations",
    "text": "Advantages and limitations\nAdvantages:\n\nODBC connections tend to be more stable than Spark connections. This is especially beneficial for content published to RStudio Connect.\nIf code is developed using a Spark connection and sparklyr, it is easy to swap out the connection type for an ODBC connection and the remaining code will still run.\nThe Spark ODBC driver provided by Databricks was benchmarked against a native Spark connection and the performance of the two is very comparable.\n\nLimitations: - Not all Spark features and functions are available through an ODBC connection."
  },
  {
    "objectID": "examples/databricks-cluster-odbc.html#driver-installation",
    "href": "examples/databricks-cluster-odbc.html#driver-installation",
    "title": "Using an ODBC connection with Databricks",
    "section": "Driver installation",
    "text": "Driver installation\nDownload and install the Spark ODBC driver from Databricks"
  },
  {
    "objectID": "examples/databricks-cluster-odbc.html#driver-configuration",
    "href": "examples/databricks-cluster-odbc.html#driver-configuration",
    "title": "Using an ODBC connection with Databricks",
    "section": "Driver configuration",
    "text": "Driver configuration\nCreate a DSN for Databricks.\n[Databricks-Spark]\nDriver=Simba\nServer=<server-hostname>\nHOST=<server-hostname>\nPORT=<port>\nSparkServerType=3\nSchema=default\nThriftTransport=2\nSSL=1\nAuthMech=3\nUID=token\nPWD=<personal-access-token>\nHTTPPath=<http-path>"
  },
  {
    "objectID": "examples/databricks-cluster-odbc.html#connect-to-databricks",
    "href": "examples/databricks-cluster-odbc.html#connect-to-databricks",
    "title": "Using an ODBC connection with Databricks",
    "section": "Connect to Databricks",
    "text": "Connect to Databricks\nThe connection can be tested from the command line using isql -v Databricks-Spark where Databricks-Spark is the DSN name for the connection. If that connects successfully, then the following code can be used to create a connection from an R session:\nlibrary(DBI)\nlibrary(odbc)\n\ncon <- dbConnect(odbc(), \"Databricks-Spark\")"
  },
  {
    "objectID": "examples/databricks-cluster-odbc.html#additional-information",
    "href": "examples/databricks-cluster-odbc.html#additional-information",
    "title": "Using an ODBC connection with Databricks",
    "section": "Additional information",
    "text": "Additional information\nFor more information about ODBC connections from R, please visit db.rstudio.com."
  },
  {
    "objectID": "examples/databricks-cluster-remote.html#overview",
    "href": "examples/databricks-cluster-remote.html#overview",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Overview",
    "text": "Overview\nWith this configuration, RStudio Workbench is installed outside of the Spark cluster and allows users to connect to Spark remotely using sparklyr with Databricks Connect.\n\nThis is the recommended configuration because it targets separate environments, involves a typical configuration process, avoids resource contention, and allows RStudio Workbench to connect to Databricks as well as other remote storage and compute resources."
  },
  {
    "objectID": "examples/databricks-cluster-remote.html#advantages-and-limitations",
    "href": "examples/databricks-cluster-remote.html#advantages-and-limitations",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Advantages and limitations",
    "text": "Advantages and limitations\nAdvantages:\n\nRStudio Workbench will remain functional if Databricks clusters are terminated\nProvides the ability to communicate with one or more Databricks clusters as a remote compute resource\nAvoids resource contention between RStudio Workbench and Databricks\n\nLimitations:\n\nDatabricks Connect does not currently support the following APIs from sparklyr: Broom APIs, Streaming APIs, Broadcast APIs, Most MLlib APIs, csv_file serialization mode, and the spark_submit API\nDatabricks Connect does not support structured streaming\nDatabricks Connect does not support running arbitrary code that is not a part of a Spark job on the remote cluster\nDatabricks Connect does not support Scala, Python, and R APIs for Delta table operations\nDatabricks Connect does not support most utilities in Databricks Utilities. However, dbutils.fs and dbutils.secrets are supported\n\nFor more information on the limitations of Databricks Connect, refer to the Limitation section of the Databricks Connect documentation."
  },
  {
    "objectID": "examples/databricks-cluster-remote.html#requirements",
    "href": "examples/databricks-cluster-remote.html#requirements",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Requirements",
    "text": "Requirements\n\nRStudio Workbench installed outside of the Databricks cluster\nJava 8 installed on the machine with RStudio Workbench\nA running Databricks cluster with a runtime version 5.5 or above"
  },
  {
    "objectID": "examples/databricks-cluster-remote.html#install-python",
    "href": "examples/databricks-cluster-remote.html#install-python",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Install Python",
    "text": "Install Python\nThe Databricks Connect client is provided as a Python library. The minor version of your Python installation must be the same as the minor Python version of your Databricks cluster.\nRefer to the steps in the install Python section of the RStudio Documentation to install Python on the same server where RStudio Workbench is installed.\nNote that you can either install Python for all users in a global location (as an administrator) or in a home directory (as an end user)."
  },
  {
    "objectID": "examples/databricks-cluster-remote.html#install-databricks-connect",
    "href": "examples/databricks-cluster-remote.html#install-databricks-connect",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Install Databricks Connect",
    "text": "Install Databricks Connect\nRun the following command to install Databricks Connect on the server with RStudio Workbench:\npip install -U databricks-connect==6.3.*  # or a different version to match your Databricks cluster\nNote that you can either install this library for all users in a global Python environment (as an administrator) or for an individual user in their Python environment (e.g., using the pip --user option or installing into a conda environment or virtual environment)."
  },
  {
    "objectID": "examples/databricks-cluster-remote.html#configure-databricks-connect",
    "href": "examples/databricks-cluster-remote.html#configure-databricks-connect",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Configure Databricks Connect",
    "text": "Configure Databricks Connect\nTo configure the Databricks Connect client, you can run the following command in a terminal when logged in as a user in RStudio Workbench:\ndatabricks-connect configure\nIn the prompts that follow, enter the following information:\n\n\n\n\n\n\n\n\nParameter\nDescription\nExample Value\n\n\n\n\nDatabricks Host\nBase address of your Databricks console URL\nhttps://dbc-01234567-89ab.cloud.databricks.com\n\n\nDatabricks Token\nUser token generated from the Databricks Console under your “User Settings”\ndapi24g06bdd96f2700b09dd336d5444c1yz\n\n\nCluster ID\nCluster ID in the Databricks console under Advanced Options > Tags > ClusterId\n0308-033548-colt989\n\n\nOrg ID\nFound in the ?o=orgId portion of your Databricks Console URL\n8498623428173033\n\n\nPort\nThe port that Databricks Connect connects to\n15001\n\n\n\nAfter you’ve completed the configuration process for Databricks Connect, you can run the following command in a terminal to test the connectivity of Databricks Connect to your Databricks cluster:\ndatabricks-connect test"
  },
  {
    "objectID": "examples/databricks-cluster-remote.html#install-sparklyr",
    "href": "examples/databricks-cluster-remote.html#install-sparklyr",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Install sparklyr",
    "text": "Install sparklyr\nThe integration of sparklyr with Databricks Connect is currently being added to the development version of sparklyr. To use this functionality now, you’ll need to install the development version of sparklyr by running the following command in an R console:\ndevtools::install_github(\"sparklyr/sparklyr\")"
  },
  {
    "objectID": "examples/databricks-cluster-remote.html#install-spark",
    "href": "examples/databricks-cluster-remote.html#install-spark",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Install Spark",
    "text": "Install Spark\nTo work with a remote Databricks cluster, you need to have a local installation of Spark that matches the version of Spark on the Databricks Cluster.\nYou can install Spark by running the following command in an R console:\nlibrary(sparklyr)\nsparklyr::spark_install()\nYou can specify the version of Spark to install along with other options. Refer to the spark_install() options in the sparklyr reference documentation for more information."
  },
  {
    "objectID": "examples/databricks-cluster-remote.html#use-sparklyr",
    "href": "examples/databricks-cluster-remote.html#use-sparklyr",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Use sparklyr",
    "text": "Use sparklyr\nIn order to connect to Databricks using sparklyr and databricks-connect, SPARK_HOME must be set to the output of the databricks-connect get-spark-home command.\nYou can set SPARK_HOME as an environment variable or directly within spark_connect(). The following R code demonstrates connecting to Databricks, copying some data into the cluster, summarizing that data using sparklyr, and disconnecting:\nlibrary(sparklyr)\nlibrary(dplyr)\n\ndatabricks_connect_spark_home <- system(\"databricks-connect get-spark-home\", intern = TRUE)\nsc <- spark_connect(method = \"databricks\", spark_home = databricks_connect_spark_home)\n\ncars_tbl <- copy_to(sc, mtcars, overwrite = TRUE)\n\ncars_tbl %>% \n  group_by(cyl) %>% \n  summarise(mean_mpg = mean(mpg, na.rm = TRUE),\n            mean_hp  = mean(hp, na.rm = TRUE))\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "examples/databricks-cluster-remote.html#additional-information",
    "href": "examples/databricks-cluster-remote.html#additional-information",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Additional information",
    "text": "Additional information\nFor more information on the setup, configuration, troubleshooting, and limitations of Databricks Connect, refer to the Databricks Connect section of the Databricks documentation."
  },
  {
    "objectID": "examples/databricks-cluster.html#overview",
    "href": "examples/databricks-cluster.html#overview",
    "title": "Using sparklyr with Databricks",
    "section": "Overview",
    "text": "Overview\nThis documentation demonstrates how to use sparklyr with Apache Spark in Databricks along with RStudio Team, RStudio Workbench, RStudio Connect, and RStudio Package Manager."
  },
  {
    "objectID": "examples/databricks-cluster.html#using-rstudio-team-with-databricks",
    "href": "examples/databricks-cluster.html#using-rstudio-team-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Using RStudio Team with Databricks",
    "text": "Using RStudio Team with Databricks\nRStudio Team is a bundle of our popular professional software for developing data science projects, publishing data products, and managing packages.\nRStudio Team and sparklyr can be used with Databricks to work with large datasets and distributed computations with Apache Spark. The most common use case is to perform interactive analysis and exploratory development with RStudio Workbench and sparklyr; write out the results to a database, file system, or cloud storage; then publish apps, reports, and APIs to RStudio Connect that query and access the results.\n\nThe sections below describe best practices and different options for configuring specific RStudio products to work with Databricks."
  },
  {
    "objectID": "examples/databricks-cluster.html#best-practices-for-working-with-databricks",
    "href": "examples/databricks-cluster.html#best-practices-for-working-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Best practices for working with Databricks",
    "text": "Best practices for working with Databricks\n\nMaintain separate installation environments - Install RStudio Workbench, RStudio Connect, and RStudio Package Manager outside of the Databricks cluster so that they are not limited to the compute resources or ephemeral nature of Databricks clusters.\nConnect to Databricks remotely - Work with Databricks as a remote compute resource, similar to how you would connect remotely to external databases, data sources, and storage systems. This can be accomplished using Databricks Connect (as described in the Connecting to Databricks remotely section below) or by performing SQL queries with JDBC/ODBC using the Databricks Spark SQL Driver on AWS or Azure.\nRestrict workloads to interactive analysis - Only perform workloads related to exploratory or interactive analysis with Spark, then write the results to a database, file system, or cloud storage for more efficient retrieval in apps, reports, and APIs.\nLoad and query results efficiently - Because of the nature of Spark computations and the associated overhead, Shiny apps that use Spark on the backend tend to have performance and runtime issues; consider reading the results from a database, file system, or cloud storage instead."
  },
  {
    "objectID": "examples/databricks-cluster.html#using-rstudio-workbench-with-databricks",
    "href": "examples/databricks-cluster.html#using-rstudio-workbench-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Using RStudio Workbench with Databricks",
    "text": "Using RStudio Workbench with Databricks\nThere are two options for using sparklyr and RStudio Workbench with Databricks:\n\nOption 1: Connecting to Databricks remotely (Recommended Option)\nOption 2: Working inside of Databricks (Alternative Option)\n\n\nOption 1 - Connecting to Databricks remotely\nWith this configuration, RStudio Workbench is installed outside of the Spark cluster and allows users to connect to Spark remotely using sparklyr with Databricks Connect.\nThis is the recommended configuration because it targets separate environments, involves a typical configuration process, avoids resource contention, and allows RStudio Workbench to connect to Databricks as well as other remote storage and compute resources.\n\n\nView steps for connecting to Databricks remotely\n\n\n  \n\n\nOption 2 - Working inside of Databricks\nIf you cannot work with Spark remotely, you should install RStudio Workbench on the Driver node of a long-running, persistent Databricks cluster as opposed to a worker node or an ephemeral cluster.\nWith this configuration, RStudio Workbench is installed on the Spark driver node and allows users to connect to Spark locally using sparklyr.\nThis configuration can result in increased complexity, limited connectivity to other storage and compute resources, resource contention between RStudio Workbench and Databricks, and maintenance concerns due to the ephemeral nature of Databricks clusters.\n\n\nView steps for working inside of Databricks"
  },
  {
    "objectID": "examples/databricks-cluster.html#using-rstudio-connect-with-databricks",
    "href": "examples/databricks-cluster.html#using-rstudio-connect-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Using RStudio Connect with Databricks",
    "text": "Using RStudio Connect with Databricks\nThe server environment within Databricks clusters is not permissive enough to support RStudio Connect or the process sandboxing mechanisms that it uses to isolate published content.\nTherefore, the only supported configuration is to install RStudio Connect outside of the Databricks cluster and connect to Databricks remotely.\nWhether RStudio Workbench is installed outside of the Databricks cluster (Recommended Option) or within the Databricks cluster (Alternative Option), you can publish content to RStudio Connect as long as HTTP/HTTPS network traffic is allowed from RStudio Workbench to RStudio Connect.\nThere are two options for using RStudio Connect with Databricks:\n\nPerforming SQL queries with ODBC using the Databricks Spark SQL Driver (Recommended Option).\nAdding calls in your R code to create and run Databricks jobs with bricksteR and the Databricks Jobs API (Alternative Option)"
  },
  {
    "objectID": "examples/databricks-cluster.html#using-rstudio-package-manager-with-databricks",
    "href": "examples/databricks-cluster.html#using-rstudio-package-manager-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Using RStudio Package Manager with Databricks",
    "text": "Using RStudio Package Manager with Databricks\nWhether RStudio Workbench is installed outside of the Databricks cluster (Recommended Option) or within the Databricks cluster (Alternative Option), you can install packages from repositories in RStudio Package Manager as long as HTTP/HTTPS network traffic is allowed from RStudio Workbench to RStudio Package Manager."
  },
  {
    "objectID": "examples/qubole-cluster.html#overview",
    "href": "examples/qubole-cluster.html#overview",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Overview",
    "text": "Overview\nQubole users can request access to RStudio Server Pro. This allows users to use sparklyr to interact directly with Spark from within the Qubole cluster."
  },
  {
    "objectID": "examples/qubole-cluster.html#advantages-and-limitations",
    "href": "examples/qubole-cluster.html#advantages-and-limitations",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Advantages and limitations",
    "text": "Advantages and limitations\nAdvantages:\n\nAbility for users to connect sparklyr directly to Spark within Qubole\nProvides a high-bandwidth connection between R and the Spark JVM processes because they are running on the same machine\nCan load data from the cluster directly into an R session since RStudio Workbench is installed within the Qubole cluster\nA unique, persistent home directory for each user\n\nLimitations:\n\nPersistent packages must be managed using Qubole Environments, not directly from within RStudio\nRStudio Workbench installed within a Qubole cluster will be limited to the compute resources and lifecycle of that particular Spark cluster\nNon-Spark jobs will use CPU and RAM resources within the Qubole cluster"
  },
  {
    "objectID": "examples/qubole-cluster.html#access-rstudio-workbench",
    "href": "examples/qubole-cluster.html#access-rstudio-workbench",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Access RStudio Workbench",
    "text": "Access RStudio Workbench\nRStudio Workbench can be accessed from the cluster resources menu:"
  },
  {
    "objectID": "examples/qubole-cluster.html#use-sparklyr",
    "href": "examples/qubole-cluster.html#use-sparklyr",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Use sparklyr",
    "text": "Use sparklyr\nUse the following R code to establish a connection from sparklyr to the Qubole cluster:\nlibrary(sparklyr)\nsc <- spark_connect(method = \"qubole\")"
  },
  {
    "objectID": "examples/qubole-cluster.html#additional-information",
    "href": "examples/qubole-cluster.html#additional-information",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Additional information",
    "text": "Additional information\nFor more information on using RStudio Workbench inside of Qubole, refer to the Qubole documentation."
  },
  {
    "objectID": "examples/qubole-overview.html#overview",
    "href": "examples/qubole-overview.html#overview",
    "title": "Using sparklyr with Qubole",
    "section": "Overview",
    "text": "Overview\nThis documentation demonstrates how to use sparklyr with Apache Spark in Qubole along with RStudio Server Pro and RStudio Connect."
  },
  {
    "objectID": "examples/qubole-overview.html#best-practices-for-working-with-qubole",
    "href": "examples/qubole-overview.html#best-practices-for-working-with-qubole",
    "title": "Using sparklyr with Qubole",
    "section": "Best practices for working with Qubole",
    "text": "Best practices for working with Qubole\n\nManage packages via Qubole Environments - Packages installed via install.packages() are not available on cluster restart. Packages managed through Qubole Environments are persistent.\nRestrict workloads to interactive analysis - Only perform workloads related to exploratory or interactive analysis with Spark, then write the results to a database, file system, or cloud storage for more efficient retrieval in apps, reports, and APIs.\nLoad and query results efficiently - Because of the nature of Spark computations and the associated overhead, Shiny apps that use Spark on the backend tend to have performance and runtime issues; consider reading the results from a database, file system, or cloud storage instead."
  },
  {
    "objectID": "examples/qubole-overview.html#using-rstudio-workbench-with-qubole",
    "href": "examples/qubole-overview.html#using-rstudio-workbench-with-qubole",
    "title": "Using sparklyr with Qubole",
    "section": "Using RStudio Workbench with Qubole",
    "text": "Using RStudio Workbench with Qubole\nThe Qubole platform includes RStudio Workbench. More details about how to request RStudio Workbench and access it from within a Qubole cluster are available from Qubole.\n\n\nView steps for running RStudio Workbench inside Qubole"
  },
  {
    "objectID": "examples/qubole-overview.html#using-rstudio-connect-with-qubole",
    "href": "examples/qubole-overview.html#using-rstudio-connect-with-qubole",
    "title": "Using sparklyr with Qubole",
    "section": "Using RStudio Connect with Qubole",
    "text": "Using RStudio Connect with Qubole\nThe best configuration for working with Qubole and RStudio Connect is to install RStudio Connect outside of the Qubole cluster and connect to Qubole remotely. This is accomplished using the Qubole ODBC Driver."
  },
  {
    "objectID": "examples/stand-alone-aws.html#overview",
    "href": "examples/stand-alone-aws.html#overview",
    "title": "Spark Standalone Deployment in AWS",
    "section": "Overview",
    "text": "Overview\nThe plan is to launch 4 identical EC2 server instances. One server will be the Master node and the other 3 the worker nodes. In one of the worker nodes, we will install RStudio server.\nWhat makes a server the Master node is only the fact that it is running the master service, while the other machines are running the slave service and are pointed to that first master. This simple setup, allows us to install the same Spark components on all 4 servers and then just add RStudio to one of them.\nThe topology will look something like this:"
  },
  {
    "objectID": "examples/stand-alone-aws.html#aws-ec-instances",
    "href": "examples/stand-alone-aws.html#aws-ec-instances",
    "title": "Spark Standalone Deployment in AWS",
    "section": "AWS EC Instances",
    "text": "AWS EC Instances\nHere are the details of the EC2 instance, just deploy one at this point:\n\nType: t2.medium\nOS: Ubuntu 16.04 LTS\nDisk space: At least 20GB\nSecurity group: Open the following ports: 8080 (Spark UI), 4040 (Spark Worker UI), 8088 (sparklyr UI) and 8787 (RStudio). Also open All TCP ports for the machines inside the security group."
  },
  {
    "objectID": "examples/stand-alone-aws.html#spark",
    "href": "examples/stand-alone-aws.html#spark",
    "title": "Spark Standalone Deployment in AWS",
    "section": "Spark",
    "text": "Spark\nPerform the steps in this section on all of the servers that will be part of the cluster.\n\nInstall Java 8\n\nWe will add the Java 8 repository, install it and set it as default\n\nsudo apt-add-repository ppa:webupd8team/java\nsudo apt-get update\nsudo apt-get install oracle-java8-installer\nsudo apt-get install oracle-java8-set-default\nsudo apt-get update\nor alternatively, run\nsudo apt install openjdk-8-jdk\nto install Open JDK version 8.\n\n\nDownload Spark\n\nDownload and unpack a pre-compiled version of Spark. Here’s is the link to the official Spark download page\n\nwget http://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-hadoop2.7.tgz\ntar -xvzf spark-2.1.0-bin-hadoop2.7.tgz\ncd spark-2.1.0-bin-hadoop2.7\n\n\nCreate and launch AMI\n\nWe will create an image of the server. In Amazon, these are called AMIs, for information please see the User Guide.\nLaunch 3 instances of the AMI"
  },
  {
    "objectID": "examples/stand-alone-aws.html#rstudio-server",
    "href": "examples/stand-alone-aws.html#rstudio-server",
    "title": "Spark Standalone Deployment in AWS",
    "section": "RStudio Server",
    "text": "RStudio Server\nSelect one of the nodes to execute this section. Please check the RStudio download page for the latest version\n\nInstall R\n\nIn order to get the latest R core, we will need to update the source list in Ubuntu.\n\nsudo sh -c 'echo \"deb http://cran.rstudio.com/bin/linux/ubuntu xenial/\" >> /etc/apt/sources.list'\ngpg --keyserver keyserver.ubuntu.com --recv-key 0x517166190x51716619e084dab9\ngpg -a --export 0x517166190x51716619e084dab9 | sudo apt-key add -\nsudo apt-get update\n\nNow we can install R\n\nsudo apt-get install r-base\nsudo apt-get install gdebi-core\n\n\nInstall RStudio\n\nWe will download and install 1.044 of RStudio Server. To find the latest version, please visit the RStudio website. In order to get the enhanced integration with Spark, RStudio version 1.044 or later will be needed.\n\nwget https://download2.rstudio.org/rstudio-server-1.0.153-amd64.deb\nsudo gdebi rstudio-server-1.0.153-amd64.deb\n\n\nInstall dependencies\n\nRun the following commands\n\nsudo apt-get -y install libcurl4-gnutls-dev\nsudo apt-get -y install libssl-dev\nsudo apt-get -y install libxml2-dev\n\n\nAdd default user\n\nRun the following command to add a default user\n\nsudo adduser rstudio-user\n\n\nStart the Master node\n\nSelect one of the servers to become your Master node\nRun the command that starts the master service\n\nsudo spark-2.1.0-bin-hadoop2.7/sbin/start-master.sh\n\nClose the terminal connection (optional)\n\n\n\nStart Worker nodes\n\nStart the slave service. Important: Use dots not dashes as separators for the Spark Master node’s address\n\nsudo spark-2.1.0-bin-hadoop2.7/sbin/start-slave.sh spark://[Master node's IP address]:7077\nsudo spark-2.1.0-bin-hadoop2.7/sbin/start-slave.sh spark://ip-172-30-1-94.us-west-2.compute.internal:7077 - Close the terminal connection (optional)\n\n\nPre-load pacakges\n\nLog into RStudio (port 8787)\nUse ‘rstudio-user’\n\ninstall.packages(\"sparklyr\")\n\n\nConnect to the Spark Master\n\nNavigate to the Spark Master’s UI, typically on port 8080 \nNote the Spark Master URL\nLogon to RStudio\nRun the following code\n\n\nlibrary(sparklyr)\n\nconf <- spark_config()\nconf$spark.executor.memory <- \"2GB\"\nconf$spark.memory.fraction <- 0.9\n\nsc <- spark_connect(master=\"[Spark Master URL]\",\n              version = \"2.1.0\",\n              config = conf,\n              spark_home = \"/home/ubuntu/spark-2.1.0-bin-hadoop2.7/\")"
  },
  {
    "objectID": "examples/yarn-cluster-emr.html#set-up-the-cluster",
    "href": "examples/yarn-cluster-emr.html#set-up-the-cluster",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Set up the cluster",
    "text": "Set up the cluster\nThis demonstration uses Amazon Web Services (AWS), but it could just as easily use Microsot, Google, or any other provider. We will use Elastic Map Reduce (EMR) to easily set up a cluster with two core nodes and one master node. Nodes use virtual servers from the Elastic Compute Cloud (EC2). Note: There is no free tier for EMR, charges will apply.\nBefore beginning this setup we assume you have:\n\nFamiliarity with and access to an AWS account\nFamiliarity with basic linux commands\nSudo privileges in order to install software from the command line\n\n\n\n\n\nBuild an EMR cluster\nBefore beginning the EMR wizard setup, make sure you create the following in AWS:\n\nAn AWS key pair (.pem key) so you can SSH into the EC2 master node\nA security group that gives you access to port 22 on your IP and port 8787 from anywhere\n\n\n\n\nStep 1: Select software\nMake sure to select Hive and Spark as part of the install. Note that by choosing Spark, R will also be installed on the master node as part of the distribution.\n\n\n\n\nStep 2: Select hardware\nInstall 2 core nodes and one master node with m3.xlarge 80 GiB storage per node. You can easily increase the number of nodes later.\n\n\n\n\nStep 3: Select general cluster settings\nClick next on the general cluster settings.\n\n\n\n\nStep 4: Select security\nEnter your EC2 key pair and security group. Make sure the security group has ports 22 and 8787 open.\n\n\n\n\n\nConnect to EMR\nThe cluster page will give you details about your EMR cluster and instructions on connecting.\n\nConnect to the master node via SSH using your key pair. Once you connect you will see the EMR welcome. ::: {.cell}\n# Log in to master node\nssh -i ~/spark-demo.pem hadoop@ec2-52-10-102-11.us-west-2.compute.amazonaws.com\n:::\n\n\n\nInstall RStudio Server\nEMR uses Amazon Linux which is based on Centos. Update your master node and install dependencies that will be used by R packages.\n\n# Update\nsudo yum update\nsudo yum install libcurl-devel openssl-devel # used for devtools\n\nThe installation of RStudio Server is easy. Download the preview version of RStudio and install on the master node.\n\n# Install RStudio Server\nwget -P /tmp https://s3.amazonaws.com/rstudio-dailybuilds/rstudio-server-rhel-0.99.1266-x86_64.rpm\nsudo yum install --nogpgcheck /tmp/rstudio-server-rhel-0.99.1266-x86_64.rpm\n\n\n\nCreate a User\nCreate a user called rstudio-user that will perform the data analysis. Create a user directory for rstudio-user on HDFS with the hadoop fs command.\n\n# Make User\nsudo useradd -m rstudio-user\nsudo passwd rstudio-user\n\n# Create new directory in hdfs\nhadoop fs -mkdir /user/rstudio-user\nhadoop fs -chmod 777 /user/rstudio-user"
  },
  {
    "objectID": "examples/yarn-cluster-emr.html#download-flights-data",
    "href": "examples/yarn-cluster-emr.html#download-flights-data",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Download flights data",
    "text": "Download flights data\nThe flights data is a well known data source representing 123 million flights over 22 years. It consumes roughly 12 GiB of storage in uncompressed CSV format in yearly files.\n\nSwitch User\nFor data loading and analysis, make sure you are logged in as regular user.\n\n# create directories on hdfs for new user\nhadoop fs -mkdir /user/rstudio-user\nhadoop fs -chmod 777 /user/rstudio-user\n\n# switch user\nsu rstudio-user\n\n\n\nDownload data\nRun the following script to download data from the web onto your master node. Download the yearly flight data and the airlines lookup table.\n\n# Make download directory\nmkdir /tmp/flights\n\n# Download flight data by year\nfor i in {1987..2008}\n  do\n    echo \"$(date) $i Download\"\n    fnam=$i.csv.bz2\n    wget -O /tmp/flights/$fnam http://stat-computing.org/dataexpo/2009/$fnam\n    echo \"$(date) $i Unzip\"\n    bunzip2 /tmp/flights/$fnam\n  done\n\n# Download airline carrier data\nwget -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS\n\n# Download airports data\nwget -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\n\n\n\nDistribute into HDFS\nCopy data into HDFS using the hadoop fs command.\n\n# Copy flight data to HDFS\nhadoop fs -mkdir /user/rstudio-user/flights/\nhadoop fs -put /tmp/flights /user/rstudio-user/\n\n# Copy airline data to HDFS\nhadoop fs -mkdir /user/rstudio-user/airlines/\nhadoop fs -put /tmp/airlines.csv /user/rstudio-user/airlines\n\n# Copy airport data to HDFS\nhadoop fs -mkdir /user/rstudio-user/airports/\nhadoop fs -put /tmp/airports.csv /user/rstudio-user/airports"
  },
  {
    "objectID": "examples/yarn-cluster-emr.html#create-hive-tables",
    "href": "examples/yarn-cluster-emr.html#create-hive-tables",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Create Hive tables",
    "text": "Create Hive tables\nLaunch Hive from the command line.\n\n# Open Hive prompt\nhive\n\nCreate the metadata that will structure the flights table. Load data into the Hive table.\n# Create metadata for flights\nCREATE EXTERNAL TABLE IF NOT EXISTS flights\n(\nyear int,\nmonth int,\ndayofmonth int,\ndayofweek int,\ndeptime int,\ncrsdeptime int,\narrtime int, \ncrsarrtime int,\nuniquecarrier string,\nflightnum int,\ntailnum string, \nactualelapsedtime int,\ncrselapsedtime int,\nairtime string,\narrdelay int,\ndepdelay int, \norigin string,\ndest string,\ndistance int,\ntaxiin string,\ntaxiout string,\ncancelled int,\ncancellationcode string,\ndiverted int,\ncarrierdelay string,\nweatherdelay string,\nnasdelay string,\nsecuritydelay string,\nlateaircraftdelay string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLINES TERMINATED BY '\\n'\nTBLPROPERTIES(\"skip.header.line.count\"=\"1\");\n\n# Load data into table\nLOAD DATA INPATH '/user/rstudio-user/flights' INTO TABLE flights;\nCreate the metadata that will structure the airlines table. Load data into the Hive table.\n# Create metadata for airlines\nCREATE EXTERNAL TABLE IF NOT EXISTS airlines\n(\nCode string,\nDescription string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE\ntblproperties(\"skip.header.line.count\"=\"1\");\n\n# Load data into table\nLOAD DATA INPATH '/user/rstudio-user/airlines' INTO TABLE airlines;\nCreate the metadata that will structure the airports table. Load data into the Hive table.\n# Create metadata for airports\nCREATE EXTERNAL TABLE IF NOT EXISTS airports\n(\nid string,\nname string,\ncity string,\ncountry string,\nfaa string,\nicao string,\nlat double,\nlon double,\nalt int,\ntz_offset double,\ndst string,\ntz_name string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE;\n\n# Load data into table\nLOAD DATA INPATH '/user/rstudio-user/airports' INTO TABLE airports;"
  },
  {
    "objectID": "examples/yarn-cluster-emr.html#connect-to-spark",
    "href": "examples/yarn-cluster-emr.html#connect-to-spark",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Connect to Spark",
    "text": "Connect to Spark\nLog in to RStudio Server by pointing a browser at your master node IP:8787.\n\n\n\nSet the environment variable SPARK_HOME and then run spark_connect. After connecting you will be able to browse the Hive metadata in the RStudio Server Spark pane.\n\n# Connect to Spark\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nSys.setenv(SPARK_HOME=\"/usr/lib/spark\")\nconfig <- spark_config()\nsc <- spark_connect(master = \"yarn-client\", config = config, version = '1.6.2')\n\nOnce you are connected, you will see the Spark pane appear along with your hive tables.\n\n\n\nYou can inspect your tables by clicking on the data icon."
  },
  {
    "objectID": "examples/yarn-cluster-emr.html#cache-the-tables-into-memory",
    "href": "examples/yarn-cluster-emr.html#cache-the-tables-into-memory",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Cache the tables into memory",
    "text": "Cache the tables into memory\nUse tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame.\n\n# Cache flights Hive table into Spark\ntbl_cache(sc, 'flights')\nflights_tbl <- tbl(sc, 'flights')\n\n# Cache airlines Hive table into Spark\ntbl_cache(sc, 'airlines')\nairlines_tbl <- tbl(sc, 'airlines')\n\n# Cache airports Hive table into Spark\ntbl_cache(sc, 'airports')\nairports_tbl <- tbl(sc, 'airports')"
  },
  {
    "objectID": "examples/yarn-cluster-emr.html#create-a-model-data-set",
    "href": "examples/yarn-cluster-emr.html#create-a-model-data-set",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Create a model data set",
    "text": "Create a model data set\nFilter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight.\n\n# Filter records and create target variable 'gain'\nmodel_data <- flights_tbl %>%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>%\n  filter(depdelay > 15 & depdelay < 240) %>%\n  filter(arrdelay > -60 & arrdelay < 360) %>%\n  filter(year >= 2003 & year <= 2007) %>%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>%\n  mutate(gain = depdelay - arrdelay) %>%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain)\n\n# Summarize data by carrier\nmodel_data %>%\n  group_by(uniquecarrier) %>%\n  summarize(description = min(description), gain=mean(gain), \n            distance=mean(distance), depdelay=mean(depdelay)) %>%\n  select(description, gain, distance, depdelay) %>%\n  arrange(gain)\n\nSource:   query [?? x 4]\nDatabase: spark connection master=yarn-client app=sparklyr local=FALSE\n\n                    description       gain  distance depdelay\n                          <chr>      <dbl>     <dbl>    <dbl>\n1        ATA Airlines d/b/a ATA -3.3480120 1134.7084 56.06583\n2  ExpressJet Airlines Inc. (1) -3.0326180  519.7125 59.41659\n3                     Envoy Air -2.5434415  416.3716 53.12529\n4       Northwest Airlines Inc. -2.2030586  779.2342 48.52828\n5          Delta Air Lines Inc. -1.8248026  868.3997 50.77174\n6   AirTran Airways Corporation -1.4331555  641.8318 54.96702\n7    Continental Air Lines Inc. -0.9617003 1116.6668 57.00553\n8        American Airlines Inc. -0.8860262 1074.4388 55.45045\n9             Endeavor Air Inc. -0.6392733  467.1951 58.47395\n10              JetBlue Airways -0.3262134 1139.0443 54.06156\n# ... with more rows"
  },
  {
    "objectID": "examples/yarn-cluster-emr.html#train-a-linear-model",
    "href": "examples/yarn-cluster-emr.html#train-a-linear-model",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Train a linear model",
    "text": "Train a linear model\nPredict time gained or lost in flight as a function of distance, departure delay, and airline carrier.\n\n# Partition the data into training and validation sets\nmodel_partition <- model_data %>% \n  sdf_partition(train = 0.8, valid = 0.2, seed = 5555)\n\n# Fit a linear model\nml1 <- model_partition$train %>%\n  ml_linear_regression(gain ~ distance + depdelay + uniquecarrier)\n\n# Summarize the linear model\nsummary(ml1)\n\nDeviance Residuals: (approximate):\n     Min       1Q   Median       3Q      Max \n-305.422   -5.593    2.699    9.750  147.871 \n\nCoefficients:\n                    Estimate  Std. Error  t value  Pr(>|t|)    \n(Intercept)      -1.24342576  0.10248281 -12.1330 < 2.2e-16 ***\ndistance          0.00326600  0.00001670 195.5709 < 2.2e-16 ***\ndepdelay         -0.01466233  0.00020337 -72.0977 < 2.2e-16 ***\nuniquecarrier_AA -2.32650517  0.10522524 -22.1098 < 2.2e-16 ***\nuniquecarrier_AQ  2.98773637  0.28798507  10.3746 < 2.2e-16 ***\nuniquecarrier_AS  0.92054894  0.11298561   8.1475 4.441e-16 ***\nuniquecarrier_B6 -1.95784698  0.11728289 -16.6934 < 2.2e-16 ***\nuniquecarrier_CO -2.52618081  0.11006631 -22.9514 < 2.2e-16 ***\nuniquecarrier_DH  2.23287189  0.11608798  19.2343 < 2.2e-16 ***\nuniquecarrier_DL -2.68848119  0.10621977 -25.3106 < 2.2e-16 ***\nuniquecarrier_EV  1.93484736  0.10724290  18.0417 < 2.2e-16 ***\nuniquecarrier_F9 -0.89788137  0.14422281  -6.2257 4.796e-10 ***\nuniquecarrier_FL -1.46706706  0.11085354 -13.2343 < 2.2e-16 ***\nuniquecarrier_HA -0.14506644  0.25031456  -0.5795    0.5622    \nuniquecarrier_HP  2.09354855  0.12337515  16.9690 < 2.2e-16 ***\nuniquecarrier_MQ -1.88297535  0.10550507 -17.8473 < 2.2e-16 ***\nuniquecarrier_NW -2.79538927  0.10752182 -25.9983 < 2.2e-16 ***\nuniquecarrier_OH  0.83520117  0.11032997   7.5700 3.730e-14 ***\nuniquecarrier_OO  0.61993842  0.10679884   5.8047 6.447e-09 ***\nuniquecarrier_TZ -4.99830389  0.15912629 -31.4109 < 2.2e-16 ***\nuniquecarrier_UA -0.68294396  0.10638099  -6.4198 1.365e-10 ***\nuniquecarrier_US -0.61589284  0.10669583  -5.7724 7.815e-09 ***\nuniquecarrier_WN  3.86386059  0.10362275  37.2878 < 2.2e-16 ***\nuniquecarrier_XE -2.59658123  0.10775736 -24.0966 < 2.2e-16 ***\nuniquecarrier_YV  3.11113140  0.11659679  26.6828 < 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nR-Squared: 0.02385\nRoot Mean Squared Error: 17.74"
  },
  {
    "objectID": "examples/yarn-cluster-emr.html#assess-model-performance",
    "href": "examples/yarn-cluster-emr.html#assess-model-performance",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Assess model performance",
    "text": "Assess model performance\nCompare the model performance using the validation data.\n\n# Calculate average gains by predicted decile\nmodel_deciles <- lapply(model_partition, function(x) {\n  sdf_predict(ml1, x) %>%\n    mutate(decile = ntile(desc(prediction), 10)) %>%\n    group_by(decile) %>%\n    summarize(gain = mean(gain)) %>%\n    select(decile, gain) %>%\n    collect()\n})\n\n# Create a summary dataset for plotting\ndeciles <- rbind(\n  data.frame(data = 'train', model_deciles$train),\n  data.frame(data = 'valid', model_deciles$valid),\n  make.row.names = FALSE\n)\n\n# Plot average gains by predicted decile\ndeciles %>%\n  ggplot(aes(factor(decile), gain, fill = data)) +\n  geom_bar(stat = 'identity', position = 'dodge') +\n  labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes')"
  },
  {
    "objectID": "examples/yarn-cluster-emr.html#visualize-predictions",
    "href": "examples/yarn-cluster-emr.html#visualize-predictions",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Visualize predictions",
    "text": "Visualize predictions\nCompare actual gains to predicted gains for an out of time sample.\n\n# Select data from an out of time sample\ndata_2008 <- flights_tbl %>%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>%\n  filter(depdelay > 15 & depdelay < 240) %>%\n  filter(arrdelay > -60 & arrdelay < 360) %>%\n  filter(year == 2008) %>%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>%\n  mutate(gain = depdelay - arrdelay) %>%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest)\n\n# Summarize data by carrier\ncarrier <- sdf_predict(ml1, data_2008) %>%\n  group_by(description) %>%\n  summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>%\n  filter(freq > 10000) %>%\n  collect\n\n# Plot actual gains and predicted gains by airline carrier\nggplot(carrier, aes(gain, prediction)) + \n  geom_point(alpha = 0.75, color = 'red', shape = 3) +\n  geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') +\n  geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) +\n  labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted')\n\n\n\n\nSome carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights."
  },
  {
    "objectID": "examples/yarn-cluster-emr.html#build-dashboard",
    "href": "examples/yarn-cluster-emr.html#build-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Build dashboard",
    "text": "Build dashboard\nAggregate the scored data by origin, destination, and airline. Save the aggregated data.\n\n# Summarize by origin, destination, and carrier\nsummary_2008 <- sdf_predict(ml1, data_2008) %>%\n  rename(carrier = uniquecarrier, airline = description) %>%\n  group_by(origin, dest, carrier, airline) %>%\n  summarize(\n    flights = n(),\n    distance = mean(distance),\n    avg_dep_delay = mean(depdelay),\n    avg_arr_delay = mean(arrdelay),\n    avg_gain = mean(gain),\n    pred_gain = mean(prediction)\n    )\n\n# Collect and save objects\npred_data <- collect(summary_2008)\nairports <- collect(select(airports_tbl, name, faa, lat, lon))\nml1_summary <- capture.output(summary(ml1))\nsave(pred_data, airports, ml1_summary, file = 'flights_pred_2008.RData')"
  },
  {
    "objectID": "examples/yarn-cluster-emr.html#publish-dashboard",
    "href": "examples/yarn-cluster-emr.html#publish-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Publish dashboard",
    "text": "Publish dashboard\nUse the saved data to build an R Markdown flexdashboard. Publish the flexdashboard to Shiny Server, Shinyapps.io or RStudio Connect."
  },
  {
    "objectID": "guides/distributed-r.html#overview",
    "href": "guides/distributed-r.html#overview",
    "title": "sparklyr",
    "section": "Overview",
    "text": "Overview\nsparklyr provides support to run arbitrary R code at scale within your Spark Cluster through spark_apply(). This is especially useful where there is a need to use functionality available only in R or R packages that is not available in Apache Spark nor Spark Packages.\nspark_apply() applies an R function to a Spark object (typically, a Spark DataFrame). Spark objects are partitioned so they can be distributed across a cluster. You can use spark_apply with the default partitions or you can define your own partitions with the group_by argument. Your R function must return another Spark DataFrame. spark_apply will run your R function on each partition and output a single Spark DataFrame.\n\nApply an R function to a Spark Object\nLets run a simple example. We will apply the identify function, I(), over a list of numbers we created with the sdf_len function.\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nsdf_len(sc, 5, repartition = 1) %>%\n  spark_apply(function(e) I(e))\n## # Source:   table<sparklyr_tmp_378c2e4fb50> [?? x 1]\n## # Database: spark_connection\n##      id\n##   <dbl>\n## 1     1\n## 2     2\n## 3     3\n## 4     4\n## 5     5\nYour R function should be designed to operate on an R data frame. The R function passed to spark_apply expects a DataFrame and will return an object that can be cast as a DataFrame. We can use the class function to verify the class of the data.\nsdf_len(sc, 10, repartition = 1) %>%\n  spark_apply(function(e) class(e))\n## # Source:   table<sparklyr_tmp_378c7ce7618d> [?? x 1]\n## # Database: spark_connection\n##           id\n##        <chr>\n## 1 data.frame\nSpark will partition your data by hash or range so it can be distributed across a cluster. In the following example we create two partitions and count the number of rows in each partition. Then we print the first record in each partition.\ntrees_tbl <- sdf_copy_to(sc, trees, repartition = 2)\n\ntrees_tbl %>%\n  spark_apply(function(e) nrow(e), names = \"n\")\n## # Source:   table<sparklyr_tmp_378c15c45eb1> [?? x 1]\n## # Database: spark_connection\n##       n\n##   <int>\n## 1    16\n## 2    15\ntrees_tbl %>%\n  spark_apply(function(e) head(e, 1))\n## # Source:   table<sparklyr_tmp_378c29215418> [?? x 3]\n## # Database: spark_connection\n##   Girth Height Volume\n##   <dbl>  <dbl>  <dbl>\n## 1   8.3     70   10.3\n## 2   8.6     65   10.3\nWe can apply any arbitrary function to the partitions in the Spark DataFrame. For instance, we can scale or jitter the columns. Notice that spark_apply applies the R function to all partitions and returns a single DataFrame.\ntrees_tbl %>%\n  spark_apply(function(e) scale(e))\n## # Source:   table<sparklyr_tmp_378c8922ba8> [?? x 3]\n## # Database: spark_connection\n##         Girth      Height     Volume\n##         <dbl>       <dbl>      <dbl>\n##  1 -1.4482330 -0.99510521 -1.1503645\n##  2 -1.3021313 -2.06675697 -1.1558670\n##  3 -0.7469449  0.68891899 -0.6826528\n##  4 -0.6592839 -1.60747764 -0.8587325\n##  5 -0.6300635  0.53582588 -0.4735581\n##  6 -0.5716229  0.38273277 -0.3855183\n##  7 -0.5424025 -0.07654655 -0.5395880\n##  8 -0.3670805 -0.22963966 -0.6661453\n##  9 -0.1040975  1.30129143  0.1427209\n## 10  0.1296653 -0.84201210 -0.3029809\n## # ... with more rows\ntrees_tbl %>%\n  spark_apply(function(e) lapply(e, jitter))\n## # Source:   table<sparklyr_tmp_378c43237574> [?? x 3]\n## # Database: spark_connection\n##        Girth   Height   Volume\n##        <dbl>    <dbl>    <dbl>\n##  1  8.319392 70.04321 10.30556\n##  2  8.801237 62.85795 10.21751\n##  3 10.719805 81.15618 18.78076\n##  4 11.009892 65.98926 15.58448\n##  5 11.089322 80.14661 22.58749\n##  6 11.309682 79.01360 24.18158\n##  7 11.418486 75.88748 21.38380\n##  8 11.982421 74.85612 19.09375\n##  9 12.907616 84.81742 33.80591\n## 10 13.691892 71.05309 25.70321\n## # ... with more rows\nBy default spark_apply() derives the column names from the input Spark data frame. Use the names argument to rename or add new columns.\ntrees_tbl %>%\n  spark_apply(\n    function(e) data.frame(2.54 * e$Girth, e),\n    names = c(\"Girth(cm)\", colnames(trees)))\n## # Source:   table<sparklyr_tmp_378c14e015b5> [?? x 4]\n## # Database: spark_connection\n##    `Girth(cm)` Girth Height Volume\n##          <dbl> <dbl>  <dbl>  <dbl>\n##  1      21.082   8.3     70   10.3\n##  2      22.352   8.8     63   10.2\n##  3      27.178  10.7     81   18.8\n##  4      27.940  11.0     66   15.6\n##  5      28.194  11.1     80   22.6\n##  6      28.702  11.3     79   24.2\n##  7      28.956  11.4     76   21.4\n##  8      30.480  12.0     75   19.1\n##  9      32.766  12.9     85   33.8\n## 10      34.798  13.7     71   25.7\n## # ... with more rows\n\n\nGroup By\nIn some cases you may want to apply your R function to specific groups in your data. For example, suppose you want to compute regression models against specific subgroups. To solve this, you can specify a group_by argument. This example counts the number of rows in iris by species and then fits a simple linear model for each species.\niris_tbl <- sdf_copy_to(sc, iris)\n\niris_tbl %>%\n  spark_apply(nrow, group_by = \"Species\")\n## # Source:   table<sparklyr_tmp_378c1b8155f3> [?? x 2]\n## # Database: spark_connection\n##      Species Sepal_Length\n##        <chr>        <int>\n## 1 versicolor           50\n## 2  virginica           50\n## 3     setosa           50\niris_tbl %>%\n  spark_apply(\n    function(e) summary(lm(Petal_Length ~ Petal_Width, e))$r.squared,\n    names = \"r.squared\",\n    group_by = \"Species\")\n## # Source:   table<sparklyr_tmp_378c30e6155> [?? x 2]\n## # Database: spark_connection\n##      Species r.squared\n##        <chr>     <dbl>\n## 1 versicolor 0.6188467\n## 2  virginica 0.1037537\n## 3     setosa 0.1099785"
  },
  {
    "objectID": "guides/distributed-r.html#distributing-packages",
    "href": "guides/distributed-r.html#distributing-packages",
    "title": "sparklyr",
    "section": "Distributing Packages",
    "text": "Distributing Packages\nWith spark_apply() you can use any R package inside Spark. For instance, you can use the broom package to create a tidy data frame from linear regression output.\nspark_apply(\n  iris_tbl,\n  function(e) broom::tidy(lm(Petal_Length ~ Petal_Width, e)),\n  names = c(\"term\", \"estimate\", \"std.error\", \"statistic\", \"p.value\"),\n  group_by = \"Species\")\n## # Source:   table<sparklyr_tmp_378c5502500b> [?? x 6]\n## # Database: spark_connection\n##      Species        term  estimate std.error statistic      p.value\n##        <chr>       <chr>     <dbl>     <dbl>     <dbl>        <dbl>\n## 1 versicolor (Intercept) 1.7812754 0.2838234  6.276000 9.484134e-08\n## 2 versicolor Petal_Width 1.8693247 0.2117495  8.827999 1.271916e-11\n## 3  virginica (Intercept) 4.2406526 0.5612870  7.555230 1.041600e-09\n## 4  virginica Petal_Width 0.6472593 0.2745804  2.357267 2.253577e-02\n## 5     setosa (Intercept) 1.3275634 0.0599594 22.141037 7.676120e-27\n## 6     setosa Petal_Width 0.5464903 0.2243924  2.435422 1.863892e-02\nTo use R packages inside Spark, your packages must be installed on the worker nodes. The first time you call spark_apply all of the contents in your local .libPaths() will be copied into each Spark worker node via the SparkConf.addFile() function. Packages will only be copied once and will persist as long as the connection remains open. It’s not uncommon for R libraries to be several gigabytes in size, so be prepared for a one-time tax while the R packages are copied over to your Spark cluster. You can disable package distribution by setting packages = FALSE. Note: packages are not copied in local mode (master=\"local\") because the packages already exist on the system."
  },
  {
    "objectID": "guides/distributed-r.html#handling-errors",
    "href": "guides/distributed-r.html#handling-errors",
    "title": "sparklyr",
    "section": "Handling Errors",
    "text": "Handling Errors\nIt can be more difficult to troubleshoot R issues in a cluster than in local mode. For instance, the following R code causes the distributed execution to fail and suggests you check the logs for details.\nspark_apply(iris_tbl, function(e) stop(\"Make this fail\"))\n Error in force(code) : \n  sparklyr worker rscript failure, check worker logs for details\nIn local mode, sparklyr will retrieve the logs for you. The logs point out the real failure as ERROR sparklyr: RScript (4190) Make this fail as you might expect.\n---- Output Log ----\n(17/07/27 21:24:18 ERROR sparklyr: Worker (2427) is shutting down with exception ,java.net.SocketException: Socket closed)\n17/07/27 21:24:18 WARN TaskSetManager: Lost task 0.0 in stage 389.0 (TID 429, localhost, executor driver): 17/07/27 21:27:21 INFO sparklyr: RScript (4190) retrieved 150 rows \n17/07/27 21:27:21 INFO sparklyr: RScript (4190) computing closure \n17/07/27 21:27:21 ERROR sparklyr: RScript (4190) Make this fail \nIt is worth mentioning that different cluster providers and platforms expose worker logs in different ways. Specific documentation for your environment will point out how to retrieve these logs."
  },
  {
    "objectID": "guides/distributed-r.html#requirements",
    "href": "guides/distributed-r.html#requirements",
    "title": "sparklyr",
    "section": "Requirements",
    "text": "Requirements\nThe R Runtime is expected to be pre-installed in the cluster for spark_apply to function. Failure to install the cluster will trigger a Cannot run program, no such file or directory error while attempting to use spark_apply(). Contact your cluster administrator to consider making the R runtime available throughout the entire cluster.\nA Homogeneous Cluster is required since the driver node distributes, and potentially compiles, packages to the workers. For instance, the driver and workers must have the same processor architecture, system libraries, etc."
  },
  {
    "objectID": "guides/distributed-r.html#configuration",
    "href": "guides/distributed-r.html#configuration",
    "title": "sparklyr",
    "section": "Configuration",
    "text": "Configuration\nThe following table describes relevant parameters while making use of spark_apply.\n\n\n\n\n\n\n\n\nValue\n\n\nDescription\n\n\n\n\n\n\nspark.r.command\n\n\nThe path to the R binary. Useful to select from multiple R versions.\n\n\n\n\nsparklyr.worker.gateway.address\n\n\nThe gateway address to use under each worker node. Defaults to sparklyr.gateway.address.\n\n\n\n\nsparklyr.worker.gateway.port\n\n\nThe gateway port to use under each worker node. Defaults to sparklyr.gateway.port.\n\n\n\n\nFor example, one could make use of an specific R version by running:\nconfig <- spark_config()\nconfig[[\"spark.r.command\"]] <- \"<path-to-r-version>\"\n\nsc <- spark_connect(master = \"local\", config = config)\nsdf_len(sc, 10) %>% spark_apply(function(e) e)"
  },
  {
    "objectID": "guides/distributed-r.html#limitations",
    "href": "guides/distributed-r.html#limitations",
    "title": "sparklyr",
    "section": "Limitations",
    "text": "Limitations\n\nClosures\nClosures are serialized using serialize, which is described as “A simple low-level interface for serializing to connections.”. One of the current limitations of serialize is that it wont serialize objects being referenced outside of it’s environment. For instance, the following function will error out since the closures references external_value:\nexternal_value <- 1\nspark_apply(iris_tbl, function(e) e + external_value)\n\n\nLivy\nCurrently, Livy connections do not support distributing packages since the client machine where the libraries are precompiled might not have the same processor architecture, not operating systems that the cluster machines.\n\n\nComputing over Groups\nWhile performing computations over groups, spark_apply() will provide partitions over the selected column; however, this implies that each partition can fit into a worker node, if this is not the case an exception will be thrown. To perform operations over groups that exceed the resources of a single node, one can consider partitioning to smaller units or use dplyr::do which is currently optimized for large partitions.\n\n\nPackage Installation\nSince packages are copied only once for the duration of the spark_connect() connection, installing additional packages is not supported while the connection is active. Therefore, if a new package needs to be installed, spark_disconnect() the connection, modify packages and reconnect."
  },
  {
    "objectID": "guides/h2o.html#overview",
    "href": "guides/h2o.html#overview",
    "title": "sparklyr",
    "section": "Overview",
    "text": "Overview\nThe rsparkling extension package provides bindings to H2O’s distributed machine learning algorithms via sparklyr. In particular, rsparkling allows you to access the machine learning routines provided by the Sparkling Water Spark package.\nTogether with sparklyr’s dplyr interface, you can easily create and tune H2O machine learning workflows on Spark, orchestrated entirely within R.\nrsparkling provides a few simple conversion functions that allow the user to transfer data between Spark DataFrames and H2O Frames. Once the Spark DataFrames are available as H2O Frames, the h2o R interface can be used to train H2O machine learning algorithms on the data.\nA typical machine learning pipeline with rsparkling might be composed of the following stages. To fit a model, you might need to:\n\nPerform SQL queries through the sparklyr dplyr interface,\nUse the sdf_* and ft_* family of functions to generate new columns, or partition your data set,\nConvert your training, validation and/or test data frames into H2O Frames using the as_h2o_frame function,\nChoose an appropriate H2O machine learning algorithm to model your data,\nInspect the quality of your model fit, and use it to make predictions with new data."
  },
  {
    "objectID": "guides/h2o.html#installation",
    "href": "guides/h2o.html#installation",
    "title": "sparklyr",
    "section": "Installation",
    "text": "Installation\nYou can install the rsparkling package from CRAN as follows:\ninstall.packages(\"rsparkling\")\nThen set the Sparkling Water version for rsparkling.:\noptions(rsparkling.sparklingwater.version = \"2.1.14\")\nFor Spark 2.0.x set rsparkling.sparklingwater.version to 2.0.3 instead, for Spark 1.6.2 use 1.6.8."
  },
  {
    "objectID": "guides/h2o.html#using-h2o",
    "href": "guides/h2o.html#using-h2o",
    "title": "sparklyr",
    "section": "Using H2O",
    "text": "Using H2O\nNow let’s walk through a simple example to demonstrate the use of H2O’s machine learning algorithms within R. We’ll use h2o.glm to fit a linear regression model. Using the built-in mtcars dataset, we’ll try to predict a car’s fuel consumption (mpg) based on its weight (wt), and the number of cylinders the engine contains (cyl).\nFirst, we will initialize a local Spark connection, and copy the mtcars dataset into Spark.\nlibrary(rsparkling)\nlibrary(sparklyr)\nlibrary(h2o)\nlibrary(dplyr)\n\nsc <- spark_connect(\"local\", version = \"2.1.0\")\n\nmtcars_tbl <- copy_to(sc, mtcars, \"mtcars\")\nNow, let’s perform some simple transformations – we’ll\n\nRemove all cars with horsepower less than 100,\nProduce a column encoding whether a car has 8 cylinders or not,\nPartition the data into separate training and test data sets,\nFit a model to our training data set,\nEvaluate our predictive performance on our test dataset.\n\n# transform our data set, and then partition into 'training', 'test'\npartitions <- mtcars_tbl %>%\n  filter(hp >= 100) %>%\n  mutate(cyl8 = cyl == 8) %>%\n  sdf_partition(training = 0.5, test = 0.5, seed = 1099)\nNow, we convert our training and test sets into H2O Frames using rsparkling conversion functions. We have already split the data into training and test frames using dplyr.\ntraining <- as_h2o_frame(sc, partitions$training, strict_version_check = FALSE)\ntest <- as_h2o_frame(sc, partitions$test, strict_version_check = FALSE)\nAlternatively, we can use the h2o.splitFrame() function instead of sdf_partition() to partition the data within H2O instead of Spark (e.g. partitions <- h2o.splitFrame(as_h2o_frame(mtcars_tbl), 0.5))\n# fit a linear model to the training dataset\nglm_model <- h2o.glm(x = c(\"wt\", \"cyl\"), \n                     y = \"mpg\", \n                     training_frame = training,\n                     lambda_search = TRUE)\nFor linear regression models produced by H2O, we can use either print() or summary() to learn a bit more about the quality of our fit. The summary() method returns some extra information about scoring history and variable importance.\nglm_model\n## Model Details:\n## ==============\n## \n## H2ORegressionModel: glm\n## Model ID:  GLM_model_R_1510348062048_1 \n## GLM Model: summary\n##     family     link                               regularization\n## 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.05468 )\n##                                                                 lambda_search\n## 1 nlambda = 100, lambda.max = 5.4682, lambda.min = 0.05468, lambda.1se = -1.0\n##   number_of_predictors_total number_of_active_predictors\n## 1                          2                           2\n##   number_of_iterations                                training_frame\n## 1                  100 frame_rdd_32_929e407384e0082416acd4c9897144a0\n## \n## Coefficients: glm coefficients\n##       names coefficients standardized_coefficients\n## 1 Intercept    32.997281                 16.625000\n## 2       cyl    -0.906688                 -1.349195\n## 3        wt    -2.712562                 -2.282649\n## \n## H2ORegressionMetrics: glm\n## ** Reported on training data. **\n## \n## MSE:  2.03293\n## RMSE:  1.425808\n## MAE:  1.306314\n## RMSLE:  0.08238032\n## Mean Residual Deviance :  2.03293\n## R^2 :  0.8265696\n## Null Deviance :93.775\n## Null D.o.F. :7\n## Residual Deviance :16.26344\n## Residual D.o.F. :5\n## AIC :36.37884\nThe output suggests that our model is a fairly good fit, and that both a cars weight, as well as the number of cylinders in its engine, will be powerful predictors of its average fuel consumption. (The model suggests that, on average, heavier cars consume more fuel.)\nLet’s use our H2O model fit to predict the average fuel consumption on our test data set, and compare the predicted response with the true measured fuel consumption. We’ll build a simple ggplot2 plot that will allow us to inspect the quality of our predictions.\nlibrary(ggplot2)\n\n# compute predicted values on our test dataset\npred <- h2o.predict(glm_model, newdata = test)\n# convert from H2O Frame to Spark DataFrame\npredicted <- as_spark_dataframe(sc, pred, strict_version_check = FALSE)\n\n# extract the true 'mpg' values from our test dataset\nactual <- partitions$test %>%\n  select(mpg) %>%\n  collect() %>%\n  `[[`(\"mpg\")\n\n# produce a data.frame housing our predicted + actual 'mpg' values\ndata <- data.frame(\n  predicted = predicted,\n  actual    = actual\n)\n# a bug in data.frame does not set colnames properly; reset here \nnames(data) <- c(\"predicted\", \"actual\")\n\n# plot predicted vs. actual values\nggplot(data, aes(x = actual, y = predicted)) +\n  geom_abline(lty = \"dashed\", col = \"red\") +\n  geom_point() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  coord_fixed(ratio = 1) +\n  labs(\n    x = \"Actual Fuel Consumption\",\n    y = \"Predicted Fuel Consumption\",\n    title = \"Predicted vs. Actual Fuel Consumption\"\n  )\n\nAlthough simple, our model appears to do a fairly good job of predicting a car’s average fuel consumption.\nAs you can see, we can easily and effectively combine dplyr data transformation pipelines with the machine learning algorithms provided by H2O’s Sparkling Water."
  },
  {
    "objectID": "guides/h2o.html#algorithms",
    "href": "guides/h2o.html#algorithms",
    "title": "sparklyr",
    "section": "Algorithms",
    "text": "Algorithms\nOnce the H2OContext is made available to Spark (as demonstrated below), all of the functions in the standard h2o R interface can be used with H2O Frames (converted from Spark DataFrames). Here is a table of the available algorithms:\n\n\n\nFunction\nDescription\n\n\n\n\nh2o.glm\nGeneralized Linear Model\n\n\nh2o.deeplearning\nMultilayer Perceptron\n\n\nh2o.randomForest\nRandom Forest\n\n\nh2o.gbm\nGradient Boosting Machine\n\n\nh2o.naiveBayes\nNaive-Bayes\n\n\nh2o.prcomp\nPrincipal Components Analysis\n\n\nh2o.svd\nSingular Value Decomposition\n\n\nh2o.glrm\nGeneralized Low Rank Model\n\n\nh2o.kmeans\nK-Means Clustering\n\n\nh2o.anomaly\nAnomaly Detection via Deep Learning Autoencoder\n\n\n\nAdditionally, the h2oEnsemble R package can be used to generate Super Learner ensembles of H2O algorithms:\n\n\n\nFunction\nDescription\n\n\n\n\nh2o.ensemble\nSuper Learner / Stacking\n\n\nh2o.stack\nSuper Learner / Stacking"
  },
  {
    "objectID": "guides/h2o.html#transformers",
    "href": "guides/h2o.html#transformers",
    "title": "sparklyr",
    "section": "Transformers",
    "text": "Transformers\nA model is often fit not on a dataset as-is, but instead on some transformation of that dataset. Spark provides feature transformers, facilitating many common transformations of data within a Spark DataFrame, and sparklyr exposes these within the ft_* family of functions. Transformers can be used on Spark DataFrames, and the final training set can be sent to the H2O cluster for machine learning.\n\n\n\n\n\n\n\n\nFunction\n\n\nDescription\n\n\n\n\n\n\nft_binarizer\n\n\nThreshold numerical features to binary (0/1) feature\n\n\n\n\nft_bucketizer\n\n\nBucketizer transforms a column of continuous features to a column of feature buckets\n\n\n\n\nft_discrete_cosine_transform\n\n\nTransforms a length NN real-valued sequence in the time domain into another length NN real-valued sequence in the frequency domain\n\n\n\n\nft_elementwise_product\n\n\nMultiplies each input vector by a provided weight vector, using element-wise multiplication.\n\n\n\n\nft_index_to_string\n\n\nMaps a column of label indices back to a column containing the original labels as strings\n\n\n\n\nft_quantile_discretizer\n\n\nTakes a column with continuous features and outputs a column with binned categorical features\n\n\n\n\nft_sql_transformer\n\n\nImplements the transformations which are defined by a SQL statement\n\n\n\n\nft_string_indexer\n\n\nEncodes a string column of labels to a column of label indices\n\n\n\n\nft_vector_assembler\n\n\nCombines a given list of columns into a single vector column"
  },
  {
    "objectID": "guides/h2o.html#examples",
    "href": "guides/h2o.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\nWe will use the iris data set to examine a handful of learning algorithms and transformers. The iris data set measures attributes for 150 flowers in 3 different species of iris.\niris_tbl <- copy_to(sc, iris, \"iris\", overwrite = TRUE)\niris_tbl\n## # Source:   table<iris> [?? x 5]\n## # Database: spark_connection\n##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl>   <chr>\n##  1          5.1         3.5          1.4         0.2  setosa\n##  2          4.9         3.0          1.4         0.2  setosa\n##  3          4.7         3.2          1.3         0.2  setosa\n##  4          4.6         3.1          1.5         0.2  setosa\n##  5          5.0         3.6          1.4         0.2  setosa\n##  6          5.4         3.9          1.7         0.4  setosa\n##  7          4.6         3.4          1.4         0.3  setosa\n##  8          5.0         3.4          1.5         0.2  setosa\n##  9          4.4         2.9          1.4         0.2  setosa\n## 10          4.9         3.1          1.5         0.1  setosa\n## # ... with more rows\nConvert to an H2O Frame:\niris_hf <- as_h2o_frame(sc, iris_tbl, strict_version_check = FALSE)\n\nK-Means Clustering\nUse H2O’s K-means clustering to partition a dataset into groups. K-means clustering partitions points into k groups, such that the sum of squares from points to the assigned cluster centers is minimized.\nkmeans_model <- h2o.kmeans(training_frame = iris_hf, \n                           x = 3:4,\n                           k = 3,\n                           seed = 1)\nTo look at particular metrics of the K-means model, we can use h2o.centroid_stats() and h2o.centers() or simply print out all the model metrics using print(kmeans_model).\n# print the cluster centers\nh2o.centers(kmeans_model)\n##   petal_length petal_width\n## 1     1.462000     0.24600\n## 2     5.566667     2.05625\n## 3     4.296154     1.32500\n# print the centroid statistics\nh2o.centroid_stats(kmeans_model)\n## Centroid Statistics: \n##   centroid     size within_cluster_sum_of_squares\n## 1        1 50.00000                       1.41087\n## 2        2 48.00000                       9.29317\n## 3        3 52.00000                       7.20274\n\n\nPCA\nUse H2O’s Principal Components Analysis (PCA) to perform dimensionality reduction. PCA is a statistical method to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate in turn has the largest variance possible.\npca_model <- h2o.prcomp(training_frame = iris_hf,\n                        x = 1:4,\n                        k = 4,\n                        seed = 1)\n## Warning in doTryCatch(return(expr), name, parentenv, handler): _train:\n## Dataset used may contain fewer number of rows due to removal of rows with\n## NA/missing values. If this is not desirable, set impute_missing argument in\n## pca call to TRUE/True/true/... depending on the client language.\npca_model\n## Model Details:\n## ==============\n## \n## H2ODimReductionModel: pca\n## Model ID:  PCA_model_R_1510348062048_3 \n## Importance of components: \n##                             pc1      pc2      pc3      pc4\n## Standard deviation     7.861342 1.455041 0.283531 0.154411\n## Proportion of Variance 0.965303 0.033069 0.001256 0.000372\n## Cumulative Proportion  0.965303 0.998372 0.999628 1.000000\n## \n## \n## H2ODimReductionMetrics: pca\n## \n## No model metrics available for PCA\n\n\nRandom Forest\nUse H2O’s Random Forest to perform regression or classification on a dataset. We will continue to use the iris dataset as an example for this problem.\nAs usual, we define the response and predictor variables using the x and y arguments. Since we’d like to do a classification, we need to ensure that the response column is encoded as a factor (enum) column.\ny <- \"Species\"\nx <- setdiff(names(iris_hf), y)\niris_hf[,y] <- as.factor(iris_hf[,y])\nWe can split the iris_hf H2O Frame into a train and test set (the split defaults to 75/25 train/test).\nsplits <- h2o.splitFrame(iris_hf, seed = 1)\nThen we can train a Random Forest model:\nrf_model <- h2o.randomForest(x = x, \n                             y = y,\n                             training_frame = splits[[1]],\n                             validation_frame = splits[[2]],\n                             nbins = 32,\n                             max_depth = 5,\n                             ntrees = 20,\n                             seed = 1)\nSince we passed a validation frame, the validation metrics will be calculated. We can retrieve individual metrics using functions such as h2o.mse(rf_model, valid = TRUE). The confusion matrix can be printed using the following:\nh2o.confusionMatrix(rf_model, valid = TRUE)\n## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n##            setosa versicolor virginica  Error     Rate\n## setosa          7          0         0 0.0000 =  0 / 7\n## versicolor      0         13         0 0.0000 = 0 / 13\n## virginica       0          1        10 0.0909 = 1 / 11\n## Totals          7         14        10 0.0323 = 1 / 31\nTo view the variable importance computed from an H2O model, you can use either the h2o.varimp() or h2o.varimp_plot() functions:\nh2o.varimp_plot(rf_model)\n\n\n\nGradient Boosting Machine\nThe Gradient Boosting Machine (GBM) is one of H2O’s most popular algorithms, as it works well on many types of data. We will continue to use the iris dataset as an example for this problem.\nUsing the same dataset and x and y from above, we can train a GBM:\ngbm_model <- h2o.gbm(x = x, \n                     y = y,\n                     training_frame = splits[[1]],\n                     validation_frame = splits[[2]],                     \n                     ntrees = 20,\n                     max_depth = 3,\n                     learn_rate = 0.01,\n                     col_sample_rate = 0.7,\n                     seed = 1)\nSince this is a multi-class problem, we may be interested in inspecting the confusion matrix on a hold-out set. Since we passed along a validatin_frame at train time, the validation metrics are already computed and we just need to retreive them from the model object.\nh2o.confusionMatrix(gbm_model, valid = TRUE)\n## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n##            setosa versicolor virginica  Error     Rate\n## setosa          7          0         0 0.0000 =  0 / 7\n## versicolor      0         13         0 0.0000 = 0 / 13\n## virginica       0          1        10 0.0909 = 1 / 11\n## Totals          7         14        10 0.0323 = 1 / 31\n\n\nDeep Learning\nUse H2O’s Deep Learning to perform regression or classification on a dataset, extact non-linear features generated by the deep neural network, and/or detect anomalies using a deep learning model with auto-encoding.\nIn this example, we will use the prostate dataset available within the h2o package:\npath <- system.file(\"extdata\", \"prostate.csv\", package = \"h2o\")\nprostate_df <- spark_read_csv(sc, \"prostate\", path)\nhead(prostate_df)\n## # Source:   lazy query [?? x 9]\n## # Database: spark_connection\n##      ID CAPSULE   AGE  RACE DPROS DCAPS   PSA   VOL GLEASON\n##   <int>   <int> <int> <int> <int> <int> <dbl> <dbl>   <int>\n## 1     1       0    65     1     2     1   1.4   0.0       6\n## 2     2       0    72     1     3     2   6.7   0.0       7\n## 3     3       0    70     1     1     2   4.9   0.0       6\n## 4     4       0    76     2     2     1  51.2  20.0       7\n## 5     5       0    69     1     1     1  12.3  55.9       6\n## 6     6       1    71     1     3     2   3.3   0.0       8\nOnce we’ve done whatever data manipulation is required to run our model we’ll get a reference to it as an h2o frame then split it into training and test sets using the h2o.splitFrame function:\nprostate_hf <- as_h2o_frame(sc, prostate_df, strict_version_check = FALSE)\nsplits <- h2o.splitFrame(prostate_hf, seed = 1)\nNext we define the response and predictor columns.\ny <- \"VOL\"\n#remove response and ID cols\nx <- setdiff(names(prostate_hf), c(\"ID\", y))\nNow we can train a deep neural net.\ndl_fit <- h2o.deeplearning(x = x, y = y,\n                           training_frame = splits[[1]],\n                           epochs = 15,\n                           activation = \"Rectifier\",\n                           hidden = c(10, 5, 10),\n                           input_dropout_ratio = 0.7)\nEvaluate performance on a test set:\nh2o.performance(dl_fit, newdata = splits[[2]])\n## H2ORegressionMetrics: deeplearning\n## \n## MSE:  253.7022\n## RMSE:  15.92803\n## MAE:  12.90077\n## RMSLE:  1.885052\n## Mean Residual Deviance :  253.7022\nNote that the above metrics are not reproducible when H2O’s Deep Learning is run on multiple cores, however, the metrics should be fairly stable across repeat runs.\n\n\nGrid Search\nH2O’s grid search capabilities currently supports traditional (Cartesian) grid search and random grid search. Grid search in R provides the following capabilities:\n\nH2OGrid class: Represents the results of the grid search\nh2o.getGrid(<grid_id>, sort_by, decreasing): Display the specified grid\nh2o.grid: Start a new grid search parameterized by\n\nmodel builder name (e.g., algorithm = \"gbm\")\nmodel parameters (e.g., ntrees = 100)\nhyper_parameters: attribute for passing a list of hyper parameters (e.g., list(ntrees=c(1,100), learn_rate=c(0.1,0.001)))\nsearch_criteria: optional attribute for specifying more a advanced search strategy\n\n\n\nCartesian Grid Search\nBy default, h2o.grid() will train a Cartesian grid search – meaning, all possible models in the specified grid. In this example, we will re-use the prostate data as an example dataset for a regression problem.\nsplits <- h2o.splitFrame(prostate_hf, seed = 1)\n\ny <- \"VOL\"\n#remove response and ID cols\nx <- setdiff(names(prostate_hf), c(\"ID\", y))\nAfter prepping the data, we define a grid and execute the grid search.\n# GBM hyperparamters\ngbm_params1 <- list(learn_rate = c(0.01, 0.1),\n                    max_depth = c(3, 5, 9),\n                    sample_rate = c(0.8, 1.0),\n                    col_sample_rate = c(0.2, 0.5, 1.0))\n\n# Train and validate a grid of GBMs\ngbm_grid1 <- h2o.grid(\"gbm\", x = x, y = y,\n                      grid_id = \"gbm_grid1\",\n                      training_frame = splits[[1]],\n                      validation_frame = splits[[1]],\n                      ntrees = 100,\n                      seed = 1,\n                      hyper_params = gbm_params1)\n\n# Get the grid results, sorted by validation MSE\ngbm_gridperf1 <- h2o.getGrid(grid_id = \"gbm_grid1\", \n                             sort_by = \"mse\", \n                             decreasing = FALSE)\ngbm_gridperf1\n## H2O Grid Details\n## ================\n## \n## Grid ID: gbm_grid1 \n## Used hyper parameters: \n##   -  col_sample_rate \n##   -  learn_rate \n##   -  max_depth \n##   -  sample_rate \n## Number of models: 36 \n## Number of failed models: 0 \n## \n## Hyper-Parameter Search Summary: ordered by increasing mse\n##   col_sample_rate learn_rate max_depth sample_rate          model_ids\n## 1             1.0        0.1         9         1.0 gbm_grid1_model_35\n## 2             0.5        0.1         9         1.0 gbm_grid1_model_34\n## 3             1.0        0.1         9         0.8 gbm_grid1_model_17\n## 4             0.5        0.1         9         0.8 gbm_grid1_model_16\n## 5             1.0        0.1         5         0.8 gbm_grid1_model_11\n##                  mse\n## 1  88.10947523138782\n## 2  102.3118989994892\n## 3 102.78632321923726\n## 4  126.4217260351778\n## 5  149.6066650109763\n## \n## ---\n##    col_sample_rate learn_rate max_depth sample_rate          model_ids\n## 31             0.5       0.01         3         0.8  gbm_grid1_model_1\n## 32             0.2       0.01         5         1.0 gbm_grid1_model_24\n## 33             0.5       0.01         3         1.0 gbm_grid1_model_19\n## 34             0.2       0.01         5         0.8  gbm_grid1_model_6\n## 35             0.2       0.01         3         1.0 gbm_grid1_model_18\n## 36             0.2       0.01         3         0.8  gbm_grid1_model_0\n##                   mse\n## 31  324.8117304723162\n## 32 325.10992525687294\n## 33 325.27898443785045\n## 34 329.36983845305735\n## 35 338.54411936919456\n## 36  339.7744828617712\n\n\nRandom Grid Search\nH2O’s Random Grid Search samples from the given parameter space until a set of constraints is met. The user can specify the total number of desired models using (e.g. max_models = 40), the amount of time (e.g. max_runtime_secs = 1000), or tell the grid to stop after performance stops improving by a specified amount. Random Grid Search is a practical way to arrive at a good model without too much effort.\nThe example below is set to run fairly quickly – increase max_runtime_secs or max_models to cover more of the hyperparameter space in your grid search. Also, you can expand the hyperparameter space of each of the algorithms by modifying the definition of hyper_param below.\n# GBM hyperparamters\ngbm_params2 <- list(learn_rate = seq(0.01, 0.1, 0.01),\n                    max_depth = seq(2, 10, 1),\n                    sample_rate = seq(0.5, 1.0, 0.1),\n                    col_sample_rate = seq(0.1, 1.0, 0.1))\nsearch_criteria2 <- list(strategy = \"RandomDiscrete\", \n                         max_models = 50)\n\n# Train and validate a grid of GBMs\ngbm_grid2 <- h2o.grid(\"gbm\", x = x, y = y,\n                      grid_id = \"gbm_grid2\",\n                      training_frame = splits[[1]],\n                      validation_frame = splits[[2]],\n                      ntrees = 100,\n                      seed = 1,\n                      hyper_params = gbm_params2,\n                      search_criteria = search_criteria2)\n\n# Get the grid results, sorted by validation MSE\ngbm_gridperf2 <- h2o.getGrid(grid_id = \"gbm_grid2\", \n                             sort_by = \"mse\", \n                             decreasing = FALSE)\nTo get the best model, as measured by validation MSE, we simply grab the first row of the gbm_gridperf2@summary_table object, since this table is already sorted such that the lowest MSE model is on top.\ngbm_gridperf2@summary_table[1,]\n## Hyper-Parameter Search Summary: ordered by increasing mse\n##   col_sample_rate learn_rate max_depth sample_rate          model_ids\n## 1             0.8       0.01         2         0.7 gbm_grid2_model_35\n##                  mse\n## 1 244.61196951586288\nIn the examples above, we generated two different grids, specified by grid_id. The first grid was called grid_id = \"gbm_grid1\" and the second was called grid_id = \"gbm_grid2\". However, if we are using the same dataset & algorithm in two grid searches, it probably makes more sense just to add the results of the second grid search to the first. If you want to add models to an existing grid, rather than create a new one, you simply re-use the same grid_id."
  },
  {
    "objectID": "guides/h2o.html#exporting-models",
    "href": "guides/h2o.html#exporting-models",
    "title": "sparklyr",
    "section": "Exporting Models",
    "text": "Exporting Models\nThere are two ways of exporting models from H2O – saving models as a binary file, or saving models as pure Java code.\n\nBinary Models\nThe more traditional method is to save a binary model file to disk using the h2o.saveModel() function. To load the models using h2o.loadModel(), the same version of H2O that generated the models is required. This method is commonly used when H2O is being used in a non-production setting.\nA binary model can be saved as follows:\nh2o.saveModel(my_model, path = \"/Users/me/h2omodels\")\n\n\nJava (POJO) Models\nOne of the most valuable features of H2O is it’s ability to export models as pure Java code, or rather, a “Plain Old Java Object” (POJO). You can learn more about H2O POJO models in this POJO quickstart guide. The POJO method is used most commonly when a model is deployed in a production setting. POJO models are ideal for when you need very fast prediction response times, and minimal requirements – the POJO is a standalone Java class with no dependencies on the full H2O stack.\nTo generate the POJO for your model, use the following command:\nh2o.download_pojo(my_model, path = \"/Users/me/h2omodels\")\nFinally, disconnect with:\nspark_disconnect_all()\n## [1] 1\nYou can learn more about how to take H2O models to production in the productionizing H2O models section of the H2O docs."
  },
  {
    "objectID": "guides/h2o.html#additional-resources",
    "href": "guides/h2o.html#additional-resources",
    "title": "sparklyr",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nMain documentation site for Sparkling Water (and all H2O software projects)\nH2O.ai website\n\nIf you are new to H2O for machine learning, we recommend you start with the Intro to H2O Tutorial, followed by the H2O Grid Search & Model Selection Tutorial. There are a number of other H2O R tutorials and demos available, as well as the H2O World 2015 Training Gitbook, and the Machine Learning with R and H2O Booklet (pdf)."
  },
  {
    "objectID": "guides/pipelines.html#introduction-to-ml-pipelines",
    "href": "guides/pipelines.html#introduction-to-ml-pipelines",
    "title": "Spark ML Pipelines",
    "section": "Introduction to ML Pipelines",
    "text": "Introduction to ML Pipelines\nThe official Apache Spark site contains a more complete overview of ML Pipelines. This article will focus in introducing the basic concepts and steps to work with ML Pipelines via sparklyr.\nThere are two important stages in building an ML Pipeline. The first one is creating a Pipeline. A good way to look at it, or call it, is as an “empty” pipeline. This step just builds the steps that the data will go through. This is the somewhat equivalent of doing this in R:\n\nlibrary(dplyr)\n\nr_pipeline <-  . %>% mutate(cyl = paste0(\"c\", cyl)) %>% lm(am ~ cyl + mpg, data = .)\nr_pipeline\n\nFunctional sequence with the following components:\n\n 1. mutate(., cyl = paste0(\"c\", cyl))\n 2. lm(am ~ cyl + mpg, data = .)\n\nUse 'functions' to extract the individual functions. \n\n\nThe r_pipeline object has all the steps needed to transform and fit the model, but it has not yet transformed any data. The second step, is to pass data through the pipeline, which in turn will output a fitted model. That is called a PipelineModel. The PipelineModel can then be used to produce predictions.\n\nr_model <- r_pipeline(mtcars)\nr_model\n\n\nCall:\nlm(formula = am ~ cyl + mpg, data = .)\n\nCoefficients:\n(Intercept)        cylc6        cylc8          mpg  \n   -0.54388      0.03124     -0.03313      0.04767  \n\n\n\nTaking advantage of Pipelines and PipelineModels\nThe two stage ML Pipeline approach produces two final data products:\n\nA PipelineModel that can be added to the daily Spark jobs which will produce new predictions for the incoming data, and again, with no R dependencies.\nA Pipeline that can be easily re-fitted on a regular interval, say every month. All that is needed is to pass a new sample to obtain the new coefficients."
  },
  {
    "objectID": "guides/pipelines.html#pipeline",
    "href": "guides/pipelines.html#pipeline",
    "title": "Spark ML Pipelines",
    "section": "Pipeline",
    "text": "Pipeline\nAn additional goal of this article is that the reader can follow along, so the data, transformations and Spark connection in this example will be kept as easy to reproduce as possible.\n\nlibrary(nycflights13)\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\")\n\nspark_flights <- copy_to(sc, flights)\n\n\nFeature Transformers\nPipelines make heavy use of Feature Transformers. If new to Spark, and sparklyr, it would be good to review what these transformers do. These functions use the Spark API directly to transform the data, and may be faster at making the data manipulations that a dplyr (SQL) transformation.\nIn sparklyr the ft functions are essentially are wrappers to original Spark feature transformer.\n\n\nft_dplyr_transformer\nThis example will start with dplyr transformations, which are ultimately SQL transformations, loaded into the df variable.\nIn sparklyr, there is one feature transformer that is not available in Spark, ft_dplyr_transformer(). The goal of this function is to convert the dplyr code to a SQL Feature Transformer that can then be used in a Pipeline.\n\ndf <- spark_flights %>%\n  filter(!is.na(dep_delay)) %>%\n  mutate(\n    month = paste0(\"m\", month),\n    day = paste0(\"d\", day)\n  ) %>%\n  select(dep_delay, sched_dep_time, month, day, distance) \n\nThis is the resulting pipeline stage produced from the dplyr code:\n\nft_dplyr_transformer(sc, df)\n\nSQLTransformer (Transformer)\n<dplyr_transformer__8ba1e82c_e9b0_4914_b571_2a71c8e1f4ea> \n (Parameters -- Column Names)\n\n\nUse the ml_param() function to extract the “statement” attribute. That attribute contains the finalized SQL statement. Notice that the flights table name has been replace with __THIS__. This allows the pipeline to accept different table names as its source, making the pipeline very modular.\n\nft_dplyr_transformer(sc, df) %>%\n  ml_param(\"statement\")\n\n[1] \"SELECT `dep_delay`, `sched_dep_time`, CONCAT(\\\"m\\\", `month`) AS `month`, CONCAT(\\\"d\\\", `day`) AS `day`, `distance`\\nFROM `__THIS__`\\nWHERE (NOT(((`dep_delay`) IS NULL)))\"\n\n\n\n\nCreating the Pipeline\nThe following step will create a 5 stage pipeline:\n\nSQL transformer - Resulting from the ft_dplyr_transformer() transformation\nBinarizer - To determine if the flight should be considered delay. The eventual outcome variable.\nBucketizer - To split the day into specific hour buckets\nR Formula - To define the model’s formula\nLogistic Model\n\n\nflights_pipeline <- ml_pipeline(sc) %>%\n  ft_dplyr_transformer(\n    tbl = df\n    ) %>%\n  ft_binarizer(\n    input_col = \"dep_delay\",\n    output_col = \"delayed\",\n    threshold = 15\n  ) %>%\n  ft_bucketizer(\n    input_col = \"sched_dep_time\",\n    output_col = \"hours\",\n    splits = c(400, 800, 1200, 1600, 2000, 2400)\n  )  %>%\n  ft_r_formula(delayed ~ month + day + hours + distance) %>% \n  ml_logistic_regression()\n\nAnother nice feature for ML Pipelines in sparklyr, is the print-out. It makes it really easy to how each stage is setup:\n\nflights_pipeline\n\nPipeline (Estimator) with 5 stages\n<pipeline__e4745ba2_5bf8_4555_88d9_4aa66aa7b6ba> \n  Stages \n  |--1 SQLTransformer (Transformer)\n  |    <dplyr_transformer__e0e59acc_d897_4a93_9760_474ebc786add> \n  |     (Parameters -- Column Names)\n  |--2 Binarizer (Transformer)\n  |    <binarizer__5b3946fe_1209_4d5d_805c_ee7c79bea1f2> \n  |     (Parameters -- Column Names)\n  |      input_col: dep_delay\n  |      output_col: delayed\n  |--3 Bucketizer (Transformer)\n  |    <bucketizer__84db4fe8_4994_4ad3_9e04_a6bbb70e5cec> \n  |     (Parameters -- Column Names)\n  |      input_col: sched_dep_time\n  |      output_col: hours\n  |--4 RFormula (Estimator)\n  |    <r_formula__b6e8b330_5824_4719_9417_d14bd05de34f> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |     (Parameters)\n  |      force_index_label: FALSE\n  |      formula: delayed ~ month + day + hours + distance\n  |      handle_invalid: error\n  |      stringIndexerOrderType: frequencyDesc\n  |--5 LogisticRegression (Estimator)\n  |    <logistic_regression__3991e32e_fa8c_46f8_b337_1ea749ffb337> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |      prediction_col: prediction\n  |      probability_col: probability\n  |      raw_prediction_col: rawPrediction\n  |     (Parameters)\n  |      aggregation_depth: 2\n  |      elastic_net_param: 0\n  |      family: auto\n  |      fit_intercept: TRUE\n  |      max_iter: 100\n  |      maxBlockSizeInMB: 0\n  |      reg_param: 0\n  |      standardization: TRUE\n  |      threshold: 0.5\n  |      tol: 1e-06\n\n\nNotice that there are no coefficients defined yet. That’s because no data has been actually processed. Even though df uses spark_flights(), recall that the final SQL transformer makes that name, so there’s no data to process yet."
  },
  {
    "objectID": "guides/pipelines.html#pipelinemodel",
    "href": "guides/pipelines.html#pipelinemodel",
    "title": "Spark ML Pipelines",
    "section": "PipelineModel",
    "text": "PipelineModel\nA quick partition of the data is created for this exercise.\n\npartitioned_flights <- sdf_random_split(\n  spark_flights,\n  training = 0.01,\n  testing = 0.01,\n  rest = 0.98\n)\n\nThe ml_fit() function produces the PipelineModel. The training partition of the partitioned_flights data is used to train the model:\n\nfitted_pipeline <- ml_fit(\n  flights_pipeline,\n  partitioned_flights$training\n)\nfitted_pipeline\n\nPipelineModel (Transformer) with 5 stages\n<pipeline__e4745ba2_5bf8_4555_88d9_4aa66aa7b6ba> \n  Stages \n  |--1 SQLTransformer (Transformer)\n  |    <dplyr_transformer__e0e59acc_d897_4a93_9760_474ebc786add> \n  |     (Parameters -- Column Names)\n  |--2 Binarizer (Transformer)\n  |    <binarizer__5b3946fe_1209_4d5d_805c_ee7c79bea1f2> \n  |     (Parameters -- Column Names)\n  |      input_col: dep_delay\n  |      output_col: delayed\n  |--3 Bucketizer (Transformer)\n  |    <bucketizer__84db4fe8_4994_4ad3_9e04_a6bbb70e5cec> \n  |     (Parameters -- Column Names)\n  |      input_col: sched_dep_time\n  |      output_col: hours\n  |--4 RFormulaModel (Transformer)\n  |    <r_formula__b6e8b330_5824_4719_9417_d14bd05de34f> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |     (Transformer Info)\n  |      formula:  chr \"delayed ~ month + day + hours + distance\" \n  |--5 LogisticRegressionModel (Transformer)\n  |    <logistic_regression__3991e32e_fa8c_46f8_b337_1ea749ffb337> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |      prediction_col: prediction\n  |      probability_col: probability\n  |      raw_prediction_col: rawPrediction\n  |     (Transformer Info)\n  |      coefficient_matrix:  num [1, 1:43] 0.47246 0.8963 0.23965 0.00185 -0.03954 ... \n  |      coefficients:  num [1:43] 0.47246 0.8963 0.23965 0.00185 -0.03954 ... \n  |      intercept:  num -2.8 \n  |      intercept_vector:  num -2.8 \n  |      num_classes:  int 2 \n  |      num_features:  int 43 \n  |      threshold:  num 0.5 \n  |      thresholds:  num [1:2] 0.5 0.5 \n\n\nNotice that the print-out for the fitted pipeline now displays the model’s coefficients.\nThe ml_transform() function can be used to run predictions, in other words it is used instead of predict() or sdf_predict().\n\npredictions <- ml_transform(\n  fitted_pipeline,\n  partitioned_flights$testing\n)\n\npredictions %>%\n  group_by(delayed, prediction) %>%\n  tally()\n\n# Source: spark<?> [?? x 3]\n# Groups: delayed\n  delayed prediction     n\n    <dbl>      <dbl> <dbl>\n1       0          1    35\n2       0          0  2569\n3       1          0   650\n4       1          1    56"
  },
  {
    "objectID": "guides/pipelines.html#save-the-pipelines-to-disk",
    "href": "guides/pipelines.html#save-the-pipelines-to-disk",
    "title": "Spark ML Pipelines",
    "section": "Save the pipelines to disk",
    "text": "Save the pipelines to disk\nThe ml_save() command can be used to save the Pipeline and PipelineModel to disk. The resulting output is a folder with the selected name, which contains all of the necessary Scala scripts:\n\nml_save(\n  flights_pipeline,\n  \"flights_pipeline\",\n  overwrite = TRUE\n)\n\nModel successfully saved.\n\nml_save(\n  fitted_pipeline,\n  \"flights_model\",\n  overwrite = TRUE\n)\n\nModel successfully saved."
  },
  {
    "objectID": "guides/pipelines.html#use-an-existing-pipelinemodel",
    "href": "guides/pipelines.html#use-an-existing-pipelinemodel",
    "title": "Spark ML Pipelines",
    "section": "Use an existing PipelineModel",
    "text": "Use an existing PipelineModel\nThe ml_load() command can be used to re-load Pipelines and PipelineModels. The saved ML Pipeline files can only be loaded into an open Spark session.\n\nreloaded_model <- ml_load(sc, \"flights_model\")\n\nA simple query can be used as the table that will be used to make the new predictions. This of course, does not have to done in R, at this time the “flights_model” can be loaded into an independent Spark session outside of R.\n\nnew_df <- spark_flights %>%\n  filter(\n    month == 7,\n    day == 5\n  )\n\nml_transform(reloaded_model, new_df)\n\n# Source: spark<?> [?? x 12]\n   dep_delay sched_dep_time month day   distance delayed hours features   label\n       <dbl>          <int> <chr> <chr>    <dbl>   <dbl> <dbl> <list>     <dbl>\n 1        39           2359 m7    d5        1617       1     4 <dbl [43]>     1\n 2       141           2245 m7    d5        2475       1     4 <dbl [43]>     1\n 3         0            500 m7    d5         529       0     0 <dbl [43]>     0\n 4        -5            536 m7    d5        1400       0     0 <dbl [43]>     0\n 5        -2            540 m7    d5        1089       0     0 <dbl [43]>     0\n 6        -7            545 m7    d5        1416       0     0 <dbl [43]>     0\n 7        -3            545 m7    d5        1576       0     0 <dbl [43]>     0\n 8        -7            600 m7    d5        1076       0     0 <dbl [43]>     0\n 9        -7            600 m7    d5          96       0     0 <dbl [43]>     0\n10        -6            600 m7    d5         937       0     0 <dbl [43]>     0\n# … with more rows, and 3 more variables: rawPrediction <list>,\n#   probability <list>, prediction <dbl>"
  },
  {
    "objectID": "guides/pipelines.html#re-fit-an-existing-pipeline",
    "href": "guides/pipelines.html#re-fit-an-existing-pipeline",
    "title": "Spark ML Pipelines",
    "section": "Re-fit an existing Pipeline",
    "text": "Re-fit an existing Pipeline\nFirst, reload the pipeline into an open Spark session:\n\nreloaded_pipeline <- ml_load(sc, \"flights_pipeline\")\n\nUse ml_fit() again to pass new data, in this case, sample_frac() is used instead of sdf_partition() to provide the new data. The idea being that the re-fitting would happen at a later date than when the model was initially fitted.\n\nnew_model <-  ml_fit(reloaded_pipeline, sample_frac(spark_flights, 0.01))\n\nnew_model\n\nPipelineModel (Transformer) with 5 stages\n<pipeline__e4745ba2_5bf8_4555_88d9_4aa66aa7b6ba> \n  Stages \n  |--1 SQLTransformer (Transformer)\n  |    <dplyr_transformer__e0e59acc_d897_4a93_9760_474ebc786add> \n  |     (Parameters -- Column Names)\n  |--2 Binarizer (Transformer)\n  |    <binarizer__5b3946fe_1209_4d5d_805c_ee7c79bea1f2> \n  |     (Parameters -- Column Names)\n  |      input_col: dep_delay\n  |      output_col: delayed\n  |--3 Bucketizer (Transformer)\n  |    <bucketizer__84db4fe8_4994_4ad3_9e04_a6bbb70e5cec> \n  |     (Parameters -- Column Names)\n  |      input_col: sched_dep_time\n  |      output_col: hours\n  |--4 RFormulaModel (Transformer)\n  |    <r_formula__b6e8b330_5824_4719_9417_d14bd05de34f> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |     (Transformer Info)\n  |      formula:  chr \"delayed ~ month + day + hours + distance\" \n  |--5 LogisticRegressionModel (Transformer)\n  |    <logistic_regression__3991e32e_fa8c_46f8_b337_1ea749ffb337> \n  |     (Parameters -- Column Names)\n  |      features_col: features\n  |      label_col: label\n  |      prediction_col: prediction\n  |      probability_col: probability\n  |      raw_prediction_col: rawPrediction\n  |     (Transformer Info)\n  |      coefficient_matrix:  num [1, 1:43] 0.111 -0.798 -0.626 -0.355 -0.19 ... \n  |      coefficients:  num [1:43] 0.111 -0.798 -0.626 -0.355 -0.19 ... \n  |      intercept:  num -2.08 \n  |      intercept_vector:  num -2.08 \n  |      num_classes:  int 2 \n  |      num_features:  int 43 \n  |      threshold:  num 0.5 \n  |      thresholds:  num [1:2] 0.5 0.5 \n\n\nThe new model can be saved using ml_save(). A new name is used in this case, but the same name as the existing PipelineModel to replace it.\n\nml_save(new_model, \"new_flights_model\", overwrite = TRUE)\n\nModel successfully saved.\n\n\nFinally, this example is complete by closing the Spark session.\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "guides/textmining.html#data-import",
    "href": "guides/textmining.html#data-import",
    "title": "Text mining with Spark & sparklyr",
    "section": "Data Import",
    "text": "Data Import\n\nConnect to Spark\nAn additional goal of this article is to encourage the reader to try it out, so a simple Spark local mode session is used.\n\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\")\n\n\n\nspark_read_text()\nThe spark_read_text() is a new function which works like readLines() but for sparklyr. It comes in handy when non-structured data, such as lines in a book, is what is available for analysis.\n\n# Imports Mark Twain's file\n\ntwain_path <- paste0(\"file:///\", here::here(), \"/mark_twain.txt\")\ntwain <-  spark_read_text(sc, \"twain\", twain_path)\n\n\n# Imports Sir Arthur Conan Doyle's file\ndoyle_path <- paste0(\"file:///\", here::here(), \"/arthur_doyle.txt\")\ndoyle <-  spark_read_text(sc, \"doyle\", doyle_path)"
  },
  {
    "objectID": "guides/textmining.html#data-transformation",
    "href": "guides/textmining.html#data-transformation",
    "title": "Text mining with Spark & sparklyr",
    "section": "Data transformation",
    "text": "Data transformation\nThe objective is to end up with a tidy table inside Spark with one row per word used. The steps will be:\n\nThe needed data transformations apply to the data from both authors. The data sets will be appended to one another\nPunctuation will be removed\nThe words inside each line will be separated, or tokenized\nFor a cleaner analysis, stop words will be removed\nTo tidy the data, each word in a line will become its own row\nThe results will be saved to Spark memory\n\n\nsdf_bind_rows()\n\nsdf_bind_rows() appends the doyle Spark Dataframe to the twain Spark Dataframe. This function can be used in lieu of a dplyr::bind_rows() wrapper function. For this exercise, the column author is added to differentiate between the two bodies of work.\n\n\nall_words <- doyle %>%\n  mutate(author = \"doyle\") %>%\n  sdf_bind_rows({\n    twain %>%\n      mutate(author = \"twain\")\n  }) %>%\n  filter(nchar(line) > 0)\n\n\n\nregexp_replace()\n\nThe Hive UDF, regexp_replace, is used as a sort of gsub() that works inside Spark. In this case it is used to remove punctuation. The usual [:punct:] regular expression did not work well during development, so a custom list is provided. For more information, see the Hive Functions section in the dplyr page.\n\n\nall_words <- all_words %>%\n  mutate(line = regexp_replace(line, \"[_\\\"\\'():;,.!?\\\\-]\", \" \"))\n\n\n\nft_tokenizer()\n\nft_tokenizer() uses the Spark API to separate each word. It creates a new list column with the results.\n\n\nall_words <- all_words %>%\n    ft_tokenizer(\n      input_col = \"line\",\n      output_col = \"word_list\"\n      )\n\nhead(all_words, 4)\n\n# Source: spark<?> [?? x 3]\n  line                          author word_list \n  <chr>                         <chr>  <list>    \n1 cover                         doyle  <list [1]>\n2 The Return of Sherlock Holmes doyle  <list [5]>\n3 by Sir Arthur Conan Doyle     doyle  <list [5]>\n4 Contents                      doyle  <list [1]>\n\n\n\n\nft_stop_words_remover()\n\nft_stop_words_remover() is a new function that, as its name suggests, takes care of removing stop words from the previous transformation. It expects a list column, so it is important to sequence it correctly after a ft_tokenizer() command. In the sample results, notice that the new wo_stop_words column contains less items than word_list.\n\n\nall_words <- all_words %>%\n  ft_stop_words_remover(\n    input_col = \"word_list\",\n    output_col = \"wo_stop_words\"\n    )\n\nhead(all_words, 4)\n\n# Source: spark<?> [?? x 4]\n  line                          author word_list  wo_stop_words\n  <chr>                         <chr>  <list>     <list>       \n1 cover                         doyle  <list [1]> <list [1]>   \n2 The Return of Sherlock Holmes doyle  <list [5]> <list [3]>   \n3 by Sir Arthur Conan Doyle     doyle  <list [5]> <list [4]>   \n4 Contents                      doyle  <list [1]> <list [1]>   \n\n\n\n\nexplode()\n\nThe Hive UDF explode performs the job of unnesting the tokens into their own row. Some further filtering and field selection is done to reduce the size of the dataset.\n\n\nall_words <- all_words %>%\n  mutate(word = explode(wo_stop_words)) %>%\n  select(word, author) %>%\n  filter(nchar(word) > 2)\n\nhead(all_words, 4)\n\n# Source: spark<?> [?? x 2]\n  word     author\n  <chr>    <chr> \n1 cover    doyle \n2 return   doyle \n3 sherlock doyle \n4 holmes   doyle \n\n\n\n\ncompute()\n\ncompute() will operate this transformation and cache the results in Spark memory. It is a good idea to pass a name to compute() to make it easier to identify it inside the Spark environment. In this case the name will be all_words\n\n\nall_words <- all_words %>%\n  compute(\"all_words\")\n\n\n\nFull code\nThis is what the code would look like on an actual analysis:\n\nall_words <- doyle %>%\n  mutate(author = \"doyle\") %>%\n  sdf_bind_rows({\n    twain %>%\n      mutate(author = \"twain\")\n  }) %>%\n  filter(nchar(line) > 0) %>%\n  mutate(line = regexp_replace(line, \"[_\\\"\\'():;,.!?\\\\-]\", \" \")) %>%\n  ft_tokenizer(\n    input_col = \"line\",\n    output_col = \"word_list\"\n  ) %>%\n  ft_stop_words_remover(\n    input_col = \"word_list\",\n    output_col = \"wo_stop_words\"\n  ) %>%\n  mutate(word = explode(wo_stop_words)) %>%\n  select(word, author) %>%\n  filter(nchar(word) > 2) %>%\n  compute(\"all_words\")"
  },
  {
    "objectID": "guides/textmining.html#data-analysis",
    "href": "guides/textmining.html#data-analysis",
    "title": "Text mining with Spark & sparklyr",
    "section": "Data Analysis",
    "text": "Data Analysis\n\nWords used the most\n\nword_count <- all_words %>%\n  count(author, word) %>% \n  ungroup()\n\nword_count\n\n# Source: spark<?> [?? x 3]\n   author word              n\n   <chr>  <chr>         <dbl>\n 1 doyle  empty           398\n 2 doyle  students        109\n 3 doyle  golden          303\n 4 doyle  abbey           164\n 5 doyle  grange           18\n 6 doyle  year            866\n 7 doyle  world          1520\n 8 doyle  circumstances   284\n 9 doyle  particulars      49\n10 doyle  crime           357\n# … with more rows\n\n\n\n\nWords used by Doyle and not Twain\n\ndoyle_unique <- filter(word_count, author == \"doyle\") %>%\n  anti_join(\n    filter(word_count, author == \"twain\"), \n    by = \"word\"\n    ) %>%\n  compute(\"doyle_unique\")\n\ndoyle_unique %>% \n  arrange(-n)\n\n# Source:     spark<?> [?? x 3]\n# Ordered by: -n\n   author word          n\n   <chr>  <chr>     <dbl>\n 1 doyle  nigel       972\n 2 doyle  alleyne     500\n 3 doyle  ezra        421\n 4 doyle  maude       337\n 5 doyle  aylward     336\n 6 doyle  lestrade    311\n 7 doyle  catinat     301\n 8 doyle  sharkey     281\n 9 doyle  summerlee   248\n10 doyle  congo       211\n# … with more rows\n\n\n\ndoyle_unique %>%\n  arrange(-n) %>%\n  head(100) %>%\n  collect() %>%\n  with(wordcloud::wordcloud(\n    word,\n    n,\n    colors = c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#56B4E9\")\n  ))\n\n\n\n\n\n\nTwain and Sherlock\nThe word cloud highlighted something interesting. The word “lestrade” is listed as one of the words used by Doyle but not Twain. Lestrade is the last name of a major character in the Sherlock Holmes books. It makes sense that the word “sherlock” appears considerably more times than “lestrade” in Doyle’s books, so why is Sherlock not in the word cloud? Did Mark Twain use the word “sherlock” in his writings?\n\nall_words %>%\n  filter(\n    author == \"twain\",\n    word == \"sherlock\"\n    ) %>%\n  count()\n\n# Source: spark<?> [?? x 1]\n      n\n  <dbl>\n1    16\n\n\nThe all_words table contains 16 instances of the word sherlock in the words used by Twain in his works. The instr Hive UDF is used to extract the lines that contain that word in the twain table. This Hive function works can be used instead of base::grep() or stringr::str_detect(). To account for any word capitalization, the lower command will be used in mutate() to make all words in the full text lower cap.\n\n\ninstr() & lower()\nMost of these lines are in a short story by Mark Twain called A Double Barrelled Detective Story. As per the Wikipedia page about this story, this is a satire by Twain on the mystery novel genre, published in 1902.\n\ntwain %>%\n  mutate(line = lower(line)) %>%\n  filter(instr(line, \"sherlock\") > 0) %>%\n  pull(line)\n\n [1] \"late sherlock holmes, and yet discernible by a member of a race charged\" \n [2] \"sherlock holmes.\"                                                        \n [3] \"“uncle sherlock! the mean luck of it!--that he should come just\"         \n [4] \"another trouble presented itself. “uncle sherlock 'll be wanting to talk\"\n [5] \"flint buckner's cabin in the frosty gloom. they were sherlock holmes and\"\n [6] \"“uncle sherlock's got some work to do, gentlemen, that 'll keep him till\"\n [7] \"“by george, he's just a duke, boys! three cheers for sherlock holmes,\"   \n [8] \"he brought sherlock holmes to the billiard-room, which was jammed with\"  \n [9] \"of interest was there--sherlock holmes. the miners stood silent and\"     \n[10] \"the room; the chair was on it; sherlock holmes, stately, imposing,\"      \n[11] \"“you have hunted me around the world, sherlock holmes, yet god is my\"    \n[12] \"“if it's only sherlock holmes that's troubling you, you needn't worry\"   \n[13] \"they sighed; then one said: “we must bring sherlock holmes. he can be\"   \n[14] \"i had small desire that sherlock holmes should hang for my deeds, as you\"\n[15] \"“my name is sherlock holmes, and i have not been doing anything.”\"       \n[16] \"late sherlock holmes, and yet discernible by a member of a race charged\" \n\n\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "guides/textmining.html#appendix",
    "href": "guides/textmining.html#appendix",
    "title": "Text mining with Spark & sparklyr",
    "section": "Appendix",
    "text": "Appendix\n\ngutenbergr package\nThis is an example of how the data for this article was pulled from the Gutenberg site:\nlibrary(gutenbergr)\n\ngutenberg_works()  %>%\n  filter(author == \"Twain, Mark\") %>%\n  pull(gutenberg_id) %>%\n  gutenberg_download(mirror = \"http://mirrors.xmission.com/gutenberg/\") %>%\n  pull(text) %>%\n  writeLines(\"mark_twain.txt\")"
  },
  {
    "objectID": "guides/arrow.html#introduction",
    "href": "guides/arrow.html#introduction",
    "title": "Using Apache Arrow",
    "section": "Introduction",
    "text": "Introduction\nApache Arrow is a cross-language development platform for in-memory data. Arrow is supported starting with sparklyr 1.0.0 to improve performance when transferring data between Spark and R. You can find some performance benchmarks under:\n\nsparklyr 1.0: Arrow, XGBoost, Broom and TFRecords.\nSpeeding up R and Apache Spark using Apache Arrow."
  },
  {
    "objectID": "guides/arrow.html#installation",
    "href": "guides/arrow.html#installation",
    "title": "Using Apache Arrow",
    "section": "Installation",
    "text": "Installation\nInstall the latest release of arrow from CRAN with\ninstall.packages(\"arrow\")\nPlease see https://arrow.apache.org/docs/r if you have any question about using arrow from R."
  },
  {
    "objectID": "guides/arrow.html#use-cases",
    "href": "guides/arrow.html#use-cases",
    "title": "Using Apache Arrow",
    "section": "Use Cases",
    "text": "Use Cases\nThere are three main use cases for arrow in sparklyr:\n\nData Copying: When copying data with copy_to(), Arrow will be used.\nData Collection: Also, when collecting either, implicitly by printing datasets or explicitly calling collect.\nR Transformations: When using spark_apply(), data will be transferred using Arrow when possible.\n\nTo use arrow in sparklyr one simply needs to import this library:\n\nlibrary(arrow)\n\nAttaching package: ‘arrow’\n\nThe following object is masked from ‘package:utils’:\n\n    timestamp\n\nThe following objects are masked from ‘package:base’:\n\n    array, table"
  },
  {
    "objectID": "guides/arrow.html#considerations",
    "href": "guides/arrow.html#considerations",
    "title": "Using Apache Arrow",
    "section": "Considerations",
    "text": "Considerations\n\nTypes\nSome data types are mapped to slightly different, one can argue more correct, types when using Arrow. For instance, consider collecting 64 bit integers in sparklyr:\n\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\ninteger64 <- sdf_len(sc, 2, type = \"integer64\")\ninteger64\n\n# Source: spark<?> [?? x 1]\n     id\n  <dbl>\n1     1\n2     2\nNotice that sparklyr collects 64 bit integers as double; however, using arrow:\n\nlibrary(arrow)\ninteger64\n\n# Source: spark<?> [?? x 1]\n  id             \n  <S3: integer64>\n1 1              \n2 2 \n64 bit integers are now being collected as proper 64 bit integer using the bit64 package.\n\n\nFallback\nThe Arrow R package supports many data types; however, in cases where a type is unsupported, sparklyr will fallback to not using arrow and print a warning.\n\nlibrary(sparklyr.nested)\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(arrow)\n\nsc <- spark_connect(master = \"local\")\ncars <- copy_to(sc, mtcars)\n\nsdf_nest(cars, hp) %>%\n  group_by(cyl) %>%\n  summarize(data = collect_list(data))\n\n# Source: spark<?> [?? x 2]\n    cyl data       \n  <dbl> <list>     \n1     6 <list [7]> \n2     4 <list [11]>\n3     8 <list [14]>\nWarning message:\nIn arrow_enabled_object.spark_jobj(sdf) :\n  Arrow disabled due to columns: data"
  },
  {
    "objectID": "guides/aws-s3.html#aws-access-keys",
    "href": "guides/aws-s3.html#aws-access-keys",
    "title": "Using Spark with AWS S3 buckets",
    "section": "AWS Access Keys",
    "text": "AWS Access Keys\nAWS Access Keys are needed to access S3 data. To learn how to setup a new keys, please review the AWS documentation: http://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html .We then pass the keys to R via Environment Variables:\nSys.setenv(AWS_ACCESS_KEY_ID=\"[Your access key]\")\nSys.setenv(AWS_SECRET_ACCESS_KEY=\"[Your secret access key]\")"
  },
  {
    "objectID": "guides/aws-s3.html#connecting-to-spark",
    "href": "guides/aws-s3.html#connecting-to-spark",
    "title": "Using Spark with AWS S3 buckets",
    "section": "Connecting to Spark",
    "text": "Connecting to Spark\nThere are four key settings needed to connect to Spark and use S3:\n\nA Hadoop-AWS package\nExecutor memory (key but not critical)\nThe master URL\nThe Spark Home\n\nTo connect to Spark, we first need to initialize a variable with the contents of sparklyr default config (spark_config) which we will then customize for our needs\nlibrary(sparklyr)\n\nconf <- spark_config()\n\nHadoop-AWS package:\nA Spark connection can be enhanced by using packages, please note that these are not R packages. For example, there are packages that tells Spark how to read CSV files, Hadoop or Hadoop in AWS.\nIn order to read S3 buckets, our Spark connection will need a package called hadoop-aws. If needed, multiple packages can be used. We experimented with many combinations of packages, and determined that for reading data in S3 we only need the one. The version we used, 2.7.3, refers to the latest Hadoop version, so as this article ages, please make sure to check this site to ensure that you are using the latest version: https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws\nconf$sparklyr.defaultPackages <- \"org.apache.hadoop:hadoop-aws:2.7.3\"\n\n\nExecutor Memory\nAs mentioned above this setting key but not critical. There are two points worth highlighting about it is:\n\nThe only performance related setting in a Spark Stand Alone cluster that can be tweaked, and in most cases because Spark defaults to a fraction of what is available, we then need to increase it by manually passing a value to that setting.\nIf more than the available RAM is requested, then Spark will set the Cores to 0, thus rendering the session unusable.\n\nconf$spark.executor.memory <- \"14g\"\n\n\nMaster URL and Spark home\nThere are three important points to mention when executing the spark_connect command:\n\nThe master will be the Spark Master’s URL. To find the URL, please see the Spark Cluster section.\nPoint the Spark Home to the location where Spark was installed in this node\nMake sure to the conf variable as the value for the config argument\n\nsc <- spark_connect(master = \"spark://ip-172-30-1-5.us-west-2.compute.internal:7077\", \n                    spark_home = \"/home/ubuntu/spark-2.1.0-bin-hadoop2.7/\",\n                    config =  conf)"
  },
  {
    "objectID": "guides/aws-s3.html#data-importwrangle-approach",
    "href": "guides/aws-s3.html#data-importwrangle-approach",
    "title": "Using Spark with AWS S3 buckets",
    "section": "Data Import/Wrangle approach",
    "text": "Data Import/Wrangle approach\nWe experimented with multiple approaches. Most of the factors for settling on a recommended approach were made based on the speed of each step. The premise is that we would rather wait longer during Data Import, if it meant that we can much faster Register and Cache our data subsets during Data Wrangling, especially since we would expect to end up with many subsets as we explore and model.The selected combination was the second slowest during the Import stage, but the fastest when caching a subset, by a lot.\nIn our tests, it took 72 seconds to read and cache the 29 columns of the 41 million rows of data, the slowest was 77 seconds. But when it comes to registering and caching a considerably sizable subset of 3 columns and almost all of the 41 million records, this approach was 17X faster than the second fastest approach. It took 1/3 of a second to register and cache the subset, the second fastest was 5 seconds.\nTo implement this approach, we need to set three arguments in the spark_csv_read() step:\n\nmemory\ninfer_schema\ncolumns\n\nAgain, this is a recommended approach. The columns argument is needed only if infer_schema is set to FALSE. When memory is set to TRUE it makes Spark load the entire dataset into memory, and setting infer_schema to FALSE prevents Spark from trying to figure out what the schema of the files are. By trying different combinations the memory and infer_schema arguments you may be able to find an approach that may better fits your needs.\n\nReading the schema\nSurprisingly, another critical detail that can easily be overlooked is choosing the right s3 URI scheme. There are two options: s3n and s3a. In most examples and tutorials I found, there was no reason give of why or when to use which one. The article the finally clarified it was this one: https://wiki.apache.org/hadoop/AmazonS3\nThe gist of it is that s3a is the recommended one going forward, especially for Hadoop versions 2.7 and above. This means that if we copy from older examples that used Hadoop 2.6 we would more likely also used s3n thus making data import much, much slower."
  },
  {
    "objectID": "guides/aws-s3.html#data-import",
    "href": "guides/aws-s3.html#data-import",
    "title": "Using Spark with AWS S3 buckets",
    "section": "Data Import",
    "text": "Data Import\nAfter the long introduction in the previous section, there is only one point to add about the following code chunk. If there are any NA values in numeric fields, then define the column as character and then convert it on later subsets using dplyr. The data import will fail if it finds any NA values on numeric fields. This is a small trade off in this approach because the next fastest one does not have this issue but is 17X slower at caching subsets.\nflights <- spark_read_csv(sc, \"flights_spark\", \n                          path =  \"s3a://flights-data/full\", \n                          memory = TRUE, \n                          columns = list(\n                            Year = \"character\",\n                            Month = \"character\",\n                            DayofMonth = \"character\",\n                            DayOfWeek = \"character\",\n                            DepTime = \"character\",\n                            CRSDepTime = \"character\",\n                            ArrTime = \"character\",\n                            CRSArrTime = \"character\",\n                            UniqueCarrier = \"character\",\n                            FlightNum = \"character\",\n                            TailNum = \"character\",\n                            ActualElapsedTime = \"character\",\n                            CRSElapsedTime = \"character\",\n                            AirTime = \"character\",\n                            ArrDelay = \"character\",\n                            DepDelay = \"character\",\n                            Origin = \"character\",\n                            Dest = \"character\",\n                            Distance = \"character\",\n                            TaxiIn = \"character\",\n                            TaxiOut = \"character\",\n                            Cancelled = \"character\",\n                            CancellationCode = \"character\",\n                            Diverted = \"character\",\n                            CarrierDelay = \"character\",\n                            WeatherDelay = \"character\",\n                            NASDelay = \"character\",\n                            SecurityDelay = \"character\",\n                            LateAircraftDelay = \"character\"), \n                         infer_schema = FALSE)"
  },
  {
    "objectID": "guides/aws-s3.html#data-wrangle",
    "href": "guides/aws-s3.html#data-wrangle",
    "title": "Using Spark with AWS S3 buckets",
    "section": "Data Wrangle",
    "text": "Data Wrangle\nThere are a few points we need to highlight about the following simple dyplr code:\nBecause there were NAs in the original fields, we have to mutate them to a number. Try coercing any variable as integer instead of numeric, this will save a lot of space when cached to Spark memory. The sdf_register command can be piped at the end of the code. After running the code, a new table will appear in the RStudio IDE’s Spark tab\ntidy_flights <- tbl(sc, \"flights_spark\") %>%\n  mutate(ArrDelay = as.integer(ArrDelay),\n         DepDelay = as.integer(DepDelay),\n         Distance = as.integer(Distance)) %>%\n  filter(!is.na(ArrDelay)) %>%\n  select(DepDelay, ArrDelay, Distance) %>%\n  sdf_register(\"tidy_spark\")\nAfter we use tbl_cache() to load the tidy_spark table into Spark memory. We can see the new table in the Storage page of our Spark session.\ntbl_cache(sc, \"tidy_spark\")"
  },
  {
    "objectID": "guides/caching.html#introduction",
    "href": "guides/caching.html#introduction",
    "title": "Understanding Spark Caching",
    "section": "Introduction",
    "text": "Introduction\nSpark also supports pulling data sets into a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small dataset or when running an iterative algorithm like random forests. Since operations in Spark are lazy, caching can help force computation. Sparklyr tools can be used to cache and uncache DataFrames. The Spark UI will tell you which DataFrames and what percentages are in memory.\nBy using a reproducible example, we will review some of the main configuration settings, commands and command arguments that can be used that can help you get the best out of Spark’s memory management options."
  },
  {
    "objectID": "guides/caching.html#preparation",
    "href": "guides/caching.html#preparation",
    "title": "Understanding Spark Caching",
    "section": "Preparation",
    "text": "Preparation\n\nDownload Test Data\nThe 2008 and 2007 Flights data from the Statistical Computing site will be used for this exercise. The spark_read_csv supports reading compressed CSV files in a bz2 format, so no additional file preparation is needed.\n\nif(!file.exists(\"2008.csv.bz2\"))\n  {download.file(\"http://stat-computing.org/dataexpo/2009/2008.csv.bz2\", \"2008.csv.bz2\")}\nif(!file.exists(\"2007.csv.bz2\"))\n  {download.file(\"http://stat-computing.org/dataexpo/2009/2007.csv.bz2\", \"2007.csv.bz2\")}\n\n\n\nStart a Spark session\nA local deployment will be used for this example.\n\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Install Spark version 2\nspark_install(version = \"2.0.0\")\n\n# Customize the connection configuration\nconf <- spark_config()\nconf$`sparklyr.shell.driver-memory` <- \"16G\"\n\n# Connect to Spark\nsc <- spark_connect(master = \"local\", config = conf, version = \"2.0.0\")"
  },
  {
    "objectID": "guides/caching.html#the-memory-argument",
    "href": "guides/caching.html#the-memory-argument",
    "title": "Understanding Spark Caching",
    "section": "The Memory Argument",
    "text": "The Memory Argument\nIn the spark_read_… functions, the memory argument controls if the data will be loaded into memory as an RDD. Setting it to FALSE means that Spark will essentially map the file, but not make a copy of it in memory. This makes the spark_read_csv command run faster, but the trade off is that any data transformation operations will take much longer.\n\nspark_read_csv(sc, \"flights_spark_2008\", \"2008.csv.bz2\", memory = FALSE)\n\nIn the RStudio IDE, the flights_spark_2008 table now shows up in the Spark tab.\n\n  \n\nTo access the Spark Web UI, click the SparkUI button in the RStudio Spark Tab. As expected, the Storage page shows no tables loaded into memory."
  },
  {
    "objectID": "guides/caching.html#loading-less-data-into-memory",
    "href": "guides/caching.html#loading-less-data-into-memory",
    "title": "Understanding Spark Caching",
    "section": "Loading Less Data into Memory",
    "text": "Loading Less Data into Memory\nUsing the pre-processing capabilities of Spark, the data will be transformed before being loaded into memory. In this section, we will continue to build on the example started in the Spark Read section\n\nLazy Transform\nThe following dplyr script will not be immediately run, so the code is processed quickly. There are some check-ups made, but for the most part it is building a Spark SQL statement in the background.\n\nflights_table <- tbl(sc,\"flights_spark_2008\") %>%\n  mutate(DepDelay = as.numeric(DepDelay),\n         ArrDelay = as.numeric(ArrDelay),\n         DepDelay > 15 , DepDelay < 240,\n         ArrDelay > -60 , ArrDelay < 360, \n         Gain = DepDelay - ArrDelay) %>%\n  filter(ArrDelay > 0) %>%\n  select(Origin, Dest, UniqueCarrier, Distance, DepDelay, ArrDelay, Gain)\n\n\n\nRegister in Spark\nsdf_register will register the resulting Spark SQL in Spark. The results will show up as a table called flights_spark. But a table of the same name is still not loaded into memory in Spark.\n\nsdf_register(flights_table, \"flights_spark\")\n\n\n  \n\n\n\nCache into Memory\nThe tbl_cache command loads the results into an Spark RDD in memory, so any analysis from there on will not need to re-read and re-transform the original file. The resulting Spark RDD is smaller than the original file because the transformations created a smaller data set than the original file.\n\ntbl_cache(sc, \"flights_spark\")\n\n\n  \n\n\n\nDriver Memory\nIn the Executors page of the Spark Web UI, we can see that the Storage Memory is at about half of the 16 gigabytes requested. This is mainly because of a Spark setting called spark.memory.fraction, which reserves by default 40% of the memory requested."
  },
  {
    "objectID": "guides/caching.html#process-on-the-fly",
    "href": "guides/caching.html#process-on-the-fly",
    "title": "Understanding Spark Caching",
    "section": "Process on the fly",
    "text": "Process on the fly\nThe plan is to read the Flights 2007 file, combine it with the 2008 file and summarize the data without bringing either file fully into memory.\n\nspark_read_csv(sc, \"flights_spark_2007\" , \"2007.csv.bz2\", memory = FALSE)\n\n\nUnion and Transform\nThe union command is akin to the bind_rows dyplyr command. It will allow us to append the 2007 file to the 2008 file, and as with the previous transform, this script will be evaluated lazily.\n\nall_flights <- tbl(sc, \"flights_spark_2008\") %>%\n  union(tbl(sc, \"flights_spark_2007\")) %>%\n  group_by(Year, Month) %>%\n  tally()\n\n\n\nCollect into R\nWhen receiving a collect command, Spark will execute the SQL statement and send the results back to R in a data frame. In this case, R only loads 24 observations into a data frame called all_flights.\n\nall_flights <- all_flights %>%\n  collect()\n\n\n  \n\n\n\nPlot in R\nNow the smaller data set can be plotted\n\nggplot(data = all_flights, aes(x = Month, y = n/1000, fill = factor(Year))) +\n  geom_area(position = \"dodge\", alpha = 0.5) +\n  geom_line(alpha = 0.4) +\n  scale_fill_brewer(palette = \"Dark2\", name = \"Year\") +\n  scale_x_continuous(breaks = 1:12, labels = c(\"J\",\"F\",\"M\",\"A\",\"M\",\"J\",\"J\",\"A\",\"S\",\"O\",\"N\",\"D\")) +\n  theme_light() +\n  labs(y=\"Number of Flights (Thousands)\", title = \"Number of Flights Year-Over-Year\")"
  },
  {
    "objectID": "guides/connections.html#local-mode",
    "href": "guides/connections.html#local-mode",
    "title": "Configuring Spark Connections",
    "section": "Local mode",
    "text": "Local mode\nLocal mode is an excellent way to learn and experiment with Spark. Local mode also provides a convenient development environment for analyses, reports, and applications that you plan to eventually deploy to a multi-node Spark cluster.\nTo work in local mode, you should first install a version of Spark for local use. You can do this using the spark_install() function, for example:\n\nRecommended properties\nThe following are the recommended Spark properties to set when connecting via R:\n\nsparklyr.cores.local - It defaults to using all of the available cores. Not a necessary property to set, unless there’s a reason to use less cores than available for a given Spark session.\nsparklyr.shell.driver-memory - The limit is the amount of RAM available in the computer minus what would be needed for OS operations.\nspark.memory.fraction - The default is set to 60% of the requested memory per executor. For more information, please see this Memory Management Overview page in the official Spark website.\n\n\n\nConnection example\nconf$`sparklyr.cores.local` <- 4\nconf$`sparklyr.shell.driver-memory` <- \"16G\"\nconf$spark.memory.fraction <- 0.9\n\nsc <- spark_connect(master = \"local\", \n                    version = \"2.1.0\",\n                    config = conf)\n\nExecutors page\nTo see how the requested configuration affected the Spark connection, go to the Executors page in the Spark Web UI available in http://localhost:4040/storage/"
  },
  {
    "objectID": "guides/connections.html#customizing-connections",
    "href": "guides/connections.html#customizing-connections",
    "title": "Configuring Spark Connections",
    "section": "Customizing connections",
    "text": "Customizing connections\nA connection to Spark can be customized by setting the values of certain Spark properties. In sparklyr, Spark properties can be set by using the config argument in the spark_connect() function.\nBy default, spark_connect() uses spark_config() as the default configuration. But that can be customized as shown in the example code below. Because of the unending number of possible combinations, spark_config() contains only a basic configuration, so it will be very likely that additional settings will be needed to properly connect to the cluster.\nconf <- spark_config()   # Load variable with spark_config()\n\nconf$spark.executor.memory <- \"16G\" # Use `$` to add or set values\n\nsc <- spark_connect(master = \"yarn-client\", \n                    config = conf)  # Pass the conf variable \n\nSpark definitions\nIt may be useful to provide some simple definitions for the Spark nomenclature:\n\nNode: A server\nWorker Node: A server that is part of the cluster and are available to run Spark jobs\nMaster Node: The server that coordinates the Worker nodes.\nExecutor: A sort of virtual machine inside a node. One Node can have multiple Executors.\nDriver Node: The Node that initiates the Spark session. Typically, this will be the server where sparklyr is located.\nDriver (Executor): The Driver Node will also show up in the Executor list.\n\n\n\nUseful concepts\n\nSpark configuration properties passed by R are just requests - In most cases, the cluster has the final say regarding the resources apportioned to a given Spark session.\nThe cluster overrides ‘silently’ - Many times, no errors are returned when more resources than allowed are requested, or if an attempt is made to change a setting fixed by the cluster."
  },
  {
    "objectID": "guides/connections.html#yarn",
    "href": "guides/connections.html#yarn",
    "title": "Configuring Spark Connections",
    "section": "YARN",
    "text": "YARN\n\nBackground\nUsing Spark and R inside a Hadoop based Data Lake is becoming a common practice at companies. Currently, there is no good way to manage user connections to the Spark service centrally. There are some caps and settings that can be applied, but in most cases there are configurations that the R user will need to customize.\nThe Running on YARN page in Spark’s official website is the best place to start for configuration settings reference, please bookmark it. Cluster administrators and users can benefit from this document. If Spark is new to the company, the YARN tunning article, courtesy of Cloudera, does a great job at explaining how the Spark/YARN architecture works.\n\n\nRecommended properties\nThe following are the recommended Spark properties to set when connecting via R:\n\nspark.executor.memory - The maximum possible is managed by the YARN cluster. See the Executor Memory Error\nspark.executor.cores - Number of cores assigned per Executor.\nspark.executor.instances - Number of executors to start. This property is acknowledged by the cluster if spark.dynamicAllocation.enabled is set to “false”.\nspark.dynamicAllocation.enabled - Overrides the mechanism that Spark provides to dynamically adjust resources. Disabling it provides more control over the number of the Executors that can be started, which in turn impact the amount of storage available for the session. For more information, please see the Dynamic Resource Allocation page in the official Spark website.\n\n\n\nClient mode\nUsing yarn-client as the value for the master argument in spark_connect() will make the server in which R is running to be the Spark’s session driver. Here is a sample connection:\nconf <- spark_config()\n\nconf$spark.executor.memory <- \"300M\"\nconf$spark.executor.cores <- 2\nconf$spark.executor.instances <- 3\nconf$spark.dynamicAllocation.enabled <- \"false\"\n\nsc <- spark_connect(master = \"yarn-client\", \n                    spark_home = \"/usr/lib/spark/\",\n                    version = \"1.6.0\",\n                    config = conf)\n\nExecutors page\nTo see how the requested configuration affected the Spark connection, go to the Executors page in the Spark Web UI. Typically, the Spark Web UI can be found using the exact same URL used for RStudio but on port 4040.\nNotice that 155.3MB per executor are assigned instead of the 300MB requested. This is because the spark.memory.fraction has been fixed by the cluster, plus, there is fixed amount of memory designated for overhead.\n\n\n\n\nCluster mode\nRunning in cluster mode means that YARN will choose where the driver of the Spark session will run. This means that the server where R is running may not necessarily be the driver for that session. Here is a good write-up explaining how running Spark applications work: Running Spark on YARN\nThe server will need to have copies of at least two files: yarn-site.xml and hive-site.xml. There may be other files needed based on your cluster’s individual setup.\nThis is an example of connecting to a Cloudera cluster:\nlibrary(sparklyr)\n\nSys.setenv(JAVA_HOME=\"/usr/lib/jvm/java-7-oracle-cloudera/\")\nSys.setenv(SPARK_HOME = '/opt/cloudera/parcels/CDH/lib/spark')\nSys.setenv(YARN_CONF_DIR = '/opt/cloudera/parcels/CDH/lib/spark/conf/yarn-conf')\n\nconf$spark.executor.memory <- \"300M\"\nconf$spark.executor.cores <- 2\nconf$spark.executor.instances <- 3\nconf$spark.dynamicAllocation.enabled <- \"false\"\nconf <- spark_config()\n\nsc <- spark_connect(master = \"yarn-cluster\", \n                    config = conf)\n\n\nExecutor memory error\nRequesting more memory or CPUs for Executors than allowed will return an error. This is one of the exceptions to the cluster’s ‘silent’ overrides. It will return a message similar to this:\n    Failed during initialize_connection: java.lang.IllegalArgumentException: Required executor memory (16384+1638 MB) is above the max threshold (8192 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'\nA cluster’s administrator is the only person who can make changes to the settings mentioned in the error. If the cluster is supported by a vendor, like Cloudera or Hortonworks, then the change can be made using the cluster’s web UI. Otherwise, changes to those settings are done directly in the yarn-default.xml file.\n\n\nKerberos\nThere are two options to access a “kerberized” data lake:\n\nUse kinit to get and cache the ticket. After kinit is installed and configured. After kinit is setup, it can used in R via a system() call prior to connecting to the cluster:\n\nsystem(\"echo '<password>' | kinit <username>\")\nFor more information visit this site: Apache - Authenticate with kinit\n\nA preferred option may be to use the out-of-the-box integration with Kerberos that the commercial version of RStudio Server offers."
  },
  {
    "objectID": "guides/connections.html#standalone-mode",
    "href": "guides/connections.html#standalone-mode",
    "title": "Configuring Spark Connections",
    "section": "Standalone mode",
    "text": "Standalone mode\n\nRecommended properties\nThe following are the recommended Spark properties to set when connecting via R:\nThe default behavior in Standalone mode is to create one executor per worker. So in a 3 worker node cluster, there will be 3 executors setup. The basic properties that can be set are:\n\nspark.executor.memory - The requested memory cannot exceed the actual RAM available.\nspark.memory.fraction - The default is set to 60% of the requested memory per executor. For more information, please see this Memory Management Overview page in the official Spark website.\nspark.executor.cores - The requested cores cannot be higher than the cores available in each worker.\n\n\nDynamic Allocation\nIf dynamic allocation is disabled, then Spark will attempt to assign all of the available cores evenly across the cluster. The property used is spark.dynamicAllocation.enabled.\nFor example, the Standalone cluster used for this article has 3 worker nodes. Each node has 14.7GB in RAM and 4 cores. This means that there are a total of 12 cores (3 workers with 4 cores) and 44.1GB in RAM (3 workers with 14.7GB in RAM each).\nIf the spark.executor.cores property is set to 2, and dynamic allocation is disabled, then Spark will spawn 6 executors. The spark.executor.memory property should be set to a level that when the value is multiplied by 6 (number of executors) it will not be over total available RAM. In this case, the value can be safely set to 7GB so that the total memory requested will be 42GB, which is under the available 44.1GB.\n\n\n\nConnection example\nconf <- spark_config()\nconf$spark.executor.memory <- \"7GB\"\nconf$spark.memory.fraction <- 0.9\nconf$spark.executor.cores <- 2\nconf$spark.dynamicAllocation.enabled <- \"false\"\n\nsc <- spark_connect(master=\"spark://master-url:7077\", \n              version = \"2.1.0\",\n              config = conf,\n              spark_home = \"/home/ubuntu/spark-2.1.0-bin-hadoop2.7/\")\n\nExecutors page\nTo see how the requested configuration affected the Spark connection, go to the Executors page in the Spark Web UI. Typically, the Spark Web UI can be found using the exact same URL used for RStudio but on port 4040:"
  },
  {
    "objectID": "guides/data-lakes.html#audience",
    "href": "guides/data-lakes.html#audience",
    "title": "Data Lakes",
    "section": "Audience",
    "text": "Audience\nThis article explains how to take advantage of Apache Spark at organizations that have a Hadoop based Big Data Lake."
  },
  {
    "objectID": "guides/data-lakes.html#introduction",
    "href": "guides/data-lakes.html#introduction",
    "title": "Data Lakes",
    "section": "Introduction",
    "text": "Introduction\nWe have noticed that the types of questions we field after a demo of sparklyr to our customers were more about high-level architecture than how the package works. To answer those questions, we put together a set of slides that illustrate and discuss important concepts, to help customers see where Spark, R, and sparklyr fit in a Big Data Platform implementation. In this article, we’ll review those slides and provide a narrative that will help you better envision how you can take advantage of our products."
  },
  {
    "objectID": "guides/data-lakes.html#r-for-data-science",
    "href": "guides/data-lakes.html#r-for-data-science",
    "title": "Data Lakes",
    "section": "R for Data Science",
    "text": "R for Data Science\nIt is very important to preface the Use Case review with some background information about where RStudio focuses its efforts when developing packages and products. Many vendors offer R integration, but in most cases, what this means is that they will add a model built in R to their pipeline or interface, and pass new inputs to that model to generate outputs that can be used in the next step in the pipeline, or in a calculation for the interface.\nIn contrast, our focus is on the process that happens before that: the discipline that produces the model, meaning Data Science.\n\nIn their R for Data Science book, Hadley Wickham and Garrett Grolemund provide a great diagram that nicely illustrates the Data Science process:\n\nWe import data into memory with R\nClean and tidy the data\nDive into a cyclical process called understand, which helps us to get to know our data, and hopefully find the answer to the question we started with. This cycle typically involves making transformations to our tidied data, using the transformed data to fit models, and visualizing results.\nOnce we find an answer to our question, we then communicate the results.\n\nData Scientists like using R because it allows them to complete a Data Science project from beginning to end inside the R environment, and in memory."
  },
  {
    "objectID": "guides/data-lakes.html#hadoop-as-a-data-source",
    "href": "guides/data-lakes.html#hadoop-as-a-data-source",
    "title": "Data Lakes",
    "section": "Hadoop as a Data Source",
    "text": "Hadoop as a Data Source\nWhat happens when the data that needs to be analyzed is very large, like the data sets found in a Hadoop cluster? It would be impossible to fit these in memory, so workarounds are normally used. Possible workarounds include using a comparatively minuscule data sample, or download as much data as possible. This becomes disruptive to Data Scientists because either the small sample may not be representative, or they have to wait a long time in every iteration of importing a lot of data, exploring a lot of data, and modeling a lot of data."
  },
  {
    "objectID": "guides/data-lakes.html#spark-as-an-analysis-engine",
    "href": "guides/data-lakes.html#spark-as-an-analysis-engine",
    "title": "Data Lakes",
    "section": "Spark as an Analysis Engine",
    "text": "Spark as an Analysis Engine\nWe noticed that a very important mental leap to make is to see Spark not just as a gateway to Hadoop (or worse, as an additional data source), but as a computing engine. As such, it is an excellent vehicle to scale our analytics. Spark has many capabilities that makes it ideal for Data Science in a data lake, such as close integration with Hadoop and Hive, the ability to cache data into memory across multiple nodes, data transformers, and its Machine Learning libraries.\nThe approach, then, is to push as much compute to the cluster as possible, using R primarily as an interface to Spark for the Data Scientist, which will then collect as few results as possible back into R memory, mostly to visualize and communicate. As shown in the slide, the more import, tidy, transform and modeling work we can push to Spark, the faster we can analyze very large data sets."
  },
  {
    "objectID": "guides/data-lakes.html#cluster-setup",
    "href": "guides/data-lakes.html#cluster-setup",
    "title": "Data Lakes",
    "section": "Cluster Setup",
    "text": "Cluster Setup\nHere is an illustration of how R, RStudio, and sparklyr can be added to the YARN managed cluster. The highlights are:\n\nR, RStudio, and sparklyr need to be installed on one node only, typically an edge node\nThe Data Scientist can access R, Spark, and the cluster via a web browser by navigating to the RStudio IDE inside the edge node"
  },
  {
    "objectID": "guides/data-lakes.html#considerations",
    "href": "guides/data-lakes.html#considerations",
    "title": "Data Lakes",
    "section": "Considerations",
    "text": "Considerations\nThere are some important considerations to keep in mind when combining your Data Lake and R for large scale analytics:\n\nSpark’s Machine Learning libraries may not contain specific models that a Data Scientist needs. For those cases, workarounds would include using a sparklyr extension such as H2O, or collecting a sample of the data into R memory for modeling.\nSpark does not have visualization functionality; currently, the best approach is to collect pre-calculated data into R for plotting. A good way to drastically reduce the number of rows being brought back into memory is to push as much computation as possible to Spark, and return just the results to be plotted. For example, the bins of a Histogram can be calculated in Spark, so that only the final bucket values would be returned to R for visualization.\nA particular use case may require a different way of scaling analytics. We have published an article that provides a very good overview of the options that are available: R for Enterprise: How to Scale Your Analytics Using R"
  },
  {
    "objectID": "guides/data-lakes.html#r-for-data-science-toolchain-with-spark",
    "href": "guides/data-lakes.html#r-for-data-science-toolchain-with-spark",
    "title": "Data Lakes",
    "section": "R for Data Science Toolchain with Spark",
    "text": "R for Data Science Toolchain with Spark\nWith sparklyr, the Data Scientist will be able to access the Data Lake’s data, and also gain an additional, very powerful understand layer via Spark. sparklyr, along with the RStudio IDE and the tidyverse packages, provides the Data Scientist with an excellent toolbox to analyze data, big and small."
  },
  {
    "objectID": "guides/streaming.html#the-sparklyr-interface",
    "href": "guides/streaming.html#the-sparklyr-interface",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "The sparklyr interface",
    "text": "The sparklyr interface\nAs stated in the Spark’s official site, Spark Streaming makes it easy to build scalable fault-tolerant streaming applications. Because is part of the Spark API, it is possible to re-use query code that queries the current state of the stream, as well as joining the streaming data with historical data. Please see Spark’s official documentation for a deeper look into Spark Streaming.\nThe sparklyr interface provides the following:\n\nAbility to run dplyr, SQL, spark_apply(), and PipelineModels against a stream\nRead in multiple formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc\nWrite stream results to Spark memory and the following file formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc\nAn out-of-the box graph visualization to monitor the stream\nA new reactiveSpark() function, that allows Shiny apps to poll the contents of the stream create Shiny apps that are able to read the contents of the stream"
  },
  {
    "objectID": "guides/streaming.html#interacting-with-a-stream",
    "href": "guides/streaming.html#interacting-with-a-stream",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Interacting with a stream",
    "text": "Interacting with a stream\nA good way of looking at the way how Spark streams update is as a three stage operation:\n\nInput - Spark reads the data inside a given folder. The folder is expected to contain multiple data files, with new files being created containing the most current stream data.\nProcessing - Spark applies the desired operations on top of the data. These operations could be data manipulations (dplyr, SQL), data transformations (sdf operations, PipelineModel predictions), or native R manipulations (spark_apply()).\nOutput - The results of processing the input files are saved in a different folder.\n\nIn the same way all of the read and write operations in sparklyr for Spark Standalone, or in sparklyr’s local mode, the input and output folders are actual OS file system folders. For Hadoop clusters, these will be folder locations inside the HDFS."
  },
  {
    "objectID": "guides/streaming.html#example-1---inputoutput",
    "href": "guides/streaming.html#example-1---inputoutput",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 1 - Input/Output",
    "text": "Example 1 - Input/Output\nThe first intro example is a small script that can be used with a local master. The result should be to see the stream_view() app showing live the number of records processed for each iteration of test data being sent to the stream.\n\nlibrary(future)\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\nstream_generate_test(iterations = 1)\nread_folder <- stream_read_csv(sc, \"source\") \nwrite_output <- stream_write_csv(read_folder, \"source-out\")\ninvisible(future(stream_generate_test(interval = 0.5)))\n\nstream_view(write_output)\n\n\n\n\nstream_stop(write_output)\nspark_disconnect(sc)\n\n\nCode breakdown\n\nOpen the Spark connection\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\n\nOptional step. This resets the input and output folders. It makes it easier to run the code multiple times in a clean manner.\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\nProduces a single test file inside the “source” folder. This allows the “read” function to infer CSV file definition.\n\nstream_generate_test(iterations = 1)\nlist.files(\"source\")\n\n[1] \"stream_1.csv\"\nPoints the stream reader to the folder where the streaming files will be placed. Since it is primed with a single CSV file, it will use as the expected layout of subsequent files. By default, stream_read_csv() creates a single integer variable data frame.\n\nread_folder <- stream_read_csv(sc, \"source\")\n\nThe output writer is what starts the streaming job. It will start monitoring the input folder, and then write the new results in the “source-out” folder. So as new records stream in, new files will be created in the “source-out” folder. Since there are no operations on the incoming data at this time, the output files will have the same exact raw data as the input files. The only difference is that the files and sub folders within “source-out” will be structured how Spark structures data folders.\n\nwrite_output <- stream_write_csv(read_folder, \"source-out\")\nlist.files(\"source-out\")\n\n[1] \"_spark_metadata\"                                     \"checkpoint\"\n[3] \"part-00000-1f29719a-2314-40e1-b93d-a647a3d57154-c000.csv\"\nThe test generation function will run 100 files every 0.2 seconds. To run the tests “out-of-sync” with the current R session, the future package is used.\n\nlibrary(future)\ninvisible(future(stream_generate_test(interval = 0.2, iterations = 100)))\n\nThe stream_view() function can be used before the 50 tests are complete because of the use of the future package. It will monitor the status of the job that write_output is pointing to and provide information on the amount of data coming into the “source” folder and going out into the “source-out” folder.\n\nstream_view(write_output)\n\nThe monitor will continue to run even after the tests are complete. To end the experiment, stop the Shiny app and then use the following to stop the stream and close the Spark session.\n\nstream_stop(write_output)\nspark_disconnect(sc)"
  },
  {
    "objectID": "guides/streaming.html#example-2---processing",
    "href": "guides/streaming.html#example-2---processing",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 2 - Processing",
    "text": "Example 2 - Processing\nThe second example builds on the first. It adds a processing step that manipulates the input data before saving it to the output folder. In this case, a new binary field is added indicating if the value from x is over 400 or not. This time, while run the second code chunk in this example a few times during the stream tests to see the aggregated values change.\n\nlibrary(future)\nlibrary(sparklyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nsc <- spark_connect(master = \"local\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\nstream_generate_test(iterations = 1)\nread_folder <- stream_read_csv(sc, \"source\") \n\nprocess_stream <- read_folder %>%\n  mutate(x = as.double(x)) %>%\n  ft_binarizer(\n    input_col = \"x\",\n    output_col = \"over\",\n    threshold = 400\n  )\n\nwrite_output <- stream_write_csv(process_stream, \"source-out\")\ninvisible(future(stream_generate_test(interval = 0.2, iterations = 100)))\n\nRun this code a few times during the experiment:\n\nspark_read_csv(sc, \"stream\", \"source-out\", memory = FALSE) %>%\n  group_by(over) %>%\n  tally()\n\nThe results would look similar to this. The n totals will increase as the experiment progresses.\n# Source:   lazy query [?? x 2]\n# Database: spark_connection\n   over     n\n  <dbl> <dbl>\n1     0 40215\n2     1 60006\nClean up after the experiment\n\nstream_stop(write_output)\nspark_disconnect(sc)\n\n\nCode breakdown\n\nThe processing starts with the read_folder variable that contains the input stream. It coerces the integer field x, into a type double. This is because the next function, ft_binarizer() does not accept integers. The binarizer determines if x is over 400 or not. This is a good illustration of how dplyr can help simplify the manipulation needed during the processing stage.\n\nprocess_stream <- read_folder %>%\n  mutate(x = as.double(x)) %>%\n  ft_binarizer(\n    input_col = \"x\",\n    output_col = \"over\",\n    threshold = 400\n  )\n\nThe output now needs to write-out the processed data instead of the raw input data. Swap read_folder with process_stream.\n\nwrite_output <- stream_write_csv(process_stream, \"source-out\")\n\nThe “source-out” folder can be treated as a if it was a single table within Spark. Using spark_read_csv(), the data can be mapped, but not brought into memory (memory = FALSE). This allows the current results to be further analyzed using regular dplyr commands.\n\nspark_read_csv(sc, \"stream\", \"source-out\", memory = FALSE) %>%\n  group_by(over) %>%\n  tally()"
  },
  {
    "objectID": "guides/streaming.html#example-3---aggregate-in-process-and-output-to-memory",
    "href": "guides/streaming.html#example-3---aggregate-in-process-and-output-to-memory",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 3 - Aggregate in process and output to memory",
    "text": "Example 3 - Aggregate in process and output to memory\nAnother option is to save the results of the processing into a in-memory Spark table. Unless intentionally saving it to disk, the table and its data will only exist while the Spark session is active.\nThe biggest advantage of using Spark memory as the target, is that it will allow for aggregation to happen during processing. This is an advantage because aggregation is not allowed for any file output, expect Kafka, on the input/process stage.\nUsing example 2 as the base, this example code will perform some aggregations to the current stream input and save only those summarized results into Spark memory:\n\nlibrary(future)\nlibrary(sparklyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nsc <- spark_connect(master = \"local\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\n\nstream_generate_test(iterations = 1)\nread_folder <- stream_read_csv(sc, \"source\") \n\nprocess_stream <- read_folder %>%\n  stream_watermark() %>%\n  group_by(timestamp) %>%\n  summarise(\n    max_x = max(x, na.rm = TRUE),\n    min_x = min(x, na.rm = TRUE),\n    count = n()\n  )\n\nwrite_output <- stream_write_memory(process_stream, name = \"stream\")\n\ninvisible(future(stream_generate_test()))\n\nRun this command a different times while the experiment is running:\n\ntbl(sc, \"stream\") \n\nClean up after the experiment\n\nstream_stop(write_output)\nspark_disconnect(sc)\n\n\nCode breakdown\n\nThe stream_watermark() functions add a new timestamp variable that is then used in the group_by() command. This is required by Spark Stream to accept summarized results as output of the stream. The second step is to simply decide what kinds of aggregations we need to perform. In this case, a simply max, min and count are performed.\n\nprocess_stream <- read_folder %>%\n  stream_watermark() %>%\n  group_by(timestamp) %>%\n  summarise(\n    max_x = max(x, na.rm = TRUE),\n    min_x = min(x, na.rm = TRUE),\n    count = n()\n  )\n\nThe spark_write_memory() function is used to write the output to Spark memory. The results will appear as a table of the Spark session with the name assigned in the name argument, in this case the name selected is: “stream”.\n\nwrite_output <- stream_write_memory(process_stream, name = \"stream\")\n\nTo query the current data in the “stream” table can be queried by using the dplyr tbl() command.\n\ntbl(sc, \"stream\")"
  },
  {
    "objectID": "guides/streaming.html#example-4---shiny-integration",
    "href": "guides/streaming.html#example-4---shiny-integration",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 4 - Shiny integration",
    "text": "Example 4 - Shiny integration\nsparklyr provides a new Shiny function called reactiveSpark(). It can take a Spark data frame, in this case the one created as a result of the stream processing, and then creates a Spark memory stream table, the same way a table is created in example 3.\n\nlibrary(future)\nlibrary(sparklyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(ggplot2)\n\nsc <- spark_connect(master = \"local\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\nstream_generate_test(iterations = 1)\nread_folder <- stream_read_csv(sc, \"source\") \n\nprocess_stream <- read_folder %>%\n  stream_watermark() %>%\n  group_by(timestamp) %>%\n  summarise(\n    max_x = max(x, na.rm = TRUE),\n    min_x = min(x, na.rm = TRUE),\n    count = n()\n  )\n\ninvisible(future(stream_generate_test(interval = 0.2, iterations = 100)))\n\nlibrary(shiny)\nui <- function(){\n  tableOutput(\"table\")\n}\nserver <- function(input, output, session){\n  \n  ps <- reactiveSpark(process_stream)\n  \n  output$table <- renderTable({\n    ps() %>%\n      mutate(timestamp = as.character(timestamp)) \n    })\n}\nrunGadget(ui, server)\n\n\n\nCode breakdown\n\nNotice that there is no stream_write_... command. The reason is that reactiveSpark() function contains the stream_write_memory() function.\nThis very basic Shiny app simply displays the output of a table in the ui section\n\nlibrary(shiny)\n\nui <- function(){\n  tableOutput(\"table\")\n}\n\nIn the server section, the reactiveSpark() function will update every time there’s a change to the stream and return a data frame. The results are saved to a variable called ps() in this script. Treat the ps() variable as a regular table that can be piped from, as shown in the example. In this case, the timestamp variable is converted to string for to make it easier to read.\n\nserver <- function(input, output, session){\n\n  ps <- reactiveSpark(process_stream)\n\n  output$table <- renderTable({\n    ps() %>%\n      mutate(timestamp = as.character(timestamp)) \n  })\n}\n\nUse runGadget() to display the Shiny app in the Viewer pane. This is optional, the app can be run using normal Shiny run functions.\n\nrunGadget(ui, server)"
  },
  {
    "objectID": "guides/streaming.html#example-5---ml-pipeline-model",
    "href": "guides/streaming.html#example-5---ml-pipeline-model",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 5 - ML Pipeline Model",
    "text": "Example 5 - ML Pipeline Model\nThis example uses a fitted Pipeline Model to process the input, and saves the predictions to the output. This approach would be used to apply Machine Learning on top of streaming data.\n\nlibrary(sparklyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nsc <- spark_connect(master = \"local\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\ndf <- data.frame(x = rep(1:1000), y = rep(2:1001))\n\nstream_generate_test(df = df, iteration = 1)\n\nmodel_sample <- spark_read_csv(sc, \"sample\", \"source\")\n\npipeline <- sc %>%\n  ml_pipeline() %>%\n  ft_r_formula(x ~ y) %>%\n  ml_linear_regression()\n\nfitted_pipeline <- ml_fit(pipeline, model_sample)\n\nml_stream <- stream_read_csv(\n    sc = sc, \n    path = \"source\", \n    columns = c(x = \"integer\", y = \"integer\")\n  )  %>%\n  ml_transform(fitted_pipeline, .)  %>%\n  select(- features) %>%\n  stream_write_csv(\"source-out\")\n\nstream_generate_test(df = df, interval = 0.5)\n\n\nspark_read_csv(sc, \"stream\", \"source-out\", memory = FALSE) \n\n### Source: spark<stream> [?? x 4]\n##       x     y label prediction\n## * <int> <int> <dbl>      <dbl>\n## 1   276   277   276       276.\n## 2   277   278   277       277.\n## 3   278   279   278       278.\n## 4   279   280   279       279.\n## 5   280   281   280       280.\n## 6   281   282   281       281.\n## 7   282   283   282       282.\n## 8   283   284   283       283.\n## 9   284   285   284       284.\n##10   285   286   285       285.\n### ... with more rows\n\nstream_stop(ml_stream)\nspark_disconnect(sc)\n\n\nCode Breakdown\n\nCreates and fits a pipeline\n\ndf <- data.frame(x = rep(1:1000), y = rep(2:1001))\nstream_generate_test(df = df, iteration = 1)\nmodel_sample <- spark_read_csv(sc, \"sample\", \"source\")\n\npipeline <- sc %>%\n  ml_pipeline() %>%\n  ft_r_formula(x ~ y) %>%\n  ml_linear_regression()\n\nfitted_pipeline <- ml_fit(pipeline, model_sample)\n\nThis example pipelines the input, process and output in a single code segment. The ml_transform() function is used to create the predictions. Because the CSV format does not support list type fields, the features column is removed before the results are sent to the output.\n\nml_stream <- stream_read_csv(\n    sc = sc, \n    path = \"source\", \n    columns = c(x = \"integer\", y = \"integer\")\n  )  %>%\n  ml_transform(fitted_pipeline, .)  %>%\n  select(- features) %>%\n  stream_write_csv(\"source-out\")\n\n\n\nstream_stop(write_output)\nspark_disconnect(sc)"
  },
  {
    "objectID": "packages/graphframes/1.0/index.html#installation",
    "href": "packages/graphframes/1.0/index.html#installation",
    "title": "sparklyr",
    "section": "Installation",
    "text": "Installation\nFor those already using sparklyr simply run:\ninstall.packages(\"graphframes\")\n# or, for the development version,\n# devtools::install_github(\"rstudio/graphframes\")\nOtherwise, install first sparklyr from CRAN using:\ninstall.packages(\"sparklyr\")\nThe examples make use of the highschool dataset from the ggplot package."
  },
  {
    "objectID": "packages/graphframes/1.0/index.html#getting-started",
    "href": "packages/graphframes/1.0/index.html#getting-started",
    "title": "sparklyr",
    "section": "Getting Started",
    "text": "Getting Started\nWe will calculate PageRank over the highschool dataset as follows:\nlibrary(graphframes)\nlibrary(sparklyr)\nlibrary(dplyr)\n\n# connect to spark using sparklyr\nsc <- spark_connect(master = \"local\", version = \"2.1.0\")\n\n# copy highschool dataset to spark\nhighschool_tbl <- copy_to(sc, ggraph::highschool, \"highschool\")\n\n# create a table with unique vertices using dplyr\nvertices_tbl <- sdf_bind_rows(\n  highschool_tbl %>% distinct(from) %>% transmute(id = from),\n  highschool_tbl %>% distinct(to) %>% transmute(id = to)\n)\n\n# create a table with <source, destination> edges\nedges_tbl <- highschool_tbl %>% transmute(src = from, dst = to)\n\ngf_graphframe(vertices_tbl, edges_tbl) %>%\n  gf_pagerank(reset_prob = 0.15, max_iter = 10L, source_id = \"1\")\n## GraphFrame\n## Vertices:\n##   $ id       <dbl> 12, 12, 59, 59, 1, 1, 20, 20, 45, 45, 8, 8, 9, 9, 26,...\n##   $ pagerank <dbl> 1.216914e-02, 1.216914e-02, 1.151867e-03, 1.151867e-0...\n## Edges:\n##   $ src    <dbl> 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,...\n##   $ dst    <dbl> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 22, 22,...\n##   $ weight <dbl> 0.02777778, 0.02777778, 0.02777778, 0.02777778, 0.02777..."
  },
  {
    "objectID": "packages/graphframes/1.0/index.html#further-reading",
    "href": "packages/graphframes/1.0/index.html#further-reading",
    "title": "sparklyr",
    "section": "Further Reading",
    "text": "Further Reading\nAppart from calculating PageRank using gf_pagerank, the following functions are available:\n\ngf_bfs: Breadth-first search (BFS).\ngf_connected_components: Connected components.\ngf_shortest_paths: Shortest paths algorithm.\ngf_scc: Strongly connected components.\ngf_triangle_count: Computes the number of triangles passing through each vertex and others.\n\nFor instance, one can calcualte the degrees of vertices using gf_degrees as follows:\ngf_graphframe(vertices_tbl, edges_tbl) %>% gf_degrees()\n## # Source:   table<sparklyr_tmp_e0776727325e> [?? x 2]\n## # Database: spark_connection\n##       id degree\n##    <dbl>  <int>\n##  1   55.     25\n##  2    6.     10\n##  3   13.     16\n##  4    7.      6\n##  5   12.     11\n##  6   63.     21\n##  7   58.      8\n##  8   41.     19\n##  9   48.     15\n## 10   59.     11\n## # ... with more rows\nIn order to visualize large graphframes, one can use sample_n and then use ggraph with igraph to visualize the graph as follows:\nlibrary(ggraph)\nlibrary(igraph)\n\ngraph <- highschool_tbl %>%\n  sample_n(20) %>%\n  collect() %>%\n  graph_from_data_frame()\n\nggraph(graph, layout = 'kk') + \n    geom_edge_link(aes(colour = factor(year))) + \n    geom_node_point() + \n    ggtitle('An example')\n\nFinally, we disconnect from Spark:\nspark_disconnect(sc)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_bfs.html#description",
    "href": "packages/graphframes/1.0/reference/gf_bfs.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nBreadth-first search (BFS)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_bfs.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_bfs.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_bfs(x, from_expr, to_expr, max_path_length = 10L, edge_filter = NULL,\n  ...)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_bfs.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_bfs.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). from_expr | Spark SQL expression specifying valid starting vertices for the BFS. to_expr | Spark SQL expression specifying valid target vertices for the BFS. max_path_length | Limit on the length of paths. edge_filter | Spark SQL expression specifying edges which may be used in the search. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_bfs.html#examples",
    "href": "packages/graphframes/1.0/reference/gf_bfs.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_bfs(g, from_expr = \"name = 'Esther'\", to_expr = \"age < 32\")"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_cache.html#description",
    "href": "packages/graphframes/1.0/reference/gf_cache.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCache the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_cache.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_cache.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_cache(x)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_cache.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_cache.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_chain.html#description",
    "href": "packages/graphframes/1.0/reference/gf_chain.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReturns a chain graph of the given size with Long ID type. The vertex IDs are 0, 1, …, n-1, and the edges are (0, 1), (1, 2), …., (n-2, n-1)."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_chain.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_chain.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_chain(sc, n)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_chain.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_chain.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSize of the graph to return."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_chain.html#examples",
    "href": "packages/graphframes/1.0/reference/gf_chain.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_chain(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_connected_components.html#description",
    "href": "packages/graphframes/1.0/reference/gf_connected_components.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nComputes the connected component membership of each vertex and returns a DataFrame of vertex information with each vertex assigned a component ID."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_connected_components.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_connected_components.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_connected_components(x, broadcast_threshold = 1000000L,\n  algorithm = c(\"graphframes\", \"graphx\"), checkpoint_interval = 2L, ...)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_connected_components.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_connected_components.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). broadcast_threshold | Broadcast threshold in propagating component assignments. algorithm | One of ‘graphframes’ or ‘graphx’. checkpoint_interval | Checkpoint interval in terms of number of iterations. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_connected_components.html#examples",
    "href": "packages/graphframes/1.0/reference/gf_connected_components.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n# checkpoint directory is required for gf_connected_components()\nspark_set_checkpoint_dir(sc, tempdir())\ng <- gf_friends(sc)\ngf_connected_components(g)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_degrees.html#description",
    "href": "packages/graphframes/1.0/reference/gf_degrees.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nDegrees of vertices"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_degrees.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_degrees.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_degrees.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_degrees.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_edge_columns.html#description",
    "href": "packages/graphframes/1.0/reference/gf_edge_columns.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nEdges column names"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_edge_columns.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_edge_columns.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_edge_columns(x)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_edge_columns.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_edge_columns.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_edges.html#description",
    "href": "packages/graphframes/1.0/reference/gf_edges.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nExtract edges DataFrame"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_edges.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_edges.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_edges(x)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_edges.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_edges.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_find.html#description",
    "href": "packages/graphframes/1.0/reference/gf_find.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nMotif finding uses a simple Domain-Specific Language (DSL) for expressing structural queries. For example, gf_find(g, “(a)-[e]->(b); (b)-[e2]->(a)”) will search for pairs of vertices a,b connected by edges in both directions. It will return a DataFrame of all such structures in the graph, with columns for each of the named elements (vertices or edges) in the motif. In this case, the returned columns will be in order of the pattern: “a, e, b, e2.”"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_find.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_find.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_find(x, pattern)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_find.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_find.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). pattern | pattern specifying a motif to search for"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_find.html#examples",
    "href": "packages/graphframes/1.0/reference/gf_find.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_friends(sc) %>%\n  gf_find(\"(a)-[e]->(b); (b)-[e2]->(a)\")"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_friends.html#description",
    "href": "packages/graphframes/1.0/reference/gf_friends.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGraph of friends in a social network."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_friends.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_friends.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_friends(sc)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_friends.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_friends.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_friends.html#examples",
    "href": "packages/graphframes/1.0/reference/gf_friends.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\ngf_friends(sc)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_graphframe.html#description",
    "href": "packages/graphframes/1.0/reference/gf_graphframe.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCreate a new GraphFrame"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_graphframe.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_graphframe.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_graphframe(vertices = NULL, edges)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_graphframe.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_graphframe.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nvertices\nA tbl_spark representing vertices.\n\n\nedges\nA tbl_psark representing edges."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_graphframe.html#examples",
    "href": "packages/graphframes/1.0/reference/gf_graphframe.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.1.0\")\nv_tbl <- sdf_copy_to(\n  sc, data.frame(id = 1:3, name = LETTERS[1:3])\n)\ne_tbl <- sdf_copy_to(\n  sc, data.frame(src = c(1, 2, 2), dst = c(2, 1, 3),\n                 action = c(\"love\", \"hate\", \"follow\"))\n)\ngf_graphframe(v_tbl, e_tbl)\ngf_graphframe(edges = e_tbl)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_grid_ising_model.html#description",
    "href": "packages/graphframes/1.0/reference/gf_grid_ising_model.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerate a grid Ising model with random parameters"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_grid_ising_model.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_grid_ising_model.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_grid_ising_model(sc, n, v_std = 1, e_std = 1)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_grid_ising_model.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_grid_ising_model.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nLength of one side of the grid. The grid will be of size n x n.\n\n\nv_std\nStandard deviation of normal distribution used to generate vertex factors “a”. Default of 1.0.\n\n\ne_std\nStandard deviation of normal distribution used to generate edge factors “b”. Default of 1.0."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_grid_ising_model.html#details",
    "href": "packages/graphframes/1.0/reference/gf_grid_ising_model.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nThis method generates a grid Ising model with random parameters. Ising models are probabilistic graphical models over binary variables xi. Each binary variable xi corresponds to one vertex, and it may take values -1 or +1. The probability distribution P(X) (over all xi) is parameterized by vertex factors ai and edge factors bij:\nP(X) = (1/Z) * exp[ i a_i x_i + {ij} b_{ij} x_i x_j ]"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_grid_ising_model.html#value",
    "href": "packages/graphframes/1.0/reference/gf_grid_ising_model.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nGraphFrame. Vertices have columns “id” and “a”. Edges have columns “src”, “dst”, and “b”. Edges are directed, but they should be treated as undirected in any algorithms run on this model. Vertex IDs are of the form “i,j”. E.g., vertex “1,3” is in the second row and fourth column of the grid."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_grid_ising_model.html#examples",
    "href": "packages/graphframes/1.0/reference/gf_grid_ising_model.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_grid_ising_model(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_in_degrees.html#description",
    "href": "packages/graphframes/1.0/reference/gf_in_degrees.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nIn-degrees of vertices"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_in_degrees.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_in_degrees.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_in_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_in_degrees.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_in_degrees.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_lpa.html#description",
    "href": "packages/graphframes/1.0/reference/gf_lpa.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRun static Label Propagation for detecting communities in networks. Each node in the network is initially assigned to its own community. At every iteration, nodes send their community affiliation to all neighbors and update their state to the mode community affiliation of incoming messages. LPA is a standard community detection algorithm for graphs. It is very inexpensive computationally, although (1) convergence is not guaranteed and (2) one can end up with trivial solutions (all nodes are identified into a single community)."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_lpa.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_lpa.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_lpa(x, max_iter, ...)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_lpa.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_lpa.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). max_iter | Maximum number of iterations. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_lpa.html#examples",
    "href": "packages/graphframes/1.0/reference/gf_lpa.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_lpa(g, max_iter = 5)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_out_degrees.html#description",
    "href": "packages/graphframes/1.0/reference/gf_out_degrees.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nOut-degrees of vertices"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_out_degrees.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_out_degrees.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_out_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_out_degrees.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_out_degrees.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_pagerank.html#description",
    "href": "packages/graphframes/1.0/reference/gf_pagerank.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPageRank"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_pagerank.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_pagerank.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_pagerank(x, tol = NULL, reset_probability = 0.15, max_iter = NULL,\n  source_id = NULL, ...)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_pagerank.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_pagerank.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). tol | Tolerance. reset_probability | Reset probability. max_iter | Maximum number of iterations. source_id | (Optional) Source vertex for a personalized pagerank. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_pagerank.html#examples",
    "href": "packages/graphframes/1.0/reference/gf_pagerank.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_pagerank(g, reset_probability = 0.15, tol = 0.01)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_persist.html#description",
    "href": "packages/graphframes/1.0/reference/gf_persist.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPersist the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_persist.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_persist.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_persist(x, storage_level = \"MEMORY_AND_DISK\")"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_persist.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_persist.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). storage_level | The storage level to be used. Please view the http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistenceSpark Documentation for information on what storage levels are accepted."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_register.html#description",
    "href": "packages/graphframes/1.0/reference/gf_register.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRegister a GraphFrame object"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_register.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_register.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_register(x)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_register.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_register.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_scc.html#description",
    "href": "packages/graphframes/1.0/reference/gf_scc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCompute the strongly connected component (SCC) of each vertex and return a DataFrame with each vertex assigned to the SCC containing that vertex."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_scc.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_scc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_scc(x, max_iter, ...)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_scc.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_scc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). max_iter | Maximum number of iterations. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_scc.html#examples",
    "href": "packages/graphframes/1.0/reference/gf_scc.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_scc(g, max_iter = 10)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_shortest_paths.html#description",
    "href": "packages/graphframes/1.0/reference/gf_shortest_paths.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nComputes shortest paths from every vertex to the given set of landmark vertices. Note that this takes edge direction into account."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_shortest_paths.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_shortest_paths.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_shortest_paths(x, landmarks, ...)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_shortest_paths.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_shortest_paths.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). landmarks | IDs of landmark vertices. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_shortest_paths.html#examples",
    "href": "packages/graphframes/1.0/reference/gf_shortest_paths.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_shortest_paths(g, landmarks = c(\"a\", \"d\"))"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_star.html#description",
    "href": "packages/graphframes/1.0/reference/gf_star.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReturns a star graph with Long ID type, consisting of a central element indexed 0 (the root) and the n other leaf vertices 1, 2, …, n."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_star.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_star.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_star(sc, n)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_star.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_star.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nThe number of leaves."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_star.html#examples",
    "href": "packages/graphframes/1.0/reference/gf_star.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_star(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_triangle_count.html#description",
    "href": "packages/graphframes/1.0/reference/gf_triangle_count.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis algorithm ignores edge direction; i.e., all edges are treated as undirected. In a multigraph, duplicate edges will be counted only once."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_triangle_count.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_triangle_count.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_triangle_count(x, ...)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_triangle_count.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_triangle_count.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_triangle_count.html#examples",
    "href": "packages/graphframes/1.0/reference/gf_triangle_count.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_triangle_count(g)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_triplets.html#description",
    "href": "packages/graphframes/1.0/reference/gf_triplets.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nTriplets of graph"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_triplets.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_triplets.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_triplets(x)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_triplets.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_triplets.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_two_blobs.html#description",
    "href": "packages/graphframes/1.0/reference/gf_two_blobs.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nTwo densely connected blobs (vertices 0->n-1 and n->2n-1) connected by a single edge (0->n)."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_two_blobs.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_two_blobs.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_two_blobs(sc, blob_size)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_two_blobs.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_two_blobs.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nblob_size\nThe size of each blob."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_two_blobs.html#examples",
    "href": "packages/graphframes/1.0/reference/gf_two_blobs.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_two_blobs(sc, 3)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_unpersist.html#description",
    "href": "packages/graphframes/1.0/reference/gf_unpersist.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nUnpersist the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_unpersist.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_unpersist.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_unpersist(x, blocking = FALSE)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_unpersist.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_unpersist.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). blocking | whether to block until all blocks are deleted"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_vertex_columns.html#description",
    "href": "packages/graphframes/1.0/reference/gf_vertex_columns.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nVertices column names"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_vertex_columns.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_vertex_columns.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_vertex_columns(x)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_vertex_columns.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_vertex_columns.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_vertices.html#description",
    "href": "packages/graphframes/1.0/reference/gf_vertices.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nExtract vertices DataFrame"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_vertices.html#usage",
    "href": "packages/graphframes/1.0/reference/gf_vertices.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_vertices(x)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/gf_vertices.html#arguments",
    "href": "packages/graphframes/1.0/reference/gf_vertices.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/1.0/reference/index.html",
    "href": "packages/graphframes/1.0/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ngf_bfs()\nBreadth-first search (BFS)\n\n\ngf_cache()\nCache the GraphFrame\n\n\ngf_chain()\nChain graph\n\n\ngf_connected_components()\nConnected components\n\n\ngf_degrees()\nDegrees of vertices\n\n\ngf_edge_columns()\nEdges column names\n\n\ngf_edges()\nExtract edges DataFrame\n\n\ngf_find()\nMotif finding: Searching the graph for structural patterns\n\n\ngf_friends()\nGraph of friends in a social network.\n\n\ngf_graphframe()\nCreate a new GraphFrame\n\n\ngf_grid_ising_model()\nGenerate a grid Ising model with random parameters\n\n\ngf_in_degrees()\nIn-degrees of vertices\n\n\ngf_lpa()\nLabel propagation algorithm (LPA)\n\n\ngf_out_degrees()\nOut-degrees of vertices\n\n\ngf_pagerank()\nPageRank\n\n\ngf_persist()\nPersist the GraphFrame\n\n\ngf_register()\nRegister a GraphFrame object\n\n\ngf_scc()\nStrongly connected components\n\n\ngf_shortest_paths()\nShortest paths\n\n\ngf_star()\nGenerate a star graph\n\n\ngf_triangle_count()\nComputes the number of triangles passing through each vertex.\n\n\ngf_triplets()\nTriplets of graph\n\n\ngf_two_blobs()\nGenerate two blobs\n\n\ngf_unpersist()\nUnpersist the GraphFrame\n\n\ngf_vertex_columns()\nVertices column names\n\n\ngf_vertices()\nExtract vertices DataFrame\n\n\nspark_graphframe() spark_graphframe()\nRetrieve a GraphFrame"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/spark_graphframe.html#description",
    "href": "packages/graphframes/1.0/reference/spark_graphframe.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieve a GraphFrame"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/spark_graphframe.html#usage",
    "href": "packages/graphframes/1.0/reference/spark_graphframe.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_graphframe(x, ...)\n\nspark_graphframe(x, ...)"
  },
  {
    "objectID": "packages/graphframes/1.0/reference/spark_graphframe.html#arguments",
    "href": "packages/graphframes/1.0/reference/spark_graphframe.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). … | additional arguments, not used"
  },
  {
    "objectID": "packages/graphframes/dev/index.html#installation",
    "href": "packages/graphframes/dev/index.html#installation",
    "title": "sparklyr",
    "section": "Installation",
    "text": "Installation\nFor those already using sparklyr simply run:\ninstall.packages(\"graphframes\")\n# or, for the development version,\n# devtools::install_github(\"rstudio/graphframes\")\nOtherwise, install first sparklyr from CRAN using:\ninstall.packages(\"sparklyr\")\nThe examples make use of the highschool dataset from the ggplot package."
  },
  {
    "objectID": "packages/graphframes/dev/index.html#getting-started",
    "href": "packages/graphframes/dev/index.html#getting-started",
    "title": "sparklyr",
    "section": "Getting Started",
    "text": "Getting Started\nWe will calculate PageRank over the built-in “friends” dataset as follows.\nlibrary(graphframes)\nlibrary(sparklyr)\nlibrary(dplyr)\n\n# connect to spark using sparklyr\nsc <- spark_connect(master = \"local\", version = \"2.3.0\")\n\n# obtain the example graph\ng <- gf_friends(sc)\n\n# compute PageRank\nresults <- gf_pagerank(g, tol = 0.01, reset_probability = 0.15)\nresults\n## GraphFrame\n## Vertices:\n##   $ id       <chr> \"f\", \"b\", \"g\", \"a\", \"d\", \"c\", \"e\"\n##   $ name     <chr> \"Fanny\", \"Bob\", \"Gabby\", \"Alice\", \"David\", \"Charlie\",...\n##   $ age      <int> 36, 36, 60, 34, 29, 30, 32\n##   $ pagerank <dbl> 0.3283607, 2.6555078, 0.1799821, 0.4491063, 0.3283607...\n## Edges:\n##   $ src          <chr> \"b\", \"c\", \"d\", \"e\", \"a\", \"a\", \"e\", \"f\"\n##   $ dst          <chr> \"c\", \"b\", \"a\", \"f\", \"e\", \"b\", \"d\", \"c\"\n##   $ relationship <chr> \"follow\", \"follow\", \"friend\", \"follow\", \"friend\",...\n##   $ weight       <dbl> 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0\nWe can then visualize the results by collecting the results to R:\nlibrary(tidygraph)\nlibrary(ggraph)\n\nvertices <- results %>%\n  gf_vertices() %>%\n  collect()\n\nedges <- results %>%\n  gf_edges() %>%\n  collect()\n\nedges %>%\n  as_tbl_graph() %>%\n  activate(nodes) %>%\n  left_join(vertices, by = c(name = \"id\")) %>%\n  ggraph(layout = \"nicely\") +\n  geom_node_label(aes(label = name.y, color = pagerank)) +\n  geom_edge_link(\n    aes(\n      alpha = weight,\n      start_cap = label_rect(node1.name.y),\n      end_cap = label_rect(node2.name.y)\n    ),\n    arrow = arrow(length = unit(4, \"mm\"))\n  ) +\n  theme_graph(fg_text_colour = 'white')"
  },
  {
    "objectID": "packages/graphframes/dev/index.html#further-reading",
    "href": "packages/graphframes/dev/index.html#further-reading",
    "title": "sparklyr",
    "section": "Further Reading",
    "text": "Further Reading\nAppart from calculating PageRank using gf_pagerank, many other functions are available, including:\n\ngf_bfs(): Breadth-first search (BFS).\ngf_connected_components(): Connected components.\ngf_shortest_paths(): Shortest paths algorithm.\ngf_scc(): Strongly connected components.\ngf_triangle_count(): Computes the number of triangles passing through each vertex and others.\ngf_degrees(): Degrees of vertices\n\nFor instance, one can calculate the degrees of vertices using gf_degrees as follows:\ngf_friends(sc) %>% gf_degrees()\n## # Source: spark<?> [?? x 2]\n##   id    degree\n## * <chr>  <int>\n## 1 f          2\n## 2 b          3\n## 3 a          3\n## 4 c          3\n## 5 e          3\n## 6 d          2\nFinally, we disconnect from Spark:\nspark_disconnect(sc)"
  },
  {
    "objectID": "packages/graphframes/dev/news.html",
    "href": "packages/graphframes/dev/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "graphframes 0.1.2\n\nUpdated dependency to graphframes 0.6.0, with support for Spark 2.3."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_bfs.html#description",
    "href": "packages/graphframes/dev/reference/gf_bfs.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nBreadth-first search (BFS)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_bfs.html#usage",
    "href": "packages/graphframes/dev/reference/gf_bfs.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_bfs(x, from_expr, to_expr, max_path_length = 10, edge_filter = NULL,\n  ...)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_bfs.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_bfs.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). from_expr | Spark SQL expression specifying valid starting vertices for the BFS. to_expr | Spark SQL expression specifying valid target vertices for the BFS. max_path_length | Limit on the length of paths. edge_filter | Spark SQL expression specifying edges which may be used in the search. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_bfs.html#examples",
    "href": "packages/graphframes/dev/reference/gf_bfs.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_bfs(g, from_expr = \"name = 'Esther'\", to_expr = \"age < 32\")"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_cache.html#description",
    "href": "packages/graphframes/dev/reference/gf_cache.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCache the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_cache.html#usage",
    "href": "packages/graphframes/dev/reference/gf_cache.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_cache(x)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_cache.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_cache.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_chain.html#description",
    "href": "packages/graphframes/dev/reference/gf_chain.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReturns a chain graph of the given size with Long ID type. The vertex IDs are 0, 1, …, n-1, and the edges are (0, 1), (1, 2), …., (n-2, n-1)."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_chain.html#usage",
    "href": "packages/graphframes/dev/reference/gf_chain.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_chain(sc, n)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_chain.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_chain.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSize of the graph to return."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_chain.html#examples",
    "href": "packages/graphframes/dev/reference/gf_chain.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_chain(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_connected_components.html#description",
    "href": "packages/graphframes/dev/reference/gf_connected_components.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nComputes the connected component membership of each vertex and returns a DataFrame of vertex information with each vertex assigned a component ID."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_connected_components.html#usage",
    "href": "packages/graphframes/dev/reference/gf_connected_components.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_connected_components(x, broadcast_threshold = 1000000L,\n  algorithm = c(\"graphframes\", \"graphx\"), checkpoint_interval = 2L,\n  ...)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_connected_components.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_connected_components.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). broadcast_threshold | Broadcast threshold in propagating component assignments. algorithm | One of ‘graphframes’ or ‘graphx’. checkpoint_interval | Checkpoint interval in terms of number of iterations. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_connected_components.html#examples",
    "href": "packages/graphframes/dev/reference/gf_connected_components.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n# checkpoint directory is required for gf_connected_components()\nspark_set_checkpoint_dir(sc, tempdir())\ng <- gf_friends(sc)\ngf_connected_components(g)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_degrees.html#description",
    "href": "packages/graphframes/dev/reference/gf_degrees.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nDegrees of vertices"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_degrees.html#usage",
    "href": "packages/graphframes/dev/reference/gf_degrees.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_degrees.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_degrees.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_edge_columns.html#description",
    "href": "packages/graphframes/dev/reference/gf_edge_columns.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nEdges column names"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_edge_columns.html#usage",
    "href": "packages/graphframes/dev/reference/gf_edge_columns.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_edge_columns(x)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_edge_columns.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_edge_columns.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_edges.html#description",
    "href": "packages/graphframes/dev/reference/gf_edges.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nExtract edges DataFrame"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_edges.html#usage",
    "href": "packages/graphframes/dev/reference/gf_edges.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_edges(x)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_edges.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_edges.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_find.html#description",
    "href": "packages/graphframes/dev/reference/gf_find.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nMotif finding uses a simple Domain-Specific Language (DSL) for expressing structural queries. For example, gf_find(g, “(a)-[e]->(b); (b)-[e2]->(a)”) will search for pairs of vertices a,b connected by edges in both directions. It will return a DataFrame of all such structures in the graph, with columns for each of the named elements (vertices or edges) in the motif. In this case, the returned columns will be in order of the pattern: “a, e, b, e2.”"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_find.html#usage",
    "href": "packages/graphframes/dev/reference/gf_find.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_find(x, pattern)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_find.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_find.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). pattern | pattern specifying a motif to search for"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_find.html#examples",
    "href": "packages/graphframes/dev/reference/gf_find.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_friends(sc) %>%\n  gf_find(\"(a)-[e]->(b); (b)-[e2]->(a)\")"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_friends.html#description",
    "href": "packages/graphframes/dev/reference/gf_friends.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGraph of friends in a social network."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_friends.html#usage",
    "href": "packages/graphframes/dev/reference/gf_friends.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_friends(sc)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_friends.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_friends.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_friends.html#examples",
    "href": "packages/graphframes/dev/reference/gf_friends.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\ngf_friends(sc)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_graphframe.html#description",
    "href": "packages/graphframes/dev/reference/gf_graphframe.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCreate a new GraphFrame"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_graphframe.html#usage",
    "href": "packages/graphframes/dev/reference/gf_graphframe.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_graphframe(vertices = NULL, edges)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_graphframe.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_graphframe.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nvertices\nA tbl_spark representing vertices.\n\n\nedges\nA tbl_psark representing edges."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_graphframe.html#examples",
    "href": "packages/graphframes/dev/reference/gf_graphframe.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.3.0\")\nv_tbl <- sdf_copy_to(\n  sc, data.frame(id = 1:3, name = LETTERS[1:3])\n)\ne_tbl <- sdf_copy_to(\n  sc, data.frame(src = c(1, 2, 2), dst = c(2, 1, 3),\n                 action = c(\"love\", \"hate\", \"follow\"))\n)\ngf_graphframe(v_tbl, e_tbl)\ngf_graphframe(edges = e_tbl)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_grid_ising_model.html#description",
    "href": "packages/graphframes/dev/reference/gf_grid_ising_model.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerate a grid Ising model with random parameters"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_grid_ising_model.html#usage",
    "href": "packages/graphframes/dev/reference/gf_grid_ising_model.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_grid_ising_model(sc, n, v_std = 1, e_std = 1)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_grid_ising_model.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_grid_ising_model.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nLength of one side of the grid. The grid will be of size n x n.\n\n\nv_std\nStandard deviation of normal distribution used to generate vertex factors “a”. Default of 1.0.\n\n\ne_std\nStandard deviation of normal distribution used to generate edge factors “b”. Default of 1.0."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_grid_ising_model.html#details",
    "href": "packages/graphframes/dev/reference/gf_grid_ising_model.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nThis method generates a grid Ising model with random parameters. Ising models are probabilistic graphical models over binary variables xi. Each binary variable xi corresponds to one vertex, and it may take values -1 or +1. The probability distribution P(X) (over all xi) is parameterized by vertex factors ai and edge factors bij:\nP(X) = (1/Z) * exp[ i a_i x_i + {ij} b_{ij} x_i x_j ]"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_grid_ising_model.html#value",
    "href": "packages/graphframes/dev/reference/gf_grid_ising_model.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nGraphFrame. Vertices have columns “id” and “a”. Edges have columns “src”, “dst”, and “b”. Edges are directed, but they should be treated as undirected in any algorithms run on this model. Vertex IDs are of the form “i,j”. E.g., vertex “1,3” is in the second row and fourth column of the grid."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_grid_ising_model.html#examples",
    "href": "packages/graphframes/dev/reference/gf_grid_ising_model.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_grid_ising_model(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_in_degrees.html#description",
    "href": "packages/graphframes/dev/reference/gf_in_degrees.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nIn-degrees of vertices"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_in_degrees.html#usage",
    "href": "packages/graphframes/dev/reference/gf_in_degrees.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_in_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_in_degrees.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_in_degrees.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_lpa.html#description",
    "href": "packages/graphframes/dev/reference/gf_lpa.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRun static Label Propagation for detecting communities in networks. Each node in the network is initially assigned to its own community. At every iteration, nodes send their community affiliation to all neighbors and update their state to the mode community affiliation of incoming messages. LPA is a standard community detection algorithm for graphs. It is very inexpensive computationally, although (1) convergence is not guaranteed and (2) one can end up with trivial solutions (all nodes are identified into a single community)."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_lpa.html#usage",
    "href": "packages/graphframes/dev/reference/gf_lpa.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_lpa(x, max_iter, ...)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_lpa.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_lpa.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). max_iter | Maximum number of iterations. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_lpa.html#examples",
    "href": "packages/graphframes/dev/reference/gf_lpa.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_lpa(g, max_iter = 5)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_out_degrees.html#description",
    "href": "packages/graphframes/dev/reference/gf_out_degrees.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nOut-degrees of vertices"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_out_degrees.html#usage",
    "href": "packages/graphframes/dev/reference/gf_out_degrees.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_out_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_out_degrees.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_out_degrees.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_pagerank.html#description",
    "href": "packages/graphframes/dev/reference/gf_pagerank.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPageRank"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_pagerank.html#usage",
    "href": "packages/graphframes/dev/reference/gf_pagerank.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_pagerank(x, tol = NULL, reset_probability = 0.15, max_iter = NULL,\n  source_id = NULL, ...)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_pagerank.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_pagerank.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). tol | Tolerance. reset_probability | Reset probability. max_iter | Maximum number of iterations. source_id | (Optional) Source vertex for a personalized pagerank. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_pagerank.html#examples",
    "href": "packages/graphframes/dev/reference/gf_pagerank.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_pagerank(g, reset_probability = 0.15, tol = 0.01)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_persist.html#description",
    "href": "packages/graphframes/dev/reference/gf_persist.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPersist the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_persist.html#usage",
    "href": "packages/graphframes/dev/reference/gf_persist.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_persist(x, storage_level = \"MEMORY_AND_DISK\")"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_persist.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_persist.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). storage_level | The storage level to be used. Please view the http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistenceSpark Documentation for information on what storage levels are accepted."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_register.html#description",
    "href": "packages/graphframes/dev/reference/gf_register.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRegister a GraphFrame object"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_register.html#usage",
    "href": "packages/graphframes/dev/reference/gf_register.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_register(x)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_register.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_register.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_scc.html#description",
    "href": "packages/graphframes/dev/reference/gf_scc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCompute the strongly connected component (SCC) of each vertex and return a DataFrame with each vertex assigned to the SCC containing that vertex."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_scc.html#usage",
    "href": "packages/graphframes/dev/reference/gf_scc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_scc(x, max_iter, ...)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_scc.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_scc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). max_iter | Maximum number of iterations. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_scc.html#examples",
    "href": "packages/graphframes/dev/reference/gf_scc.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_scc(g, max_iter = 10)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_shortest_paths.html#description",
    "href": "packages/graphframes/dev/reference/gf_shortest_paths.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nComputes shortest paths from every vertex to the given set of landmark vertices. Note that this takes edge direction into account."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_shortest_paths.html#usage",
    "href": "packages/graphframes/dev/reference/gf_shortest_paths.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_shortest_paths(x, landmarks, ...)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_shortest_paths.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_shortest_paths.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). landmarks | IDs of landmark vertices. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_shortest_paths.html#examples",
    "href": "packages/graphframes/dev/reference/gf_shortest_paths.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_shortest_paths(g, landmarks = c(\"a\", \"d\"))"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_star.html#description",
    "href": "packages/graphframes/dev/reference/gf_star.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReturns a star graph with Long ID type, consisting of a central element indexed 0 (the root) and the n other leaf vertices 1, 2, …, n."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_star.html#usage",
    "href": "packages/graphframes/dev/reference/gf_star.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_star(sc, n)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_star.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_star.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nThe number of leaves."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_star.html#examples",
    "href": "packages/graphframes/dev/reference/gf_star.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_star(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_triangle_count.html#description",
    "href": "packages/graphframes/dev/reference/gf_triangle_count.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis algorithm ignores edge direction; i.e., all edges are treated as undirected. In a multigraph, duplicate edges will be counted only once."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_triangle_count.html#usage",
    "href": "packages/graphframes/dev/reference/gf_triangle_count.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_triangle_count(x, ...)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_triangle_count.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_triangle_count.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_triangle_count.html#examples",
    "href": "packages/graphframes/dev/reference/gf_triangle_count.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_triangle_count(g)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_triplets.html#description",
    "href": "packages/graphframes/dev/reference/gf_triplets.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nTriplets of graph"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_triplets.html#usage",
    "href": "packages/graphframes/dev/reference/gf_triplets.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_triplets(x)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_triplets.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_triplets.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_two_blobs.html#description",
    "href": "packages/graphframes/dev/reference/gf_two_blobs.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nTwo densely connected blobs (vertices 0->n-1 and n->2n-1) connected by a single edge (0->n)."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_two_blobs.html#usage",
    "href": "packages/graphframes/dev/reference/gf_two_blobs.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_two_blobs(sc, blob_size)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_two_blobs.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_two_blobs.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nblob_size\nThe size of each blob."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_two_blobs.html#examples",
    "href": "packages/graphframes/dev/reference/gf_two_blobs.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_two_blobs(sc, 3)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_unpersist.html#description",
    "href": "packages/graphframes/dev/reference/gf_unpersist.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nUnpersist the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_unpersist.html#usage",
    "href": "packages/graphframes/dev/reference/gf_unpersist.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_unpersist(x, blocking = FALSE)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_unpersist.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_unpersist.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). blocking | whether to block until all blocks are deleted"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_vertex_columns.html#description",
    "href": "packages/graphframes/dev/reference/gf_vertex_columns.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nVertices column names"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_vertex_columns.html#usage",
    "href": "packages/graphframes/dev/reference/gf_vertex_columns.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_vertex_columns(x)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_vertex_columns.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_vertex_columns.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_vertices.html#description",
    "href": "packages/graphframes/dev/reference/gf_vertices.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nExtract vertices DataFrame"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_vertices.html#usage",
    "href": "packages/graphframes/dev/reference/gf_vertices.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_vertices(x)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/gf_vertices.html#arguments",
    "href": "packages/graphframes/dev/reference/gf_vertices.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/dev/reference/index.html",
    "href": "packages/graphframes/dev/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ngf_bfs()\nBreadth-first search (BFS)\n\n\ngf_cache()\nCache the GraphFrame\n\n\ngf_chain()\nChain graph\n\n\ngf_connected_components()\nConnected components\n\n\ngf_degrees()\nDegrees of vertices\n\n\ngf_edge_columns()\nEdges column names\n\n\ngf_edges()\nExtract edges DataFrame\n\n\ngf_find()\nMotif finding: Searching the graph for structural patterns\n\n\ngf_friends()\nGraph of friends in a social network.\n\n\ngf_graphframe()\nCreate a new GraphFrame\n\n\ngf_grid_ising_model()\nGenerate a grid Ising model with random parameters\n\n\ngf_in_degrees()\nIn-degrees of vertices\n\n\ngf_lpa()\nLabel propagation algorithm (LPA)\n\n\ngf_out_degrees()\nOut-degrees of vertices\n\n\ngf_pagerank()\nPageRank\n\n\ngf_persist()\nPersist the GraphFrame\n\n\ngf_register()\nRegister a GraphFrame object\n\n\ngf_scc()\nStrongly connected components\n\n\ngf_shortest_paths()\nShortest paths\n\n\ngf_star()\nGenerate a star graph\n\n\ngf_triangle_count()\nComputes the number of triangles passing through each vertex.\n\n\ngf_triplets()\nTriplets of graph\n\n\ngf_two_blobs()\nGenerate two blobs\n\n\ngf_unpersist()\nUnpersist the GraphFrame\n\n\ngf_vertex_columns()\nVertices column names\n\n\ngf_vertices()\nExtract vertices DataFrame\n\n\nspark_graphframe() spark_graphframe()\nRetrieve a GraphFrame"
  },
  {
    "objectID": "packages/graphframes/dev/reference/spark_graphframe.html#description",
    "href": "packages/graphframes/dev/reference/spark_graphframe.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieve a GraphFrame"
  },
  {
    "objectID": "packages/graphframes/dev/reference/spark_graphframe.html#usage",
    "href": "packages/graphframes/dev/reference/spark_graphframe.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_graphframe(x, ...)\n\nspark_graphframe(x, ...)"
  },
  {
    "objectID": "packages/graphframes/dev/reference/spark_graphframe.html#arguments",
    "href": "packages/graphframes/dev/reference/spark_graphframe.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). … | additional arguments, not used"
  },
  {
    "objectID": "packages/graphframes/latest/index.html#installation",
    "href": "packages/graphframes/latest/index.html#installation",
    "title": "sparklyr",
    "section": "Installation",
    "text": "Installation\nFor those already using sparklyr simply run:\ninstall.packages(\"graphframes\")\n# or, for the development version,\n# devtools::install_github(\"rstudio/graphframes\")\nOtherwise, install first sparklyr from CRAN using:\ninstall.packages(\"sparklyr\")\nThe examples make use of the highschool dataset from the ggplot package."
  },
  {
    "objectID": "packages/graphframes/latest/index.html#getting-started",
    "href": "packages/graphframes/latest/index.html#getting-started",
    "title": "sparklyr",
    "section": "Getting Started",
    "text": "Getting Started\nWe will calculate PageRank over the built-in “friends” dataset as follows.\nlibrary(graphframes)\nlibrary(sparklyr)\nlibrary(dplyr)\n\n# connect to spark using sparklyr\nsc <- spark_connect(master = \"local\", version = \"2.3.0\")\n\n# obtain the example graph\ng <- gf_friends(sc)\n\n# compute PageRank\nresults <- gf_pagerank(g, tol = 0.01, reset_probability = 0.15)\nresults\n## GraphFrame\n## Vertices:\n##   $ id       <chr> \"f\", \"b\", \"g\", \"a\", \"d\", \"c\", \"e\"\n##   $ name     <chr> \"Fanny\", \"Bob\", \"Gabby\", \"Alice\", \"David\", \"Charlie\",...\n##   $ age      <int> 36, 36, 60, 34, 29, 30, 32\n##   $ pagerank <dbl> 0.3283607, 2.6555078, 0.1799821, 0.4491063, 0.3283607...\n## Edges:\n##   $ src          <chr> \"b\", \"c\", \"d\", \"e\", \"a\", \"a\", \"e\", \"f\"\n##   $ dst          <chr> \"c\", \"b\", \"a\", \"f\", \"e\", \"b\", \"d\", \"c\"\n##   $ relationship <chr> \"follow\", \"follow\", \"friend\", \"follow\", \"friend\",...\n##   $ weight       <dbl> 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0\nWe can then visualize the results by collecting the results to R:\nlibrary(tidygraph)\nlibrary(ggraph)\n\nvertices <- results %>%\n  gf_vertices() %>%\n  collect()\n\nedges <- results %>%\n  gf_edges() %>%\n  collect()\n\nedges %>%\n  as_tbl_graph() %>%\n  activate(nodes) %>%\n  left_join(vertices, by = c(name = \"id\")) %>%\n  ggraph(layout = \"nicely\") +\n  geom_node_label(aes(label = name.y, color = pagerank)) +\n  geom_edge_link(\n    aes(\n      alpha = weight,\n      start_cap = label_rect(node1.name.y),\n      end_cap = label_rect(node2.name.y)\n    ),\n    arrow = arrow(length = unit(4, \"mm\"))\n  ) +\n  theme_graph(fg_text_colour = 'white')"
  },
  {
    "objectID": "packages/graphframes/latest/index.html#further-reading",
    "href": "packages/graphframes/latest/index.html#further-reading",
    "title": "sparklyr",
    "section": "Further Reading",
    "text": "Further Reading\nAppart from calculating PageRank using gf_pagerank, many other functions are available, including:\n\ngf_bfs(): Breadth-first search (BFS).\ngf_connected_components(): Connected components.\ngf_shortest_paths(): Shortest paths algorithm.\ngf_scc(): Strongly connected components.\ngf_triangle_count(): Computes the number of triangles passing through each vertex and others.\ngf_degrees(): Degrees of vertices\n\nFor instance, one can calculate the degrees of vertices using gf_degrees as follows:\ngf_friends(sc) %>% gf_degrees()\n## # Source: spark<?> [?? x 2]\n##   id    degree\n## * <chr>  <int>\n## 1 f          2\n## 2 b          3\n## 3 a          3\n## 4 c          3\n## 5 e          3\n## 6 d          2\nFinally, we disconnect from Spark:\nspark_disconnect(sc)"
  },
  {
    "objectID": "packages/graphframes/latest/news.html",
    "href": "packages/graphframes/latest/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "graphframes 0.1.2\n\nUpdated dependency to graphframes 0.6.0, with support for Spark 2.3."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#description",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nBreadth-first search (BFS)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#usage",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_bfs(x, from_expr, to_expr, max_path_length = 10, edge_filter = NULL,\n  ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). from_expr | Spark SQL expression specifying valid starting vertices for the BFS. to_expr | Spark SQL expression specifying valid target vertices for the BFS. max_path_length | Limit on the length of paths. edge_filter | Spark SQL expression specifying edges which may be used in the search. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_bfs.html#examples",
    "href": "packages/graphframes/latest/reference/gf_bfs.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_bfs(g, from_expr = \"name = 'Esther'\", to_expr = \"age < 32\")"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_cache.html#description",
    "href": "packages/graphframes/latest/reference/gf_cache.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCache the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_cache.html#usage",
    "href": "packages/graphframes/latest/reference/gf_cache.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_cache(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_cache.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_cache.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#description",
    "href": "packages/graphframes/latest/reference/gf_chain.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReturns a chain graph of the given size with Long ID type. The vertex IDs are 0, 1, …, n-1, and the edges are (0, 1), (1, 2), …., (n-2, n-1)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#usage",
    "href": "packages/graphframes/latest/reference/gf_chain.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_chain(sc, n)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_chain.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nSize of the graph to return."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_chain.html#examples",
    "href": "packages/graphframes/latest/reference/gf_chain.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_chain(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#description",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nComputes the connected component membership of each vertex and returns a DataFrame of vertex information with each vertex assigned a component ID."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#usage",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_connected_components(x, broadcast_threshold = 1000000L,\n  algorithm = c(\"graphframes\", \"graphx\"), checkpoint_interval = 2L,\n  ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). broadcast_threshold | Broadcast threshold in propagating component assignments. algorithm | One of ‘graphframes’ or ‘graphx’. checkpoint_interval | Checkpoint interval in terms of number of iterations. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_connected_components.html#examples",
    "href": "packages/graphframes/latest/reference/gf_connected_components.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\n# checkpoint directory is required for gf_connected_components()\nspark_set_checkpoint_dir(sc, tempdir())\ng <- gf_friends(sc)\ngf_connected_components(g)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_degrees.html#description",
    "href": "packages/graphframes/latest/reference/gf_degrees.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nDegrees of vertices"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_degrees.html#usage",
    "href": "packages/graphframes/latest/reference/gf_degrees.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_degrees.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_degrees.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edge_columns.html#description",
    "href": "packages/graphframes/latest/reference/gf_edge_columns.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nEdges column names"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edge_columns.html#usage",
    "href": "packages/graphframes/latest/reference/gf_edge_columns.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_edge_columns(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edge_columns.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_edge_columns.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edges.html#description",
    "href": "packages/graphframes/latest/reference/gf_edges.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nExtract edges DataFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edges.html#usage",
    "href": "packages/graphframes/latest/reference/gf_edges.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_edges(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_edges.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_edges.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#description",
    "href": "packages/graphframes/latest/reference/gf_find.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nMotif finding uses a simple Domain-Specific Language (DSL) for expressing structural queries. For example, gf_find(g, “(a)-[e]->(b); (b)-[e2]->(a)”) will search for pairs of vertices a,b connected by edges in both directions. It will return a DataFrame of all such structures in the graph, with columns for each of the named elements (vertices or edges) in the motif. In this case, the returned columns will be in order of the pattern: “a, e, b, e2.”"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#usage",
    "href": "packages/graphframes/latest/reference/gf_find.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_find(x, pattern)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_find.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). pattern | pattern specifying a motif to search for"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_find.html#examples",
    "href": "packages/graphframes/latest/reference/gf_find.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_friends(sc) %>%\n  gf_find(\"(a)-[e]->(b); (b)-[e2]->(a)\")"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#description",
    "href": "packages/graphframes/latest/reference/gf_friends.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGraph of friends in a social network."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#usage",
    "href": "packages/graphframes/latest/reference/gf_friends.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_friends(sc)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_friends.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_friends.html#examples",
    "href": "packages/graphframes/latest/reference/gf_friends.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\ngf_friends(sc)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#description",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCreate a new GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#usage",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_graphframe(vertices = NULL, edges)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nvertices\nA tbl_spark representing vertices.\n\n\nedges\nA tbl_psark representing edges."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_graphframe.html#examples",
    "href": "packages/graphframes/latest/reference/gf_graphframe.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.3.0\")\nv_tbl <- sdf_copy_to(\n  sc, data.frame(id = 1:3, name = LETTERS[1:3])\n)\ne_tbl <- sdf_copy_to(\n  sc, data.frame(src = c(1, 2, 2), dst = c(2, 1, 3),\n                 action = c(\"love\", \"hate\", \"follow\"))\n)\ngf_graphframe(v_tbl, e_tbl)\ngf_graphframe(edges = e_tbl)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#description",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nGenerate a grid Ising model with random parameters"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#usage",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_grid_ising_model(sc, n, v_std = 1, e_std = 1)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nLength of one side of the grid. The grid will be of size n x n.\n\n\nv_std\nStandard deviation of normal distribution used to generate vertex factors “a”. Default of 1.0.\n\n\ne_std\nStandard deviation of normal distribution used to generate edge factors “b”. Default of 1.0."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#details",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nThis method generates a grid Ising model with random parameters. Ising models are probabilistic graphical models over binary variables xi. Each binary variable xi corresponds to one vertex, and it may take values -1 or +1. The probability distribution P(X) (over all xi) is parameterized by vertex factors ai and edge factors bij:\nP(X) = (1/Z) * exp[ i a_i x_i + {ij} b_{ij} x_i x_j ]"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#value",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nGraphFrame. Vertices have columns “id” and “a”. Edges have columns “src”, “dst”, and “b”. Edges are directed, but they should be treated as undirected in any algorithms run on this model. Vertex IDs are of the form “i,j”. E.g., vertex “1,3” is in the second row and fourth column of the grid."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_grid_ising_model.html#examples",
    "href": "packages/graphframes/latest/reference/gf_grid_ising_model.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_grid_ising_model(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_in_degrees.html#description",
    "href": "packages/graphframes/latest/reference/gf_in_degrees.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nIn-degrees of vertices"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_in_degrees.html#usage",
    "href": "packages/graphframes/latest/reference/gf_in_degrees.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_in_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_in_degrees.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_in_degrees.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#description",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRun static Label Propagation for detecting communities in networks. Each node in the network is initially assigned to its own community. At every iteration, nodes send their community affiliation to all neighbors and update their state to the mode community affiliation of incoming messages. LPA is a standard community detection algorithm for graphs. It is very inexpensive computationally, although (1) convergence is not guaranteed and (2) one can end up with trivial solutions (all nodes are identified into a single community)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#usage",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_lpa(x, max_iter, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). max_iter | Maximum number of iterations. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_lpa.html#examples",
    "href": "packages/graphframes/latest/reference/gf_lpa.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_lpa(g, max_iter = 5)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_out_degrees.html#description",
    "href": "packages/graphframes/latest/reference/gf_out_degrees.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nOut-degrees of vertices"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_out_degrees.html#usage",
    "href": "packages/graphframes/latest/reference/gf_out_degrees.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_out_degrees(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_out_degrees.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_out_degrees.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#description",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPageRank"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#usage",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_pagerank(x, tol = NULL, reset_probability = 0.15, max_iter = NULL,\n  source_id = NULL, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). tol | Tolerance. reset_probability | Reset probability. max_iter | Maximum number of iterations. source_id | (Optional) Source vertex for a personalized pagerank. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_pagerank.html#examples",
    "href": "packages/graphframes/latest/reference/gf_pagerank.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_pagerank(g, reset_probability = 0.15, tol = 0.01)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_persist.html#description",
    "href": "packages/graphframes/latest/reference/gf_persist.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nPersist the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_persist.html#usage",
    "href": "packages/graphframes/latest/reference/gf_persist.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_persist(x, storage_level = \"MEMORY_AND_DISK\")"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_persist.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_persist.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). storage_level | The storage level to be used. Please view the http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistenceSpark Documentation for information on what storage levels are accepted."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_register.html#description",
    "href": "packages/graphframes/latest/reference/gf_register.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRegister a GraphFrame object"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_register.html#usage",
    "href": "packages/graphframes/latest/reference/gf_register.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_register(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_register.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_register.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#description",
    "href": "packages/graphframes/latest/reference/gf_scc.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nCompute the strongly connected component (SCC) of each vertex and return a DataFrame with each vertex assigned to the SCC containing that vertex."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#usage",
    "href": "packages/graphframes/latest/reference/gf_scc.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_scc(x, max_iter, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_scc.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). max_iter | Maximum number of iterations. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_scc.html#examples",
    "href": "packages/graphframes/latest/reference/gf_scc.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_scc(g, max_iter = 10)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#description",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nComputes shortest paths from every vertex to the given set of landmark vertices. Note that this takes edge direction into account."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#usage",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_shortest_paths(x, landmarks, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). landmarks | IDs of landmark vertices. … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_shortest_paths.html#examples",
    "href": "packages/graphframes/latest/reference/gf_shortest_paths.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_shortest_paths(g, landmarks = c(\"a\", \"d\"))"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#description",
    "href": "packages/graphframes/latest/reference/gf_star.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReturns a star graph with Long ID type, consisting of a central element indexed 0 (the root) and the n other leaf vertices 1, 2, …, n."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#usage",
    "href": "packages/graphframes/latest/reference/gf_star.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_star(sc, n)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_star.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nn\nThe number of leaves."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_star.html#examples",
    "href": "packages/graphframes/latest/reference/gf_star.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_star(sc, 5)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#description",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis algorithm ignores edge direction; i.e., all edges are treated as undirected. In a multigraph, duplicate edges will be counted only once."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#usage",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_triangle_count(x, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). … | Optional arguments, currently not used."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triangle_count.html#examples",
    "href": "packages/graphframes/latest/reference/gf_triangle_count.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ng <- gf_friends(sc)\ngf_triangle_count(g)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triplets.html#description",
    "href": "packages/graphframes/latest/reference/gf_triplets.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nTriplets of graph"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triplets.html#usage",
    "href": "packages/graphframes/latest/reference/gf_triplets.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_triplets(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_triplets.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_triplets.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#description",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nTwo densely connected blobs (vertices 0->n-1 and n->2n-1) connected by a single edge (0->n)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#usage",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_two_blobs(sc, blob_size)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA Spark connection.\n\n\nblob_size\nThe size of each blob."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_two_blobs.html#examples",
    "href": "packages/graphframes/latest/reference/gf_two_blobs.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ngf_two_blobs(sc, 3)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_unpersist.html#description",
    "href": "packages/graphframes/latest/reference/gf_unpersist.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nUnpersist the GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_unpersist.html#usage",
    "href": "packages/graphframes/latest/reference/gf_unpersist.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_unpersist(x, blocking = FALSE)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_unpersist.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_unpersist.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). blocking | whether to block until all blocks are deleted"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertex_columns.html#description",
    "href": "packages/graphframes/latest/reference/gf_vertex_columns.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nVertices column names"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertex_columns.html#usage",
    "href": "packages/graphframes/latest/reference/gf_vertex_columns.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_vertex_columns(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertex_columns.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_vertex_columns.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertices.html#description",
    "href": "packages/graphframes/latest/reference/gf_vertices.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nExtract vertices DataFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertices.html#usage",
    "href": "packages/graphframes/latest/reference/gf_vertices.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ngf_vertices(x)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/gf_vertices.html#arguments",
    "href": "packages/graphframes/latest/reference/gf_vertices.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe)."
  },
  {
    "objectID": "packages/graphframes/latest/reference/index.html",
    "href": "packages/graphframes/latest/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ngf_bfs()\nBreadth-first search (BFS)\n\n\ngf_cache()\nCache the GraphFrame\n\n\ngf_chain()\nChain graph\n\n\ngf_connected_components()\nConnected components\n\n\ngf_degrees()\nDegrees of vertices\n\n\ngf_edge_columns()\nEdges column names\n\n\ngf_edges()\nExtract edges DataFrame\n\n\ngf_find()\nMotif finding: Searching the graph for structural patterns\n\n\ngf_friends()\nGraph of friends in a social network.\n\n\ngf_graphframe()\nCreate a new GraphFrame\n\n\ngf_grid_ising_model()\nGenerate a grid Ising model with random parameters\n\n\ngf_in_degrees()\nIn-degrees of vertices\n\n\ngf_lpa()\nLabel propagation algorithm (LPA)\n\n\ngf_out_degrees()\nOut-degrees of vertices\n\n\ngf_pagerank()\nPageRank\n\n\ngf_persist()\nPersist the GraphFrame\n\n\ngf_register()\nRegister a GraphFrame object\n\n\ngf_scc()\nStrongly connected components\n\n\ngf_shortest_paths()\nShortest paths\n\n\ngf_star()\nGenerate a star graph\n\n\ngf_triangle_count()\nComputes the number of triangles passing through each vertex.\n\n\ngf_triplets()\nTriplets of graph\n\n\ngf_two_blobs()\nGenerate two blobs\n\n\ngf_unpersist()\nUnpersist the GraphFrame\n\n\ngf_vertex_columns()\nVertices column names\n\n\ngf_vertices()\nExtract vertices DataFrame\n\n\nspark_graphframe() spark_graphframe()\nRetrieve a GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/spark_graphframe.html#description",
    "href": "packages/graphframes/latest/reference/spark_graphframe.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRetrieve a GraphFrame"
  },
  {
    "objectID": "packages/graphframes/latest/reference/spark_graphframe.html#usage",
    "href": "packages/graphframes/latest/reference/spark_graphframe.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_graphframe(x, ...)\n\nspark_graphframe(x, ...)"
  },
  {
    "objectID": "packages/graphframes/latest/reference/spark_graphframe.html#arguments",
    "href": "packages/graphframes/latest/reference/spark_graphframe.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn object coercable to a GraphFrame (typically, a\n\n\n\ngf_graphframe). … | additional arguments, not used"
  },
  {
    "objectID": "packages/mleap/0.1.0/index.html#getting-started",
    "href": "packages/mleap/0.1.0/index.html#getting-started",
    "title": "sparklyr",
    "section": "Getting started",
    "text": "Getting started\nmleap can be installed from CRAN via\ninstall.packages(\"mleap\")\nor, for the latest development version from GitHub, using\ndevtools::install_github(\"rstudio/mleap\")\nOnce mleap has been installed, we can install the external dependencies using\nlibrary(mleap)\ninstall_maven()\n# Alternatively, if you already have Maven installed, you can \n#  set options(maven.home = \"path/to/maven\")\ninstall_mleap()\nWe can now export Spark ML pipelines from sparklyr.\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\n\n# Create a pipeline and fit it\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\n\n# A transformed data frame with the appropriate schema is required\n#   for exporting the pipeline model\ntransformed_tbl <- ml_transform(pipeline_model, mtcars_tbl)\n\n# Export model\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, transformed_tbl, model_path)\n\n# Disconnect from Spark\nspark_disconnect(sc)\nAt this point, we can share mtcars_model.zip with our deployment/implementation engineers, and they would be able to embed the model in another application. See the MLeap docs for details.\nWe also provide R functions for testing that the saved models behave as expected. Here we load the previously saved model:\nmodel <- mleap_load_bundle(model_path)\nmodel\n## MLeap Transformer\n## <fcf4647e-31ee-4d8e-b620-05b28c23a4c0> \n##   Name: pipeline_930b6090bbf9 \n##   Format: json \n##   MLeap Version: 0.10.0-SNAPSHOT\nWe can retrieve the schema associated with the model:\nmleap_model_schema(model)\n## # A tibble: 6 x 4\n##   name       type   nullable dimension\n##   <chr>      <chr>  <lgl>    <chr>    \n## 1 qsec       double TRUE     <NA>     \n## 2 hp         double FALSE    <NA>     \n## 3 wt         double TRUE     <NA>     \n## 4 big_hp     double FALSE    <NA>     \n## 5 features   double TRUE     (3)      \n## 6 prediction double FALSE    <NA>\nThen, we create a new data frame to be scored, and make predictions using our model:\nnewdata <- tibble::tribble(\n  ~qsec, ~hp, ~wt,\n  16.2,  101, 2.68,\n  18.1,  99,  3.08\n)\n\n# Transform the data frame\ntransformed_df <- mleap_transform(model, newdata)\ndplyr::glimpse(transformed_df)\n## Observations: 2\n## Variables: 6\n## $ qsec       <dbl> 16.2, 18.1\n## $ hp         <dbl> 101, 99\n## $ wt         <dbl> 2.68, 3.08\n## $ big_hp     <dbl> 1, 0\n## $ features   <list> [[[1, 2.68, 16.2], [3]], [[0, 3.08, 18.1], [3]]]\n## $ prediction <dbl> 21.06529, 22.36667"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/index.html",
    "href": "packages/mleap/0.1.0/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ninstall_maven()\nInstall Maven\n\n\ninstall_mleap()\nInstall MLeap runtime\n\n\nml_write_bundle()\nExport a Spark pipeline for serving\n\n\nmleap_installed_versions()\nFind existing MLeap installations\n\n\nmleap_load_bundle()\nLoads an MLeap bundle\n\n\nmleap_model_schema()\nMLeap model schema\n\n\nmleap_transform()\nTransform data using an MLeap model"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/install_maven.html#description",
    "href": "packages/mleap/0.1.0/reference/install_maven.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis function installs Apache Maven."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/install_maven.html#usage",
    "href": "packages/mleap/0.1.0/reference/install_maven.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninstall_maven(dir = NULL, version = NULL)"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/install_maven.html#arguments",
    "href": "packages/mleap/0.1.0/reference/install_maven.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\ndir\n(Optional) Directory to install maven in.\n\n\n\nDefaults to maven/ under user’s home directory. version | Version of Maven to install, defaults to the latest version tested with this package."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/install_maven.html#examples",
    "href": "packages/mleap/0.1.0/reference/install_maven.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ninstall_maven()"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/install_mleap.html#description",
    "href": "packages/mleap/0.1.0/reference/install_mleap.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nInstall MLeap runtime"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/install_mleap.html#usage",
    "href": "packages/mleap/0.1.0/reference/install_mleap.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninstall_mleap(dir = NULL, version = NULL, use_temp_cache = TRUE)"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/install_mleap.html#arguments",
    "href": "packages/mleap/0.1.0/reference/install_mleap.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\ndir\n(Optional) Directory to save the jars\n\n\nversion\nVersion of MLeap to install, defaults to the latest version tested with this package.\n\n\nuse_temp_cache\nWhether to use a temporary Maven cache directory for downloading.\n\n\n\nSetting this to TRUE prevents Maven from creating a persistent .m2/ directory. Defaults to TRUE."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/install_mleap.html#examples",
    "href": "packages/mleap/0.1.0/reference/install_mleap.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ninstall_mleap()"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/ml_write_bundle.html#description",
    "href": "packages/mleap/0.1.0/reference/ml_write_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis functions serializes a Spark pipeline model into an MLeap bundle."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/ml_write_bundle.html#usage",
    "href": "packages/mleap/0.1.0/reference/ml_write_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_write_bundle(x, dataset, path, overwrite = FALSE)"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/ml_write_bundle.html#arguments",
    "href": "packages/mleap/0.1.0/reference/ml_write_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark pipeline model object.\n\n\ndataset\nA Spark DataFrame with the schema of the transformed DataFrame.\n\n\npath\nWhere to save the bundle.\n\n\noverwrite\nWhether to overwrite an existing file, defaults to FALSE."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/ml_write_bundle.html#examples",
    "href": "packages/mleap/0.1.0/reference/ml_write_bundle.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, \n                ml_transform(pipeline_model, mtcars_tbl),\n                model_path,\n                overwrite = TRUE)"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_installed_versions.html#description",
    "href": "packages/mleap/0.1.0/reference/mleap_installed_versions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFind existing MLeap installations"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_installed_versions.html#usage",
    "href": "packages/mleap/0.1.0/reference/mleap_installed_versions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_installed_versions()"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_installed_versions.html#value",
    "href": "packages/mleap/0.1.0/reference/mleap_installed_versions.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA data frame of MLeap Runtime installation versions and their locations."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_load_bundle.html#description",
    "href": "packages/mleap/0.1.0/reference/mleap_load_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nLoads an MLeap bundle"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_load_bundle.html#usage",
    "href": "packages/mleap/0.1.0/reference/mleap_load_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_load_bundle(path)"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_load_bundle.html#arguments",
    "href": "packages/mleap/0.1.0/reference/mleap_load_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\npath\nPath to the exported bundle zip file."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_load_bundle.html#value",
    "href": "packages/mleap/0.1.0/reference/mleap_load_bundle.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_model_schema.html#description",
    "href": "packages/mleap/0.1.0/reference/mleap_model_schema.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReturns the schema of an MLeap transformer."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_model_schema.html#usage",
    "href": "packages/mleap/0.1.0/reference/mleap_model_schema.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_model_schema(x)"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_model_schema.html#arguments",
    "href": "packages/mleap/0.1.0/reference/mleap_model_schema.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_model_schema.html#value",
    "href": "packages/mleap/0.1.0/reference/mleap_model_schema.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA data frame of the model schema."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_transform.html#description",
    "href": "packages/mleap/0.1.0/reference/mleap_transform.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis functions transforms an R data frame using an MLeap model."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_transform.html#usage",
    "href": "packages/mleap/0.1.0/reference/mleap_transform.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_transform(model, data)"
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_transform.html#arguments",
    "href": "packages/mleap/0.1.0/reference/mleap_transform.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nmodel\nAn MLeap model object, obtained by mleap_load_bundle().\n\n\ndata\nAn R data frame."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_transform.html#value",
    "href": "packages/mleap/0.1.0/reference/mleap_transform.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA transformed data frame."
  },
  {
    "objectID": "packages/mleap/0.1.0/reference/mleap_transform.html#see-also",
    "href": "packages/mleap/0.1.0/reference/mleap_transform.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\n[mleap_load_bundle()]"
  },
  {
    "objectID": "packages/mleap/0.1.3/index.html#getting-started",
    "href": "packages/mleap/0.1.3/index.html#getting-started",
    "title": "sparklyr",
    "section": "Getting started",
    "text": "Getting started\nmleap can be installed from CRAN via\ninstall.packages(\"mleap\")\nor, for the latest development version from GitHub, using\ndevtools::install_github(\"rstudio/mleap\")\nOnce mleap has been installed, we can install the external dependencies using\nlibrary(mleap)\ninstall_maven()\n# Alternatively, if you already have Maven installed, you can \n#  set options(maven.home = \"path/to/maven\")\ninstall_mleap()\nWe can now export Spark ML pipelines from sparklyr.\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.3.0\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\n\n# Create a pipeline and fit it\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\n\n# A transformed data frame with the appropriate schema is required\n#   for exporting the pipeline model\ntransformed_tbl <- ml_transform(pipeline_model, mtcars_tbl)\n\n# Export model\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, transformed_tbl, model_path)\n\n# Disconnect from Spark\nspark_disconnect(sc)\nAt this point, we can share mtcars_model.zip with our deployment/implementation engineers, and they would be able to embed the model in another application. See the MLeap docs for details.\nWe also provide R functions for testing that the saved models behave as expected. Here we load the previously saved model:\nmodel <- mleap_load_bundle(model_path)\nmodel\n## MLeap Transformer\n## <97ff1e90-5c3e-40fc-99dd-1919276e76be> \n##   Name: pipeline_1b49362281ef \n##   Format: json \n##   MLeap Version: 0.12.0\nWe can retrieve the schema associated with the model:\nmleap_model_schema(model)\n## # A tibble: 6 x 4\n##   name       type   nullable dimension\n##   <chr>      <chr>  <lgl>    <chr>    \n## 1 qsec       double TRUE     <NA>     \n## 2 hp         double FALSE    <NA>     \n## 3 wt         double TRUE     <NA>     \n## 4 big_hp     double FALSE    <NA>     \n## 5 features   double TRUE     (3)      \n## 6 prediction double FALSE    <NA>\nThen, we create a new data frame to be scored, and make predictions using our model:\nnewdata <- tibble::tribble(\n  ~qsec, ~hp, ~wt,\n  16.2,  101, 2.68,\n  18.1,  99,  3.08\n)\n\n# Transform the data frame\ntransformed_df <- mleap_transform(model, newdata)\ndplyr::glimpse(transformed_df)\n## Observations: 2\n## Variables: 6\n## $ qsec       <dbl> 16.2, 18.1\n## $ hp         <dbl> 101, 99\n## $ wt         <dbl> 2.68, 3.08\n## $ big_hp     <dbl> 1, 0\n## $ features   <list> [[[1, 2.68, 16.2], [3]], [[0, 3.08, 18.1], [3]]]\n## $ prediction <dbl> 21.00084, 20.56445"
  },
  {
    "objectID": "packages/mleap/0.1.3/news.html",
    "href": "packages/mleap/0.1.3/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "mleap 0.1.3\n\nSupport MLeap 0.12.0.\n\n\n\nmleap 0.1.2\n\nSupport MLeap 0.10.1 and Spark 2.3.0.\n\n\n\nmleap 0.1.1\n\nAllow heterogenous inputs (numeric and character) in mleap_transform() (#16)."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/index.html",
    "href": "packages/mleap/0.1.3/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ninstall_maven()\nInstall Maven\n\n\ninstall_mleap()\nInstall MLeap runtime\n\n\nml_write_bundle()\nExport a Spark pipeline for serving\n\n\nmleap_installed_versions()\nFind existing MLeap installations\n\n\nmleap_load_bundle()\nLoads an MLeap bundle\n\n\nmleap_model_schema()\nMLeap model schema\n\n\nmleap_transform()\nTransform data using an MLeap model"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/install_maven.html#description",
    "href": "packages/mleap/0.1.3/reference/install_maven.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis function installs Apache Maven."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/install_maven.html#usage",
    "href": "packages/mleap/0.1.3/reference/install_maven.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninstall_maven(dir = NULL, version = NULL)"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/install_maven.html#arguments",
    "href": "packages/mleap/0.1.3/reference/install_maven.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\ndir\n(Optional) Directory to install maven in.\n\n\n\nDefaults to maven/ under user’s home directory. version | Version of Maven to install, defaults to the latest version tested with this package."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/install_maven.html#examples",
    "href": "packages/mleap/0.1.3/reference/install_maven.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ninstall_maven()"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/install_mleap.html#description",
    "href": "packages/mleap/0.1.3/reference/install_mleap.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nInstall MLeap runtime"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/install_mleap.html#usage",
    "href": "packages/mleap/0.1.3/reference/install_mleap.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninstall_mleap(dir = NULL, version = NULL, use_temp_cache = TRUE)"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/install_mleap.html#arguments",
    "href": "packages/mleap/0.1.3/reference/install_mleap.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\ndir\n(Optional) Directory to save the jars\n\n\nversion\nVersion of MLeap to install, defaults to the latest version tested with this package.\n\n\nuse_temp_cache\nWhether to use a temporary Maven cache directory for downloading.\n\n\n\nSetting this to TRUE prevents Maven from creating a persistent .m2/ directory. Defaults to TRUE."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/install_mleap.html#examples",
    "href": "packages/mleap/0.1.3/reference/install_mleap.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ninstall_mleap()"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/ml_write_bundle.html#description",
    "href": "packages/mleap/0.1.3/reference/ml_write_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis functions serializes a Spark pipeline model into an MLeap bundle."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/ml_write_bundle.html#usage",
    "href": "packages/mleap/0.1.3/reference/ml_write_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_write_bundle(x, dataset, path, overwrite = FALSE)"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/ml_write_bundle.html#arguments",
    "href": "packages/mleap/0.1.3/reference/ml_write_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark pipeline model object.\n\n\ndataset\nA Spark DataFrame with the schema of the transformed DataFrame.\n\n\npath\nWhere to save the bundle.\n\n\noverwrite\nWhether to overwrite an existing file, defaults to FALSE."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/ml_write_bundle.html#examples",
    "href": "packages/mleap/0.1.3/reference/ml_write_bundle.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, \n                ml_transform(pipeline_model, mtcars_tbl),\n                model_path,\n                overwrite = TRUE)"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_installed_versions.html#description",
    "href": "packages/mleap/0.1.3/reference/mleap_installed_versions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFind existing MLeap installations"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_installed_versions.html#usage",
    "href": "packages/mleap/0.1.3/reference/mleap_installed_versions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_installed_versions()"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_installed_versions.html#value",
    "href": "packages/mleap/0.1.3/reference/mleap_installed_versions.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA data frame of MLeap Runtime installation versions and their locations."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_load_bundle.html#description",
    "href": "packages/mleap/0.1.3/reference/mleap_load_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nLoads an MLeap bundle"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_load_bundle.html#usage",
    "href": "packages/mleap/0.1.3/reference/mleap_load_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_load_bundle(path)"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_load_bundle.html#arguments",
    "href": "packages/mleap/0.1.3/reference/mleap_load_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\npath\nPath to the exported bundle zip file."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_load_bundle.html#value",
    "href": "packages/mleap/0.1.3/reference/mleap_load_bundle.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_model_schema.html#description",
    "href": "packages/mleap/0.1.3/reference/mleap_model_schema.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReturns the schema of an MLeap transformer."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_model_schema.html#usage",
    "href": "packages/mleap/0.1.3/reference/mleap_model_schema.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_model_schema(x)"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_model_schema.html#arguments",
    "href": "packages/mleap/0.1.3/reference/mleap_model_schema.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_model_schema.html#value",
    "href": "packages/mleap/0.1.3/reference/mleap_model_schema.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA data frame of the model schema."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_transform.html#description",
    "href": "packages/mleap/0.1.3/reference/mleap_transform.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis functions transforms an R data frame using an MLeap model."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_transform.html#usage",
    "href": "packages/mleap/0.1.3/reference/mleap_transform.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_transform(model, data)"
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_transform.html#arguments",
    "href": "packages/mleap/0.1.3/reference/mleap_transform.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nmodel\nAn MLeap model object, obtained by mleap_load_bundle().\n\n\ndata\nAn R data frame."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_transform.html#value",
    "href": "packages/mleap/0.1.3/reference/mleap_transform.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA transformed data frame."
  },
  {
    "objectID": "packages/mleap/0.1.3/reference/mleap_transform.html#see-also",
    "href": "packages/mleap/0.1.3/reference/mleap_transform.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\n[mleap_load_bundle()]"
  },
  {
    "objectID": "packages/mleap/dev/index.html#getting-started",
    "href": "packages/mleap/dev/index.html#getting-started",
    "title": "sparklyr",
    "section": "Getting started",
    "text": "Getting started\nmleap can be installed from CRAN via\ninstall.packages(\"mleap\")\nor, for the latest development version from GitHub, using\ndevtools::install_github(\"rstudio/mleap\")\nOnce mleap has been installed, we can install the external dependencies using\nlibrary(mleap)\ninstall_maven()\n# Alternatively, if you already have Maven installed, you can \n#  set options(maven.home = \"path/to/maven\")\ninstall_mleap()\nWe can now export Spark ML pipelines from sparklyr.\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.4.0\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\n\n# Create a pipeline and fit it\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\n\n# Export model\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, sample_input = mtcars_tbl, path = model_path)\n\n# Disconnect from Spark\nspark_disconnect(sc)\n## NULL\nAt this point, we can share mtcars_model.zip with our deployment/implementation engineers, and they would be able to embed the model in another application. See the MLeap docs for details.\nWe also provide R functions for testing that the saved models behave as expected. Here we load the previously saved model:\nmodel <- mleap_load_bundle(model_path)\nmodel\n## MLeap Transformer\n## <7e2f61ed-154b-4c9e-9926-85fa326d69ef> \n##   Name: pipeline_c1754f374a53 \n##   Format: json \n##   MLeap Version: 0.14.0\nWe can retrieve the schema associated with the model:\nmleap_model_schema(model)\n## # A tibble: 6 x 5\n##   name       type   nullable dimension io    \n##   <chr>      <chr>  <lgl>    <chr>     <chr> \n## 1 qsec       double TRUE     <NA>      input \n## 2 hp         double FALSE    <NA>      input \n## 3 wt         double TRUE     <NA>      input \n## 4 big_hp     double FALSE    <NA>      output\n## 5 features   double TRUE     (3)       output\n## 6 prediction double FALSE    <NA>      output\nThen, we create a new data frame to be scored, and make predictions using our model:\nnewdata <- tibble::tribble(\n  ~qsec, ~hp, ~wt,\n  16.2,  101, 2.68,\n  18.1,  99,  3.08\n)\n\n# Transform the data frame\ntransformed_df <- mleap_transform(model, newdata)\ndplyr::glimpse(transformed_df)\n## Observations: 2\n## Variables: 6\n## $ qsec       <dbl> 16.2, 18.1\n## $ hp         <dbl> 101, 99\n## $ wt         <dbl> 2.68, 3.08\n## $ big_hp     <dbl> 1, 0\n## $ features   <list> [[[1, 2.68, 16.2], [3]], [[0, 3.08, 18.1], [3]]]\n## $ prediction <dbl> 21.00084, 20.56445"
  },
  {
    "objectID": "packages/mleap/dev/news.html",
    "href": "packages/mleap/dev/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "mleap 1.0.0\n\nBreaking change: ml_write_bundle() now takes the sample_input instead of dataset.\nSupport for Spark 2.4 and MLeap 0.14.\nInteger inputs are now supported (#36).\n\n\n\nmleap 0.1.3\n\nSupport MLeap 0.12.0.\n\n\n\nmleap 0.1.2\n\nSupport MLeap 0.10.1 and Spark 2.3.0.\n\n\n\nmleap 0.1.1\n\nAllow heterogenous inputs (numeric and character) in mleap_transform() (#16)."
  },
  {
    "objectID": "packages/mleap/dev/reference/index.html",
    "href": "packages/mleap/dev/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ninstall_maven()\nInstall Maven\n\n\ninstall_mleap()\nInstall MLeap runtime\n\n\nml_write_bundle()\nExport a Spark pipeline for serving\n\n\nmleap_installed_versions()\nFind existing MLeap installations\n\n\nmleap_load_bundle()\nLoads an MLeap bundle\n\n\nmleap_model_schema()\nMLeap model schema\n\n\nmleap_transform()\nTransform data using an MLeap model"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#description",
    "href": "packages/mleap/dev/reference/install_maven.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis function installs Apache Maven."
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#usage",
    "href": "packages/mleap/dev/reference/install_maven.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninstall_maven(dir = NULL, version = NULL)"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#arguments",
    "href": "packages/mleap/dev/reference/install_maven.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\ndir\n(Optional) Directory to install maven in.\n\n\n\nDefaults to maven/ under user’s home directory. version | Version of Maven to install, defaults to the latest version tested with this package."
  },
  {
    "objectID": "packages/mleap/dev/reference/install_maven.html#examples",
    "href": "packages/mleap/dev/reference/install_maven.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ninstall_maven()"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#description",
    "href": "packages/mleap/dev/reference/install_mleap.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nInstall MLeap runtime"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#usage",
    "href": "packages/mleap/dev/reference/install_mleap.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninstall_mleap(dir = NULL, version = NULL, use_temp_cache = TRUE)"
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#arguments",
    "href": "packages/mleap/dev/reference/install_mleap.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\ndir\n(Optional) Directory to save the jars\n\n\nversion\nVersion of MLeap to install, defaults to the latest version tested with this package.\n\n\nuse_temp_cache\nWhether to use a temporary Maven cache directory for downloading.\n\n\n\nSetting this to TRUE prevents Maven from creating a persistent .m2/ directory. Defaults to TRUE."
  },
  {
    "objectID": "packages/mleap/dev/reference/install_mleap.html#examples",
    "href": "packages/mleap/dev/reference/install_mleap.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ninstall_mleap()"
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#description",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis functions serializes a Spark pipeline model into an MLeap bundle."
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#usage",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_write_bundle(x, sample_input, path, overwrite = FALSE)"
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#arguments",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark pipeline model object.\n\n\nsample_input\nA sample input Spark DataFrame with the expected schema.\n\n\npath\nWhere to save the bundle.\n\n\noverwrite\nWhether to overwrite an existing file, defaults to FALSE."
  },
  {
    "objectID": "packages/mleap/dev/reference/ml_write_bundle.html#examples",
    "href": "packages/mleap/dev/reference/ml_write_bundle.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, \n                mtcars_tbl,\n                model_path,\n                overwrite = TRUE)"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_installed_versions.html#description",
    "href": "packages/mleap/dev/reference/mleap_installed_versions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFind existing MLeap installations"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_installed_versions.html#usage",
    "href": "packages/mleap/dev/reference/mleap_installed_versions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_installed_versions()"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_installed_versions.html#value",
    "href": "packages/mleap/dev/reference/mleap_installed_versions.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA data frame of MLeap Runtime installation versions and their locations."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#description",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nLoads an MLeap bundle"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#usage",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_load_bundle(path)"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#arguments",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\npath\nPath to the exported bundle zip file."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_load_bundle.html#value",
    "href": "packages/mleap/dev/reference/mleap_load_bundle.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#description",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReturns the schema of an MLeap transformer."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#usage",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_model_schema(x)"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#arguments",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_model_schema.html#value",
    "href": "packages/mleap/dev/reference/mleap_model_schema.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA data frame of the model schema."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#description",
    "href": "packages/mleap/dev/reference/mleap_transform.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis functions transforms an R data frame using an MLeap model."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#usage",
    "href": "packages/mleap/dev/reference/mleap_transform.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_transform(model, data)"
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#arguments",
    "href": "packages/mleap/dev/reference/mleap_transform.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nmodel\nAn MLeap model object, obtained by mleap_load_bundle().\n\n\ndata\nAn R data frame."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#value",
    "href": "packages/mleap/dev/reference/mleap_transform.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA transformed data frame."
  },
  {
    "objectID": "packages/mleap/dev/reference/mleap_transform.html#see-also",
    "href": "packages/mleap/dev/reference/mleap_transform.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\n[mleap_load_bundle()]"
  },
  {
    "objectID": "packages/mleap/latest/index.html#getting-started",
    "href": "packages/mleap/latest/index.html#getting-started",
    "title": "sparklyr",
    "section": "Getting started",
    "text": "Getting started\nmleap can be installed from CRAN via\ninstall.packages(\"mleap\")\nor, for the latest development version from GitHub, using\ndevtools::install_github(\"rstudio/mleap\")\nOnce mleap has been installed, we can install the external dependencies using\nlibrary(mleap)\ninstall_maven()\n# Alternatively, if you already have Maven installed, you can \n#  set options(maven.home = \"path/to/maven\")\ninstall_mleap()\nWe can now export Spark ML pipelines from sparklyr.\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", version = \"2.4.0\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\n\n# Create a pipeline and fit it\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\n\n# Export model\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, sample_input = mtcars_tbl, path = model_path)\n\n# Disconnect from Spark\nspark_disconnect(sc)\n## NULL\nAt this point, we can share mtcars_model.zip with our deployment/implementation engineers, and they would be able to embed the model in another application. See the MLeap docs for details.\nWe also provide R functions for testing that the saved models behave as expected. Here we load the previously saved model:\nmodel <- mleap_load_bundle(model_path)\nmodel\n## MLeap Transformer\n## <7e2f61ed-154b-4c9e-9926-85fa326d69ef> \n##   Name: pipeline_c1754f374a53 \n##   Format: json \n##   MLeap Version: 0.14.0\nWe can retrieve the schema associated with the model:\nmleap_model_schema(model)\n## # A tibble: 6 x 5\n##   name       type   nullable dimension io    \n##   <chr>      <chr>  <lgl>    <chr>     <chr> \n## 1 qsec       double TRUE     <NA>      input \n## 2 hp         double FALSE    <NA>      input \n## 3 wt         double TRUE     <NA>      input \n## 4 big_hp     double FALSE    <NA>      output\n## 5 features   double TRUE     (3)       output\n## 6 prediction double FALSE    <NA>      output\nThen, we create a new data frame to be scored, and make predictions using our model:\nnewdata <- tibble::tribble(\n  ~qsec, ~hp, ~wt,\n  16.2,  101, 2.68,\n  18.1,  99,  3.08\n)\n\n# Transform the data frame\ntransformed_df <- mleap_transform(model, newdata)\ndplyr::glimpse(transformed_df)\n## Observations: 2\n## Variables: 6\n## $ qsec       <dbl> 16.2, 18.1\n## $ hp         <dbl> 101, 99\n## $ wt         <dbl> 2.68, 3.08\n## $ big_hp     <dbl> 1, 0\n## $ features   <list> [[[1, 2.68, 16.2], [3]], [[0, 3.08, 18.1], [3]]]\n## $ prediction <dbl> 21.00084, 20.56445"
  },
  {
    "objectID": "packages/mleap/latest/news.html",
    "href": "packages/mleap/latest/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "mleap 1.0.0\n\nBreaking change: ml_write_bundle() now takes the sample_input instead of dataset.\nSupport for Spark 2.4 and MLeap 0.14.\nInteger inputs are now supported (#36).\n\n\n\nmleap 0.1.3\n\nSupport MLeap 0.12.0.\n\n\n\nmleap 0.1.2\n\nSupport MLeap 0.10.1 and Spark 2.3.0.\n\n\n\nmleap 0.1.1\n\nAllow heterogenous inputs (numeric and character) in mleap_transform() (#16)."
  },
  {
    "objectID": "packages/mleap/latest/reference/index.html",
    "href": "packages/mleap/latest/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\ninstall_maven()\nInstall Maven\n\n\ninstall_mleap()\nInstall MLeap runtime\n\n\nml_write_bundle()\nExport a Spark pipeline for serving\n\n\nmleap_installed_versions()\nFind existing MLeap installations\n\n\nmleap_load_bundle()\nLoads an MLeap bundle\n\n\nmleap_model_schema()\nMLeap model schema\n\n\nmleap_transform()\nTransform data using an MLeap model"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#description",
    "href": "packages/mleap/latest/reference/install_maven.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis function installs Apache Maven."
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#usage",
    "href": "packages/mleap/latest/reference/install_maven.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninstall_maven(dir = NULL, version = NULL)"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#arguments",
    "href": "packages/mleap/latest/reference/install_maven.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\ndir\n(Optional) Directory to install maven in.\n\n\n\nDefaults to maven/ under user’s home directory. version | Version of Maven to install, defaults to the latest version tested with this package."
  },
  {
    "objectID": "packages/mleap/latest/reference/install_maven.html#examples",
    "href": "packages/mleap/latest/reference/install_maven.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ninstall_maven()"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#description",
    "href": "packages/mleap/latest/reference/install_mleap.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nInstall MLeap runtime"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#usage",
    "href": "packages/mleap/latest/reference/install_mleap.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\ninstall_mleap(dir = NULL, version = NULL, use_temp_cache = TRUE)"
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#arguments",
    "href": "packages/mleap/latest/reference/install_mleap.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\ndir\n(Optional) Directory to save the jars\n\n\nversion\nVersion of MLeap to install, defaults to the latest version tested with this package.\n\n\nuse_temp_cache\nWhether to use a temporary Maven cache directory for downloading.\n\n\n\nSetting this to TRUE prevents Maven from creating a persistent .m2/ directory. Defaults to TRUE."
  },
  {
    "objectID": "packages/mleap/latest/reference/install_mleap.html#examples",
    "href": "packages/mleap/latest/reference/install_mleap.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\ninstall_mleap()"
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#description",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis functions serializes a Spark pipeline model into an MLeap bundle."
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#usage",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nml_write_bundle(x, sample_input, path, overwrite = FALSE)"
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#arguments",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark pipeline model object.\n\n\nsample_input\nA sample input Spark DataFrame with the expected schema.\n\n\npath\nWhere to save the bundle.\n\n\noverwrite\nWhether to overwrite an existing file, defaults to FALSE."
  },
  {
    "objectID": "packages/mleap/latest/reference/ml_write_bundle.html#examples",
    "href": "packages/mleap/latest/reference/ml_write_bundle.html#examples",
    "title": "sparklyr",
    "section": "Examples",
    "text": "Examples\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\nmtcars_tbl <- sdf_copy_to(sc, mtcars, overwrite = TRUE)\npipeline <- ml_pipeline(sc) %>%\n  ft_binarizer(\"hp\", \"big_hp\", threshold = 100) %>%\n  ft_vector_assembler(c(\"big_hp\", \"wt\", \"qsec\"), \"features\") %>%\n  ml_gbt_regressor(label_col = \"mpg\")\npipeline_model <- ml_fit(pipeline, mtcars_tbl)\nmodel_path <- file.path(tempdir(), \"mtcars_model.zip\")\nml_write_bundle(pipeline_model, \n                mtcars_tbl,\n                model_path,\n                overwrite = TRUE)"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_installed_versions.html#description",
    "href": "packages/mleap/latest/reference/mleap_installed_versions.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nFind existing MLeap installations"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_installed_versions.html#usage",
    "href": "packages/mleap/latest/reference/mleap_installed_versions.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_installed_versions()"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_installed_versions.html#value",
    "href": "packages/mleap/latest/reference/mleap_installed_versions.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA data frame of MLeap Runtime installation versions and their locations."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#description",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nLoads an MLeap bundle"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#usage",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_load_bundle(path)"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#arguments",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\npath\nPath to the exported bundle zip file."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_load_bundle.html#value",
    "href": "packages/mleap/latest/reference/mleap_load_bundle.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#description",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nReturns the schema of an MLeap transformer."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#usage",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_model_schema(x)"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#arguments",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nAn MLeap model object."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_model_schema.html#value",
    "href": "packages/mleap/latest/reference/mleap_model_schema.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA data frame of the model schema."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#description",
    "href": "packages/mleap/latest/reference/mleap_transform.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nThis functions transforms an R data frame using an MLeap model."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#usage",
    "href": "packages/mleap/latest/reference/mleap_transform.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nmleap_transform(model, data)"
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#arguments",
    "href": "packages/mleap/latest/reference/mleap_transform.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nmodel\nAn MLeap model object, obtained by mleap_load_bundle().\n\n\ndata\nAn R data frame."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#value",
    "href": "packages/mleap/latest/reference/mleap_transform.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nA transformed data frame."
  },
  {
    "objectID": "packages/mleap/latest/reference/mleap_transform.html#see-also",
    "href": "packages/mleap/latest/reference/mleap_transform.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\n[mleap_load_bundle()]"
  },
  {
    "objectID": "packages/sparklyr/news.html#distributed-r-7",
    "href": "packages/sparklyr/news.html#distributed-r-7",
    "title": "sparklyr",
    "section": "Distributed R",
    "text": "Distributed R\n\nThe memory parameter in spark_apply() now defaults to FALSE when the name parameter is not specified."
  },
  {
    "objectID": "packages/sparklyr/news.html#other",
    "href": "packages/sparklyr/news.html#other",
    "title": "sparklyr",
    "section": "Other",
    "text": "Other\n\nRemoved dreprecated sdf_mutate().\nRemove exported ensure_ functions which were deprecated.\nFixed missing Hive tables not rendering under some Spark distributions (#1823).\nRemove dependency on broom.\nFixed re-entrancy job progress issues when running RStudio 1.2.\nTables with periods supported by setting sparklyr.dplyr.period.splits to FALSE.\nsdf_len(), sdf_along() and sdf_seq() default to 32 bit integers but allow support for 64 bits through bits parameter.\nSupport for detecting Spark version using spark-submit."
  },
  {
    "objectID": "packages/sparklyr/news.html#batches-1",
    "href": "packages/sparklyr/news.html#batches-1",
    "title": "sparklyr",
    "section": "Batches",
    "text": "Batches\n\nAdded support for spark_submit() to assist submitting non-interactive Spark jobs.\n\n\nSpark ML\n\n(Breaking change) The formula API for ML classification algorithms no longer indexes numeric labels, to avoid the confusion of 0 being mapped to \"1\" and vice versa. This means that if the largest numeric label is N, Spark will fit a N+1-class classification model, regardless of how many distinct labels there are in the provided training set (#1591).\nFix retrieval of coefficients in ml_logistic_regression() (@shabbybanks, #1596).\n(Breaking change) For model objects, lazy val and def attributes have been converted to closures, so they are not evaluated at object instantiation (#1453).\nInput and output column names are no longer required to construct pipeline objects to be consistent with Spark (#1513).\nVector attributes of pipeline stages are now printed correctly (#1618).\nDeprecate various aliases favoring method names in Spark.\n\nml_binary_classification_eval()\nml_classification_eval()\nml_multilayer_perceptron()\nml_survival_regression()\nml_als_factorization()\n\nDeprecate incompatible signatures for sdf_transform() and ml_transform() families of methods; the former should take a tbl_spark as the first argument while the latter should take a model object as the first argument.\nInput and output column names are no longer required to construct pipeline objects to be consistent with Spark (#1513).\n\n\n\nData\n\nImplemented support for DBI::db_explain() (#1623).\nFixed for timestamp fields when using copy_to() (#1312, @yutannihilation).\nAdded support to read and write ORC files using spark_read_orc() and spark_write_orc() (#1548).\n\n\n\nLivy\n\nFixed must share the same src error for sdf_broadcast() and other functions when using Livy connections.\nAdded support for logging sparklyr server events and logging sparklyr invokes as comments in the Livy UI.\nAdded support to open the Livy UI from the connections viewer while using RStudio.\nImprove performance in Livy for long execution queries, fixed livy.session.command.timeout and support for livy.session.command.interval to control max polling while waiting for command response (#1538).\nFixed Livy version with MapR distributions.\nRemoved install column from livy_available_versions().\n\n\n\nDistributed R\n\nAdded name parameter to spark_apply() to optionally name resulting table.\nFix to spark_apply() to retain column types when NAs are present (#1665).\nspark_apply() now supports rlang anonymous functions. For example, sdf_len(sc, 3) %>% spark_apply(~.x+1).\nBreaking Change: spark_apply() no longer defaults to the input column names when the columns parameter is nos specified.\nSupport for reading column names from the R data frame returned by spark_apply().\nFix to support retrieving empty data frames in grouped spark_apply() operations (#1505).\nAdded support for sparklyr.apply.packages to configure default behavior for spark_apply() parameters (#1530).\nAdded support for spark.r.libpaths to configure package library in spark_apply() (#1530).\n\n\n\nConnections\n\nDefault to Spark 2.3.1 for installation and local connections (#1680).\nml_load() no longer keeps extraneous table views which was cluttering up the RStudio Connections pane (@randomgambit, #1549).\nAvoid preparing windows environment in non-local connections.\n\n\n\nExtensions\n\nThe ensure_* family of functions is deprecated in favor of forge which doesn’t use NSE and provides more informative errors messages for debugging (#1514).\nSupport for sparklyr.invoke.trace and sparklyr.invoke.trace.callstack configuration options to trace all invoke() calls.\nSupport to invoke methods with char types using single character strings (@lawremi, #1395).\n\n\n\nSerialization\n\nFixed collection of Date types to support correct local JVM timezone to UTC ().\n\n\n\nDocumentation\n\nMany new examples for ft_binarizer(), ft_bucketizer(), ft_min_max_scaler, ft_max_abs_scaler(), ft_standard_scaler(), ml_kmeans(), ml_pca(), ml_bisecting_kmeans(), ml_gaussian_mixture(), ml_naive_bayes(), ml_decision_tree(), ml_random_forest(), ml_multilayer_perceptron_classifier(), ml_linear_regression(), ml_logistic_regression(), ml_gradient_boosted_trees(), ml_generalized_linear_regression(), ml_cross_validator(), ml_evaluator(), ml_clustering_evaluator(), ml_corr(), ml_chisquare_test() and sdf_pivot() (@samuelmacedo83).\n\n\n\nBroom\n\nImplemented tidy(), augment(), and glance() for ml_aft_survival_regression(), ml_isotonic_regression(), ml_naive_bayes(), ml_logistic_regression(), ml_decision_tree(), ml_random_forest(), ml_gradient_boosted_trees(), ml_bisecting_kmeans(), ml_kmeans()and ml_gaussian_mixture() models (@samuelmacedo83)\n\n\n\nConfiguration\n\nDeprecated configuration option sparklyr.dplyr.compute.nocache.\nAdded spark_config_settings() to list all sparklyr configuration settings and describe them, cleaned all settings and grouped by area while maintaining support for previous settings.\nStatic SQL configuration properties are now respected for Spark 2.3, and spark.sql.catalogImplementation defaults to hive to maintain Hive support (#1496, #415).\nspark_config() values can now also be specified as options().\nSupport for functions as values in entries to spark_config() to enable advanced configuration workflows."
  },
  {
    "objectID": "packages/sparktf/index.html#overview",
    "href": "packages/sparktf/index.html#overview",
    "title": "sparklyr",
    "section": "Overview",
    "text": "Overview\nsparktf is a sparklyr extension that allows writing of Spark DataFrames to TFRecord, the recommended format for persisting data to be used in training with TensorFlow."
  },
  {
    "objectID": "packages/sparktf/index.html#installation",
    "href": "packages/sparktf/index.html#installation",
    "title": "sparklyr",
    "section": "Installation",
    "text": "Installation\nYou can install the development version of sparktf from GitHub with:\ndevtools::install_github(\"rstudio/sparktf\")"
  },
  {
    "objectID": "packages/sparktf/index.html#example",
    "href": "packages/sparktf/index.html#example",
    "title": "sparklyr",
    "section": "Example",
    "text": "Example\nWe first attach the required packages and establish a Spark connection.\nlibrary(sparktf)\nlibrary(sparklyr)\nlibrary(keras)\nuse_implementation(\"tensorflow\")\nlibrary(tensorflow)\ntfe_enable_eager_execution()\nlibrary(tfdatasets)\n\nsc <- spark_connect(master = \"local\")\nCopied a sample dataset to Spark then write it to disk via spark_write_tfrecord().\ndata_path <- file.path(tempdir(), \"iris\")\niris_tbl <- sdf_copy_to(sc, iris)\n\niris_tbl %>%\n  ft_string_indexer_model(\n    \"Species\", \"label\",\n    labels = c(\"setosa\", \"versicolor\", \"virginica\")\n  ) %>%\n  spark_write_tfrecord(\n    path = data_path,\n    write_locality = \"local\"\n  )\nWe now read the saved TFRecord file and parse the contents to create a dataset object. For details, refer to the package website for tfdatasets.\ndataset <- tfrecord_dataset(list.files(data_path, full.names = TRUE)) %>%\n  dataset_map(function(example_proto) {\n    features <- list(\n      label = tf$FixedLenFeature(shape(), tf$float32),\n      Sepal_Length = tf$FixedLenFeature(shape(), tf$float32),\n      Sepal_Width = tf$FixedLenFeature(shape(), tf$float32),\n      Petal_Length = tf$FixedLenFeature(shape(), tf$float32),\n      Petal_Width = tf$FixedLenFeature(shape(), tf$float32)\n    )\n\n    features <- tf$parse_single_example(example_proto, features)\n    x <- list(\n      features$Sepal_Length, features$Sepal_Width,\n      features$Petal_Length, features$Petal_Width\n      )\n    y <- tf$one_hot(tf$cast(features$label, tf$int32), 3L)\n    list(x, y)\n  }) %>%\n  dataset_shuffle(150) %>%\n  dataset_batch(16)\nNow, we can define a Keras model using the keras package and fit it by feeding the dataset object defined above.\nmodel <- keras_model_sequential() %>%\n  layer_dense(32, activation = \"relu\", input_shape = 4) %>%\n  layer_dense(3, activation = \"softmax\")\n\nmodel %>%\n  compile(loss = \"categorical_crossentropy\", optimizer = tf$train$AdamOptimizer())\n\nhistory <- model %>%\n  fit(dataset, epochs = 100, verbose = 0)\nFinally, we can use the trained model to make some predictions.\nnew_data <- tf$constant(c(4.9, 3.2, 1.4, 0.2), shape = c(1, 4))\nmodel(new_data)\n#> tf.Tensor([[0.76382965 0.19407341 0.04209692]], shape=(1, 3), dtype=float32)"
  },
  {
    "objectID": "packages/sparktf/news.html",
    "href": "packages/sparktf/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "sparktf 0.1.0\nsparkdf is a sparklyr extension for reading and writing TensorFlow TFRecord files via Apache Spark."
  },
  {
    "objectID": "packages/sparktf/reference/index.html",
    "href": "packages/sparktf/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\nspark_read_tfrecord()\nRead a TFRecord File\n\n\nspark_write_tfrecord()\nWrite a Spark DataFrame to a TFRecord file"
  },
  {
    "objectID": "packages/sparktf/reference/spark_read_tfrecord.html#description",
    "href": "packages/sparktf/reference/spark_read_tfrecord.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nRead a TFRecord file as a Spark DataFrame."
  },
  {
    "objectID": "packages/sparktf/reference/spark_read_tfrecord.html#usage",
    "href": "packages/sparktf/reference/spark_read_tfrecord.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_read_tfrecord(sc, name, path, schema = NULL,\n  record_type = c(\"Example\", \"SequenceExample\"), overwrite = TRUE)"
  },
  {
    "objectID": "packages/sparktf/reference/spark_read_tfrecord.html#arguments",
    "href": "packages/sparktf/reference/spark_read_tfrecord.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nsc\nA spark conneciton.\n\n\nname\nThe name to assign to the newly generated table.\n\n\npath\nThe path to the file. Needs to be accessible from the cluster. Supports the “hdfs://”, “s3a://” and “file://” protocols.\n\n\nschema\n(Currently unsupported.) Schema of TensorFlow records. If not provided, the schema is inferred from TensorFlow records.\n\n\nrecord_type\nInput format of TensorFlow records. By default it is Example.\n\n\noverwrite\nBoolean; overwrite the table with the given name if it already exists?"
  },
  {
    "objectID": "packages/sparktf/reference/spark_write_tfrecord.html#description",
    "href": "packages/sparktf/reference/spark_write_tfrecord.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nSerialize a Spark DataFrame to the TensorFlow TFRecord format for training or inference."
  },
  {
    "objectID": "packages/sparktf/reference/spark_write_tfrecord.html#usage",
    "href": "packages/sparktf/reference/spark_write_tfrecord.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nspark_write_tfrecord(x, path, record_type = c(\"Example\",\n  \"SequenceExample\"), write_locality = c(\"distributed\", \"local\"),\n  mode = NULL)"
  },
  {
    "objectID": "packages/sparktf/reference/spark_write_tfrecord.html#arguments",
    "href": "packages/sparktf/reference/spark_write_tfrecord.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA Spark DataFrame\n\n\npath\nThe path to the file. Needs to be accessible from the cluster.\n\n\n\nSupports the “hdfs://”, “s3a://”, and “file://” protocols. record_type | Output format of TensorFlow records. One of \"Example\" and \"SequenceExample\". write_locality | Determines whether the TensorFlow records are written locally on the workers or on a distributed file system. One of \"distributed\" and \"local\". See Details for more information. mode | A character element. Specifies the behavior when data or table already exists. Supported values include: ‘error’, ‘append’, ‘overwrite’ and ‘ignore’. Notice that ‘overwrite’ will also change the column structure.\nFor more details see also http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes for your version of Spark."
  },
  {
    "objectID": "packages/sparktf/reference/spark_write_tfrecord.html#details",
    "href": "packages/sparktf/reference/spark_write_tfrecord.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nFor write_locality = local, each of the workers stores on the local disk a subset of the data. The subset that is stored on each worker is determined by the partitioning of the DataFrame. Each of the partitions is coalesced into a single TFRecord file and written on the node where the partition lives. This is useful in the context of distributed training, in which each of the workers gets a subset of the data to work on. When this mode is activated, the path provided to the writer is interpreted as a base path that is created on each of the worker nodes, and that will be populated with data from the DataFrame."
  },
  {
    "objectID": "packages/sparkxgb/index.html#overview",
    "href": "packages/sparkxgb/index.html#overview",
    "title": "sparklyr",
    "section": "Overview",
    "text": "Overview\nsparkxgb is a sparklyr extension that provides an interface to XGBoost on Spark."
  },
  {
    "objectID": "packages/sparkxgb/index.html#installation",
    "href": "packages/sparkxgb/index.html#installation",
    "title": "sparklyr",
    "section": "Installation",
    "text": "Installation\nYou can install the development version of sparkxgb with:\n# sparkxgb requires the development version of sparklyr\ndevtools::install_github(\"rstudio/sparklyr\")\ndevtools::install_github(\"rstudio/sparkxgb\")"
  },
  {
    "objectID": "packages/sparkxgb/index.html#example",
    "href": "packages/sparkxgb/index.html#example",
    "title": "sparklyr",
    "section": "Example",
    "text": "Example\nsparkxgb supports the familiar formula interface for specifying models:\nlibrary(sparkxgb)\nlibrary(sparklyr)\nlibrary(dplyr)\n\nsc <- spark_connect(master = \"local\")\niris_tbl <- sdf_copy_to(sc, iris)\n\nxgb_model <- xgboost_classifier(\n  iris_tbl,\n  Species ~ .,\n  num_class = 3,\n  num_round = 50,\n  max_depth = 4\n)\n\nxgb_model %>%\n  ml_predict(iris_tbl) %>%\n  select(Species, predicted_label, starts_with(\"probability_\")) %>%\n  glimpse()\n#> Observations: ??\n#> Variables: 5\n#> Database: spark_connection\n#> $ Species                <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"…\n#> $ predicted_label        <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"…\n#> $ probability_versicolor <dbl> 0.003566429, 0.003564076, 0.003566429, 0.…\n#> $ probability_virginica  <dbl> 0.001423170, 0.002082058, 0.001423170, 0.…\n#> $ probability_setosa     <dbl> 0.9950104, 0.9943539, 0.9950104, 0.995010…\nIt also provides a Pipelines API, which means you can use a xgboost_classifier or xgboost_regressor in a pipeline as any Estimator, and do things like hyperparameter tuning:\npipeline <- ml_pipeline(sc) %>%\n  ft_r_formula(Species ~ .) %>%\n  xgboost_classifier(num_class = 3)\n\nparam_grid <- list(\n  xgboost = list(\n    max_depth = c(1, 5),\n    num_round = c(10, 50)\n  )\n)\n\ncv <- ml_cross_validator(\n  sc,\n  estimator = pipeline,\n  evaluator = ml_multiclass_classification_evaluator(\n    sc,\n    label_col = \"label\",\n    raw_prediction_col = \"rawPrediction\"\n  ),\n  estimator_param_maps = param_grid\n)\n\ncv_model <- cv %>%\n  ml_fit(iris_tbl)\n\nsummary(cv_model)\n#> Summary for CrossValidatorModel\n#>             <cross_validator_ebc61803a06b>\n#>\n#> Tuned Pipeline\n#>   with metric f1\n#>   over 4 hyperparameter sets\n#>   via 3-fold cross validation\n#>\n#> Estimator: Pipeline\n#>            <pipeline_ebc62f635bb6>\n#> Evaluator: MulticlassClassificationEvaluator\n#>            <multiclass_classification_evaluator_ebc65fbf8a19>\n#>\n#> Results Summary:\n#>          f1 num_round_1 max_depth_1\n#> 1 0.9549670          10           1\n#> 2 0.9674460          10           5\n#> 3 0.9488665          50           1\n#> 4 0.9613854          50           5"
  },
  {
    "objectID": "packages/sparkxgb/news.html",
    "href": "packages/sparkxgb/news.html",
    "title": "sparklyr",
    "section": "",
    "text": "sparkxgb 0.1.2\n\nEdgar Ruiz (https://github.com/edgararuiz) will be the new maintainer of this package moving forward.\n\n\n\nsparkxgb 0.1.1\n\nsparkxgb is a sparklyr extension that provides an interface to XGBoost on Spark."
  },
  {
    "objectID": "packages/sparkxgb/reference/index.html",
    "href": "packages/sparkxgb/reference/index.html",
    "title": "sparklyr",
    "section": "",
    "text": "Function(s)\nDescription\n\n\n\n\nxgboost_classifier()\nXGBoost Classifier\n\n\nxgboost_regressor()\nXGBoost Regressor"
  },
  {
    "objectID": "packages/sparkxgb/reference/xgboost_classifier.html#description",
    "href": "packages/sparkxgb/reference/xgboost_classifier.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nXGBoost classifier for Spark."
  },
  {
    "objectID": "packages/sparkxgb/reference/xgboost_classifier.html#usage",
    "href": "packages/sparkxgb/reference/xgboost_classifier.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nxgboost_classifier(\n  x,\n  formula = NULL,\n  eta = 0.3,\n  gamma = 0,\n  max_depth = 6,\n  min_child_weight = 1,\n  max_delta_step = 0,\n  grow_policy = \"depthwise\",\n  max_bins = 16,\n  subsample = 1,\n  colsample_bytree = 1,\n  colsample_bylevel = 1,\n  lambda = 1,\n  alpha = 0,\n  tree_method = \"auto\",\n  sketch_eps = 0.03,\n  scale_pos_weight = 1,\n  sample_type = \"uniform\",\n  normalize_type = \"tree\",\n  rate_drop = 0,\n  skip_drop = 0,\n  lambda_bias = 0,\n  tree_limit = 0,\n  num_round = 1,\n  num_workers = 1,\n  nthread = 1,\n  use_external_memory = FALSE,\n  silent = 0,\n  custom_obj = NULL,\n  custom_eval = NULL,\n  missing = NaN,\n  seed = 0,\n  timeout_request_workers = 30 * 60 * 1000,\n  checkpoint_path = \"\",\n  checkpoint_interval = -1,\n  objective = \"multi:softprob\",\n  base_score = 0.5,\n  train_test_ratio = 1,\n  num_early_stopping_rounds = 0,\n  objective_type = \"classification\",\n  eval_metric = NULL,\n  maximize_evaluation_metrics = FALSE,\n  num_class = NULL,\n  base_margin_col = NULL,\n  thresholds = NULL,\n  weight_col = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  probability_col = \"probability\",\n  raw_prediction_col = \"rawPrediction\",\n  uid = random_string(\"xgboost_classifier_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparkxgb/reference/xgboost_classifier.html#arguments",
    "href": "packages/sparkxgb/reference/xgboost_classifier.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\neta\nStep size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features and eta actually shrinks the feature weights to make the boosting process more conservative. [default=0.3] range: [0,1]\n\n\ngamma\nMinimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be. [default=0]\n\n\nmax_depth\nMaximum depth of a tree, increase this value will make model more complex / likely to be overfitting. [default=6]\n\n\nmin_child_weight\nMinimum sum of instance weight(hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be. [default=1]\n\n\nmax_delta_step\nMaximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update. [default=0]\n\n\ngrow_policy\nGrowth policy for fast histogram algorithm.\n\n\nmax_bins\nMaximum number of bins in histogram.\n\n\nsubsample\nSubsample ratio of the training instance. Setting it to 0.5 means that XGBoost randomly collected half of the data instances to grow trees and this will prevent overfitting. [default=1] range:(0,1]\n\n\ncolsample_bytree\nSubsample ratio of columns when constructing each tree. [default=1] range: (0,1]\n\n\ncolsample_bylevel\nSubsample ratio of columns for each split, in each level. [default=1] range: (0,1]\n\n\nlambda\nL2 regularization term on weights, increase this value will make model more conservative. [default=1]\n\n\nalpha\nL1 regularization term on weights, increase this value will make model more conservative, defaults to 0.\n\n\ntree_method\nThe tree construction algorithm used in XGBoost. options: ‘auto’, ‘exact’, ‘approx’ [default=‘auto’]\n\n\nsketch_eps\nThis is only used for approximate greedy algorithm. This roughly translated into O(1 / sketch_eps) number of bins. Compared to directly select number of bins, this comes with theoretical guarantee with sketch accuracy. [default=0.03] range: (0, 1)\n\n\nscale_pos_weight\nControl the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative cases) / sum(positive cases). [default=1]\n\n\nsample_type\nParameter for Dart booster. Type of sampling algorithm. “uniform”: dropped trees are selected uniformly. “weighted”: dropped trees are selected in proportion to weight. [default=“uniform”]\n\n\nnormalize_type\nParameter of Dart booster. type of normalization algorithm, options: ‘tree’, ‘forest’. [default=“tree”]\n\n\nrate_drop\nParameter of Dart booster. dropout rate. [default=0.0] range: [0.0, 1.0]\n\n\nskip_drop\nParameter of Dart booster. probability of skip dropout. If a dropout is skipped, new trees are added in the same manner as gbtree. [default=0.0] range: [0.0, 1.0]\n\n\nlambda_bias\nParameter of linear booster L2 regularization term on bias, default 0 (no L1 reg on bias because it is not important.)\n\n\ntree_limit\nLimit number of trees in the prediction; defaults to 0 (use all trees.)\n\n\nnum_round\nThe number of rounds for boosting.\n\n\nnum_workers\nnumber of workers used to train xgboost model. Defaults to 1.\n\n\nnthread\nNumber of threads used by per worker. Defaults to 1.\n\n\nuse_external_memory\nThe tree construction algorithm used in XGBoost. options: ‘auto’, ‘exact’, ‘approx’ [default=‘auto’]\n\n\nsilent\n0 means printing running messages, 1 means silent mode. default: 0\n\n\ncustom_obj\nCustomized objective function provided by user. Currently unsupported.\n\n\ncustom_eval\nCustomized evaluation function provided by user. Currently unsupported.\n\n\nmissing\nThe value treated as missing. default: Float.NaN\n\n\nseed\nRandom seed for the C++ part of XGBoost and train/test splitting.\n\n\ntimeout_request_workers\nthe maximum time to wait for the job requesting new workers. default: 30 minutes\n\n\ncheckpoint_path\nThe hdfs folder to load and save checkpoint boosters.\n\n\ncheckpoint_interval\nParam for set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the trained model will get checkpointed every 10 iterations. Note: checkpoint_path must also be set if the checkpoint interval is greater than 0.\n\n\nobjective\nSpecify the learning task and the corresponding learning objective. options: reg:linear, reg:logistic, binary:logistic, binary:logitraw, count:poisson, multi:softmax, multi:softprob, rank:pairwise, reg:gamma. default: reg:linear.\n\n\nbase_score\nParam for initial prediction (aka base margin) column name. Defaults to 0.5.\n\n\ntrain_test_ratio\nFraction of training points to use for testing.\n\n\nnum_early_stopping_rounds\nIf non-zero, the training will be stopped after a specified number of consecutive increases in any evaluation metric.\n\n\nobjective_type\nThe learning objective type of the specified custom objective and eval. Corresponding type will be assigned if custom objective is defined options: regression, classification.\n\n\neval_metric\nEvaluation metrics for validation data, a default metric will be assigned according to objective(rmse for regression, and error for classification, mean average precision for ranking). options: rmse, mae, logloss, error, merror, mlogloss, auc, aucpr, ndcg, map, gamma-deviance\n\n\nmaximize_evaluation_metrics\nWhether to maximize evaluation metrics. Defaults to FALSE (for minization.)\n\n\nnum_class\nNumber of classes.\n\n\nbase_margin_col\nParam for initial prediction (aka base margin) column name.\n\n\nthresholds\nThresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class’s threshold.\n\n\nweight_col\nWeight column.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nprobability_col\nColumn name for predicted class conditional probabilities.\n\n\nraw_prediction_col\nRaw prediction (a.k.a. confidence) column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparkxgb/reference/xgboost_regressor.html#description",
    "href": "packages/sparkxgb/reference/xgboost_regressor.html#description",
    "title": "sparklyr",
    "section": "Description",
    "text": "Description\nXGBoost regressor for Spark."
  },
  {
    "objectID": "packages/sparkxgb/reference/xgboost_regressor.html#usage",
    "href": "packages/sparkxgb/reference/xgboost_regressor.html#usage",
    "title": "sparklyr",
    "section": "Usage",
    "text": "Usage\nxgboost_regressor(\n  x,\n  formula = NULL,\n  eta = 0.3,\n  gamma = 0,\n  max_depth = 6,\n  min_child_weight = 1,\n  max_delta_step = 0,\n  grow_policy = \"depthwise\",\n  max_bins = 16,\n  subsample = 1,\n  colsample_bytree = 1,\n  colsample_bylevel = 1,\n  lambda = 1,\n  alpha = 0,\n  tree_method = \"auto\",\n  sketch_eps = 0.03,\n  scale_pos_weight = 1,\n  sample_type = \"uniform\",\n  normalize_type = \"tree\",\n  rate_drop = 0,\n  skip_drop = 0,\n  lambda_bias = 0,\n  tree_limit = 0,\n  num_round = 1,\n  num_workers = 1,\n  nthread = 1,\n  use_external_memory = FALSE,\n  silent = 0,\n  custom_obj = NULL,\n  custom_eval = NULL,\n  missing = NaN,\n  seed = 0,\n  timeout_request_workers = 30 * 60 * 1000,\n  checkpoint_path = \"\",\n  checkpoint_interval = -1,\n  objective = \"reg:linear\",\n  base_score = 0.5,\n  train_test_ratio = 1,\n  num_early_stopping_rounds = 0,\n  objective_type = \"regression\",\n  eval_metric = NULL,\n  maximize_evaluation_metrics = FALSE,\n  base_margin_col = NULL,\n  weight_col = NULL,\n  features_col = \"features\",\n  label_col = \"label\",\n  prediction_col = \"prediction\",\n  uid = random_string(\"xgboost_regressor_\"),\n  ...\n)"
  },
  {
    "objectID": "packages/sparkxgb/reference/xgboost_regressor.html#arguments",
    "href": "packages/sparkxgb/reference/xgboost_regressor.html#arguments",
    "title": "sparklyr",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\nx\nA spark_connection, ml_pipeline, or a tbl_spark.\n\n\nformula\nUsed when x is a tbl_spark. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see ft_r_formula for details.\n\n\neta\nStep size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features and eta actually shrinks the feature weights to make the boosting process more conservative. [default=0.3] range: [0,1]\n\n\ngamma\nMinimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be. [default=0]\n\n\nmax_depth\nMaximum depth of a tree, increase this value will make model more complex / likely to be overfitting. [default=6]\n\n\nmin_child_weight\nMinimum sum of instance weight(hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be. [default=1]\n\n\nmax_delta_step\nMaximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update. [default=0]\n\n\ngrow_policy\nGrowth policy for fast histogram algorithm.\n\n\nmax_bins\nMaximum number of bins in histogram.\n\n\nsubsample\nSubsample ratio of the training instance. Setting it to 0.5 means that XGBoost randomly collected half of the data instances to grow trees and this will prevent overfitting. [default=1] range:(0,1]\n\n\ncolsample_bytree\nSubsample ratio of columns when constructing each tree. [default=1] range: (0,1]\n\n\ncolsample_bylevel\nSubsample ratio of columns for each split, in each level. [default=1] range: (0,1]\n\n\nlambda\nL2 regularization term on weights, increase this value will make model more conservative. [default=1]\n\n\nalpha\nL1 regularization term on weights, increase this value will make model more conservative, defaults to 0.\n\n\ntree_method\nThe tree construction algorithm used in XGBoost. options: ‘auto’, ‘exact’, ‘approx’ [default=‘auto’]\n\n\nsketch_eps\nThis is only used for approximate greedy algorithm. This roughly translated into O(1 / sketch_eps) number of bins. Compared to directly select number of bins, this comes with theoretical guarantee with sketch accuracy. [default=0.03] range: (0, 1)\n\n\nscale_pos_weight\nControl the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative cases) / sum(positive cases). [default=1]\n\n\nsample_type\nParameter for Dart booster. Type of sampling algorithm. “uniform”: dropped trees are selected uniformly. “weighted”: dropped trees are selected in proportion to weight. [default=“uniform”]\n\n\nnormalize_type\nParameter of Dart booster. type of normalization algorithm, options: ‘tree’, ‘forest’. [default=“tree”]\n\n\nrate_drop\nParameter of Dart booster. dropout rate. [default=0.0] range: [0.0, 1.0]\n\n\nskip_drop\nParameter of Dart booster. probability of skip dropout. If a dropout is skipped, new trees are added in the same manner as gbtree. [default=0.0] range: [0.0, 1.0]\n\n\nlambda_bias\nParameter of linear booster L2 regularization term on bias, default 0 (no L1 reg on bias because it is not important.)\n\n\ntree_limit\nLimit number of trees in the prediction; defaults to 0 (use all trees.)\n\n\nnum_round\nThe number of rounds for boosting.\n\n\nnum_workers\nnumber of workers used to train xgboost model. Defaults to 1.\n\n\nnthread\nNumber of threads used by per worker. Defaults to 1.\n\n\nuse_external_memory\nThe tree construction algorithm used in XGBoost. options: ‘auto’, ‘exact’, ‘approx’ [default=‘auto’]\n\n\nsilent\n0 means printing running messages, 1 means silent mode. default: 0\n\n\ncustom_obj\nCustomized objective function provided by user. Currently unsupported.\n\n\ncustom_eval\nCustomized evaluation function provided by user. Currently unsupported.\n\n\nmissing\nThe value treated as missing. default: Float.NaN\n\n\nseed\nRandom seed for the C++ part of XGBoost and train/test splitting.\n\n\ntimeout_request_workers\nthe maximum time to wait for the job requesting new workers. default: 30 minutes\n\n\ncheckpoint_path\nThe hdfs folder to load and save checkpoint boosters.\n\n\ncheckpoint_interval\nParam for set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the trained model will get checkpointed every 10 iterations. Note: checkpoint_path must also be set if the checkpoint interval is greater than 0.\n\n\nobjective\nSpecify the learning task and the corresponding learning objective. options: reg:linear, reg:logistic, binary:logistic, binary:logitraw, count:poisson, multi:softmax, multi:softprob, rank:pairwise, reg:gamma. default: reg:linear.\n\n\nbase_score\nParam for initial prediction (aka base margin) column name. Defaults to 0.5.\n\n\ntrain_test_ratio\nFraction of training points to use for testing.\n\n\nnum_early_stopping_rounds\nIf non-zero, the training will be stopped after a specified number of consecutive increases in any evaluation metric.\n\n\nobjective_type\nThe learning objective type of the specified custom objective and eval. Corresponding type will be assigned if custom objective is defined options: regression, classification.\n\n\neval_metric\nEvaluation metrics for validation data, a default metric will be assigned according to objective(rmse for regression, and error for classification, mean average precision for ranking). options: rmse, mae, logloss, error, merror, mlogloss, auc, aucpr, ndcg, map, gamma-deviance\n\n\nmaximize_evaluation_metrics\nWhether to maximize evaluation metrics. Defaults to FALSE (for minization.)\n\n\nbase_margin_col\nParam for initial prediction (aka base margin) column name.\n\n\nweight_col\nWeight column.\n\n\nfeatures_col\nFeatures column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by ft_r_formula.\n\n\nlabel_col\nLabel column name. The column should be a numeric column. Usually this column is output by ft_r_formula.\n\n\nprediction_col\nPrediction column name.\n\n\nuid\nA character string used to uniquely identify the ML estimator.\n\n\n…\nOptional arguments; see Details."
  },
  {
    "objectID": "packages/sparkxgb/reference/xgboost_regressor.html#details",
    "href": "packages/sparkxgb/reference/xgboost_regressor.html#details",
    "title": "sparklyr",
    "section": "Details",
    "text": "Details\nWhen x is a tbl_spark and formula (alternatively, response and features) is specified, the function returns a ml_model object wrapping a ml_pipeline_model which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. For classification, an optional argument predicted_label_col (defaults to \"predicted_label\") can be used to specify the name of the predicted label column. In addition to the fitted ml_pipeline_model, ml_model objects also contain a ml_pipeline object where the ML predictor stage is an estimator ready to be fit against data. This is utilized by ml_save with type = \"pipeline\" to faciliate model refresh workflows."
  },
  {
    "objectID": "packages/sparkxgb/reference/xgboost_regressor.html#value",
    "href": "packages/sparkxgb/reference/xgboost_regressor.html#value",
    "title": "sparklyr",
    "section": "Value",
    "text": "Value\nThe object returned depends on the class of x.\n\nspark_connection: When x is a spark_connection, the function returns an instance of a ml_estimator object. The object contains a pointer to a Spark Predictor object and can be used to compose Pipeline objects.\nml_pipeline: When x is a ml_pipeline, the function returns a ml_pipeline with the predictor appended to the pipeline.\ntbl_spark: When x is a tbl_spark, a predictor is constructed then immediately fit with the input tbl_spark, returning a prediction model.\ntbl_spark, with formula: specified When formula is specified, the input tbl_spark is first transformed using a RFormula transformer before being fit by the predictor. The object returned in this case is a ml_model which is a wrapper of a ml_pipeline_model."
  },
  {
    "objectID": "packages/sparkxgb/reference/xgboost_regressor.html#see-also",
    "href": "packages/sparkxgb/reference/xgboost_regressor.html#see-also",
    "title": "sparklyr",
    "section": "See Also",
    "text": "See Also\nSee http://spark.apache.org/docs/latest/ml-classification-regression.html for more information on the set of supervised learning algorithms."
  },
  {
    "objectID": "guides/index.html",
    "href": "guides/index.html",
    "title": "Guides",
    "section": "",
    "text": "Data Lakes"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "sparklyr: An interface for Apache Spark",
    "section": "",
    "text": "Connect to Spark from R. The sparklyr package provides a complete dplyr backend.\nFilter and aggregate Spark datasets  then bring them into R for analysis and visualization.\nUse Spark’s distributed machine learning library from R.\nCreate extensions that call the full  Spark API and provide interfaces to  Spark packages."
  },
  {
    "objectID": "guides/streaming.html#the-sparklyr-interface-1",
    "href": "guides/streaming.html#the-sparklyr-interface-1",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "The sparklyr interface",
    "text": "The sparklyr interface\nAs stated in the Spark’s official site, Spark Streaming makes it easy to build scalable fault-tolerant streaming applications. Because is part of the Spark API, it is possible to re-use query code that queries the current state of the stream, as well as joining the streaming data with historical data. Please see Spark’s official documentation for a deeper look into Spark Streaming.\nThe sparklyr interface provides the following:\n\nAbility to run dplyr, SQL, spark_apply(), and PipelineModels against a stream\nRead in multiple formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc\nWrite stream results to Spark memory and the following file formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc\nAn out-of-the box graph visualization to monitor the stream\nA new reactiveSpark() function, that allows Shiny apps to poll the contents of the stream create Shiny apps that are able to read the contents of the stream"
  },
  {
    "objectID": "guides/streaming.html#interacting-with-a-stream-1",
    "href": "guides/streaming.html#interacting-with-a-stream-1",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Interacting with a stream",
    "text": "Interacting with a stream\nA good way of looking at the way how Spark streams update is as a three stage operation:\n\nInput - Spark reads the data inside a given folder. The folder is expected to contain multiple data files, with new files being created containing the most current stream data.\nProcessing - Spark applies the desired operations on top of the data. These operations could be data manipulations (dplyr, SQL), data transformations (sdf operations, PipelineModel predictions), or native R manipulations (spark_apply()).\nOutput - The results of processing the input files are saved in a different folder.\n\nIn the same way all of the read and write operations in sparklyr for Spark Standalone, or in sparklyr’s local mode, the input and output folders are actual OS file system folders. For Hadoop clusters, these will be folder locations inside the HDFS."
  },
  {
    "objectID": "guides/streaming.html#example-1---inputoutput-1",
    "href": "guides/streaming.html#example-1---inputoutput-1",
    "title": "Intro to Spark Streaming with sparklyr",
    "section": "Example 1 - Input/Output",
    "text": "Example 1 - Input/Output\nThe first intro example is a small script that can be used with a local master. The result should be to see the stream_view() app showing live the number of records processed for each iteration of test data being sent to the stream.\n\nlibrary(future)\nlibrary(sparklyr)\n\nsc <- spark_connect(master = \"local\")\n\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n\nstream_generate_test(iterations = 1)\nread_folder <- stream_read_csv(sc, \"source\") \nwrite_output <- stream_write_csv(read_folder, \"source-out\")\ninvisible(future(stream_generate_test(interval = 0.5)))\n\nstream_view(write_output)\n\n\n\n\n\n\n\nstream_stop(write_output)\nspark_disconnect(sc)\n\n\nCode breakdown\n\nOpen the Spark connection\n::: {.cell}\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\", spark_version = \"2.3.0\")\n:::\nOptional step. This resets the input and output folders. It makes it easier to run the code multiple times in a clean manner.\n::: {.cell}\nif(file.exists(\"source\")) unlink(\"source\", TRUE)\nif(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)\n:::\nProduces a single test file inside the “source” folder. This allows the “read” function to infer CSV file definition.\n::: {.cell}\nstream_generate_test(iterations = 1)\nlist.files(\"source\")\n:::\n[1] \"stream_1.csv\"\nPoints the stream reader to the folder where the streaming files will be placed. Since it is primed with a single CSV file, it will use as the expected layout of subsequent files. By default, stream_read_csv() creates a single integer variable data frame.\n::: {.cell}\nread_folder <- stream_read_csv(sc, \"source\")\n:::\nThe output writer is what starts the streaming job. It will start monitoring the input folder, and then write the new results in the “source-out” folder. So as new records stream in, new files will be created in the “source-out” folder. Since there are no operations on the incoming data at this time, the output files will have the same exact raw data as the input files. The only difference is that the files and sub folders within “source-out” will be structured how Spark structures data folders.\n::: {.cell}\nwrite_output <- stream_write_csv(read_folder, \"source-out\")\nlist.files(\"source-out\")\n::: [1] \"_spark_metadata\"                                     \"checkpoint\"  [3] \"part-00000-1f29719a-2314-40e1-b93d-a647a3d57154-c000.csv\"\nThe test generation function will run 100 files every 0.2 seconds. To run the tests “out-of-sync” with the current R session, the future package is used.\n::: {.cell}\nlibrary(future)\ninvisible(future(stream_generate_test(interval = 0.2, iterations = 100)))\n:::\nThe stream_view() function can be used before the 50 tests are complete because of the use of the future package. It will monitor the status of the job that write_output is pointing to and provide information on the amount of data coming into the “source” folder and going out into the “source-out” folder.\n::: {.cell}\nstream_view(write_output)\n:::\nThe monitor will continue to run even after the tests are complete. To end the experiment, stop the Shiny app and then use the following to stop the stream and close the Spark session.\n::: {.cell}\nstream_stop(write_output)\nspark_disconnect(sc)\n:::"
  },
  {
    "objectID": "guides/extensions.html#introduction",
    "href": "guides/extensions.html#introduction",
    "title": "Creating Extensions for sparklyr",
    "section": "Introduction",
    "text": "Introduction\nThe sparklyr package provides a dplyr interface to Spark DataFrames as well as an R interface to Spark’s distributed machine learning pipelines. However, since Spark is a general-purpose cluster computing system there are many other R interfaces that could be built (e.g. interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).\nThe facilities used internally by sparklyr for its dplyr and machine learning interfaces are available to extension packages. This guide describes how you can use these tools to create your own custom R interfaces to Spark.\n\nExamples\nHere’s an example of an extension function that calls the text file line counting function available via the SparkContext:\n\nlibrary(sparklyr)\ncount_lines <- function(sc, file) {\n  spark_context(sc) %>% \n    invoke(\"textFile\", file, 1L) %>% \n    invoke(\"count\")\n}\n\nThe count_lines function takes a spark_connection (sc) argument which enables it to obtain a reference to the SparkContext object, and in turn call the textFile().count() method.\nYou can use this function with an existing sparklyr connection as follows:\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\ncount_lines(sc, \"hdfs://path/data.csv\")\n\nHere are links to some additional examples of extension packages:\n\n\n\n\n\n\n\nPackage\nDescription\n\n\n\n\nspark.sas7bdat\nRead in SAS data in parallel into Apache Spark.\n\n\nrsparkling\nExtension for using H2O machine learning algorithms against Spark Data Frames.\n\n\nsparkhello\nSimple example of including a custom JAR file within an extension package.\n\n\nrddlist\nImplements some methods of an R list as a Spark RDD (resilient distributed dataset).\n\n\nsparkwarc\nLoad WARC files into Apache Spark with sparklyr.\n\n\nsparkavro\nLoad Avro data into Spark with sparklyr. It is a wrapper of spark-avro\n\n\ncrassy\nConnect to Cassandra with sparklyr using the Spark-Cassandra-Connector.\n\n\nsparklygraphs\nR interface for GraphFrames which aims to provide the functionality of GraphX.\n\n\nsparklyr.nested\nExtension for working with nested data.\n\n\nsparklyudf\nSimple example registering an Scala UDF within an extension package.\n\n\nmleap\nR Interface to MLeap.\n\n\nsparkbq\nSparklyr extension package to connect to Google BigQuery.\n\n\nsparkgeo\nSparklyr extension package providing geospatial analytics capabilities.\n\n\nsparklytd\nSpaklyr plugin for td-spark to connect TD from R.\n\n\nsparkts\nExtensions for the spark-timeseries framework.\n\n\nsparkxgb\nR interface for XGBoost on Spark.\n\n\nsparktf\nR interface to Spark TensorFlow Connector.\n\n\ngeospark\nR interface to GeoSpark to perform spatial analysis in Spark.\n\n\nmmlspark\nMicrosoft Machine Learning for Apache Spark."
  },
  {
    "objectID": "guides/extensions.html#core-types",
    "href": "guides/extensions.html#core-types",
    "title": "Creating Extensions for sparklyr",
    "section": "Core Types",
    "text": "Core Types\nThree classes are defined for representing the fundamental types of the R to Java bridge:\n\n\n\nFunction\nDescription\n\n\n\n\nspark_connection\nConnection between R and the Spark shell process\n\n\nspark_jobj\nInstance of a remote Spark object\n\n\nspark_dataframe\nInstance of a remote Spark DataFrame object\n\n\n\nS3 methods are defined for each of these classes so they can be easily converted to or from objects that contain or wrap them. Note that for any given spark_jobj it’s possible to discover the underlying spark_connection."
  },
  {
    "objectID": "guides/extensions.html#calling-spark-from-r",
    "href": "guides/extensions.html#calling-spark-from-r",
    "title": "Creating Extensions for sparklyr",
    "section": "Calling Spark from R",
    "text": "Calling Spark from R\nThere are several functions available for calling the methods of Java objects and static methods of Java classes:\n\n\n\nFunction\nDescription\n\n\n\n\ninvoke\nCall a method on an object\n\n\ninvoke_new\nCreate a new object by invoking a constructor\n\n\ninvoke_static\nCall a static method on an object\n\n\n\nFor example, to create a new instance of the java.math.BigInteger class and then call the longValue() method on it you would use code like this:\n\nbillionBigInteger <- invoke_new(sc, \"java.math.BigInteger\", \"1000000000\")\nbillion <- invoke(billionBigInteger, \"longValue\")\n\nNote the sc argument: that’s the spark_connection object which is provided by the front-end package (e.g. sparklyr).\nThe previous example can be re-written to be more compact and clear using magrittr pipes:\n\nbillion <- sc %>% \n  invoke_new(\"java.math.BigInteger\", \"1000000000\") %>%\n    invoke(\"longValue\")\n\nThis syntax is similar to the method-chaining syntax often used with Scala code so is generally preferred.\nCalling a static method of a class is also straightforward. For example, to call the Math::hypot() static function you would use this code:\n\nhypot <- sc %>% \n  invoke_static(\"java.lang.Math\", \"hypot\", 10, 20)"
  },
  {
    "objectID": "guides/extensions.html#wrapper-functions",
    "href": "guides/extensions.html#wrapper-functions",
    "title": "Creating Extensions for sparklyr",
    "section": "Wrapper Functions",
    "text": "Wrapper Functions\nCreating an extension typically consists of writing R wrapper functions for a set of Spark services. In this section we’ll describe the typical form of these functions as well as how to handle special types like Spark DataFrames.\nHere’s the wrapper function for textFile().count() which we defined earlier:\n\ncount_lines <- function(sc, file) {\n  spark_context(sc) %>% \n    invoke(\"textFile\", file, 1L) %>% \n      invoke(\"count\")\n}\n\nThe count_lines function takes a spark_connection (sc) argument which enables it to obtain a reference to the SparkContext object, and in turn call the textFile().count() method.\nThe following functions are useful for implementing wrapper functions of various kinds:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nspark_connection\nGet the Spark connection associated with an object (S3)\n\n\nspark_jobj\nGet the Spark jobj associated with an object (S3)\n\n\nspark_dataframe\nGet the Spark DataFrame associated with an object (S3)\n\n\nspark_context\nGet the SparkContext for a spark_connection\n\n\nhive_context\nGet the HiveContext for a spark_connection\n\n\nspark_version\nGet the version of Spark (as a numeric_version) for a spark_connection\n\n\n\nThe use of these functions is illustrated in this simple example:\n\nanalyze <- function(x, features) {\n  \n  # normalize whatever we were passed (e.g. a dplyr tbl) into a DataFrame\n  df <- spark_dataframe(x)\n  \n  # get the underlying connection so we can create new objects\n  sc <- spark_connection(df)\n  \n  # create an object to do the analysis and call its `analyze` and `summary`\n  # methods (note that the df and features are passed to the analyze function)\n  summary <- sc %>%  \n    invoke_new(\"com.example.tools.Analyzer\") %>% \n      invoke(\"analyze\", df, features) %>% \n      invoke(\"summary\")\n\n  # return the results\n  summary\n}\n\nThe first argument is an object that can be accessed using the Spark DataFrame API (this might be an actual reference to a DataFrame or could rather be a dplyr tbl which has a DataFrame reference inside it).\nAfter using the spark_dataframe function to normalize the reference, we extract the underlying Spark connection associated with the data frame using the spark_connection function. Finally, we create a new Analyzer object, call it’s analyze method with the DataFrame and list of features, and then call the summary method on the results of the analysis.\nAccepting a spark_jobj or spark_dataframe as the first argument of a function makes it very easy to incorporate into magrittr pipelines so this pattern is highly recommended when possible."
  },
  {
    "objectID": "guides/extensions.html#dependencies",
    "href": "guides/extensions.html#dependencies",
    "title": "Creating Extensions for sparklyr",
    "section": "Dependencies",
    "text": "Dependencies\nWhen creating R packages which implement interfaces to Spark you may need to include additional dependencies. Your dependencies might be a set of Spark Packages or might be a custom JAR file. In either case, you’ll need a way to specify that these dependencies should be included during the initialization of a Spark session. A Spark dependency is defined using the spark_dependency function:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nspark_dependency\nDefine a Spark dependency consisting of JAR files and Spark packages\n\n\n\nYour extension package can specify it’s dependencies by implementing a function named spark_dependencies within the package (this function should not be publicly exported). For example, let’s say you were creating an extension package named sparkds that needs to include a custom JAR as well as the Redshift and Apache Avro packages:\n\nspark_dependencies <- function(spark_version, scala_version, ...) {\n  spark_dependency(\n    jars = c(\n      system.file(\n        sprintf(\"java/sparkds-%s-%s.jar\", spark_version, scala_version), \n        package = \"sparkds\"\n      )\n    ),\n    packages = c(\n      sprintf(\"com.databricks:spark-redshift_%s:0.6.0\", scala_version),\n      sprintf(\"com.databricks:spark-avro_%s:2.0.1\", scala_version)\n    )\n  )\n}\n\n.onLoad <- function(libname, pkgname) {\n  sparklyr::register_extension(pkgname)\n}\n\nThe spark_version argument is provided so that a package can support multiple Spark versions for it’s JARs. Note that the argument will include just the major and minor versions (e.g. 1.6 or 2.0) and will not include the patch level (as JARs built for a given major/minor version are expected to work for all patch levels).\nThe scala_version argument is provided so that a single package can support multiple Scala compiler versions for it’s JARs and packages (currently Scala 1.6 downloadable binaries are compiled with Scala 2.10 and Scala 2.0 downloadable binaries are compiled with Scala 2.11).\nThe ... argument is unused but nevertheless should be included to ensure compatibility if new arguments are added to spark_dependencies in the future.\nThe .onLoad function registers your extension package so that it’s spark_dependencies function will be automatically called when new connections to Spark are made via spark_connect:\n\nlibrary(sparklyr)\nlibrary(sparkds)\nsc <- spark_connect(master = \"local\")\n\n\nCompiling JARs\nThe sparklyr package includes a utility function (compile_package_jars) that will automatically compile a JAR file from your Scala source code for the required permutations of Spark and Scala compiler versions. To use the function just invoke it from the root directory of your R package as follows:\n\nsparklyr::compile_package_jars()\n\nNote that a prerequisite to calling compile_package_jars is the installation of the Scala 2.10 and 2.11 compilers to one of the following paths:\n\n/opt/scala\n/opt/local/scala\n/usr/local/scala\n~/scala (Windows-only)\n\nSee the sparkhello repository for a complete example of including a custom JAR within an extension package.\n\nCRAN\nWhen including a JAR file within an R package distributed on CRAN, you should follow the guidelines provided in Writing R Extensions:\n\nJava code is a special case: except for very small programs, .java files should be byte-compiled (to a .class file) and distributed as part of a .jar file: the conventional location for the .jar file(s) is inst/java. It is desirable (and required under an Open Source license) to make the Java source files available: this is best done in a top-level java directory in the package – the source files should not be installed."
  },
  {
    "objectID": "guides/extensions.html#data-types",
    "href": "guides/extensions.html#data-types",
    "title": "Creating Extensions for sparklyr",
    "section": "Data Types",
    "text": "Data Types\nThe ensure_* family of functions can be used to enforce specific data types that are passed to a Spark routine. For example, Spark routines that require an integer will not accept an R numeric element. Use these functions ensure certain parameters are scalar integers, or scalar doubles, and so on.\n\nensure_scalar_integer\nensure_scalar_double\nensure_scalar_boolean\nensure_scalar_character\n\nIn order to match the correct data types while calling Scala code from R, or retrieving results from Scala back to R, consider the following types mapping:\n\n\n\nFrom R\nScala\nTo R\n\n\n\n\nNULL\nvoid\nNULL\n\n\ninteger\nInt\ninteger\n\n\ncharacter\nString\ncharacter\n\n\nlogical\nBoolean\nlogical\n\n\ndouble\nDouble\ndouble\n\n\nnumeric\nDouble\ndouble\n\n\n\nFloat\ndouble\n\n\n\nDecimal\ndouble\n\n\n\nLong\ndouble\n\n\nraw\nArray[Byte]\nraw\n\n\nDate\nDate\nDate\n\n\nPOSIXlt\nTime\n\n\n\nPOSIXct\nTime\nPOSIXct\n\n\nlist\nArray[T]\nlist\n\n\nenvironment\nMap[String, T]\n\n\n\njobj\nObject\njobj"
  },
  {
    "objectID": "guides/extensions.html#compiling",
    "href": "guides/extensions.html#compiling",
    "title": "Creating Extensions for sparklyr",
    "section": "Compiling",
    "text": "Compiling\nMost Spark extensions won’t need to define their own compilation specification, and can instead rely on the default behavior of compile_package_jars. For users who would like to take more control over where the scalac compilers should be looked up, use the spark_compilation_spec fucnction. The Spark compilation specification is used when compiling Spark extension Java Archives, and defines which versions of Spark, as well as which versions of Scala, should be used for compilation."
  },
  {
    "objectID": "guides/deployment.html#deployment",
    "href": "guides/deployment.html#deployment",
    "title": "Deployment and Configuration",
    "section": "Deployment",
    "text": "Deployment\nThere are two well supported deployment modes for sparklyr:\n\nLocal — Working on a local desktop typically with smaller/sampled datasets\nCluster — Working directly within or alongside a Spark cluster (standalone, YARN, Mesos, etc.)\n\n\nLocal Deployment\nLocal mode is an excellent way to learn and experiment with Spark. Local mode also provides a convenient development environment for analyses, reports, and applications that you plan to eventually deploy to a multi-node Spark cluster.\nTo work in local mode you should first install a version of Spark for local use. You can do this using the spark_install function, for example:\n\nsparklyr::spark_install(version = \"2.1.0\")\n\nTo connect to the local Spark instance you pass “local” as the value of the Spark master node to spark_connect:\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"local\")\n\nFor the local development scenario, see the Configuration section below for details on how to have the same code work seamlessly in both development and production environments.\n\n\nCluster Deployment\nA common deployment strategy is to submit your application from a gateway machine that is physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster). In this setup, client mode is appropriate. In client mode, the driver is launched directly within the spark-submit process which acts as a client to the cluster. The input and output of the application is attached to the console. Thus, this mode is especially suitable for applications that involve the REPL (e.g. Spark shell). For more information see Submitting Applications.\nTo use spaklyr with a Spark cluster you should locate your R session on a machine that is either directly on one of the cluster nodes or is close to the cluster (for networking performance). In the case where R is not running directly on the cluster you should also ensure that the machine has a Spark version and configuration identical to that of the cluster nodes.\nThe most straightforward way to run R within or near to the cluster is either a remote SSH session or via RStudio Server.\nIn cluster mode you use the version of Spark already deployed on the cluster node. This version is located via the SPARK_HOME environment variable, so you should be sure that this variable is correctly defined on your server before attempting a connection. This would typically be done within the Renviron.site configuration file. For example:\nSPARK_HOME=/opt/spark/spark-2.0.0-bin-hadoop2.6\nTo connect, pass the address of the master node to spark_connect, for example:\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"spark://local:7077\")\n\nFor a Hadoop YARN cluster, you can connect using the YARN master, for example:\n\nlibrary(sparklyr)\nsc <- spark_connect(master = \"yarn-client\")\n\nIf you are running on EC2 using the Spark EC2 deployment scripts then you can read the master from /root/spark-ec2/cluster-url, for example:\n\nlibrary(sparklyr)\ncluster_url <- system('cat /root/spark-ec2/cluster-url', intern=TRUE)\nsc <- spark_connect(master = cluster_url)\n\n\n\nLivy Connections\nLivy, “An Open Source REST Service for Apache Spark (Apache License)” , is available starting in sparklyr 0.5 as an experimental feature. Among many scenarios, this enables connections from the RStudio desktop to Apache Spark when Livy is available and correctly configured in the remote cluster.\nTo work with Livy locally, sparklyr supports livy_install() which installs Livy in your local environment, this is similar to spark_install(). Since Livy is a service to enable remote connections into Apache Spark, the service needs to be started with livy_service_start(). Once the service is running, spark_connect() needs to reference the running service and use method = \"Livy\", then sparklyr can be used as usual. A short example follows:\n\nlivy_install()\nlivy_service_start()\n\nsc <- spark_connect(master = \"http://localhost:8998\", method = \"livy\")\ncopy_to(sc, iris)\n\nspark_disconnect(sc)\nlivy_service_stop()\n\n\n\nConnection Tools\nYou can view the Spark web UI via the spark_web function, and view the Spark log via the spark_log function:\n\nspark_web(sc)\nspark_log(sc)\n\nYou can disconnect from Spark using the spark_disconnect function:\n\nspark_disconnect(sc)\n\n\n\nCollect\nThe collect function transfers data from Spark into R. The data are collected from a cluster environment and transfered into local R memory. In the process, all data is first transfered from executor nodes to the driver node. Therefore, the driver node must have enough memory to collect all the data.\nCollecting data on the driver node is relatively slow. The process also inflates the data as it moves from the executor nodes to the driver node. Caution should be used when collecting large data.\nThe following parameters could be adjusted to avoid OutOfMemory and Timeout errors:\n\nspark.executor.heartbeatInterval\nspark.network.timeout\nspark.driver.extraJavaOptions\nspark.driver.memory\nspark.yarn.driver.memoryOverhead\nspark.driver.maxResultSize"
  },
  {
    "objectID": "guides/deployment.html#configuration",
    "href": "guides/deployment.html#configuration",
    "title": "Deployment and Configuration",
    "section": "Configuration",
    "text": "Configuration\nThis section describes the various options available for configuring both the behavior of the sparklyr package as well as the underlying Spark cluster. Creating multiple configuration profiles (e.g. development, test, production) is also covered.\n\nConfig Files\nThe configuration for a Spark connection is specified via the config parameter of the spark_connect function. By default the configuration is established by calling the spark_config function. This code represents the default behavior:\n\nspark_connect(master = \"local\", config = spark_config())\n\nBy default the spark_config function reads configuration data from a file named config.yml located in the current working directory (or in parent directories if not located in the working directory). This file is not required and only need be provided for overriding default behavior. You can also specify an alternate config file name and/or location.\nThe config.yml file is in turn processed using the config package, which enables support for multiple named configuration profiles.\n\n\nPackage Options\nThere are a number of options available to configure the behavior of the sparklyr package:\nFor example, this configuration file sets the number of local cores to 4 and the amount of memory allocated for the Spark driver to 4G:\ndefault:\n  sparklyr.cores.local: 4\n  sparklyr.shell.driver-memory: 4G\nNote that the use of default will be explained below in Multiple Profiles.\n\nSpark\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nsparklyr.shell.*\nCommand line parameters to pass to spark-submit. For example, sparklyr.shell.executor-memory: 20G configures --executor-memory 20G (see the Spark documentation for details on supported options).\n\n\n\n\n\nRuntime\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nsparklyr.cores.local\nNumber of cores to use when running in local mode (defaults to parallel::detectCores).\n\n\nsparklyr.sparkui.url\nConfigures the url to the Spark UI web interface when calling spark_web.\n\n\nsparklyr.defaultPackages\nList of default Spark packages to install in the cluster (defaults to “com.databricks:spark-csv_2.11:1.3.0” and “com.amazonaws:aws-java-sdk-pom:1.10.34”).\n\n\nsparklyr.sanitize.column.names\nAllows Spark to automatically rename column names to conform to Spark naming restrictions.\n\n\n\n\n\nDiagnostics\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nsparklyr.backend.threads\nNumber of threads to use in the sparklyr backend to process incoming connections form the sparklyr client.\n\n\nsparklyr.app.jar\nThe application jar to be submitted in Spark submit.\n\n\nsparklyr.ports.file\nPath to the ports file used to share connection information to the sparklyr backend.\n\n\nsparklyr.ports.wait.seconds\nNumber of seconds to wait while for the Spark connection to initialize.\n\n\nsparklyr.verbose\nProvide additional feedback while performing operations. Currently used to communicate which column names are being sanitized in sparklyr.sanitize.column.names.\n\n\n\n\n\n\nSpark Options\nYou can also use config.yml to specify arbitrary Spark configuration properties:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nspark.*\nConfiguration settings for the Spark context (applied by creating a SparkConf containing the specified properties). For example, spark.executor.memory: 1g configures the memory available in each executor (see Spark Configuration for additional options.)\n\n\nspark.sql.*\nConfiguration settings for the Spark SQL context (applied using SET). For instance, spark.sql.shuffle.partitions configures number of partitions to use while shuffling (see SQL Programming Guide for additional options).\n\n\n\nFor example, this configuration file sets a custom scratch directory for Spark and specifies 100 as the number of partitions to use when shuffling data for joins or aggregations:\ndefault:\n  spark.local.dir: /tmp/spark-scratch\n  spark.sql.shuffle.partitions: 100\n\n\nUser Options\nYou can also include arbitrary custom user options within the config.yml file. These can be named anything you like so long as they do not use either spark or sparklyr as a prefix. For example, this configuration file defines dataset and sample-size options:\ndefault:\n  dataset: \"observations.parquet\"\n  sample-size: 10000\n\n\nMultiple Profiles\nThe config package enables the definition of multiple named configuration profiles for different environments (e.g. default, test, production). All environments automatically inherit from the default environment and can optionally also inherit from each other.\nFor example, you might want to use a distinct datasets for development and testing or might want to use custom Spark configuration properties that are only applied when running on a production cluster. Here’s how that would be expressed in config.yml:\ndefault:\n  dataset: \"observations-dev.parquet\"\n  sample-size: 10000\n\nproduction:\n  spark.memory.fraction: 0.9\n  spark.rdd.compress: true\n  dataset: \"observations.parquet\"\n  sample-size: null\nYou can also use this feature to specify distinct Spark master nodes for different environments, for example:\ndefault:\n  spark.master: \"local\"\n\nproduction:\n  spark.master: \"spark://local:7077\"\nWith this configuration, you can omit the master argument entirely from the call to spark_connect:\n\nsc <- spark_connect()\n\nNote that the currently active configuration is determined via the value of R_CONFIG_ACTIVE environment variable. See the config package documentation for additional details.\n\n\nTuning\nIn general, you will need to tune a Spark cluster for it to perform well. Spark applications tend to consume a lot of resources. There are many knobs to control the performance of Yarn and executor (i.e. worker) nodes in a cluster. Some of the parameters to pay attention to are as follows:\n\nspark.executor.heartbeatInterval\nspark.network.timeout\nspark.executor.extraJavaOptions\nspark.executor.memory\nspark.yarn.executor.memoryOverhead\nspark.executor.cores\nspark.executor.instances (if is not enabled)\n\n\nExample Config\nHere is an example spark configuration for an EMR cluster on AWS with 1 master and 2 worker nodes. Eache node has 8 vCPU and 61 GiB of memory.\n\n\n\nParameter\nValue\n\n\n\n\nspark.driver.extraJavaOptions\nappend -XX:MaxPermSize=30G\n\n\nspark.driver.maxResultSize\n0\n\n\nspark.driver.memory\n30G\n\n\nspark.yarn.driver.memoryOverhead\n4096\n\n\nspark.yarn.executor.memoryOverhead\n4096\n\n\nspark.executor.memory\n4G\n\n\nspark.executor.cores\n2\n\n\nspark.dynamicAllocation.maxExecutors\n15\n\n\n\nConfiguration parameters can be set in the config R object or can be set in the config.yml. Alternatively, they can be set in the spark-defaults.conf.\n\nConfiguration in R script\n\nconfig <- spark_config()\nconfig$spark.executor.cores <- 2\nconfig$spark.executor.memory <- \"4G\"\nsc <- spark_connect(master = \"yarn-client\", config = config, version = '2.0.0')\n\n\n\nConfiguration in YAML script\ndefault:\n  spark.executor.cores: 2\n  spark.executor.memory: 4G"
  },
  {
    "objectID": "guides/deployment.html#rstudio-server",
    "href": "guides/deployment.html#rstudio-server",
    "title": "Deployment and Configuration",
    "section": "RStudio Server",
    "text": "RStudio Server\nRStudio Server provides a web-based IDE interface to a remote R session, making it ideal for use as a front-end to a Spark cluster. This section covers some additional configuration options that are useful for RStudio Server.\n\nConnection Options\nThe RStudio IDE Spark pane provides a New Connection dialog to assist in connecting with both local instances of Spark and Spark clusters:\n\nYou can configure which connection choices are presented using the rstudio.spark.connections option. By default, users are presented with possibility of both local and cluster connections, however, you can modify this behavior to present only one of these, or even a specific Spark master URL. Some commonly used combinations of connection choices include:\n\n\n\n\n\n\n\nValue\nDescription\n\n\n\n\nc(\"local\", \"cluster\")\nDefault. Present connections to both local and cluster Spark instances.\n\n\n\"local\"\nPresent only connections to local Spark instances.\n\n\n\"spark://local:7077\"\nPresent only a connection to a specific Spark cluster.\n\n\nc(\"spark://local:7077\", \"cluster\")\nPresent a connection to a specific Spark cluster and other clusters.\n\n\n\nThis option should generally be set within Rprofile.site. For example:\n\noptions(rstudio.spark.connections = \"spark://local:7077\")\n\n\n\nSpark Installations\nIf you are running within local mode (as opposed to cluster mode) you may want to provide pre-installed Spark version(s) to be shared by all users of the server. You can do this by installing Spark versions within a shared directory (e.g. /opt/spark) then designating it as the Spark installation directory.\nFor example, after installing one or more versions of Spark to /opt/spark you would add the following to Rprofile.site:\n\noptions(spark.install.dir = \"/opt/spark\")\n\nIf this directory is read-only for ordinary users then RStudio will not offer installation of additional versions, which will help guide users to a version that is known to be compatible with versions of Spark deployed on clusters in the same organization."
  },
  {
    "objectID": "deployment/cloudera-aws.html#hive-data",
    "href": "deployment/cloudera-aws.html#hive-data",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Hive data",
    "text": "Hive data\nFor this demo, we have created and populated 3 tables in Hive. The table names are: flights, airlines and airports. Using Hue, we can see the loaded tables. For the links to the data files and their Hive import scripts please see Appendix A."
  },
  {
    "objectID": "deployment/cloudera-aws.html#cache-the-tables-into-memory",
    "href": "deployment/cloudera-aws.html#cache-the-tables-into-memory",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Cache the tables into memory",
    "text": "Cache the tables into memory\nUse tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame.\n\n# Cache flights Hive table into Spark\ntbl_cache(sc, 'flights')\nflights_tbl <- tbl(sc, 'flights')\n\n# Cache airlines Hive table into Spark\ntbl_cache(sc, 'airlines')\nairlines_tbl <- tbl(sc, 'airlines')\n\n# Cache airports Hive table into Spark\ntbl_cache(sc, 'airports')\nairports_tbl <- tbl(sc, 'airports')"
  },
  {
    "objectID": "deployment/cloudera-aws.html#create-a-model-data-set",
    "href": "deployment/cloudera-aws.html#create-a-model-data-set",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Create a model data set",
    "text": "Create a model data set\nFilter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight.\n\n# Filter records and create target variable 'gain'\nmodel_data <- flights_tbl %>%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>%\n  filter(depdelay > 15 & depdelay < 240) %>%\n  filter(arrdelay > -60 & arrdelay < 360) %>%\n  filter(year >= 2003 & year <= 2007) %>%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>%\n  mutate(gain = depdelay - arrdelay) %>%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain)\n\n# Summarize data by carrier\nmodel_data %>%\n  group_by(uniquecarrier) %>%\n  summarize(description = min(description), gain=mean(gain), \n            distance=mean(distance), depdelay=mean(depdelay)) %>%\n  select(description, gain, distance, depdelay) %>%\n  arrange(gain)\n\nSource:   query [?? x 4]\nDatabase: spark connection master=yarn-client app=sparklyr local=FALSE\n\n                    description       gain  distance depdelay\n                          <chr>      <dbl>     <dbl>    <dbl>\n1        ATA Airlines d/b/a ATA -5.5679651 1240.7219 61.84391\n2       Northwest Airlines Inc. -3.1134556  779.1926 48.84979\n3                     Envoy Air -2.2056576  437.0883 54.54923\n4             PSA Airlines Inc. -1.9267647  500.6955 55.60335\n5  ExpressJet Airlines Inc. (1) -1.5886314  537.3077 61.58386\n6               JetBlue Airways -1.3742524 1087.2337 59.80750\n7         SkyWest Airlines Inc. -1.1265678  419.6489 54.04198\n8          Delta Air Lines Inc. -0.9829374  956.9576 50.19338\n9        American Airlines Inc. -0.9631200 1066.8396 56.78222\n10  AirTran Airways Corporation -0.9411572  665.6574 53.38363\n# ... with more rows"
  },
  {
    "objectID": "deployment/cloudera-aws.html#train-a-linear-model",
    "href": "deployment/cloudera-aws.html#train-a-linear-model",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Train a linear model",
    "text": "Train a linear model\nPredict time gained or lost in flight as a function of distance, departure delay, and airline carrier.\n\n# Partition the data into training and validation sets\nmodel_partition <- model_data %>% \n  sdf_partition(train = 0.8, valid = 0.2, seed = 5555)\n\n# Fit a linear model\nml1 <- model_partition$train %>%\n  ml_linear_regression(gain ~ distance + depdelay + uniquecarrier)\n\n# Summarize the linear model\nsummary(ml1)\n\nCall: ml_linear_regression(., gain ~ distance + depdelay + uniquecarrier)\n\nDeviance Residuals: (approximate):\n     Min       1Q   Median       3Q      Max \n-302.343   -5.669    2.714    9.832  104.130 \n\nCoefficients:\n                    Estimate  Std. Error  t value  Pr(>|t|)    \n(Intercept)      -1.26566581  0.10385870 -12.1864 < 2.2e-16 ***\ndistance          0.00308711  0.00002404 128.4155 < 2.2e-16 ***\ndepdelay         -0.01397013  0.00028816 -48.4812 < 2.2e-16 ***\nuniquecarrier_AA -2.18483090  0.10985406 -19.8885 < 2.2e-16 ***\nuniquecarrier_AQ  3.14330242  0.29114487  10.7964 < 2.2e-16 ***\nuniquecarrier_AS  0.09210380  0.12825003   0.7182 0.4726598    \nuniquecarrier_B6 -2.66988794  0.12682192 -21.0523 < 2.2e-16 ***\nuniquecarrier_CO -1.11611186  0.11795564  -9.4621 < 2.2e-16 ***\nuniquecarrier_DL -1.95206198  0.11431110 -17.0767 < 2.2e-16 ***\nuniquecarrier_EV  1.70420830  0.11337215  15.0320 < 2.2e-16 ***\nuniquecarrier_F9 -1.03178176  0.15384863  -6.7065 1.994e-11 ***\nuniquecarrier_FL -0.99574060  0.12034738  -8.2739 2.220e-16 ***\nuniquecarrier_HA -1.16970713  0.34894788  -3.3521 0.0008020 ***\nuniquecarrier_MQ -1.55569040  0.10975613 -14.1741 < 2.2e-16 ***\nuniquecarrier_NW -3.58502418  0.11534938 -31.0797 < 2.2e-16 ***\nuniquecarrier_OH -1.40654797  0.12034858 -11.6873 < 2.2e-16 ***\nuniquecarrier_OO -0.39069404  0.11132164  -3.5096 0.0004488 ***\nuniquecarrier_TZ -7.26285217  0.34428509 -21.0955 < 2.2e-16 ***\nuniquecarrier_UA -0.56995737  0.11186757  -5.0949 3.489e-07 ***\nuniquecarrier_US -0.52000028  0.11218498  -4.6352 3.566e-06 ***\nuniquecarrier_WN  4.22838982  0.10629405  39.7801 < 2.2e-16 ***\nuniquecarrier_XE -1.13836940  0.11332176 -10.0455 < 2.2e-16 ***\nuniquecarrier_YV  3.17149538  0.11709253  27.0854 < 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nR-Squared: 0.02301\nRoot Mean Squared Error: 17.83"
  },
  {
    "objectID": "deployment/cloudera-aws.html#assess-model-performance",
    "href": "deployment/cloudera-aws.html#assess-model-performance",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Assess model performance",
    "text": "Assess model performance\nCompare the model performance using the validation data.\n\n# Calculate average gains by predicted decile\nmodel_deciles <- lapply(model_partition, function(x) {\n  sdf_predict(ml1, x) %>%\n    mutate(decile = ntile(desc(prediction), 10)) %>%\n    group_by(decile) %>%\n    summarize(gain = mean(gain)) %>%\n    select(decile, gain) %>%\n    collect()\n})\n\n# Create a summary dataset for plotting\ndeciles <- rbind(\n  data.frame(data = 'train', model_deciles$train),\n  data.frame(data = 'valid', model_deciles$valid),\n  make.row.names = FALSE\n)\n\n# Plot average gains by predicted decile\ndeciles %>%\n  ggplot(aes(factor(decile), gain, fill = data)) +\n  geom_bar(stat = 'identity', position = 'dodge') +\n  labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes')"
  },
  {
    "objectID": "deployment/cloudera-aws.html#visualize-predictions",
    "href": "deployment/cloudera-aws.html#visualize-predictions",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Visualize predictions",
    "text": "Visualize predictions\nCompare actual gains to predicted gains for an out of time sample.\n\n# Select data from an out of time sample\ndata_2008 <- flights_tbl %>%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>%\n  filter(depdelay > 15 & depdelay < 240) %>%\n  filter(arrdelay > -60 & arrdelay < 360) %>%\n  filter(year == 2008) %>%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>%\n  mutate(gain = depdelay - arrdelay) %>%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest)\n\n# Summarize data by carrier\ncarrier <- sdf_predict(ml1, data_2008) %>%\n  group_by(description) %>%\n  summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>%\n  filter(freq > 10000) %>%\n  collect\n\n# Plot actual gains and predicted gains by airline carrier\nggplot(carrier, aes(gain, prediction)) + \n  geom_point(alpha = 0.75, color = 'red', shape = 3) +\n  geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') +\n  geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) +\n  labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted')\n\n\n\n\nSome carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights."
  },
  {
    "objectID": "deployment/cloudera-aws.html#build-dashboard",
    "href": "deployment/cloudera-aws.html#build-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Build dashboard",
    "text": "Build dashboard\nAggregate the scored data by origin, destination, and airline. Save the aggregated data.\n\n# Summarize by origin, destination, and carrier\nsummary_2008 <- sdf_predict(ml1, data_2008) %>%\n  rename(carrier = uniquecarrier, airline = description) %>%\n  group_by(origin, dest, carrier, airline) %>%\n  summarize(\n    flights = n(),\n    distance = mean(distance),\n    avg_dep_delay = mean(depdelay),\n    avg_arr_delay = mean(arrdelay),\n    avg_gain = mean(gain),\n    pred_gain = mean(prediction)\n    )\n\n# Collect and save objects\npred_data <- collect(summary_2008)\nairports <- collect(select(airports_tbl, name, faa, lat, lon))\nml1_summary <- capture.output(summary(ml1))\nsave(pred_data, airports, ml1_summary, file = 'flights_pred_2008.RData')"
  },
  {
    "objectID": "deployment/cloudera-aws.html#publish-dashboard",
    "href": "deployment/cloudera-aws.html#publish-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Publish dashboard",
    "text": "Publish dashboard\nUse the saved data to build an R Markdown flexdashboard. Publish the flexdashboard\n\n#Appendix\n\nAppendix A - Data files\nRun the following script to download data from the web onto your master node. Download the yearly flight data and the airlines lookup table.\n\n# Make download directory\nmkdir /tmp/flights\n\n# Download flight data by year\nfor i in {2006..2008}\n  do\n    echo \"$(date) $i Download\"\n    fnam=$i.csv.bz2\n    wget -O /tmp/flights/$fnam http://stat-computing.org/dataexpo/2009/$fnam\n    echo \"$(date) $i Unzip\"\n    bunzip2 /tmp/flights/$fnam\n  done\n\n# Download airline carrier data\nwget -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS\n\n# Download airports data\nwget -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\n\n\n\nHive tables\nWe used the Hue interface, logged in as ‘admin’ to load the data into HDFS and then into Hive.\nCREATE EXTERNAL TABLE IF NOT EXISTS flights\n(\nyear int,\nmonth int,\ndayofmonth int,\ndayofweek int,\ndeptime int,\ncrsdeptime int,\narrtime int, \ncrsarrtime int,\nuniquecarrier string,\nflightnum int,\ntailnum string, \nactualelapsedtime int,\ncrselapsedtime int,\nairtime string,\narrdelay int,\ndepdelay int, \norigin string,\ndest string,\ndistance int,\ntaxiin string,\ntaxiout string,\ncancelled int,\ncancellationcode string,\ndiverted int,\ncarrierdelay string,\nweatherdelay string,\nnasdelay string,\nsecuritydelay string,\nlateaircraftdelay string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLINES TERMINATED BY '\\n'\nTBLPROPERTIES(\"skip.header.line.count\"=\"1\");\nLOAD DATA INPATH '/user/admin/flights/2006.csv/' INTO TABLE flights;\nLOAD DATA INPATH '/user/admin/flights/2007.csv/' INTO TABLE flights;\nLOAD DATA INPATH '/user/admin/flights/2008.csv/' INTO TABLE flights;\n# Create metadata for airlines\nCREATE EXTERNAL TABLE IF NOT EXISTS airlines\n(\nCode string,\nDescription string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE\ntblproperties(\"skip.header.line.count\"=\"1\");\nLOAD DATA INPATH '/user/admin/L_UNIQUE_CARRIERS.csv' INTO TABLE airlines;\nCREATE EXTERNAL TABLE IF NOT EXISTS airports\n(\nid string,\nname string,\ncity string,\ncountry string,\nfaa string,\nicao string,\nlat double,\nlon double,\nalt int,\ntz_offset double,\ndst string,\ntz_name string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE;\nLOAD DATA INPATH '/user/admin/airports.dat' INTO TABLE airports;"
  },
  {
    "objectID": "deployment/databricks-cluster-odbc.html#overview",
    "href": "deployment/databricks-cluster-odbc.html#overview",
    "title": "Using an ODBC connection with Databricks",
    "section": "Overview",
    "text": "Overview\nThis configuration details how to connect to Databricks using an ODBC connection. With this setup, R can connect to Databricks using the odbc and DBI R packages. This type of configuration is the recommended approach for connecting to Databricks from RStudio Connect and can also be used from RStudio Workbench."
  },
  {
    "objectID": "deployment/databricks-cluster-odbc.html#advantages-and-limitations",
    "href": "deployment/databricks-cluster-odbc.html#advantages-and-limitations",
    "title": "Using an ODBC connection with Databricks",
    "section": "Advantages and limitations",
    "text": "Advantages and limitations\nAdvantages:\n\nODBC connections tend to be more stable than Spark connections. This is especially beneficial for content published to RStudio Connect.\nIf code is developed using a Spark connection and sparklyr, it is easy to swap out the connection type for an ODBC connection and the remaining code will still run.\nThe Spark ODBC driver provided by Databricks was benchmarked against a native Spark connection and the performance of the two is very comparable.\n\nLimitations: - Not all Spark features and functions are available through an ODBC connection."
  },
  {
    "objectID": "deployment/databricks-cluster-odbc.html#driver-installation",
    "href": "deployment/databricks-cluster-odbc.html#driver-installation",
    "title": "Using an ODBC connection with Databricks",
    "section": "Driver installation",
    "text": "Driver installation\nDownload and install the Spark ODBC driver from Databricks"
  },
  {
    "objectID": "deployment/databricks-cluster-odbc.html#driver-configuration",
    "href": "deployment/databricks-cluster-odbc.html#driver-configuration",
    "title": "Using an ODBC connection with Databricks",
    "section": "Driver configuration",
    "text": "Driver configuration\nCreate a DSN for Databricks.\n[Databricks-Spark]\nDriver=Simba\nServer=<server-hostname>\nHOST=<server-hostname>\nPORT=<port>\nSparkServerType=3\nSchema=default\nThriftTransport=2\nSSL=1\nAuthMech=3\nUID=token\nPWD=<personal-access-token>\nHTTPPath=<http-path>"
  },
  {
    "objectID": "deployment/databricks-cluster-odbc.html#connect-to-databricks",
    "href": "deployment/databricks-cluster-odbc.html#connect-to-databricks",
    "title": "Using an ODBC connection with Databricks",
    "section": "Connect to Databricks",
    "text": "Connect to Databricks\nThe connection can be tested from the command line using isql -v Databricks-Spark where Databricks-Spark is the DSN name for the connection. If that connects successfully, then the following code can be used to create a connection from an R session:\nlibrary(DBI)\nlibrary(odbc)\n\ncon <- dbConnect(odbc(), \"Databricks-Spark\")"
  },
  {
    "objectID": "deployment/databricks-cluster-odbc.html#additional-information",
    "href": "deployment/databricks-cluster-odbc.html#additional-information",
    "title": "Using an ODBC connection with Databricks",
    "section": "Additional information",
    "text": "Additional information\nFor more information about ODBC connections from R, please visit db.rstudio.com."
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#overview",
    "href": "deployment/databricks-cluster-local.html#overview",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Overview",
    "text": "Overview\nIf the recommended path of connecting to Spark remotely with Databricks Connect does not apply to your use case, then you can install RStudio Workbench directly within a Databricks cluster as described in the sections below.\nWith this configuration, RStudio Workbench is installed on the Spark driver node and allows users to work locally with Spark using sparklyr.\n\nThis configuration can result in increased complexity, limited connectivity to other storage and compute resources, resource contention between RStudio Workbench and Databricks, and maintenance concerns due to the ephemeral nature of Databricks clusters.\nFor additional details, refer to the FAQ for RStudio in the Databricks Documentation."
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#advantages-and-limitations",
    "href": "deployment/databricks-cluster-local.html#advantages-and-limitations",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Advantages and limitations",
    "text": "Advantages and limitations\nAdvantages:\n\nAbility for users to connect sparklyr to Spark without configuring remote connectivity\nProvides a high-bandwidth connection between R and the Spark JVM processes because they are running on the same machine\nCan load data from the cluster directly into an R session since RStudio Workbench is installed within the Databricks cluster\n\nLimitations:\n\nIf the Databricks cluster is restarted or terminated, then the instance of RStudio Workbench will be terminated and its configuration will be lost\nIf users do not persist their code through version control or the Databricks File System, then you risk losing user’s work if the cluster is restarted or terminated\nRStudio Workbench (and other RStudio products) installed within a Databricks cluster will be limited to the compute resources and lifecycle of that particular Spark cluster\nNon-Spark jobs will use CPU and RAM resources within the Databricks cluster\nNeed to install one instance of RStudio Workbench per Spark cluster that you want to run jobs on"
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#requirements",
    "href": "deployment/databricks-cluster-local.html#requirements",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Requirements",
    "text": "Requirements\n\nA running Databricks cluster with a runtime version 4.1 or above\nThe cluster must not have “table access control” or “automatic termination” enabled\nYou must have “Can Attach To” permission for the Databricks cluster"
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#preparation",
    "href": "deployment/databricks-cluster-local.html#preparation",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Preparation",
    "text": "Preparation\nThe following steps walk through the process to install RStudio Workbench on the Spark driver node within your Databricks cluster.\nThe recommended method for installing RStudio Workbench to the Spark driver node is via SSH. However, an alternative method is available if you are not able to access the Spark driver node via SSH."
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#configure-ssh-access-to-the-spark-driver-node",
    "href": "deployment/databricks-cluster-local.html#configure-ssh-access-to-the-spark-driver-node",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Configure SSH access to the Spark driver node",
    "text": "Configure SSH access to the Spark driver node\nConfigure SSH access to the Spark driver node in Databricks by following the steps in the SSH access to clusters section of the Databricks Cluster configurations documentation.\nNote: If you are unable to configure SSH access or connect to the Spark driver node via SSH, then you can follow the steps in the Get started with RStudio Workbench section of the RStudio on Databricks documentation to install RStudio Workbench from a Databricks notebook, then skip to the access RStudio Workbench section of this documentation."
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#connect-to-the-spark-driver-node-via-ssh",
    "href": "deployment/databricks-cluster-local.html#connect-to-the-spark-driver-node-via-ssh",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Connect to the Spark driver node via SSH",
    "text": "Connect to the Spark driver node via SSH\nConnect to the Spark driver node via SSH on port 2200 by using the following command on your local machine:\nssh ubuntu@<spark-driver-node-address> -p 2200 -i <path-to-private-SSH-key>\nReplace <spark-driver-node-address> with the DNS name or IP address of the Spark driver node, and <path-to-private-SSH-key> with the path to your private SSH key on your local machine."
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#install-rstudio-workbench-on-the-spark-driver-node",
    "href": "deployment/databricks-cluster-local.html#install-rstudio-workbench-on-the-spark-driver-node",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Install RStudio Workbench on the Spark driver node",
    "text": "Install RStudio Workbench on the Spark driver node\nAfter you SSH into the Spark driver node, then you can follow the typical steps to install RStudio Workbench in the RStudio documentation. In the installation steps, you can select Ubuntu as the target Linux distribution."
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#configure-rstudio-workbench",
    "href": "deployment/databricks-cluster-local.html#configure-rstudio-workbench",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Configure RStudio Workbench",
    "text": "Configure RStudio Workbench\nThe following configuration steps are required to be able to use RStudio Workbench with Databricks.\nAdd the following configuration lines to /etc/rstudio/rserver.conf to use proxied authentication with Databricks and enable the administrator dashboard:\nauth-proxy=1\nauth-proxy-user-header-rewrite=^(.*)$ $1\nauth-proxy-sign-in-url=<domain>/login.html\nadmin-enabled=1\nAdd the following configuration line to /etc/rstudio/rsession-profile to set the PATH to be used with RStudio Workbench:\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:$PATH\nAdd the following configuration lines to /etc/rstudio/rsession.conf to configure sessions in RStudio Workbench to work with Databricks:\nsession-rprofile-on-resume-default=1\nallow-terminal-websockets=0\nRestart RStudio Workbench:\nsudo rstudio-server restart"
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#access-rstudio-workbench",
    "href": "deployment/databricks-cluster-local.html#access-rstudio-workbench",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Access RStudio Workbench",
    "text": "Access RStudio Workbench\nFrom the Databricks console, click on the Databricks cluster that you want to work with:\n\nFrom within the Databricks cluster, click on the Apps tab:\n\nClick on the Set up RStudio button:\n\nTo access RStudio Workbench, click on the link to Open RStudio:\n\nIf you configured proxied authentication in RStudio Workbench as described in the previous section, then you do not need to use the username or password that is displayed. Instead, RStudio Workbench will automatically login and start a new RStudio session as your logged-in Databricks user:\n\nOther users can access RStudio Workbench from the Databricks console by following the same steps described above. You do not need to create those users in RStudio Workbench or their home directory beforehand."
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#configure-sparklyr",
    "href": "deployment/databricks-cluster-local.html#configure-sparklyr",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Configure sparklyr",
    "text": "Configure sparklyr\nUse the following R code to establish a connection from sparklyr to the Databricks cluster:\nSparkR::sparkR.session()\nlibrary(sparklyr)\nsc <- spark_connect(method = \"databricks\")"
  },
  {
    "objectID": "deployment/databricks-cluster-local.html#additional-information",
    "href": "deployment/databricks-cluster-local.html#additional-information",
    "title": "Option 2 - Working inside of Databricks",
    "section": "Additional information",
    "text": "Additional information\nFor more information on using RStudio Workbench inside of Databricks, refer to the sections on RStudio on Databricks (AWS) or RStudio on Databricks (Azure) in the Databricks documentation."
  },
  {
    "objectID": "deployment/qubole-cluster.html#overview",
    "href": "deployment/qubole-cluster.html#overview",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Overview",
    "text": "Overview\nQubole users can request access to RStudio Server Pro. This allows users to use sparklyr to interact directly with Spark from within the Qubole cluster."
  },
  {
    "objectID": "deployment/qubole-cluster.html#advantages-and-limitations",
    "href": "deployment/qubole-cluster.html#advantages-and-limitations",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Advantages and limitations",
    "text": "Advantages and limitations\nAdvantages:\n\nAbility for users to connect sparklyr directly to Spark within Qubole\nProvides a high-bandwidth connection between R and the Spark JVM processes because they are running on the same machine\nCan load data from the cluster directly into an R session since RStudio Workbench is installed within the Qubole cluster\nA unique, persistent home directory for each user\n\nLimitations:\n\nPersistent packages must be managed using Qubole Environments, not directly from within RStudio\nRStudio Workbench installed within a Qubole cluster will be limited to the compute resources and lifecycle of that particular Spark cluster\nNon-Spark jobs will use CPU and RAM resources within the Qubole cluster"
  },
  {
    "objectID": "deployment/qubole-cluster.html#access-rstudio-workbench",
    "href": "deployment/qubole-cluster.html#access-rstudio-workbench",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Access RStudio Workbench",
    "text": "Access RStudio Workbench\nRStudio Workbench can be accessed from the cluster resources menu:"
  },
  {
    "objectID": "deployment/qubole-cluster.html#use-sparklyr",
    "href": "deployment/qubole-cluster.html#use-sparklyr",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Use sparklyr",
    "text": "Use sparklyr\nUse the following R code to establish a connection from sparklyr to the Qubole cluster:\nlibrary(sparklyr)\nsc <- spark_connect(method = \"qubole\")"
  },
  {
    "objectID": "deployment/qubole-cluster.html#additional-information",
    "href": "deployment/qubole-cluster.html#additional-information",
    "title": "Using RStudio Workbench inside of Qubole",
    "section": "Additional information",
    "text": "Additional information\nFor more information on using RStudio Workbench inside of Qubole, refer to the Qubole documentation."
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#overview",
    "href": "deployment/databricks-cluster-remote.html#overview",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Overview",
    "text": "Overview\nWith this configuration, RStudio Workbench is installed outside of the Spark cluster and allows users to connect to Spark remotely using sparklyr with Databricks Connect.\n\nThis is the recommended configuration because it targets separate environments, involves a typical configuration process, avoids resource contention, and allows RStudio Workbench to connect to Databricks as well as other remote storage and compute resources."
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#advantages-and-limitations",
    "href": "deployment/databricks-cluster-remote.html#advantages-and-limitations",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Advantages and limitations",
    "text": "Advantages and limitations\nAdvantages:\n\nRStudio Workbench will remain functional if Databricks clusters are terminated\nProvides the ability to communicate with one or more Databricks clusters as a remote compute resource\nAvoids resource contention between RStudio Workbench and Databricks\n\nLimitations:\n\nDatabricks Connect does not currently support the following APIs from sparklyr: Broom APIs, Streaming APIs, Broadcast APIs, Most MLlib APIs, csv_file serialization mode, and the spark_submit API\nDatabricks Connect does not support structured streaming\nDatabricks Connect does not support running arbitrary code that is not a part of a Spark job on the remote cluster\nDatabricks Connect does not support Scala, Python, and R APIs for Delta table operations\nDatabricks Connect does not support most utilities in Databricks Utilities. However, dbutils.fs and dbutils.secrets are supported\n\nFor more information on the limitations of Databricks Connect, refer to the Limitation section of the Databricks Connect documentation."
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#requirements",
    "href": "deployment/databricks-cluster-remote.html#requirements",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Requirements",
    "text": "Requirements\n\nRStudio Workbench installed outside of the Databricks cluster\nJava 8 installed on the machine with RStudio Workbench\nA running Databricks cluster with a runtime version 5.5 or above"
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#install-python",
    "href": "deployment/databricks-cluster-remote.html#install-python",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Install Python",
    "text": "Install Python\nThe Databricks Connect client is provided as a Python library. The minor version of your Python installation must be the same as the minor Python version of your Databricks cluster.\nRefer to the steps in the install Python section of the RStudio Documentation to install Python on the same server where RStudio Workbench is installed.\nNote that you can either install Python for all users in a global location (as an administrator) or in a home directory (as an end user)."
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#install-databricks-connect",
    "href": "deployment/databricks-cluster-remote.html#install-databricks-connect",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Install Databricks Connect",
    "text": "Install Databricks Connect\nRun the following command to install Databricks Connect on the server with RStudio Workbench:\npip install -U databricks-connect==6.3.*  # or a different version to match your Databricks cluster\nNote that you can either install this library for all users in a global Python environment (as an administrator) or for an individual user in their Python environment (e.g., using the pip --user option or installing into a conda environment or virtual environment)."
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#configure-databricks-connect",
    "href": "deployment/databricks-cluster-remote.html#configure-databricks-connect",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Configure Databricks Connect",
    "text": "Configure Databricks Connect\nTo configure the Databricks Connect client, you can run the following command in a terminal when logged in as a user in RStudio Workbench:\ndatabricks-connect configure\nIn the prompts that follow, enter the following information:\n\n\n\n\n\n\n\n\nParameter\nDescription\nExample Value\n\n\n\n\nDatabricks Host\nBase address of your Databricks console URL\nhttps://dbc-01234567-89ab.cloud.databricks.com\n\n\nDatabricks Token\nUser token generated from the Databricks Console under your “User Settings”\ndapi24g06bdd96f2700b09dd336d5444c1yz\n\n\nCluster ID\nCluster ID in the Databricks console under Advanced Options > Tags > ClusterId\n0308-033548-colt989\n\n\nOrg ID\nFound in the ?o=orgId portion of your Databricks Console URL\n8498623428173033\n\n\nPort\nThe port that Databricks Connect connects to\n15001\n\n\n\nAfter you’ve completed the configuration process for Databricks Connect, you can run the following command in a terminal to test the connectivity of Databricks Connect to your Databricks cluster:\ndatabricks-connect test"
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#install-sparklyr",
    "href": "deployment/databricks-cluster-remote.html#install-sparklyr",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Install sparklyr",
    "text": "Install sparklyr\nThe integration of sparklyr with Databricks Connect is currently being added to the development version of sparklyr. To use this functionality now, you’ll need to install the development version of sparklyr by running the following command in an R console:\ndevtools::install_github(\"sparklyr/sparklyr\")"
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#install-spark",
    "href": "deployment/databricks-cluster-remote.html#install-spark",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Install Spark",
    "text": "Install Spark\nTo work with a remote Databricks cluster, you need to have a local installation of Spark that matches the version of Spark on the Databricks Cluster.\nYou can install Spark by running the following command in an R console:\nlibrary(sparklyr)\nsparklyr::spark_install()\nYou can specify the version of Spark to install along with other options. Refer to the spark_install() options in the sparklyr reference documentation for more information."
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#use-sparklyr",
    "href": "deployment/databricks-cluster-remote.html#use-sparklyr",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Use sparklyr",
    "text": "Use sparklyr\nIn order to connect to Databricks using sparklyr and databricks-connect, SPARK_HOME must be set to the output of the databricks-connect get-spark-home command.\nYou can set SPARK_HOME as an environment variable or directly within spark_connect(). The following R code demonstrates connecting to Databricks, copying some data into the cluster, summarizing that data using sparklyr, and disconnecting:\nlibrary(sparklyr)\nlibrary(dplyr)\n\ndatabricks_connect_spark_home <- system(\"databricks-connect get-spark-home\", intern = TRUE)\nsc <- spark_connect(method = \"databricks\", spark_home = databricks_connect_spark_home)\n\ncars_tbl <- copy_to(sc, mtcars, overwrite = TRUE)\n\ncars_tbl %>% \n  group_by(cyl) %>% \n  summarise(mean_mpg = mean(mpg, na.rm = TRUE),\n            mean_hp  = mean(hp, na.rm = TRUE))\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "deployment/databricks-cluster-remote.html#additional-information",
    "href": "deployment/databricks-cluster-remote.html#additional-information",
    "title": "Option 1 - Connecting to Databricks remotely",
    "section": "Additional information",
    "text": "Additional information\nFor more information on the setup, configuration, troubleshooting, and limitations of Databricks Connect, refer to the Databricks Connect section of the Databricks documentation."
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#set-up-the-cluster",
    "href": "deployment/yarn-cluster-emr.html#set-up-the-cluster",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Set up the cluster",
    "text": "Set up the cluster\nThis demonstration uses Amazon Web Services (AWS), but it could just as easily use Microsot, Google, or any other provider. We will use Elastic Map Reduce (EMR) to easily set up a cluster with two core nodes and one master node. Nodes use virtual servers from the Elastic Compute Cloud (EC2). Note: There is no free tier for EMR, charges will apply.\nBefore beginning this setup we assume you have:\n\nFamiliarity with and access to an AWS account\nFamiliarity with basic linux commands\nSudo privileges in order to install software from the command line\n\n\n\n\n\nBuild an EMR cluster\nBefore beginning the EMR wizard setup, make sure you create the following in AWS:\n\nAn AWS key pair (.pem key) so you can SSH into the EC2 master node\nA security group that gives you access to port 22 on your IP and port 8787 from anywhere\n\n\n\n\nStep 1: Select software\nMake sure to select Hive and Spark as part of the install. Note that by choosing Spark, R will also be installed on the master node as part of the distribution.\n\n\n\n\nStep 2: Select hardware\nInstall 2 core nodes and one master node with m3.xlarge 80 GiB storage per node. You can easily increase the number of nodes later.\n\n\n\n\nStep 3: Select general cluster settings\nClick next on the general cluster settings.\n\n\n\n\nStep 4: Select security\nEnter your EC2 key pair and security group. Make sure the security group has ports 22 and 8787 open.\n\n\n\n\n\nConnect to EMR\nThe cluster page will give you details about your EMR cluster and instructions on connecting.\n\nConnect to the master node via SSH using your key pair. Once you connect you will see the EMR welcome. ::: {.cell}\n# Log in to master node\nssh -i ~/spark-demo.pem hadoop@ec2-52-10-102-11.us-west-2.compute.amazonaws.com\n:::\n\n\n\nInstall RStudio Server\nEMR uses Amazon Linux which is based on Centos. Update your master node and install dependencies that will be used by R packages.\n\n# Update\nsudo yum update\nsudo yum install libcurl-devel openssl-devel # used for devtools\n\nThe installation of RStudio Server is easy. Download the preview version of RStudio and install on the master node.\n\n# Install RStudio Server\nwget -P /tmp https://s3.amazonaws.com/rstudio-dailybuilds/rstudio-server-rhel-0.99.1266-x86_64.rpm\nsudo yum install --nogpgcheck /tmp/rstudio-server-rhel-0.99.1266-x86_64.rpm\n\n\n\nCreate a User\nCreate a user called rstudio-user that will perform the data analysis. Create a user directory for rstudio-user on HDFS with the hadoop fs command.\n\n# Make User\nsudo useradd -m rstudio-user\nsudo passwd rstudio-user\n\n# Create new directory in hdfs\nhadoop fs -mkdir /user/rstudio-user\nhadoop fs -chmod 777 /user/rstudio-user"
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#download-flights-data",
    "href": "deployment/yarn-cluster-emr.html#download-flights-data",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Download flights data",
    "text": "Download flights data\nThe flights data is a well known data source representing 123 million flights over 22 years. It consumes roughly 12 GiB of storage in uncompressed CSV format in yearly files.\n\nSwitch User\nFor data loading and analysis, make sure you are logged in as regular user.\n\n# create directories on hdfs for new user\nhadoop fs -mkdir /user/rstudio-user\nhadoop fs -chmod 777 /user/rstudio-user\n\n# switch user\nsu rstudio-user\n\n\n\nDownload data\nRun the following script to download data from the web onto your master node. Download the yearly flight data and the airlines lookup table.\n\n# Make download directory\nmkdir /tmp/flights\n\n# Download flight data by year\nfor i in {1987..2008}\n  do\n    echo \"$(date) $i Download\"\n    fnam=$i.csv.bz2\n    wget -O /tmp/flights/$fnam http://stat-computing.org/dataexpo/2009/$fnam\n    echo \"$(date) $i Unzip\"\n    bunzip2 /tmp/flights/$fnam\n  done\n\n# Download airline carrier data\nwget -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS\n\n# Download airports data\nwget -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\n\n\n\nDistribute into HDFS\nCopy data into HDFS using the hadoop fs command.\n\n# Copy flight data to HDFS\nhadoop fs -mkdir /user/rstudio-user/flights/\nhadoop fs -put /tmp/flights /user/rstudio-user/\n\n# Copy airline data to HDFS\nhadoop fs -mkdir /user/rstudio-user/airlines/\nhadoop fs -put /tmp/airlines.csv /user/rstudio-user/airlines\n\n# Copy airport data to HDFS\nhadoop fs -mkdir /user/rstudio-user/airports/\nhadoop fs -put /tmp/airports.csv /user/rstudio-user/airports"
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#create-hive-tables",
    "href": "deployment/yarn-cluster-emr.html#create-hive-tables",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Create Hive tables",
    "text": "Create Hive tables\nLaunch Hive from the command line.\n\n# Open Hive prompt\nhive\n\nCreate the metadata that will structure the flights table. Load data into the Hive table.\n# Create metadata for flights\nCREATE EXTERNAL TABLE IF NOT EXISTS flights\n(\nyear int,\nmonth int,\ndayofmonth int,\ndayofweek int,\ndeptime int,\ncrsdeptime int,\narrtime int, \ncrsarrtime int,\nuniquecarrier string,\nflightnum int,\ntailnum string, \nactualelapsedtime int,\ncrselapsedtime int,\nairtime string,\narrdelay int,\ndepdelay int, \norigin string,\ndest string,\ndistance int,\ntaxiin string,\ntaxiout string,\ncancelled int,\ncancellationcode string,\ndiverted int,\ncarrierdelay string,\nweatherdelay string,\nnasdelay string,\nsecuritydelay string,\nlateaircraftdelay string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLINES TERMINATED BY '\\n'\nTBLPROPERTIES(\"skip.header.line.count\"=\"1\");\n\n# Load data into table\nLOAD DATA INPATH '/user/rstudio-user/flights' INTO TABLE flights;\nCreate the metadata that will structure the airlines table. Load data into the Hive table.\n# Create metadata for airlines\nCREATE EXTERNAL TABLE IF NOT EXISTS airlines\n(\nCode string,\nDescription string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE\ntblproperties(\"skip.header.line.count\"=\"1\");\n\n# Load data into table\nLOAD DATA INPATH '/user/rstudio-user/airlines' INTO TABLE airlines;\nCreate the metadata that will structure the airports table. Load data into the Hive table.\n# Create metadata for airports\nCREATE EXTERNAL TABLE IF NOT EXISTS airports\n(\nid string,\nname string,\ncity string,\ncountry string,\nfaa string,\nicao string,\nlat double,\nlon double,\nalt int,\ntz_offset double,\ndst string,\ntz_name string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES\n(\n\"separatorChar\" = '\\,',\n\"quoteChar\"     = '\\\"'\n)\nSTORED AS TEXTFILE;\n\n# Load data into table\nLOAD DATA INPATH '/user/rstudio-user/airports' INTO TABLE airports;"
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#connect-to-spark",
    "href": "deployment/yarn-cluster-emr.html#connect-to-spark",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Connect to Spark",
    "text": "Connect to Spark\nLog in to RStudio Server by pointing a browser at your master node IP:8787.\n\n\n\nSet the environment variable SPARK_HOME and then run spark_connect. After connecting you will be able to browse the Hive metadata in the RStudio Server Spark pane.\n\n# Connect to Spark\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nSys.setenv(SPARK_HOME=\"/usr/lib/spark\")\nconfig <- spark_config()\nsc <- spark_connect(master = \"yarn-client\", config = config, version = '1.6.2')\n\nOnce you are connected, you will see the Spark pane appear along with your hive tables.\n\n\n\nYou can inspect your tables by clicking on the data icon."
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#cache-the-tables-into-memory",
    "href": "deployment/yarn-cluster-emr.html#cache-the-tables-into-memory",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Cache the tables into memory",
    "text": "Cache the tables into memory\nUse tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame.\n\n# Cache flights Hive table into Spark\ntbl_cache(sc, 'flights')\nflights_tbl <- tbl(sc, 'flights')\n\n# Cache airlines Hive table into Spark\ntbl_cache(sc, 'airlines')\nairlines_tbl <- tbl(sc, 'airlines')\n\n# Cache airports Hive table into Spark\ntbl_cache(sc, 'airports')\nairports_tbl <- tbl(sc, 'airports')"
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#create-a-model-data-set",
    "href": "deployment/yarn-cluster-emr.html#create-a-model-data-set",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Create a model data set",
    "text": "Create a model data set\nFilter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight.\n\n# Filter records and create target variable 'gain'\nmodel_data <- flights_tbl %>%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>%\n  filter(depdelay > 15 & depdelay < 240) %>%\n  filter(arrdelay > -60 & arrdelay < 360) %>%\n  filter(year >= 2003 & year <= 2007) %>%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>%\n  mutate(gain = depdelay - arrdelay) %>%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain)\n\n# Summarize data by carrier\nmodel_data %>%\n  group_by(uniquecarrier) %>%\n  summarize(description = min(description), gain=mean(gain), \n            distance=mean(distance), depdelay=mean(depdelay)) %>%\n  select(description, gain, distance, depdelay) %>%\n  arrange(gain)\n\nSource:   query [?? x 4]\nDatabase: spark connection master=yarn-client app=sparklyr local=FALSE\n\n                    description       gain  distance depdelay\n                          <chr>      <dbl>     <dbl>    <dbl>\n1        ATA Airlines d/b/a ATA -3.3480120 1134.7084 56.06583\n2  ExpressJet Airlines Inc. (1) -3.0326180  519.7125 59.41659\n3                     Envoy Air -2.5434415  416.3716 53.12529\n4       Northwest Airlines Inc. -2.2030586  779.2342 48.52828\n5          Delta Air Lines Inc. -1.8248026  868.3997 50.77174\n6   AirTran Airways Corporation -1.4331555  641.8318 54.96702\n7    Continental Air Lines Inc. -0.9617003 1116.6668 57.00553\n8        American Airlines Inc. -0.8860262 1074.4388 55.45045\n9             Endeavor Air Inc. -0.6392733  467.1951 58.47395\n10              JetBlue Airways -0.3262134 1139.0443 54.06156\n# ... with more rows"
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#train-a-linear-model",
    "href": "deployment/yarn-cluster-emr.html#train-a-linear-model",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Train a linear model",
    "text": "Train a linear model\nPredict time gained or lost in flight as a function of distance, departure delay, and airline carrier.\n\n# Partition the data into training and validation sets\nmodel_partition <- model_data %>% \n  sdf_partition(train = 0.8, valid = 0.2, seed = 5555)\n\n# Fit a linear model\nml1 <- model_partition$train %>%\n  ml_linear_regression(gain ~ distance + depdelay + uniquecarrier)\n\n# Summarize the linear model\nsummary(ml1)\n\nDeviance Residuals: (approximate):\n     Min       1Q   Median       3Q      Max \n-305.422   -5.593    2.699    9.750  147.871 \n\nCoefficients:\n                    Estimate  Std. Error  t value  Pr(>|t|)    \n(Intercept)      -1.24342576  0.10248281 -12.1330 < 2.2e-16 ***\ndistance          0.00326600  0.00001670 195.5709 < 2.2e-16 ***\ndepdelay         -0.01466233  0.00020337 -72.0977 < 2.2e-16 ***\nuniquecarrier_AA -2.32650517  0.10522524 -22.1098 < 2.2e-16 ***\nuniquecarrier_AQ  2.98773637  0.28798507  10.3746 < 2.2e-16 ***\nuniquecarrier_AS  0.92054894  0.11298561   8.1475 4.441e-16 ***\nuniquecarrier_B6 -1.95784698  0.11728289 -16.6934 < 2.2e-16 ***\nuniquecarrier_CO -2.52618081  0.11006631 -22.9514 < 2.2e-16 ***\nuniquecarrier_DH  2.23287189  0.11608798  19.2343 < 2.2e-16 ***\nuniquecarrier_DL -2.68848119  0.10621977 -25.3106 < 2.2e-16 ***\nuniquecarrier_EV  1.93484736  0.10724290  18.0417 < 2.2e-16 ***\nuniquecarrier_F9 -0.89788137  0.14422281  -6.2257 4.796e-10 ***\nuniquecarrier_FL -1.46706706  0.11085354 -13.2343 < 2.2e-16 ***\nuniquecarrier_HA -0.14506644  0.25031456  -0.5795    0.5622    \nuniquecarrier_HP  2.09354855  0.12337515  16.9690 < 2.2e-16 ***\nuniquecarrier_MQ -1.88297535  0.10550507 -17.8473 < 2.2e-16 ***\nuniquecarrier_NW -2.79538927  0.10752182 -25.9983 < 2.2e-16 ***\nuniquecarrier_OH  0.83520117  0.11032997   7.5700 3.730e-14 ***\nuniquecarrier_OO  0.61993842  0.10679884   5.8047 6.447e-09 ***\nuniquecarrier_TZ -4.99830389  0.15912629 -31.4109 < 2.2e-16 ***\nuniquecarrier_UA -0.68294396  0.10638099  -6.4198 1.365e-10 ***\nuniquecarrier_US -0.61589284  0.10669583  -5.7724 7.815e-09 ***\nuniquecarrier_WN  3.86386059  0.10362275  37.2878 < 2.2e-16 ***\nuniquecarrier_XE -2.59658123  0.10775736 -24.0966 < 2.2e-16 ***\nuniquecarrier_YV  3.11113140  0.11659679  26.6828 < 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nR-Squared: 0.02385\nRoot Mean Squared Error: 17.74"
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#assess-model-performance",
    "href": "deployment/yarn-cluster-emr.html#assess-model-performance",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Assess model performance",
    "text": "Assess model performance\nCompare the model performance using the validation data.\n\n# Calculate average gains by predicted decile\nmodel_deciles <- lapply(model_partition, function(x) {\n  sdf_predict(ml1, x) %>%\n    mutate(decile = ntile(desc(prediction), 10)) %>%\n    group_by(decile) %>%\n    summarize(gain = mean(gain)) %>%\n    select(decile, gain) %>%\n    collect()\n})\n\n# Create a summary dataset for plotting\ndeciles <- rbind(\n  data.frame(data = 'train', model_deciles$train),\n  data.frame(data = 'valid', model_deciles$valid),\n  make.row.names = FALSE\n)\n\n# Plot average gains by predicted decile\ndeciles %>%\n  ggplot(aes(factor(decile), gain, fill = data)) +\n  geom_bar(stat = 'identity', position = 'dodge') +\n  labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes')"
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#visualize-predictions",
    "href": "deployment/yarn-cluster-emr.html#visualize-predictions",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Visualize predictions",
    "text": "Visualize predictions\nCompare actual gains to predicted gains for an out of time sample.\n\n# Select data from an out of time sample\ndata_2008 <- flights_tbl %>%\n  filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>%\n  filter(depdelay > 15 & depdelay < 240) %>%\n  filter(arrdelay > -60 & arrdelay < 360) %>%\n  filter(year == 2008) %>%\n  left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>%\n  mutate(gain = depdelay - arrdelay) %>%\n  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest)\n\n# Summarize data by carrier\ncarrier <- sdf_predict(ml1, data_2008) %>%\n  group_by(description) %>%\n  summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>%\n  filter(freq > 10000) %>%\n  collect\n\n# Plot actual gains and predicted gains by airline carrier\nggplot(carrier, aes(gain, prediction)) + \n  geom_point(alpha = 0.75, color = 'red', shape = 3) +\n  geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') +\n  geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) +\n  labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted')\n\n\n\n\nSome carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights."
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#build-dashboard",
    "href": "deployment/yarn-cluster-emr.html#build-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Build dashboard",
    "text": "Build dashboard\nAggregate the scored data by origin, destination, and airline. Save the aggregated data.\n\n# Summarize by origin, destination, and carrier\nsummary_2008 <- sdf_predict(ml1, data_2008) %>%\n  rename(carrier = uniquecarrier, airline = description) %>%\n  group_by(origin, dest, carrier, airline) %>%\n  summarize(\n    flights = n(),\n    distance = mean(distance),\n    avg_dep_delay = mean(depdelay),\n    avg_arr_delay = mean(arrdelay),\n    avg_gain = mean(gain),\n    pred_gain = mean(prediction)\n    )\n\n# Collect and save objects\npred_data <- collect(summary_2008)\nairports <- collect(select(airports_tbl, name, faa, lat, lon))\nml1_summary <- capture.output(summary(ml1))\nsave(pred_data, airports, ml1_summary, file = 'flights_pred_2008.RData')"
  },
  {
    "objectID": "deployment/yarn-cluster-emr.html#publish-dashboard",
    "href": "deployment/yarn-cluster-emr.html#publish-dashboard",
    "title": "Using sparklyr with an Apache Spark cluster",
    "section": "Publish dashboard",
    "text": "Publish dashboard\nUse the saved data to build an R Markdown flexdashboard. Publish the flexdashboard to Shiny Server, Shinyapps.io or RStudio Connect."
  },
  {
    "objectID": "deployment/qubole-overview.html#overview",
    "href": "deployment/qubole-overview.html#overview",
    "title": "Using sparklyr with Qubole",
    "section": "Overview",
    "text": "Overview\nThis documentation demonstrates how to use sparklyr with Apache Spark in Qubole along with RStudio Server Pro and RStudio Connect."
  },
  {
    "objectID": "deployment/qubole-overview.html#best-practices-for-working-with-qubole",
    "href": "deployment/qubole-overview.html#best-practices-for-working-with-qubole",
    "title": "Using sparklyr with Qubole",
    "section": "Best practices for working with Qubole",
    "text": "Best practices for working with Qubole\n\nManage packages via Qubole Environments - Packages installed via install.packages() are not available on cluster restart. Packages managed through Qubole Environments are persistent.\nRestrict workloads to interactive analysis - Only perform workloads related to exploratory or interactive analysis with Spark, then write the results to a database, file system, or cloud storage for more efficient retrieval in apps, reports, and APIs.\nLoad and query results efficiently - Because of the nature of Spark computations and the associated overhead, Shiny apps that use Spark on the backend tend to have performance and runtime issues; consider reading the results from a database, file system, or cloud storage instead."
  },
  {
    "objectID": "deployment/qubole-overview.html#using-rstudio-workbench-with-qubole",
    "href": "deployment/qubole-overview.html#using-rstudio-workbench-with-qubole",
    "title": "Using sparklyr with Qubole",
    "section": "Using RStudio Workbench with Qubole",
    "text": "Using RStudio Workbench with Qubole\nThe Qubole platform includes RStudio Workbench. More details about how to request RStudio Workbench and access it from within a Qubole cluster are available from Qubole.\n\n\nView steps for running RStudio Workbench inside Qubole"
  },
  {
    "objectID": "deployment/qubole-overview.html#using-rstudio-connect-with-qubole",
    "href": "deployment/qubole-overview.html#using-rstudio-connect-with-qubole",
    "title": "Using sparklyr with Qubole",
    "section": "Using RStudio Connect with Qubole",
    "text": "Using RStudio Connect with Qubole\nThe best configuration for working with Qubole and RStudio Connect is to install RStudio Connect outside of the Qubole cluster and connect to Qubole remotely. This is accomplished using the Qubole ODBC Driver."
  },
  {
    "objectID": "deployment/databricks-cluster.html#overview",
    "href": "deployment/databricks-cluster.html#overview",
    "title": "Using sparklyr with Databricks",
    "section": "Overview",
    "text": "Overview\nThis documentation demonstrates how to use sparklyr with Apache Spark in Databricks along with RStudio Team, RStudio Workbench, RStudio Connect, and RStudio Package Manager."
  },
  {
    "objectID": "deployment/databricks-cluster.html#using-rstudio-team-with-databricks",
    "href": "deployment/databricks-cluster.html#using-rstudio-team-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Using RStudio Team with Databricks",
    "text": "Using RStudio Team with Databricks\nRStudio Team is a bundle of our popular professional software for developing data science projects, publishing data products, and managing packages.\nRStudio Team and sparklyr can be used with Databricks to work with large datasets and distributed computations with Apache Spark. The most common use case is to perform interactive analysis and exploratory development with RStudio Workbench and sparklyr; write out the results to a database, file system, or cloud storage; then publish apps, reports, and APIs to RStudio Connect that query and access the results.\n\nThe sections below describe best practices and different options for configuring specific RStudio products to work with Databricks."
  },
  {
    "objectID": "deployment/databricks-cluster.html#best-practices-for-working-with-databricks",
    "href": "deployment/databricks-cluster.html#best-practices-for-working-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Best practices for working with Databricks",
    "text": "Best practices for working with Databricks\n\nMaintain separate installation environments - Install RStudio Workbench, RStudio Connect, and RStudio Package Manager outside of the Databricks cluster so that they are not limited to the compute resources or ephemeral nature of Databricks clusters.\nConnect to Databricks remotely - Work with Databricks as a remote compute resource, similar to how you would connect remotely to external databases, data sources, and storage systems. This can be accomplished using Databricks Connect (as described in the Connecting to Databricks remotely section below) or by performing SQL queries with JDBC/ODBC using the Databricks Spark SQL Driver on AWS or Azure.\nRestrict workloads to interactive analysis - Only perform workloads related to exploratory or interactive analysis with Spark, then write the results to a database, file system, or cloud storage for more efficient retrieval in apps, reports, and APIs.\nLoad and query results efficiently - Because of the nature of Spark computations and the associated overhead, Shiny apps that use Spark on the backend tend to have performance and runtime issues; consider reading the results from a database, file system, or cloud storage instead."
  },
  {
    "objectID": "deployment/databricks-cluster.html#using-rstudio-workbench-with-databricks",
    "href": "deployment/databricks-cluster.html#using-rstudio-workbench-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Using RStudio Workbench with Databricks",
    "text": "Using RStudio Workbench with Databricks\nThere are two options for using sparklyr and RStudio Workbench with Databricks:\n\nOption 1: Connecting to Databricks remotely (Recommended Option)\nOption 2: Working inside of Databricks (Alternative Option)\n\n\nOption 1 - Connecting to Databricks remotely\nWith this configuration, RStudio Workbench is installed outside of the Spark cluster and allows users to connect to Spark remotely using sparklyr with Databricks Connect.\nThis is the recommended configuration because it targets separate environments, involves a typical configuration process, avoids resource contention, and allows RStudio Workbench to connect to Databricks as well as other remote storage and compute resources.\n\n\nView steps for connecting to Databricks remotely\n\n\n  \n\n\nOption 2 - Working inside of Databricks\nIf you cannot work with Spark remotely, you should install RStudio Workbench on the Driver node of a long-running, persistent Databricks cluster as opposed to a worker node or an ephemeral cluster.\nWith this configuration, RStudio Workbench is installed on the Spark driver node and allows users to connect to Spark locally using sparklyr.\nThis configuration can result in increased complexity, limited connectivity to other storage and compute resources, resource contention between RStudio Workbench and Databricks, and maintenance concerns due to the ephemeral nature of Databricks clusters.\n\n\nView steps for working inside of Databricks"
  },
  {
    "objectID": "deployment/databricks-cluster.html#using-rstudio-connect-with-databricks",
    "href": "deployment/databricks-cluster.html#using-rstudio-connect-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Using RStudio Connect with Databricks",
    "text": "Using RStudio Connect with Databricks\nThe server environment within Databricks clusters is not permissive enough to support RStudio Connect or the process sandboxing mechanisms that it uses to isolate published content.\nTherefore, the only supported configuration is to install RStudio Connect outside of the Databricks cluster and connect to Databricks remotely.\nWhether RStudio Workbench is installed outside of the Databricks cluster (Recommended Option) or within the Databricks cluster (Alternative Option), you can publish content to RStudio Connect as long as HTTP/HTTPS network traffic is allowed from RStudio Workbench to RStudio Connect.\nThere are two options for using RStudio Connect with Databricks:\n\nPerforming SQL queries with ODBC using the Databricks Spark SQL Driver (Recommended Option).\nAdding calls in your R code to create and run Databricks jobs with bricksteR and the Databricks Jobs API (Alternative Option)"
  },
  {
    "objectID": "deployment/databricks-cluster.html#using-rstudio-package-manager-with-databricks",
    "href": "deployment/databricks-cluster.html#using-rstudio-package-manager-with-databricks",
    "title": "Using sparklyr with Databricks",
    "section": "Using RStudio Package Manager with Databricks",
    "text": "Using RStudio Package Manager with Databricks\nWhether RStudio Workbench is installed outside of the Databricks cluster (Recommended Option) or within the Databricks cluster (Alternative Option), you can install packages from repositories in RStudio Package Manager as long as HTTP/HTTPS network traffic is allowed from RStudio Workbench to RStudio Package Manager."
  },
  {
    "objectID": "deployment/stand-alone-aws.html#overview",
    "href": "deployment/stand-alone-aws.html#overview",
    "title": "Spark Standalone Deployment in AWS",
    "section": "Overview",
    "text": "Overview\nThe plan is to launch 4 identical EC2 server instances. One server will be the Master node and the other 3 the worker nodes. In one of the worker nodes, we will install RStudio server.\nWhat makes a server the Master node is only the fact that it is running the master service, while the other machines are running the slave service and are pointed to that first master. This simple setup, allows us to install the same Spark components on all 4 servers and then just add RStudio to one of them.\nThe topology will look something like this:"
  },
  {
    "objectID": "deployment/stand-alone-aws.html#aws-ec-instances",
    "href": "deployment/stand-alone-aws.html#aws-ec-instances",
    "title": "Spark Standalone Deployment in AWS",
    "section": "AWS EC Instances",
    "text": "AWS EC Instances\nHere are the details of the EC2 instance, just deploy one at this point:\n\nType: t2.medium\nOS: Ubuntu 16.04 LTS\nDisk space: At least 20GB\nSecurity group: Open the following ports: 8080 (Spark UI), 4040 (Spark Worker UI), 8088 (sparklyr UI) and 8787 (RStudio). Also open All TCP ports for the machines inside the security group."
  },
  {
    "objectID": "deployment/stand-alone-aws.html#spark",
    "href": "deployment/stand-alone-aws.html#spark",
    "title": "Spark Standalone Deployment in AWS",
    "section": "Spark",
    "text": "Spark\nPerform the steps in this section on all of the servers that will be part of the cluster.\n\nInstall Java 8\n\nWe will add the Java 8 repository, install it and set it as default\n\nsudo apt-add-repository ppa:webupd8team/java\nsudo apt-get update\nsudo apt-get install oracle-java8-installer\nsudo apt-get install oracle-java8-set-default\nsudo apt-get update\nor alternatively, run\nsudo apt install openjdk-8-jdk\nto install Open JDK version 8.\n\n\nDownload Spark\n\nDownload and unpack a pre-compiled version of Spark. Here’s is the link to the official Spark download page\n\nwget http://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-hadoop2.7.tgz\ntar -xvzf spark-2.1.0-bin-hadoop2.7.tgz\ncd spark-2.1.0-bin-hadoop2.7\n\n\nCreate and launch AMI\n\nWe will create an image of the server. In Amazon, these are called AMIs, for information please see the User Guide.\nLaunch 3 instances of the AMI"
  },
  {
    "objectID": "deployment/stand-alone-aws.html#rstudio-server",
    "href": "deployment/stand-alone-aws.html#rstudio-server",
    "title": "Spark Standalone Deployment in AWS",
    "section": "RStudio Server",
    "text": "RStudio Server\nSelect one of the nodes to execute this section. Please check the RStudio download page for the latest version\n\nInstall R\n\nIn order to get the latest R core, we will need to update the source list in Ubuntu.\n\nsudo sh -c 'echo \"deb http://cran.rstudio.com/bin/linux/ubuntu xenial/\" >> /etc/apt/sources.list'\ngpg --keyserver keyserver.ubuntu.com --recv-key 0x517166190x51716619e084dab9\ngpg -a --export 0x517166190x51716619e084dab9 | sudo apt-key add -\nsudo apt-get update\n\nNow we can install R\n\nsudo apt-get install r-base\nsudo apt-get install gdebi-core\n\n\nInstall RStudio\n\nWe will download and install 1.044 of RStudio Server. To find the latest version, please visit the RStudio website. In order to get the enhanced integration with Spark, RStudio version 1.044 or later will be needed.\n\nwget https://download2.rstudio.org/rstudio-server-1.0.153-amd64.deb\nsudo gdebi rstudio-server-1.0.153-amd64.deb\n\n\nInstall dependencies\n\nRun the following commands\n\nsudo apt-get -y install libcurl4-gnutls-dev\nsudo apt-get -y install libssl-dev\nsudo apt-get -y install libxml2-dev\n\n\nAdd default user\n\nRun the following command to add a default user\n\nsudo adduser rstudio-user\n\n\nStart the Master node\n\nSelect one of the servers to become your Master node\nRun the command that starts the master service\n\nsudo spark-2.1.0-bin-hadoop2.7/sbin/start-master.sh\n\nClose the terminal connection (optional)\n\n\n\nStart Worker nodes\n\nStart the slave service. Important: Use dots not dashes as separators for the Spark Master node’s address\n\nsudo spark-2.1.0-bin-hadoop2.7/sbin/start-slave.sh spark://[Master node's IP address]:7077\nsudo spark-2.1.0-bin-hadoop2.7/sbin/start-slave.sh spark://ip-172-30-1-94.us-west-2.compute.internal:7077 - Close the terminal connection (optional)\n\n\nPre-load pacakges\n\nLog into RStudio (port 8787)\nUse ‘rstudio-user’\n\ninstall.packages(\"sparklyr\")\n\n\nConnect to the Spark Master\n\nNavigate to the Spark Master’s UI, typically on port 8080 \nNote the Spark Master URL\nLogon to RStudio\nRun the following code\n\n\nlibrary(sparklyr)\n\nconf <- spark_config()\nconf$spark.executor.memory <- \"2GB\"\nconf$spark.memory.fraction <- 0.9\n\nsc <- spark_connect(master=\"[Spark Master URL]\",\n              version = \"2.1.0\",\n              config = conf,\n              spark_home = \"/home/ubuntu/spark-2.1.0-bin-hadoop2.7/\")"
  }
]