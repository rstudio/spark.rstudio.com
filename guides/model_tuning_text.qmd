---
title: "Model Tuning - Part II"
execute:
  eval: true
  freeze: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup}
#| include: false
library(sparklyr)
library(dplyr)
library(modeldata)
```

## Goals

The aim of this article is to build on the knowledge from [Model
Tuning](model_tuning.qmd){target="_blank"} by covering the following:

-   Show how to re-use the code of an existing ML Pipeline as the base
    for the hyper-parameter tuning
-   Show how tuning parameters are not limited to only model's
    parameters. We will cover how to tune data transformation parameters

## Recreating "Tuning Text Analysis"

The example in this article is based on the [Tuning Text
Analysis](https://tune.tidymodels.org/articles/extras/text_analysis.html){target="_blank"}
article found in the `tidymodels`' `tune` website. In that article, they
use *Amazon's Fine Food Reviews* text data to perform hyper parameter
tuning.

As its name suggest, Model Tuning has two main phases: the modeling, and
the tuning. In `tidymodels`, the modeling is done using the `recepies`
and `parsnip` packages, while the tuning is done with `tune`. `tune` is
able to modify the the `recipe` and model's arguments for each
experiment.

In Spark, the modeling is done with an ML Pipeline. In itself, preparing
the data, and setting up the model can be complex enough to merit its
own walk-through. For the walk-through of the model used in this
article, please see [Text Modeling](textmodeling.qmd){target="_blank"}.

The basics of the tuning part are covered in [Model
Tuning](model_tuning.qmd){target="_blank"}. To avoid duplication, any
foundational explanation will be linked back to that article.

## Setup

For this example, we will start a local Spark session, and then copy the
*Fine Food Reviews* data to it.

```{r}
library(sparklyr)
library(modeldata)

data("small_fine_foods")

sc <- spark_connect(master = "local", version = "3.3")

sff_training_data <- copy_to(sc, training_data)
sff_testing_data <- copy_to(sc, testing_data)
```

## Pipeline

The data preparation and modeling in [Text
Modeling](textmodeling.qmd){target="_blank"} was based on the [Tuning
Text
Analysis](https://tune.tidymodels.org/articles/extras/text_analysis.html){target="_blank"}
article. The main steps from from the [Recipe and Model
Specification](https://tune.tidymodels.org/articles/extras/text_analysis.html#recipe-and-model-specifications-1){target="_blank"}
section were recreated using Spark, via the `sparklyr` API. The `recipe`
steps were recreated with Feature Transformer functions, and the
`parsnip` model was recreated using the equivalent
`ml_logistic_regression()` model.

During tuning, any parameter value used in the ML Pipeline will be
overwritten with values from the [grid](#grid). This means that it
doesn't matter that we use the exact same code for developing, and
tuning the pipeline. We can literally copy-paste, and run the resulting
pipeline code from [Text
Modeling](textmodeling.qmd#prepare-the-model-with-an-ml-pipeline).

```{r}
sff_pipeline <- ml_pipeline(sc) %>% 
  ft_tokenizer(
    input_col = "review",
    output_col = "word_list"
  ) %>% 
  ft_stop_words_remover(
    input_col = "word_list", 
    output_col = "wo_stop_words"
    ) %>% 
  ft_hashing_tf(
    input_col = "wo_stop_words", 
    output_col = "hashed_features", 
    binary = TRUE, 
    num_features = 1024
    ) %>%
  ft_normalizer(
    input_col = "hashed_features", 
    output_col = "normal_features"
    ) %>% 
  ft_r_formula(score ~ normal_features) %>% 
  ml_logistic_regression()

sff_pipeline
```

It is also worth pointing out that in a"real life" exercise,
`sff_pipeline` would probably already be loaded into our environment.
That is because we just finished modeling and, decided to test to see if
we could tune the model. Spark can re-use the exact same ML Pipeline
object for the cross validation step.

## Grid {#grid}

There is a big advantage to transforming, and modeling the data in a
single ML Pipeline. It opens the door for Spark to also alter parameters
used for data transformation, in addition to the model's parameters.
This means that we can include the parameters of the tokenization,
cleaning, hashing, and normalization steps as possible candidate for the
model tuning.

The *Text Analysis* article uses three tuning parameters. Two are in the
model, and one is in the hashing step. Here are the parameters, and how
they map between `tidymodels` and `sparklyr`:

| Parameter                             | `tidymodels` | `sparklyr`          |
|---------------------------------------|--------------|---------------------|
| Number of Terms to Hash               | `num_terms`  | `num_features`      |
| Amount of regularization in the model | `penalty`    | `elastic_net_param` |
| Proportion of pure vs ridge Lasso     | `mixture`    | `reg_param`         |

The values to tune with are taken from the [Grid
Search](https://tune.tidymodels.org/articles/extras/text_analysis.html#grid-search-1){target="_blank"}
section in the *Text Analysis* article. All that is left to do is to
create the grid itself. Just like we did in the first [Model
Tuning](model_tuning.html#grid){target="_blank"} article, we use partial
name matching to the steps we want to tune:

-   `hashing_ft` will be the name of the list object containing the
    `num_features` values

-   `logistic_regression` will be the of the list object containing the
    values of the other two parameters

Notice that the R code of the values themselves are a direct copy of the
ones used in the *Text Analysis* article.

```{r}
sff_grid <-  list(
    hashing_tf = list(
      num_features = 2^c(8, 10, 12)  
    ),
    logistic_regression = list(
      elastic_net_param = 10^seq(-3, 0, length = 20), 
      reg_param = seq(0, 1, length = 5)    
    )
  )

sff_grid
```

## Evaluator

```{r}
sff_evaluator <- ml_binary_classification_evaluator(sc)
```

## Model Tuning

```{r}
sff_cv <- ml_cross_validator(
  x = sc,
  estimator = sff_pipeline, 
  estimator_param_maps = sff_grid,
  evaluator = sff_evaluator,
  num_folds = 3,
  parallelism = 4,
  seed = 100
)

sff_cv
```

```{r}
sff_model <- ml_fit(
  x = sff_cv, 
  dataset = sff_training_data
  )
```

## Validation metrics

```{r}
sff_metrics <- ml_validation_metrics(sff_model)
```

```{r}
library(ggplot2)

sff_metrics %>% 
  mutate(reg_param_1 = as.factor(reg_param_1)) %>% 
  ggplot(aes(
    x = elastic_net_param_1, 
    y = areaUnderROC, 
    color = reg_param_1
    )) +
  geom_line() +
  geom_point(size = 0.5) +
  scale_x_continuous(trans = "log10") +
  facet_wrap(~ num_features_2) +
  theme_light(base_size = 9)
```

```{r}
library(dplyr)

sff_metrics %>% 
  arrange(desc(areaUnderROC)) %>% 
  head()
```

## Model selection

```{r}
new_sff_pipeline <- ml_pipeline(sc) %>% 
  ft_tokenizer(
    input_col = "review",
    output_col = "word_list"
  ) %>% 
  ft_stop_words_remover(
    input_col = "word_list", 
    output_col = "wo_stop_words"
    ) %>% 
  ft_hashing_tf(
    input_col = "wo_stop_words", 
    output_col = "hashed_features", 
    binary = TRUE, 
    num_features = 4096
    ) %>%
  ft_normalizer(
    input_col = "hashed_features", 
    output_col = "normal_features"
    ) %>% 
  ft_r_formula(score ~ normal_features) %>% 
  ml_logistic_regression(elastic_net_param = 0.05, reg_param = 0.25)
```

```{r}
new_sff_fitted <- new_sff_pipeline %>% 
  ml_fit(sff_training_data)
```

## Test data metrics

```{r}
new_sff_fitted %>% 
  ml_transform(sff_testing_data) %>% 
  ml_metrics_binary()
```

```{r}
#| include: false
spark_disconnect(sc)
```
