---
title: Snowflake
subtitle: Snowpark Connect
format:
  html:
    theme: default
    toc: true
execute:
    eval: true
    freeze: true
editor: 
  markdown: 
    wrap: 72
---

*Last updated: `r lubridate::date()`*

## Intro

Snowflake's [Snowpark Connect](https://docs.snowflake.com/en/developer-guide/snowpark-connect/snowpark-connect-overview)
enables running Spark workloads remotely.  Snowpark Connect for Spark supports 
using the Spark Dataframe API on Snowflake. 

Snowpark Connect is based on [Spark Connect](https://spark.apache.org/docs/latest/spark-connect-overview.html).
This architecture decouples client  and server, so that Spark code can run
remotely against the Snowflake compute engine without your needing to manage a 
Spark cluster.

## Package Installation

To access Snowpark Connect, you will need the following two packages:

-   `sparklyr` 
-   `pysparklyr` 

This feature is currently in development, so you will need to install `pysparklyr`
from GitHub:

``` r
install.packages("sparklyr")
pak::pak("mlverse/pysparklyr")
```

