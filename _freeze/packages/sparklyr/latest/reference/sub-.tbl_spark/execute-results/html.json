{
  "hash": "6bc3e96dd2d5d7c6b9ec1f4eef3d52ae",
  "result": {
    "markdown": "---\ntitle: \"Subsetting operator for Spark dataframe\"\nexecute:\n  freeze: true\n---\n\n\n\n\n*R/sdf_interface.R*\n\n## [.tbl_spark\n\n## Description\nSusetting operator for Spark dataframe allowing a subset of column(s) to be selected using syntaxes similar to those supported by R dataframes \n\n\n## Usage\n```r\n## S3 method for class 'tbl_spark'\n[(x, i) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | The Spark dataframe |\n| i | Expression specifying subset of column(s) to include or exclude from the result (e.g., `[\"col1\"]`, `[c(\"col1\", \"col2\")]`, `[1:10]`, `[-1]`, `[NULL]`, or `[]`) |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr) \nsc <- spark_connect(master = \"spark://HOST:PORT\") \nexample_sdf <- copy_to(sc, tibble::tibble(a = 1, b = 2)) \nexample_sdf[\"a\"] %>% print() \n#> # A tibble: 1 Ã— 1\n#>       a\n#>   <dbl>\n#> 1     1\n```\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}