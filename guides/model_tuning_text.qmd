---
title: "Model Tuning - Part II"
execute:
  eval: false
  freeze: true
---



```{r setup}
#| include: false
library(sparklyr)
library(dplyr)
library(modeldata)
```

## Goals

The aim of this article is to build on the knowledge from [Model Tuning](model_tuning.qmd){target="_blank"} 
by covering the following: 

- Show how to re-use the code of an existing ML Pipeline as the base 
of the hyper-parameter tuning 
- Show how tuning parameters are not limited to only model's parameters. We will
cover how to tune data transformation parameters
 
## Recreating "Tuning Text Analysis"

The example in this article is based on the [Tuning Text Analysis](https://tune.tidymodels.org/articles/extras/text_analysis.html){target="_blank"}
article found in the `tidymodels`' `tune` website. In that article, they use 
*Amazon's Fine Foods Reviews* text data to perform hyper parameter tuning. 

As its name suggest, Model Tuning has two main phases: the modeling, and the tuning.
In `tidymodels`, the modeling is done using the `recepies` and `parsnip` packages,
while the tuning is done with `tune`. `tune` is able to modify the 
the `recipe` and model's arguments for each experiment. 

In Spark, the modeling is done with an ML Pipeline. In itself, preparing the data, 
and setting up the model can be complex enough to merit its own walk-through. 
For the walk-through of the model used in this article, please see
[Text Modeling](textmodeling.qmd){target="_blank"}.

## Setup

```{r}
library(sparklyr)
library(modeldata)

data("small_fine_foods")

sc <- spark_connect(master = "local", version = "3.3")

sff_training_data <- copy_to(sc, training_data)
sff_testing_data <- copy_to(sc, testing_data)
```

## Pipeline

The [Text Modeling](textmodeling.qmd#prepare-the-model-with-an-ml-pipeline){target="_blank"}
article was 

```{r}
sff_pipeline <- ml_pipeline(sc) %>% 
  ft_tokenizer(
    input_col = "review",
    output_col = "word_list"
  ) %>% 
  ft_stop_words_remover(
    input_col = "word_list", 
    output_col = "wo_stop_words"
    ) %>% 
  ft_hashing_tf(
    input_col = "wo_stop_words", 
    output_col = "hashed_features", 
    binary = TRUE, 
    num_features = 1024
    ) %>%
  ft_normalizer(
    input_col = "hashed_features", 
    output_col = "normal_features"
    ) %>% 
  ft_r_formula(score ~ normal_features) %>% 
  ml_logistic_regression()
```

## Grid

[Grid Search](https://tune.tidymodels.org/articles/extras/text_analysis.html#grid-search-1){target="_blank"}
section in the `tidymodels` article. 

  penalty = 10^seq(-3, 0, length = 20)
  mixture = seq(0, 1, length = 5)
  num_terms = 2^c(8, 10, 12)
```{r}
sff_grid <-  list(
    hashing_tf = list(
      num_features = 2^c(8, 10, 12)  
    ),
    logistic_regression = list(
      elastic_net_param = 10^seq(-3, 0, length = 20), 
      reg_param = seq(0, 1, length = 5)    
    )
  )

sff_grid
```

```{r}
sff_grid <-  list(
    hashing_tf = list(
      num_features = 2^c(12)  
    ),
    logistic_regression = list(
      elastic_net_param = 10^seq(-3, 0, length = 5), 
      reg_param = seq(0, 1, length = 3)    
    )
  )

sff_grid
```

## Evaluator

```{r}
sff_evaluator <- ml_binary_classification_evaluator(sc)
```

## Model Tuning

```{r}
sff_cv <- ml_cross_validator(
  x = sc,
  estimator = sff_pipeline, 
  estimator_param_maps = sff_grid,
  evaluator = sff_evaluator,
  num_folds = 3,
  parallelism = 4
)

sff_cv
```


```{r}
sff_model <- ml_fit(
  x = sff_cv, 
  dataset = sff_training_data
  )
```

## Validation metrics

```{r}
sff_metrics <- ml_validation_metrics(sff_model)
```


```{r}
library(ggplot2)

sff_metrics %>% 
  mutate(reg_param_1 = as.factor(reg_param_1)) %>% 
  ggplot() +
  geom_line(aes(
    x = elastic_net_param_1, 
    y = areaUnderROC, 
    color = reg_param_1
    )) +
  facet_wrap(~ num_features_2) +
  theme_light()
```

```{r}
library(dplyr)

sff_metrics %>% 
  arrange(desc(areaUnderROC)) %>% 
  head()
```

## Model selection

```{r}
new_sff_pipeline <- ml_pipeline(sc) %>% 
  ft_tokenizer(
    input_col = "review",
    output_col = "word_list"
  ) %>% 
  ft_stop_words_remover(
    input_col = "word_list", 
    output_col = "wo_stop_words"
    ) %>% 
  ft_hashing_tf(
    input_col = "wo_stop_words", 
    output_col = "hashed_features", 
    binary = TRUE, 
    num_features = 4096
    ) %>%
  ft_normalizer(
    input_col = "hashed_features", 
    output_col = "normal_features"
    ) %>% 
  ft_r_formula(score ~ normal_features) %>% 
  ml_logistic_regression(elastic_net_param = 0.018, reg_param = 0.5)
```

```{r}
new_sff_fitted <- new_sff_pipeline %>% 
  ml_fit(sff_training_data)
```

## Test data metrics

```{r}
new_sff_fitted %>% 
  ml_transform(sff_testing_data) %>% 
  ml_metrics_binary()
```

```{r}
#| include: false
spark_disconnect(sc)
```



