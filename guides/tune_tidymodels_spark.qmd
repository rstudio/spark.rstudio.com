---
title: "Tune `tidymodels` in Spark"
execute:
  eval: false
  freeze: false
---

```{r}
#| include: false

library(sparklyr)
library(tidymodels)
library(readmission)

if(FALSE) { # if (!interactive()) {
  pysparklyr::install_pyspark("4.1.0", python_version = "3.12")  
  reticulate::py_install("rpy2", envname = "r-sparklyr-pyspark-4.1")
}

# TODO: Remove before publishing
# readmission <- readmission[1:1000, ]
```


The ability of Spark to easily distribute tasks across multiple nodes (servers)
makes it a great target to run "embarrassing parallel" jobs, such as hyper-parameter
tuning. For R users, the challenge is to have to learn how to run experiments 
using the Spark ML components. Ideally, we would like to use the already familiar
modeling R packages but have the computation happen somewhere else, faster,
and with little to no changes to our original code. 

**It is important to note
that we are not speaking of "big data", but rather of something more like 
"big processing"**.  What takes the job a long time to complete is the fact that
there are numerous tuning combination that need to be discretely processed 
(fit training data, predict on evaluation data, calculate metrics), and not that
we have to train on gigabytes and gigabytes of data. 

Using `sparklyr`, it is now possible to use the exact same R objects created
to process the model tuning, but have the tuning itself occur in Spark. `sparklyr`
will upload the R objects to your Spark session automatically, and have all
of the parallel jobs run remotely. Everything made possible by running a
very similar function call. Instead of running `tune_grid()`, we would run
`tune_grid_spark()`, which has the extact same arguments as `tune_grid()`, with
a couple of additions, such as requiring a current Spark connection.


```{r}
#| eval: false

# TODO: Replace with actual installation instructions at release time

remotes::install_github("sparklyr/sparklyr", ref = "grid2")
remotes::install_github("tidymodels/tune", ref = "grid2")
remotes::install_github("mlverse/pysparklyr", ref = "grid2")
```

## Prepare the workflow and re-samples locally

The standard framework for Machine Learning in R is 
[Tidymodels](https://www.tidymodels.org/). It is a collection of packages that
are designed to work together to provide everything from re-sampling, to 
pre-processing, to model tuning, to performance measurement. 

The following code prepares all the components needed to run hyper-parameter model
tuning in order to identify the best combination of parameters:


```{r}
library(tidymodels)
library(readmission)

set.seed(1)

# Data set resampling 
readmission_splits <- initial_split(readmission, strata = readmitted)

readmission_folds <- vfold_cv(
  data = training(readmission_splits),
  strata = readmitted
  )

# Data pre-processing
recipe_basic <- recipe(readmitted ~ ., data = readmission) |>
  step_mutate(
    race = factor(case_when(
      !(race %in% c("Caucasian", "African American")) ~ "Other",
      .default = race
    ))
  ) |>
  step_unknown(all_nominal_predictors()) |>
  step_YeoJohnson(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors())

# Model specification
spec_bt <- boost_tree(
  mode = "classification",
  mtry = tune(), # Part of the hyper-parameters to tune
  learn_rate = tune(), # Part of the hyper-parameters to tune
  trees = 10
  )
```

Typically, the next step would be to execute the hyper-parameter tuning by
calling `tune_grid()`. This is where we will diverge in order to use Spark
instead of our laptop to tune the model.

## Tune in Spark Connect

The feature of tuning remotely it is only available for Spark Connect connections,
and its vendor variants such as Databricks Connect. The variants will need to 
have R installed and available in their Spark nodes.  

```{r}
#| eval: false
library(sparklyr)
```


### Start Spark Connect locally

To show a full example throughout this article, we have included the steps to
start a local Spark Connect service. Please keep in mind that it is not necessary
to create anything local if you already have a Spark Connect service available
somewhere else, such as a vendor provided one. 

1. Install Spark locally if the version you wish to use is not available locally
yet

```{r}
spark_install("4.1.0")
```

2. Start the Spark Connect service locally. Make sure to match the Spark version
you recently installed

```{r}
pysparklyr::spark_connect_service_start("4.1.0", python_version = "3.12")
```


### Tune the model

```{r}
sc <- spark_connect(
  "sc://localhost",
  method = "spark_connect",
  version = "4.1.0"
  )
```


```{r}
spark_results <- tune_grid_spark(sc, spec_bt, recipe_basic, readmission_folds)
```

```{r}
spark_results
```

## Compare with local results

```{r}
results <- tune_grid(spec_bt, recipe_basic, readmission_folds)
```

```{r}
show_best(results)
show_best(spark_results)
```

TODO: Make sure to mention that R and package versions will affect results but the output will be reproducible

## Other features 

### Number of parallel Spark tasks

### How to group parallel jobs 


```{r}
pysparklyr::spark_connect_service_stop()
```
