{
  "hash": "ddb088aff6f89c3e6e2a841ee23466a7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Feature Transformation -- Word2Vec (Estimator)\"\nexecute:\n  freeze: true\n---\n\n\n\n*R/ml_feature_word2vec.R*\n\n## ft_word2vec\n\n## Description\n Word2Vec transforms a word into a code for further natural language processing or machine learning process. \n\n\n## Usage\n```r\n \nft_word2vec( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  vector_size = 100, \n  min_count = 5, \n  max_sentence_length = 1000, \n  num_partitions = 1, \n  step_size = 0.025, \n  max_iter = 1, \n  seed = NULL, \n  uid = random_string(\"word2vec_\"), \n  ... \n) \n \nml_find_synonyms(model, word, num) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | A `spark_connection`, `ml_pipeline`, or a `tbl_spark`. |\n| input_col | The name of the input column. |\n| output_col | The name of the output column. |\n| vector_size | The dimension of the code that you want to transform from words. Default: 100 |\n| min_count | The minimum number of times a token must appear to be included in the word2vec model's vocabulary. Default: 5 |\n| max_sentence_length | (Spark 2.0.0+) Sets the maximum length (in words) of each sentence in the input data. Any sentence longer than this threshold will be divided into chunks of up to `max_sentence_length` size. Default: 1000 |\n| num_partitions | Number of partitions for sentences of words. Default: 1 |\n| step_size | Param for Step size to be used for each iteration of optimization (> 0). |\n| max_iter | The maximum number of iterations to use. |\n| seed | A random seed. Set this value if you need your results to be reproducible across repeated calls. |\n| uid | A character string used to uniquely identify the feature transformer. |\n| ... | Optional arguments; currently unused. |\n| model | A fitted `Word2Vec` model, returned by `ft_word2vec()`. |\n| word | A word, as a length-one character vector. |\n| num | Number of words closest in similarity to the given word to find. |\n\n## Details\n In the case where `x` is a `tbl_spark`, the estimator fits against `x` to obtain a transformer, returning a `tbl_spark`. \n\n\n## Value\n The object returned depends on the class of `x`. If it is a `spark_connection`, the function returns a `ml_estimator` or a `ml_estimator` object. If it is a `ml_pipeline`, it will return a pipeline with the transformer or estimator appended to it. If a `tbl_spark`, it will return a `tbl_spark` with the transformation  applied to it.  `ml_find_synonyms()` returns a DataFrame of synonyms and cosine similarities \n\n\n\n## See Also\n Other feature transformers:  `ft_binarizer()`, `ft_bucketizer()`, `ft_chisq_selector()`, `ft_count_vectorizer()`, `ft_dct()`, `ft_elementwise_product()`, `ft_feature_hasher()`, `ft_hashing_tf()`, `ft_idf()`, `ft_imputer()`, `ft_index_to_string()`, `ft_interaction()`, `ft_lsh`, `ft_max_abs_scaler()`, `ft_min_max_scaler()`, `ft_ngram()`, `ft_normalizer()`, `ft_one_hot_encoder()`, `ft_one_hot_encoder_estimator()`, `ft_pca()`, `ft_polynomial_expansion()`, `ft_quantile_discretizer()`, `ft_r_formula()`, `ft_regex_tokenizer()`, `ft_robust_scaler()`, `ft_sql_transformer()`, `ft_standard_scaler()`, `ft_stop_words_remover()`, `ft_string_indexer()`, `ft_tokenizer()`, `ft_vector_assembler()`, `ft_vector_indexer()`, `ft_vector_slicer()` \n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}