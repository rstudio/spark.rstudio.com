{
  "hash": "6be8f4053d0621b60fd3f1a0460a1df6",
  "result": {
    "markdown": "---\ntitle: \"Feature Transformation -- MaxAbsScaler (Estimator)\"\nexecute:\n  freeze: true\n---\n\n\n\n\n*R/ml_feature_max_abs_scaler.R*\n\n## ft_max_abs_scaler\n\n## Description\n Rescale each feature individually to range [-1, 1] by dividing through the   largest maximum absolute value in each feature. It does not shift/center the   data, and thus does not destroy any sparsity. \n\n\n## Usage\n```r\n \nft_max_abs_scaler( \n  x, \n  input_col = NULL, \n  output_col = NULL, \n  uid = random_string(\"max_abs_scaler_\"), \n  ... \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | A `spark_connection`, `ml_pipeline`, or a `tbl_spark`. |\n| input_col | The name of the input column. |\n| output_col | The name of the output column. |\n| uid | A character string used to uniquely identify the feature transformer. |\n| ... | Optional arguments; currently unused. |\n\n## Details\n In the case where `x` is a `tbl_spark`, the estimator fits against `x`   to obtain a transformer, which is then immediately used to transform `x`, returning a `tbl_spark`. \n\n\n## Value\n\n The object returned depends on the class of `x`.     \n\n- `spark_connection`: When `x` is a `spark_connection`, the function returns a `ml_transformer`,   a `ml_estimator`, or one of their subclasses. The object contains a pointer to   a Spark `Transformer` or `Estimator` object and can be used to compose   `Pipeline` objects.    \n\n- `ml_pipeline`: When `x` is a `ml_pipeline`, the function returns a `ml_pipeline` with   the transformer or estimator appended to the pipeline.    \n\n- `tbl_spark`: When `x` is a `tbl_spark`, a transformer is constructed then   immediately applied to the input `tbl_spark`, returning a `tbl_spark` \n\n \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n \nsc <- spark_connect(master = \"local\") \niris_tbl <- sdf_copy_to(sc, iris, name = \"iris_tbl\", overwrite = TRUE) \n \nfeatures <- c(\"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\") \n \niris_tbl %>% \n  ft_vector_assembler( \n    input_col = features, \n    output_col = \"features_temp\" \n  ) %>% \n  ft_max_abs_scaler( \n    input_col = \"features_temp\", \n    output_col = \"features\" \n  ) \n#> # Source: spark<?> [?? x 7]\n#>    Sepal_L…¹ Sepal…² Petal…³ Petal…⁴ Species featu…⁵ featu…⁶\n#>        <dbl>   <dbl>   <dbl>   <dbl> <chr>   <list>  <list> \n#>  1       5.1     3.5     1.4     0.2 setosa  <dbl>   <dbl>  \n#>  2       4.9     3       1.4     0.2 setosa  <dbl>   <dbl>  \n#>  3       4.7     3.2     1.3     0.2 setosa  <dbl>   <dbl>  \n#>  4       4.6     3.1     1.5     0.2 setosa  <dbl>   <dbl>  \n#>  5       5       3.6     1.4     0.2 setosa  <dbl>   <dbl>  \n#>  6       5.4     3.9     1.7     0.4 setosa  <dbl>   <dbl>  \n#>  7       4.6     3.4     1.4     0.3 setosa  <dbl>   <dbl>  \n#>  8       5       3.4     1.5     0.2 setosa  <dbl>   <dbl>  \n#>  9       4.4     2.9     1.4     0.2 setosa  <dbl>   <dbl>  \n#> 10       4.9     3.1     1.5     0.1 setosa  <dbl>   <dbl>  \n#> # … with more rows, and abbreviated variable names\n#> #   ¹​Sepal_Length, ²​Sepal_Width, ³​Petal_Length,\n#> #   ⁴​Petal_Width, ⁵​features_temp, ⁶​features\n```\n:::\n\n\n## See Also\n See [https://spark.apache.org/docs/latest/ml-features.html](https://spark.apache.org/docs/latest/ml-features.html) for   more information on the set of transformations available for DataFrame   columns in Spark.  Other feature transformers:  `ft_binarizer()`, `ft_bucketizer()`, `ft_chisq_selector()`, `ft_count_vectorizer()`, `ft_dct()`, `ft_elementwise_product()`, `ft_feature_hasher()`, `ft_hashing_tf()`, `ft_idf()`, `ft_imputer()`, `ft_index_to_string()`, `ft_interaction()`, `ft_lsh`, `ft_min_max_scaler()`, `ft_ngram()`, `ft_normalizer()`, `ft_one_hot_encoder_estimator()`, `ft_one_hot_encoder()`, `ft_pca()`, `ft_polynomial_expansion()`, `ft_quantile_discretizer()`, `ft_r_formula()`, `ft_regex_tokenizer()`, `ft_robust_scaler()`, `ft_sql_transformer()`, `ft_standard_scaler()`, `ft_stop_words_remover()`, `ft_string_indexer()`, `ft_tokenizer()`, `ft_vector_assembler()`, `ft_vector_indexer()`, `ft_vector_slicer()`, `ft_word2vec()` \n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}