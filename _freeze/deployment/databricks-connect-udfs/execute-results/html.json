{
  "hash": "300113218886a144e5d05ee560b19abc",
  "result": {
    "markdown": "---\ntitle: Run R inside Databricks Connect\nformat:\n  html:\n    theme: default\n    toc: true\nexecute:\n    eval: true\n    freeze: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n\n*Last updated: Tue Feb  6 11:50:14 2024*\n\n## Intro\n\nSupport for `spark_apply()` is currently available in the development\nversions of `sparklyr`, and `pysparklyr`. To install, run the following:\n\n``` r\nremotes::install_github(\"sparklyr/sparklyr\")\nremotes::install_github(\"mlverse/pysparklyr\")\n```\n\nDatabricks Connect is now able to run regular Python code inside Spark.\n`sparklyr` takes advantage of this capability by having Python transport\nand run the R code. It does this via the `rpy2` Python library. Using\nthis library also guarantees Arrow support.\n\n::: {#fig-connect}\n\n```{mermaid}\n%%| fig-width: 6\n%%| eval: true\nflowchart LR\n  subgraph mm[My machine]\n    sp[R <br> **********  <br>sparklyr]\n    rp[Python<br> **************** <br>rpy2 'packages'<br> the R code]\n  end\n  subgraph db[Databricks]\n    subgraph sr[Spark]\n      pt[Python<br> ********************* <br>rpy2 runs the R code]\n    end\n  end\n\nsp --> rp\nrp --> sr\n\nstyle mm   fill:#fff,stroke:#666,color:#000\nstyle sp   fill:#fff,stroke:#666,color:#000\nstyle rp   fill:#fff,stroke:#666,color:#000\nstyle db   fill:#fff,stroke:#666,color:#000\nstyle sr   fill:#fff,stroke:#666,color:#000\nstyle pt   fill:#fff,stroke:#666,color:#000\n```\n\n\nHow `sparklyr` uses rpy2 to run R code in Databricks Connect\n:::\n\n## Getting started\n\nIf you have been using `sparklyr` with Databricks Connect v2 already,\nthen after upgrading the packages, you will be prompted to install\n`rpy2` in your Python environment. The prompt will occur the first time\nyou use `spark_apply()` in an interactive R session. If this is the\nfirst time you are using `sparklyr` with Databricks Connect v2, please\nrefer to our intro article[\"Databricks Connect\nv2\"](/deployment/databricks-connect.qmd) to learn how to setup your\nenvironment.\n\nAs shown in the diagram on the previous section, `rpy2` is needed on the\nDatabricks cluster you plan to use. This means that you will need to\n\"manually\" install the library in the cluster. This is a simple\noperation that is done via your Databricks web portal. Here are the\ninstructions that shows you how to do that: [Databricks - Cluster\nLibraries](https://docs.databricks.com/en/libraries/cluster-libraries.html).\n\n## What is supported\n\n| Argument                      | Supported? | Notes                                                                                                                                                                                                                                                                                                                                                                                                 |\n|------------------------|------------|------------------------------------|\n| `x`                           | Yes        |                                                                                                                                                                                                                                                                                                                                                                                                       |\n| `f`                           | Yes        |                                                                                                                                                                                                                                                                                                                                                                                                       |\n| `columns`                     | Yes        | Requires a string entry that contains the name of the column and its Spark variable type. Accepted values are: `long`, `decimal`, `string`, `datetime` and `bool`. Example: `columns = \"x long, y string\"`. If not provided, `sparklyr` will automatically create one, by examining the first 10 records of `x`, and it will provide a `columns` spec you can use when running `spark_apply()` again. |\n| `memory`                      | Yes        |                                                                                                                                                                                                                                                                                                                                                                                                       |\n| `group_by`                    | Yes        |                                                                                                                                                                                                                                                                                                                                                                                                       |\n| `packages`                    | **No**     | You will need to pre-install the needed R packages in your cluster via the Databricks web portal, see [R packages](#r-packages)                                                                                                                                                                                                                                                                       |\n| `context`                     | **No**     |                                                                                                                                                                                                                                                                                                                                                                                                       |\n| `name`                        | Yes        |                                                                                                                                                                                                                                                                                                                                                                                                       |\n| `barrier`                     | Yes        | Supports only on not-grouped data. In other words, it is valid when the `group_by` argument is used.                                                                                                                                                                                                                                                                                                  |\n| `fetch_result_as_sdf`         | Yes        | At this time, `spark_apply()` inside Databricks Connect only supports rectangular data, so seeing to `FALSE` will always return a data frame.                                                                                                                                                                                                                                                         |\n| `partition_index_param`       | **No**     |                                                                                                                                                                                                                                                                                                                                                                                                       |\n| `arrow_max_records_per_batch` | Yes        | Supports only on not-grouped data. In other words, it is valid when the `group_by` argument is used.                                                                                                                                                                                                                                                                                                  |\n| `auto_deps`                   | **No**     |                                                                                                                                                                                                                                                                                                                                                                                                       |\n| `...`                         |            |                                                                                                                                                                                                                                                                                                                                                                                                       |\n\n## R packages\n\nIf your `spark_apply()` call uses specific R packages, you will need to\npre-install those specific packages in your target cluster. This is a\nsimple operation, because you can do this via your Databricks web\nportal, please see [Databricks - Cluster\nLibraries](https://docs.databricks.com/en/libraries/cluster-libraries.html)\nto learn how to do this.\n\n::: callout-caution\n## Only CRAN packages supported\n\nThe Databricks cluster library interface is able to source packages from\nCRAN only. This means that packages installed from GitHub, or another\nalternative sources, will not be available.\n:::\n\n### Additional info\n\nIn previous implementation, `spark_apply()` was able to easily copy the\nlocally installed R packages in order to ensure that your code will run\nin the cluster. This was possible because R, and RStudio, was running in\none of the matching servers in the Spark cluster. Because `sparklyr` is\nrunning on a remote machine, more likely a laptop, this is no longer an\noption. In the vast majority of cases, the remote machine will be on\ndifferent a Operating System than the cluster. Additionally,\ntransmitting the unpacked, compiled, R packages would take a long time\nover a broadband Internet connection.\n\n## Limitations\n\n`spark_apply()` **will only work on Databricks \"Single Access\" mode**.\n\"Shared Access\" mode does not currently support `mapInPandas()`, and\n`applyInPandas()` (see [Databricks - Access mode\nlimitations](https://docs.databricks.com/en/compute/access-mode-limitations.html#udf-limitations-for-unity-catalog-shared-access-mode)).\nThese are the Python functions that `sparklyr` uses to run the Python\ncode, which in turn runs the R code via `rpy2`.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}