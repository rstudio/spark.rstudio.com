{
  "hash": "5f103251a10c4487d6a8838773b7a6ba",
  "result": {
    "markdown": "---\ntitle: \"Model Tuning - Part II\"\nexecute:\n  eval: false\n  freeze: true\neditor_options: \n  markdown: \n    wrap: 72\n---\n\n\n\n\n## Goals\n\nThe aim of this article is to build on the knowledge from [Model\nTuning](model_tuning.qmd){target=\"_blank\"} by covering the following:\n\n-   Show how to re-use the code of an existing ML Pipeline as the base\n    for the hyper-parameter tuning\n-   Show how tuning parameters are not limited to only model's\n    parameters. We will cover how to tune data transformation parameters\n\n## Recreating \"Tuning Text Analysis\"\n\nThe example in this article is based on the [Tuning Text\nAnalysis](https://tune.tidymodels.org/articles/extras/text_analysis.html){target=\"_blank\"}\narticle found in the `tidymodels`' `tune` website. In that article, they\nuse *Amazon's Fine Food Reviews* text data to perform hyper parameter\ntuning.\n\nAs its name suggest, Model Tuning has two main phases: the modeling, and\nthe tuning. In `tidymodels`, the modeling is done using the `recepies`\nand `parsnip` packages, while the tuning is done with `tune`. `tune` is\nable to modify the the `recipe` and model's arguments for each\nexperiment.\n\nIn Spark, the modeling is done with an ML Pipeline. In itself, preparing\nthe data, and setting up the model can be complex enough to merit its\nown walk-through. For the walk-through of the model used in this\narticle, please see [Text Modeling](textmodeling.qmd){target=\"_blank\"}.\n\nThe basics of the tuning part are covered in [Model\nTuning](model_tuning.qmd){target=\"_blank\"}. To avoid duplication, any\nfoundational explanation will be linked back to that article.\n\n## Setup\n\nFor this example, we will start a local Spark session, and then copy the\n*Fine Food Reviews* data to it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nlibrary(modeldata)\n\ndata(\"small_fine_foods\")\n\nsc <- spark_connect(master = \"local\", version = \"3.3\")\n\nsff_training_data <- copy_to(sc, training_data)\nsff_testing_data <- copy_to(sc, testing_data)\n```\n:::\n\n\n## Pipeline\n\nThe data preparation and modeling in [Text\nModeling](textmodeling.qmd){target=\"_blank\"} was based on the [Tuning\nText\nAnalysis](https://tune.tidymodels.org/articles/extras/text_analysis.html){target=\"_blank\"}\narticle. The main steps from from the [Recipe and Model\nSpecification](https://tune.tidymodels.org/articles/extras/text_analysis.html#recipe-and-model-specifications-1){target=\"_blank\"}\nsection were recreated using Spark, via the `sparklyr` API. The `recipe`\nsteps were recreated with Feature Transformer functions, and the\n`parsnip` model was recreated using the equivalent\n`ml_logistic_regression()` model.\n\nDuring tuning, any parameter value used in the ML Pipeline will be\noverwritten with values from the [grid](#grid). This means that it\ndoesn't matter that we use the exact same code for developing, and\ntuning the pipeline. We can literally copy-paste, and run the resulting\npipeline code from [Text\nModeling](textmodeling.qmd#prepare-the-model-with-an-ml-pipeline).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_pipeline <- ml_pipeline(sc) %>% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %>% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %>% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 1024\n    ) %>%\n  ft_normalizer(\n    input_col = \"hashed_features\", \n    output_col = \"normal_features\"\n    ) %>% \n  ft_r_formula(score ~ normal_features) %>% \n  ml_logistic_regression()\n\nsff_pipeline\n```\n:::\n\n\nIt is also worth pointing out that in a\"real life\" exercise,\n`sff_pipeline` would probably already be loaded into our environment.\nThat is because we just finished modeling and, decided to test to see if\nwe could tune the model. Spark can re-use the exact same ML Pipeline\nobject for the cross validation step.\n\n## Grid {#grid}\n\nThere is a big advantage to transforming, and modeling the data in a\nsingle ML Pipeline. It opens the door for Spark to also alter parameters\nused for data transformation, in addition to the model's parameters.\nThis means that we can include the parameters of the tokenization,\ncleaning, hashing, and normalization steps as possible candidate for the\nmodel tuning.\n\nThe *Text Analysis* article uses three tuning parameters. Two are in the\nmodel, and one is in the hashing step. Here are the parameters, and how\nthey map between `tidymodels` and `sparklyr`:\n\n| Parameter                             | `tidymodels` | `sparklyr`          |\n|---------------------------------------|--------------|---------------------|\n| Number of Terms to Hash               | `num_terms`  | `num_features`      |\n| Amount of regularization in the model | `penalty`    | `elastic_net_param` |\n| Proportion of pure vs ridge Lasso     | `mixture`    | `reg_param`         |\n\nThe values to tune with are taken from the [Grid\nSearch](https://tune.tidymodels.org/articles/extras/text_analysis.html#grid-search-1){target=\"_blank\"}\nsection in the *Text Analysis* article. All that is left to do is to\ncreate the grid itself. Just like we did in the first [Model\nTuning](model_tuning.html#grid){target=\"_blank\"} article, we use partial\nname matching to the steps we want to tune:\n\n-   `hashing_ft` will be the name of the list object containing the\n    `num_features` values\n\n-   `logistic_regression` will be the of the list object containing the\n    values of the other two parameters\n\nNotice that the R code of the values themselves are a direct copy of the\nones used in the *Text Analysis* article.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_grid <-  list(\n    hashing_tf = list(\n      num_features = 2^c(8, 10, 12)  \n    ),\n    logistic_regression = list(\n      elastic_net_param = 10^seq(-3, 0, length = 20), \n      reg_param = seq(0, 1, length = 5)    \n    )\n  )\n\nsff_grid\n```\n:::\n\n\n## Evaluator\n\nIn the *Text Analysis* article, ROC AUC is used to measure performance.\nThe is the default metric of `ml_binary_classification_evaluator()` , so\nwe only need to pass the connection variable to the evaluator function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_evaluator <- ml_binary_classification_evaluator(sc)\n```\n:::\n\n\n## Model Tuning\n\nWe will use `ml_cross_validator()` to prepare a tuning specification\ninside Spark. We recommend to set the `seed` argument in order to\nincrease reproducibility.\n\nSpark will automatically create the grid combinations when tuning the\nmodel. In this case, `sff_grid` contains three parameters:\n\n-    `num_features` has 3 values\n\n-    `elastic_net_param` has 20 values\n\n-   `reg_parm` has 5 values\n\nThis means that there will be 300 combinations for the tuning\nparameters (3 x 20 x 5). Because we set the number of folds to 3\n(`num_folds`), Spark will run a total of 900 models\n(3 x 300).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_cv <- ml_cross_validator(\n  x = sc,\n  estimator = sff_pipeline, \n  estimator_param_maps = sff_grid,\n  evaluator = sff_evaluator,\n  num_folds = 3,\n  parallelism = 4,\n  seed = 100\n)\n\nsff_cv\n```\n:::\n\n\nThis is the step that will take the longest time. The `ml_fit()` function will \nrun the 900 models using the training data. There is no need to pre-prepare the \nre-sampling folds, Spark will take care of that. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_model <- ml_fit(\n  x = sff_cv, \n  dataset = sff_training_data\n  )\n```\n:::\n\n\n## Validation metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_metrics <- ml_validation_metrics(sff_model)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nsff_metrics %>% \n  mutate(reg_param_1 = as.factor(reg_param_1)) %>% \n  ggplot(aes(\n    x = elastic_net_param_1, \n    y = areaUnderROC, \n    color = reg_param_1\n    )) +\n  geom_line() +\n  geom_point(size = 0.5) +\n  scale_x_continuous(trans = \"log10\") +\n  facet_wrap(~ num_features_2) +\n  theme_light(base_size = 9)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\nsff_metrics %>% \n  arrange(desc(areaUnderROC)) %>% \n  head()\n```\n:::\n\n\n## Model selection\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_pipeline <- ml_pipeline(sc) %>% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %>% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %>% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 4096\n    ) %>%\n  ft_normalizer(\n    input_col = \"hashed_features\", \n    output_col = \"normal_features\"\n    ) %>% \n  ft_r_formula(score ~ normal_features) %>% \n  ml_logistic_regression(elastic_net_param = 0.05, reg_param = 0.25)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_fitted <- new_sff_pipeline %>% \n  ml_fit(sff_training_data)\n```\n:::\n\n\n## Test data metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_fitted %>% \n  ml_transform(sff_testing_data) %>% \n  ml_metrics_binary()\n```\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}