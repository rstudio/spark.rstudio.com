{
  "hash": "9fec747f89592979aef95cbbd31373d9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Copy an R Data Frame to Spark\"\nexecute:\n  freeze: true\n---\n\n\n\n*R/dplyr_spark.R*\n\n## copy_to.spark_connection\n\n## Description\n Copy an R `data.frame` to Spark, and return a reference to the generated Spark DataFrame as a `tbl_spark`. The returned object will act as a `dplyr`-compatible interface to the underlying Spark table. \n\n\n## Usage\n```r\n \n## S3 method for class 'spark_connection'\ncopy_to( \n  dest, \n  df, \n  name = spark_table_name(substitute(df)), \n  overwrite = FALSE, \n  memory = TRUE, \n  repartition = 0L, \n  ... \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dest | A `spark_connection`. |\n| df | An `R` `data.frame`. |\n| name | The name to assign to the copied table in Spark. |\n| overwrite | Boolean; overwrite a pre-existing table with the name `name` if one already exists? |\n| memory | Boolean; should the table be cached into memory? |\n| repartition | The number of partitions to use when distributing the table across the Spark cluster. The default (0) can be used to avoid partitioning. |\n| ... | Optional arguments; currently unused. |\n\n\n\n## Value\n A `tbl_spark`, representing a `dplyr`-compatible interface   to a Spark DataFrame. \n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}