---
title: "Spark Connect, and Databrick Connect v2"
format:
  html:
    theme: default
    toc: true
execute:
    eval: false
    freeze: false
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| include: false

install_github("sparklyr/sparklyr")
install_github("mlverse/pysparklyr")

library(remotes)
library(dplyr)
library(dbplyr)
library(sparklyr)
library(pysparklyr)

# install_pyspark()
```

*Last updated: `r lubridate::date()`*

## Intro

Intoduced in Spark 3.4, [Spark
Connect](https://spark.apache.org/docs/latest/spark-connect-overview.html) is a new
architecture that enables remote connectivity to Spark Clusters.  Spark Connect
will make it much easier for R users to interact with Spark clusters right from
their laptops.
[Databricks Connect](https://docs.databricks.com/dev-tools/databricks-connect.html), 
available in Databricks Runtime version 13 and above, is based on this new
architecture. 

Because it is a new architecture, the way `sparklyr` currently integrates with 
Spark does not work with Connect.  This is why we are working on a solution that
extends `sparklyr`'s capabilities in order to enable R users to work within the 
Connect framework.

## The Solution

`sparklyr` integrates with Spark using the Spark Shell. In order for the Spark 
Shell to work in your laptop, or server, you would need to setup Java.

As mentioned in the previous section, `sparklyr` cannot use its current method
to integrate with Connect. It requires the use of a remote procedure call
framework called gRPC.  The Spark team are developing solutions to integrate with
Connect via Python, and Scala. These solutions interact directly with gRPC.  

In `sparklyr`, we will use `reticulate` to interact with the Python solution. At
this time, `reticulate` offers a very robust integration with Python, that is very
easy to setup and use.  While it is possible to use the `reticulate` package to start
and interact with Connect directly, the additional functionality that `sparklyr`
offers would not be present. Such functionality includes: `dplyr` back-end, `DBI` 
back-end, RStudio's Connections pane integration, and others.

In order to quickly iterate enhancements and bug fixes, we have decided to isolate
the Python integration into its own package. The new package, called `pysparklyr`,
is an extension of `sparklyr`. 


## Package Installation

As mentioned above, we are still working integrating `sparklyr` with Connect.
For now, to access the new capabilities, you will need the development versions
of the `sparklyr`, and `pysparklyr`, packages. These versions are available in
their respective GitHub repositories. To install use: 

``` r
library(remotes)

install_github("sparklyr/sparklyr")
install_github("mlverse/pysparklyr")
```

## Initial Setup

To communicate with Connect, `sparklyr` needs specific Python components.
We provide a helper function that installs those components. 
You should only need to run this command the first time you install `pysparklyr`:

``` r
pysparklyr::install_pyspark()
```

### Additional setup details

The function will attempt to install Python if one is not found. It will also
create a new Python Virtual Environment. The new virtual environment will contain
the necessary Python libraries that `sparklyr` will use to interact with Connect. 
In addition, `install_pyspark()` installs specific versions of the libraries in order
to insure compatability. It does this by taking advantage of the snapshot
capabilities of 
[Posit's Package Manager](https://packagemanager.posit.co/client/#/repos/5/packages/appdirs).

By default, `sparklyr` will use whatever virtual environment is currently loaded
in your session. If none is loaded, then `sparklyr` will load the one installed
by `install_pyspark()`.  The default name of the Virtual Environment that `sparklyr`
uses is `r-sparklyr`.  If you wish to use your own Python environment, then 
just make sure to load it before calling `spark_connect()`. If there is a
Python environment already loaded when you connect to your Spark cluster, then
`sparklyr` will use that environment instead.


## Databricks Connect (ML 13+)

### Getting started

To use with Databricks Connect, in run-time 13 or above, you will need three 
configuration items: 

- Your [Workspace Instance URL](https://docs.databricks.com/workspace/workspace-details.html#workspace-url)
- The ID of a currently running [cluster](https://docs.databricks.com/workspace/workspace-details.html#cluster-url-and-id) within your workspace
- Authentication setup. At this time, `sparklyr` supports [PAT](https://docs.databricks.com/dev-tools/auth.html#pat)

In `spark_connect()` those items can be passed as function arguments. Also note 
that to let `sparklyr` know that you are attempting to use Databricks Connect,
use `method = databricks_connect`:

```r
library(sparklyr)

sc <- spark_connect(
 host = "[Your Workspace Instance URL]", 
 cluster_id = "[Cluster ID]"
 token = "[Your PAT - Please do not include 
          it as plain text here,  please refer 
          to the next section]"
  )
```

### Safely connect

For your safety and convenience, you can save your authentication token
(PAT), and your Workspace Instance URL in a environment variable. The
two variables are:

-   `DATABRICKS_TOKEN`
-   `DATABRICKS_HOST`

This will prevent your token to be shown in plain text in your code. You
can set the environment variables at the beginning of your R session
with `Sys.setenv()`.

Preferably, the two variables can be set for all R
sessions by saving them to the `.Renviron` file. The `usethis` package
has a handy function that opens the file so you can edit it:
`usethis::edit_r_environ()`. Then simply append the two entries to your
`.Renviron` file.

After that, use `spark_connect()` to open the connection to Databricks.
You will need to only pass the `cluster_id` of your cluster, and tell
`sparklyr` that you are connecting to a Spark cluster with run time
13 or above: 

```r
library(sparklyr)

sc <- spark_connect(
  cluster_id = "0608-170338-jwkec0wi",
  method = "databricks_connect"
)
```

If you wish to further remove identifying values from your connection code,
you can also save the `cluster_id` in the `DATABRICKS_CLUSTER_ID` environment
variable.  After you save it using `Sys.setenv()`, or by updating your `.Renviron`
file, your connection code will look like this:

```r
library(sparklyr)

sc <- spark_connect(
  method = "databricks_connect"
)
```

### RStudio's Connection pane

Thanks to the new way we are integrating with Spark, it is now possible to 
display the same structure displayed in the Databricks Data Explorer page.
In Databricks, the current data structure levels are:

- Catalog
  - Database
    - Table

In RStudio, you can navigate the data structure by expanding from the top level,
all the way down to the table you wish to explore. Once expanded, the table's 
fields, and their types are displayed. 

![](/images/deployment/connect/rstudio-connection.png)

In the Connection Pane, you can click on the **table** icon, situated to the
right of the table name, to preview the first 1,000 rows:

![](/images/deployment/connect/preview.png)

### Using 

After connecting,  you can use `dbplyr`'s `in_catalog()` function to access 
any table in your data catalog. You will only need to pass the respective names
of the three levels as comma separated character entries to `in_catalog()` in 
this order: Catalog, Database, and Table. 

Here is an example of using `tbl()` and `in_catalog()` to point to the **trips**
table, which is inside **nyctaxi** database, which is a database inside the
**samples** catalog:

```{r}
library(dplyr)
library(dbplyr)

trips <- tbl(sc, in_catalog("samples", "nyctaxi", "trips"))

trips
```

After pointing `tbl()` to that  specific table, you can then use `dplyr`.

```{r}
trips %>%
  group_by(pickup_zip) %>%
  summarise(
    count = n(),
    avg_distance = mean(trip_distance, na.rm = TRUE)
  )
```

```{r}
spark_disconnect(sc)
```

## Spark Connect

### Connecting

### Using

## What is supported
