{
  "hash": "487b5046c5a9e84cf301d812bffe02a7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Grid Search Tuning\"\nexecute:\n  eval: false\n  freeze: true\neditor_options: \n  markdown: \n    wrap: 72\n---\n\n\n\nIn this article, will cover the following four points:\n\n1.  Overview of Grid Search, and Cross Validation\n\n2.  Show how easy it is to run Grid Search model tuning in Spark\n\n3.  Provide a compelling reason to use ML Pipelines in our daily work\n\n4.  Highlight the advantages of using Spark, and `sparklyr`, for model\n    tuning\n\n## Grid Search\n\nThe main goal of hyper-parameter tuning is to find the ideal set of\nmodel parameter values. For example, finding out the ideal number of\ntrees to use for a model. We use model tuning to try several, and\nincreasing values. That will tell us at what point a increasing the\nnumber of trees does not improve the model's performance.\n\nIn Grid Search, we provide a provide a set of specific parameters, and\nspecific values to test for each parameter. The total number of\ncombinations will be the product of all the specific values of each\nparameter.\n\nFor example, suppose we are going to try **two** parameters. For the\nfirst parameter we provide 5 values to try, and for the second parameter\nwe provide 10 values to try. The total number of combinations will be\n50. The number of combinations grows quickly the more parameters we use.\nAdding a **third** parameter, with only 2 values, will mean that the\nnumber of combinations would double to 100 (5x10x2).\n\nThe model tuning returns a performance metric for each combination. We\ncan compare the results, and decide which model to use.\n\nIn Spark, we use an **ML Pipeline**, and a list of the parameters and\nthe values to try (**Grid**). We also specify the **metric** it should\nuse to measure performance (See @fig-grid). Spark will then take care of\nfiguring out the combinations, and fits the corresponding models.\n\n::: {#fig-grid}\n```{mermaid}\n%%| fig-width: 6.5\nflowchart LR\n  subgraph id1 [ ]\n    subgraph id2 [ ]\n      subgraph id3 [ML Pipeline]\n        d[Prepare] --> m[Model]\n      end\n      subgraph id4 [Grid Search Tuning]\n        m --> gv[Grid]\n        gv-- Combo 1 -->ft1[Fit Models]\n        ft1 --> ev1[Metric]\n        gv-- Combo 2 -->ft2[Fit Models]\n        ft2 --> ev2[Metric]     \n        gv-- Combo 3 -->ft3[Fit Models]\n        ft3 --> ev3[Metric]  \n        gv-- Combo n -->ft4[Fit Models]\n        ft4 --> ev4[Metric]          \n      end\n    end\n  end\n  style id1 fill:#eee,stroke:#eee\n  style id2 fill:#eee,stroke:#eee\n  style d fill:#99ccff,stroke#666\n  style m fill:#99ffcc,stroke:#666\n  style gv fill:#ffcc99,stroke:#666\n  style ft1 fill:#ccff99,stroke:#666\n  style ft2 fill:#ccff99,stroke:#666\n  style ft3 fill:#ccff99,stroke:#666\n  style ft4 fill:#ccff99,stroke:#666\n```\n\nGrid Search Tuning in Spark\n:::\n\n## Cross Validation\n\nIn Cross Validation, multiple models are fitted with the same\ncombination of parameters. The difference is the data used for training,\nand validation. These are called folds. The training data, and the\nvalidation data is different for each fold, that is called re-sampling.\n\nThe model is fitted with the current fold's training data, and then it\nis evaluated using the validation data. The average of the evaluation\nresults become the official metric value of the combination of\nparameters. @fig-cv, is a \"zoomed\" look of what happens inside\n`Fit Models` of @fig-grid.\n\nThe total number of models, will be the total number of combinations\ntimes the number of folds. For example, if we use 3 parameters, with 5\nvalues each, that would be 125 combinations. If tuning with 3 folds,\nCross Validation will fit, and validate, a total of 375 models.\n\nIn Spark, running **the 375 discrete models can be distributed across\nthe entire cluster**, thus significantly reducing the amount of time we\nwould have to wait to see the results.\n\n::: {#fig-cv}\n```{mermaid}\n%%| fig-width: 6.5\nflowchart LR\n  subgraph id1 [ ]\n    subgraph id2 [ ]\n      gv[Grid] -- Combo n -->re[Resample]\n      re -- Fold 1 --> ft1[Fit]\n      subgraph id4 [Fit Models with Cross Validation]\n        re -- Fold 2 --> ft2[Fit]\n        re -- Fold 3 --> ft3[Fit]\n        ft1 --> ev1[Evaluate]\n        ft2 --> ev2[Evaluate]     \n        ft3 --> ev3[Evaluate]  \n        ev1 --> eva[Avgerage]\n        ev2 --> eva\n        ev3 --> eva\n      end\n      eva --> mt[Metric]\n    end\n  end\n  style id1 fill:#eee,stroke:#eee\n  style id4 fill:#ccff99,stroke:#666\n  style gv fill:#ffcc99,stroke:#666\n  style ft1 fill:#ccffff,stroke:#666\n  style ft2 fill:#ccffff,stroke:#666\n  style ft3 fill:#ccffff,stroke:#666\n  style re fill:#ffccff,stroke:#666\n  style ev1 fill:#ffff99,stroke:#666\n  style ev2 fill:#ffff99,stroke:#666\n  style ev3 fill:#ffff99,stroke:#666\n  style eva fill:#ffff66,stroke:#666\n```\n\nCross Validation in Spark\n:::\n\n## Reproducing \"Tuning Text Analysis\" in Spark\n\nIn this article, we will reproduce the *Grid Search* tuning example\nfound in the `tune` package's website: [Tuning Text\nAnalysis](https://tune.tidymodels.org/articles/extras/text_analysis.html){target=\"_blank\"}.\nThat example analyzes *Amazon's Fine Food Reviews* text data. The goal\nis to tune the model using the exact same tuning parameters, and values,\nthat were used in the `tune`'s website example.\n\n::: callout-tip\nThis article builds on the knowledge of two previous articles, [Text\nModeling](textmodeling.qmd){target=\"_blank\"} and [Intro to Model\nTuning](model_tuning.qmd){target=\"_blank\"}. We encourage you to\nfamiliarize yourself with the concepts and code from those articles.\n:::\n\n## Spark and Data Setup\n\nFor this example, we will start a local Spark session, and then copy the\n*Fine Food Reviews* data to it. For more information about the data,\nplease see the [Data](textmodeling.qmd#data) section of the *Text\nModeling* article.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nlibrary(modeldata)\n\ndata(\"small_fine_foods\")\n\nsc <- spark_connect(master = \"local\", version = \"3.3\")\n\nsff_training_data <- copy_to(sc, training_data)\nsff_testing_data <- copy_to(sc, testing_data)\n```\n:::\n\n\n## ML Pipeline\n\nAs mentioned before, the data preparation and modeling in [Text\nModeling](textmodeling.qmd#recipe-and-model-specifications-1){target=\"_blank\"}\nare based on the same example from the [`tune`\nwebsite](https://tune.tidymodels.org/articles/extras/text_analysis.html){target=\"_blank\"}'s\narticle. The `recipe` steps, and `parsnip` model are recreated with\nFeature Transformers, and an ML model respectively.\n\nUnlike `tidymodels`, there is no need to \"pre-define\" the arguments that\nwill need tuning. **At execution, Spark will automatically override the\nparameters specified in the [grid](#grid).** This means that it doesn't\nmatter that we use the exact same code for developing, and tuning the\npipeline. **We can literally copy-paste, and run the resulting pipeline\ncode from [Text\nModeling](textmodeling.qmd#prepare-the-model-with-an-ml-pipeline).**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_pipeline <- ml_pipeline(sc) %>% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %>% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %>% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 1024\n    ) %>%\n  ft_normalizer(\n    input_col = \"hashed_features\", \n    output_col = \"normal_features\"\n    ) %>% \n  ft_r_formula(score ~ normal_features) %>% \n  ml_logistic_regression()\n\nsff_pipeline\n```\n:::\n\n\nIt is also worth pointing out that in a\"real life\" exercise,\n`sff_pipeline` would probably already be loaded into our environment.\nThat is because we just finished modeling and, decided to test to see if\nwe could tune the model (See @fig-pipeline). Spark can re-use the exact\nsame ML Pipeline object for the cross validation step.\n\n## Grid {#grid}\n\nThere is a big advantage to transforming, and modeling the data in a\nsingle ML Pipeline. It opens the door for Spark to also alter parameters\nused for data transformation, in addition to the model's parameters.\nThis means that we can include the parameters of the tokenization,\ncleaning, hashing, and normalization steps as possible candidates for\nthe model tuning.\n\nThe *Tuning Text Analysis* article uses three tuning parameters. Two\nparameters are in the model, and one is in the hashing step. Here are\nthe parameters, and how they map between `tidymodels` and `sparklyr`:\n\n| Parameter                             | `tidymodels` | `sparklyr`          |\n|---------------------------------------|--------------|---------------------|\n| Number of Terms to Hash               | `num_terms`  | `num_features`      |\n| Amount of regularization in the model | `penalty`    | `elastic_net_param` |\n| Proportion of pure vs ridge Lasso     | `mixture`    | `reg_param`         |\n\nUsing partial name matching, we map the parameters to the steps we want\nto tune:\n\n-   `hashing_ft` will be the name of the list object containing the\n    `num_features` values\n\n-   `logistic_regression` will be the of the list object with the model\n    parameters\n\nFor more about partial name matching, see in the [Intro Model\nTuning](model_tuning.qmd#grid) article. For the parameters values, we\ncan copy the exact same values from the [`tune`\nwebsite](https://tune.tidymodels.org/articles/extras/text_analysis.html#grid-search-1){target=\"_blank\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_grid <-  list(\n    hashing_tf = list(\n      num_features = 2^c(8, 10, 12)  \n    ),\n    logistic_regression = list(\n      elastic_net_param = 10^seq(-3, 0, length = 20), \n      reg_param = seq(0, 1, length = 5)    \n    )\n  )\n\nsff_grid\n```\n:::\n\n\n## Evaluate\n\nIn the [`tune`\nwebsite](https://tune.tidymodels.org/articles/extras/text_analysis.html){target=\"_blank\"}'s\narticle, ROC AUC is used to measure performance. The is the default\nmetric of `ml_binary_classification_evaluator()` , so we only need to\npass the connection variable to the evaluator function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_evaluator <- ml_binary_classification_evaluator(sc)\n```\n:::\n\n\n## Model Tuning\n\nWe will use `ml_cross_validator()` to prepare a tuning specification\ninside Spark. Spark will automatically create the parameter combinations\nwhen tuning the model. In this case, `sff_grid` contains three\nparameters:\n\n-   `num_features` with 3 values\n\n-   `elastic_net_param` with 20 values\n\n-   `reg_parm` with 5 values\n\nThis means that there will be **300 combinations** for the tuning\nparameters (3 x 20 x 5). Because we set the number of folds to 3\n(`num_folds`), Spark will run a total of **900 models** (3 x 300).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_cv <- ml_cross_validator(\n  x = sc,\n  estimator = sff_pipeline, \n  estimator_param_maps = sff_grid,\n  evaluator = sff_evaluator,\n  num_folds = 3,\n  parallelism = 4,\n  seed = 100\n)\n\nsff_cv\n```\n:::\n\n\n::: callout-tip\nWe recommend to set the `seed` argument in order to increase\nreproducibility.\n:::\n\nThis is the step that will take the longest time. The `ml_fit()`\nfunction will run the 900 models using the training data. There is no\nneed to pre-prepare the re-sampling folds, Spark will take care of that.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_model <- ml_fit(\n  x = sff_cv, \n  dataset = sff_training_data\n  )\n```\n:::\n\n\n## Metrics\n\nWe can now extract the metrics from `sff_model` using\n`ml_validation_metrics()`. The ROC AUC values will be in a column called\n`areaUnderROC`. We can then take a look at the best performing models\nusing `dplyr`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsff_metrics <- ml_validation_metrics(sff_model)\n\nlibrary(dplyr)\n\nsff_metrics %>% \n  arrange(desc(areaUnderROC)) %>% \n  head()\n```\n:::\n\n\nWe will now plot the results. We will match the approach used in the\n[Grid\nSearch](https://tune.tidymodels.org/articles/extras/text_analysis.html#grid-search-1){target=\"_blank\"}\nsection of the *Tuning Text Analysis* article.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nsff_metrics %>% \n  mutate(reg_param_1 = as.factor(reg_param_1)) %>% \n  ggplot(aes(\n    x = elastic_net_param_1, \n    y = areaUnderROC, \n    color = reg_param_1\n    )) +\n  geom_line() +\n  geom_point(size = 0.5) +\n  scale_x_continuous(trans = \"log10\") +\n  facet_wrap(~ num_features_2) +\n  theme_light(base_size = 9)\n```\n:::\n\n\nIn the plot, we can see the effects of the three parameters, and the\nvalues that look to be the best. These effects are very similar to the\noriginal `tune`'s website article.\n\n## Model selection\n\nWe can create a new ML Pipeline using the same code as the original\npipeline. We only need to change the 3 parameters values, with values\nthat performed best.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_pipeline <- ml_pipeline(sc) %>% \n  ft_tokenizer(\n    input_col = \"review\",\n    output_col = \"word_list\"\n  ) %>% \n  ft_stop_words_remover(\n    input_col = \"word_list\", \n    output_col = \"wo_stop_words\"\n    ) %>% \n  ft_hashing_tf(\n    input_col = \"wo_stop_words\", \n    output_col = \"hashed_features\", \n    binary = TRUE, \n    num_features = 4096      \n    ) %>%\n  ft_normalizer(\n    input_col = \"hashed_features\", \n    output_col = \"normal_features\"\n    ) %>% \n  ft_r_formula(score ~ normal_features) %>% \n  ml_logistic_regression(\n    elastic_net_param = 0.05,\n    reg_param = 0.25  \n    )\n```\n:::\n\n\nNow, we create a final model using the new ML Pipeline.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_fitted <- new_sff_pipeline %>% \n  ml_fit(sff_training_data)\n```\n:::\n\n\nThe test data set is now used to confirm that the performance gains\nhold. We use it to run predictions with the new ML Pipeline Model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_sff_fitted %>% \n  ml_transform(sff_testing_data) %>% \n  ml_metrics_binary()\n```\n:::\n\n\n## Benefits of ML Pipelines for everyday work\n\nIn the previous section, the metrics show an increase performance\ncompared to the model in the [Text\nModeling](textmodeling.qmd#fit-and-predict) article.\n\nThe gains in performance were easy to obtain. **We literally took the\nexact same pipeline we used in developing the model, and ran it through\nthe tuning process**. All we had to create was a simple grid, and a\nprovide the metric function.\n\nThis highlights an advantage of using ML Pipelines. Because\ntransitioning from modeling to tuning in Spark, will be a simple\noperation. An operation that has the potential to yield great benefits,\nwith little cost of effort. (@fig-pipeline)\n\n::: {#fig-pipeline}\n```{mermaid}\n%%| fig-width: 6.5\nflowchart LR\n  subgraph id1 [ ]\n  subgraph id2 [ ]\n    subgraph si [ML Pipeline]\n      dm[Data prep<br> & model] \n    end\n    dm -- Development<br>cycle --> t[Test] \n    t --> dm\n    t -- Selected<br>ML Pipeline  --> tn[Tune]\n    gm[Grid & Metric] --> tn\n    subgraph sp [ML Pipeline]\n      fm[Final model] \n    end\n    tn -- Best<br>parameters --> fm\n  end\n  end\n  style t fill:#ffcc00,stroke:#000\n  style tn fill:#ffcc99,stroke:#000\n  style dm fill:#ffff99,stroke:#000\n  style fm fill:#ffff99,stroke:#000\n  style id1 fill:#eee,stroke:#eee\n  style id2 fill:#eee,stroke:#eee\n  style gm fill:#fff,stroke:#000\n```\n\nDeveloping models with ML Pipelines\n:::\n\n## Accelerate model tuning with Spark\n\nAs highlighter in the previous section, Spark, and `sparklyr`, provide\nan easy way to go from exploration, to modeling, and tuning. Even\nwithout a \"formal\" Spark cluster, it is possible to take advantage of\nthese capabilities right from our personal computer.\n\nIf we add an actual cluster to the mix, the advantage of using Spark\nraises dramatically. **Usually, we talk about Spark for \"big data\"\nanalysis, but in this case, we can leverage it to \"parallelize\" hundreds\nof models across multiple machines.** The ability to distribute the\nmodels across the cluster will cut down the tuning processing time\n(@fig-cluster). The resources available to the cluster, and the given\nSpark session, will also determine the the amount of time saved. There\nis really no other open-source technology that is capable of this.\n\n::: {#fig-cluster}\n```{mermaid}\n%%| fig-width: 6\n\nclassDiagram\n  class Driver {\n  }\n  class Node1{\n    Job 1 - Model 1\n    Job 2 - Model 2\n    Job 3 - Model 3\n    Job 4 - Model 4    \n  }\n  class Node2{\n    Job 1 - Model 5\n    Job 2 - Model 6\n    Job 3 - Model 7\n    Job 4 - Model 8       \n  }\n  class Node3{\n    Job 1 - Model 9\n    Job 2 - Model 10\n    Job 3 - Model 11\n    Job 4 - Model 12      \n  }  \n  class Node4{\n    Job 1 - Model 13\n    Job 2 - Model 14\n    Job 3 - Model 15\n    Job 4 - Model 16      \n  }    \n  Driver --> Node1\n  Driver --> Node2\n  Driver --> Node3\n  Driver --> Node4\n\n```\n\nModel tuning in a Spark cluster\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}