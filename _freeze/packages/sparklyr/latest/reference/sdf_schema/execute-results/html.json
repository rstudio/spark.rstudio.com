{
  "hash": "19157fb03376e50846b9f52852e9e97d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Read the Schema of a Spark DataFrame\"\nexecute:\n  freeze: true\n---\n\n\n\n*R/sdf_wrapper.R*\n\n## sdf_schema\n\n## Description\n Read the schema of a Spark DataFrame. \n\n\n## Usage\n```r\n \nsdf_schema(x, expand_nested_cols = FALSE, expand_struct_cols = FALSE) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | A `spark_connection`, `ml_pipeline`, or a `tbl_spark`. |\n| expand_nested_cols | Whether to expand columns containing nested array of structs (which are usually created by tidyr::nest on a Spark data frame) |\n| expand_struct_cols | Whether to expand columns containing structs |\n\n## Details\n The `type` column returned gives the string representation of the underlying Spark  type for that column; for example, a vector of numeric values would be returned with the type `\"DoubleType\"`. Please see the [Spark Scala API Documentation](https://spark.apache.org/docs/latest/api/scala/index.html) for information on what types are available and exposed by Spark. \n\n\n## Value\n An `R` `list`, with each `list` element describing the   `name` and `type` of a column. \n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}