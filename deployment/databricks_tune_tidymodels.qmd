---
title: "Tune `tidymodels` remotely in Databricks Connect"
execute:
  eval: true
  freeze: true
---

```{r}
#| include: false
run_r <- TRUE

library(tidymodels)
library(sparklyr)
```

## Intro

`sparklyr` makes it easy to take advantage of Databricks Connect to tune
Tidymodels. 
Simply call `tune_spark_grid()` instead of `tune_spark()`. `sparklyr` will take 
care of all the setup and parallel job scheduling inside Databricks, see @fig-diagram. 
The output from `tune_spark_grid()` is the exact same type of R object
`tune_grid()` returns. 

:::{#fig-diagram}
![](/images/guides/tidymodels-db/diagram-1.png)

How *tune_grid_spark()* works with Databricks
:::

`sparklyr` accepts the exact same arguments as the function in `tune`. The only
additional required argument is the Databricks connection object. In @fig-code
we show how the exact same parsnip model, recipe and resamples can be passed
to both.

:::{#fig-code}
:::{.columns}
:::{.column width="48%"}
```{r}
#| eval: false
tune::tune_grid(
  object = my_model, 
  preprocessor = my_recipe, 
  resamples = my_resamples
  
  )
```

Run locally

:::
:::{.column width="4%"}
:::
:::{.column width="48%"}
```{r}
#| eval: false
sparklyr::tune_grid_spark(
  object = my_model, 
  preprocessor = my_recipe, 
  resamples = my_resamples,
  sc = my_conn # Only additional requirement
  )
```

Run remotely in Databricks

:::
:::
Comparing tune and sparklyr function calls
:::

## Example

### Setup the Tidymodels objects

The model building, the data pre-processing and re-sampling steps remain the 
exact same. `sparklyr` will upload the R objects resulting from each step onto
the Databricks cluster, so that it can process the hyper-tuning using the exact
same information.

To demonstrate the functionality in this article, we start with a standard
tuning workflow. In other words, there is nothing different to do in Tidymodels
to setup for the actual model tuning process:

```{r}
#| eval: !expr 'run_r'

library(tidymodels)
library(readmission)

set.seed(1)

# Data set resampling 
readmission_splits <- initial_split(readmission, strata = readmitted)

readmission_folds <- vfold_cv(
  data = training(readmission_splits),
  strata = readmitted
  )

# Data pre-processing
recipe_basic <- recipe(readmitted ~ ., data = readmission) |>
  step_mutate(
    race = factor(case_when(
      !(race %in% c("Caucasian", "African American")) ~ "Other",
      .default = race
    ))
  ) |>
  step_unknown(all_nominal_predictors()) |>
  step_YeoJohnson(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors())

# Model specification
spec_bt <- boost_tree(
  mode = "classification",
  mtry = tune(), # Part of the hyper-parameters to tune
  learn_rate = tune(), # Part of the hyper-parameters to tune
  trees = 10
  )
```

### Tune in Databricks Connect

The first step is to connect to the Databricks cluster. This operation is
only needed one time during the R session: 

```{r}
#| eval: !expr 'run_r'
library(sparklyr)

sc <- spark_connect(
  method = "databricks_connect", 
  cluster_id = "1218-000327-q970zsow" # Replace with your own cluster's ID
  )
```

The next step is to call `tune_grid_spark()`. We will pass the model, recipe
and re-sample objects prepared in the previous section. The only addition is
to have the process run in `verbose` mode, this is an optional argument. It 
will display the steps and timings for each:

```{r}
#| eval: !expr 'run_r'
spark_results <- tune_grid_spark(
  sc = sc, 
  object = spec_bt, 
  preprocessor = recipe_basic, 
  resamples = readmission_folds,
  control = control_grid(verbose = TRUE)
  )
```

The `spark_results` object can be now used as if it was a tuned object that
was prepared locally. 

```{r}
#| eval: !expr 'run_r'
autoplot(spark_results)
```


## R and Python libraries

There are Python and R packages that need to be pre-installed in the Databricks
cluster:

**Python**

- `rpy2`

**R**

- `tidymodels`

- `reticulate`

- The modeling package used by `parsnip` in the tuning 

Installation is a simple operation that is done via your Databricks web portal. 
Here are the instructions that shows you how to do that: [Databricks - Cluster
Libraries](https://docs.databricks.com/en/libraries/cluster-libraries.html).


### Install programmatically (optional)

The [`brickster`](https://databrickslabs.github.io/brickster/) package is a 
complete toolkit for interacting with Databricks. It can be used to 
programmatically install the Python and R libraries. It requires that the
user has rights to modify the libraries in target cluster.

```{r}
#| eval: false
library(brickster)

# DBRs have a fixed snapshot date to source the R packages. Redirecting  
# to the 'latest' snapshot to get the most recent package versions.
repo <- "https://databricks.packagemanager.posit.co/cran/__linux__/noble/latest"

# Builds the list of R packages, pointing them to the 'latest' snapshot
r_libs <- libraries(
  lib_cran("reticulate", repo = repo),
  lib_cran("tidymodels", repo = repo),
  lib_cran("xgboost", repo = repo) # (optional) Used in the example
)

db_libs_install("1218-000327-q970zsow", r_libs)

# Builds the Python package object. Specifying the version of `rpy2` to install.
py_lib <- lib_pypi("rpy2==3.6.4") |>
  libraries()

db_libs_install("1218-000327-q970zsow", py_lib)
```

As time goes by, there may be other modeling R packages that need to be installed
in Spark. Here is a template that can be used to install a single package: 

```{r}
#| eval: false
library(brickster)

lib_cran(
  package = "[Missing package]", 
  repo = "https://databricks.packagemanager.posit.co/cran/__linux__/noble/latest"
  ) |> 
  libraries() |> 
  db_libs_install("[Your cluster ID]", libraries =  _)

```
