---
title: Run R inside Databricks Connect
format:
  html:
    theme: default
    toc: true
execute:
    eval: true
    freeze: true
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| include: false

library(remotes)
library(dplyr)
library(dbplyr)
library(sparklyr)
library(pysparklyr)
```

*Last updated: `r lubridate::date()`*

## Background

Support for `spark_apply()` is currently available in the development
versions of `sparklyr`, and `pysparklyr`. To install, run the following:

``` r
remotes::install_github("sparklyr/sparklyr")
remotes::install_github("mlverse/pysparklyr")
```

Databricks Connect is now able to run regular Python code inside Spark.
`sparklyr` takes advantage of this capability by having Python transport
and run the R code. It does this via the `rpy2` Python library. Using
this library also guarantees Arrow support.

::: {#fig-connect}
```{mermaid}
%%| fig-width: 6
%%| eval: true
flowchart LR
  subgraph mm[My machine]
    sp[R <br> **********  <br>sparklyr]
    rp[Python<br> **************** <br>rpy2 'packages'<br> the R code]
  end
  subgraph db[Databricks]
    subgraph sr[Spark]
      pt[Python<br> ********************* <br>rpy2 runs the R code]
    end
  end

sp --> rp
rp --> sr

style mm   fill:#fff,stroke:#666,color:#000
style sp   fill:#fff,stroke:#666,color:#000
style rp   fill:#fff,stroke:#666,color:#000
style db   fill:#fff,stroke:#666,color:#000
style sr   fill:#fff,stroke:#666,color:#000
style pt   fill:#fff,stroke:#666,color:#000
```

How `sparklyr` uses rpy2 to run R code in Databricks Connect
:::

In Python, `sparklyr` uses the `mapInPandas()` and `applyInPandas()` to
run the Python code, that in turn, runs the R code. `mapInPandas()` is
used to run R code against un-grouped data, and `applyInPandas()` is
used for grouped data, meaning when the `group_by` argument, in
`spark_apply()` is used.
