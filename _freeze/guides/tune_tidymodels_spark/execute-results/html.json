{
  "hash": "85658ced8a4c6ac130bb7c1f6ddfc78c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Tune `tidymodels` in Spark\"\nexecute:\n  eval: true\n  freeze: true\n---\n\n\n\n\nThe ability of Spark to easily distribute tasks across multiple nodes (servers)\nmakes it a great target to run \"embarrassing parallel\" jobs, such as hyper-parameter\ntuning. For R users, the challenge is to have to learn how to run experiments \nusing the Spark ML components. Ideally, we would like to use the already familiar\nmodeling R packages but have the computation happen somewhere else, faster,\nand with little to no changes to our original code. \n\n**It is important to note\nthat we are not speaking of \"big data\", but rather of something more like \n\"big processing\"**.  What takes the job a long time to complete is the fact that\nthere are numerous tuning combination that need to be discretely processed \n(fit training data, predict on evaluation data, calculate metrics), and not that\nwe have to train on gigabytes and gigabytes of data. \n\nUsing `sparklyr`, it is now possible to use the exact same R objects created\nto process the model tuning, but have the tuning itself occur in Spark. `sparklyr`\nwill upload the R objects to your Spark session automatically, and have all\nof the parallel jobs run remotely. Everything made possible by running a\nvery similar function call. Instead of running `tune_grid()`, we would run\n`tune_grid_spark()`, which has the extact same arguments as `tune_grid()`, with\na couple of additions, such as requiring a current Spark connection.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# TODO: Replace with actual installation instructions at release time\n\nremotes::install_github(\"sparklyr/sparklyr\", ref = \"grid2\")\nremotes::install_github(\"tidymodels/tune\", ref = \"grid2\")\nremotes::install_github(\"mlverse/pysparklyr\", ref = \"grid2\")\n```\n:::\n\n\n## Prepare the workflow and re-samples locally\n\nToday, the framework that has become the standard for machine learning in R is \n`tidymodels`. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(readmission)\n\nset.seed(1)\n\nreadmission_splits <- initial_split(readmission, strata = readmitted)\n\nreadmission_folds <- vfold_cv(\n  data = training(readmission_splits),\n  strata = readmitted\n  )\n\nrecipe_basic <- recipe(readmitted ~ ., data = readmission) |>\n  step_mutate(\n    race = factor(case_when(\n      !(race %in% c(\"Caucasian\", \"African American\")) ~ \"Other\",\n      .default = race\n    ))\n  ) |>\n  step_unknown(all_nominal_predictors()) |>\n  step_YeoJohnson(all_numeric_predictors()) |>\n  step_normalize(all_numeric_predictors()) |>\n  step_dummy(all_nominal_predictors())\n\nspec_bt <- boost_tree(\n  mode = \"classification\",\n  mtry = tune(),\n  learn_rate = tune(),\n  trees = 10\n  )\n```\n:::\n\n\n## Tune in Spark Connect\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\n```\n:::\n\n\n\n### Start Spark Connect locally\n\n1. Install Spark locally if the version you wish to use is not available locally\nyet\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_install(\"4.1.0\")\n```\n:::\n\n\n2. Start the Spark Connect service locally. Make sure to match the Spark version\nyou recently installed\n\n\n::: {.cell}\n\n```{.r .cell-code}\npysparklyr::spark_connect_service_start(\"4.1.0\", python_version = \"3.12\")\n#> Starting Spark Connect locally ...\n#> openjdk version \"17.0.14\" 2025-01-21\n#> OpenJDK Runtime Environment Homebrew (build 17.0.14+0)\n#> OpenJDK 64-Bit Server VM Homebrew (build 17.0.14+0, mixed mode, sharing)\n#> ℹ Attempting to load 'r-sparklyr-pyspark-4.1'\n#> ✔ Python environment: 'r-sparklyr-pyspark-4.1' [379ms]\n#> \n#>   org.apache.spark.sql.connect.service.SparkConnectServer running as process\n#>   34949.  Stop it first.\n```\n:::\n\n\n3. Connect to the Spark Connect services\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsc <- spark_connect(\n  \"sc://localhost\",\n  method = \"spark_connect\",\n  version = \"4.1.0\"\n  )\n#> ℹ Attempting to load 'r-sparklyr-pyspark-4.1'\n#> ✔ Python environment: 'r-sparklyr-pyspark-4.1' [7ms]\n#> \n```\n:::\n\n\n### Tune the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_results <- tune_grid_spark(sc, spec_bt, recipe_basic, readmission_folds)\n#> i Creating pre-processing data to finalize 1 unknown parameter: \"mtry\"\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_results\n#> # Tuning results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 × 5\n#>    splits               id     .seeds    .metrics          .notes          \n#>    <list>               <chr>  <list>    <list>            <list>          \n#>  1 <split [48272/5364]> Fold01 <int [7]> <tibble [30 × 6]> <tibble [0 × 4]>\n#>  2 <split [48272/5364]> Fold02 <int [7]> <tibble [30 × 6]> <tibble [0 × 4]>\n#>  3 <split [48272/5364]> Fold03 <int [7]> <tibble [30 × 6]> <tibble [0 × 4]>\n#>  4 <split [48272/5364]> Fold04 <int [7]> <tibble [30 × 6]> <tibble [0 × 4]>\n#>  5 <split [48272/5364]> Fold05 <int [7]> <tibble [30 × 6]> <tibble [0 × 4]>\n#>  6 <split [48272/5364]> Fold06 <int [7]> <tibble [30 × 6]> <tibble [0 × 4]>\n#>  7 <split [48273/5363]> Fold07 <int [7]> <tibble [30 × 6]> <tibble [0 × 4]>\n#>  8 <split [48273/5363]> Fold08 <int [7]> <tibble [30 × 6]> <tibble [0 × 4]>\n#>  9 <split [48273/5363]> Fold09 <int [7]> <tibble [30 × 6]> <tibble [0 × 4]>\n#> 10 <split [48273/5363]> Fold10 <int [7]> <tibble [30 × 6]> <tibble [0 × 4]>\n```\n:::\n\n\n## Compare with local results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- tune_grid(spec_bt, recipe_basic, readmission_folds)\n#> i Creating pre-processing data to finalize 1 unknown parameter: \"mtry\"\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(results)\n#> Warning in show_best(results): No value of `metric` was given; \"roc_auc\" will\n#> be used.\n#> # A tibble: 5 × 8\n#>    mtry learn_rate .metric .estimator  mean     n std_err .config         \n#>   <int>      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>           \n#> 1     4     0.0880 roc_auc binary     0.600    10 0.00425 pre0_mod02_post0\n#> 2     7     0.001  roc_auc binary     0.600    10 0.00434 pre0_mod03_post0\n#> 3    10     0.0129 roc_auc binary     0.599    10 0.00467 pre0_mod04_post0\n#> 4    13     0.167  roc_auc binary     0.596    10 0.00391 pre0_mod05_post0\n#> 5    20     0.0245 roc_auc binary     0.596    10 0.00509 pre0_mod07_post0\nshow_best(spark_results)\n#> Warning in show_best(spark_results): No value of `metric` was given; \"roc_auc\"\n#> will be used.\n#> # A tibble: 5 × 8\n#>    mtry learn_rate .metric .estimator  mean     n std_err .config         \n#>   <int>      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>           \n#> 1     4    0.0880  roc_auc binary     0.600    10 0.00425 pre0_mod02_post0\n#> 2     7    0.00100 roc_auc binary     0.600    10 0.00434 pre0_mod03_post0\n#> 3    10    0.0129  roc_auc binary     0.599    10 0.00467 pre0_mod04_post0\n#> 4    13    0.167   roc_auc binary     0.596    10 0.00391 pre0_mod05_post0\n#> 5    20    0.0245  roc_auc binary     0.596    10 0.00509 pre0_mod07_post0\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npysparklyr::spark_connect_service_stop()\n#> \n#> ── Stopping Spark Connect\n#>   - Shutdown command sent\n```\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}