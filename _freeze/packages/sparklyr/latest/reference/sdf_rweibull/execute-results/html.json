{
  "hash": "693350f8424b8fd015a7ba5e7c455260",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Generate random samples from a Weibull distribution.\"\nexecute:\n  freeze: true\n---\n\n\n\n*R/sdf_stat.R*\n\n## sdf_rweibull\n\n## Description\n Generator method for creating a single-column Spark dataframes comprised of i.i.d. samples from a Weibull distribution. \n\n\n## Usage\n```r\n \nsdf_rweibull( \n  sc, \n  n, \n  shape, \n  scale = 1, \n  num_partitions = NULL, \n  seed = NULL, \n  output_col = \"x\" \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| sc | A Spark connection. |\n| n | Sample Size (default: 1000). |\n| shape | The shape of the Weibull distribution. |\n| scale | The scale of the Weibull distribution (default: 1). |\n| num_partitions | Number of partitions in the resulting Spark dataframe (default: default parallelism of the Spark cluster). |\n| seed | Random seed (default: a random long integer). |\n| output_col | Name of the output column containing sample values (default: \"x\"). |\n\n\n\n\n\n\n## See Also\n Other Spark statistical routines:  `sdf_rbeta()`, `sdf_rbinom()`, `sdf_rcauchy()`, `sdf_rchisq()`, `sdf_rexp()`, `sdf_rgamma()`, `sdf_rgeom()`, `sdf_rhyper()`, `sdf_rlnorm()`, `sdf_rnorm()`, `sdf_rpois()`, `sdf_rt()`, `sdf_runif()` \n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}